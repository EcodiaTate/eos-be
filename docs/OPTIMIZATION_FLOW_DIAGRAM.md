# LLM Cost Optimization â€” Flow Diagrams

## 1. Request Flow with Optimization

```
System (Nova, Voxis, etc) wants LLM response
    â†“
[Check Token Budget]
    â”œâ”€ Can use LLM? â”€â”€â†’ YES â”€â”
    â”‚                        â”‚
    â””â”€ Red/Yellow tier â”€â†’ NO â”˜
                              â†“
                    [Try Prompt Cache]
                              â†“
                    Cache hit? â”€â”€â†’ YES â”€â”€â†’ Return cached result
                              â”‚
                              NO
                              â†“
                    [Call LLM Provider]
                    (charges budget)
                              â†“
                    [Validate Output]
                    (extract_json, etc)
                              â†“
                    Valid? â”€â”€â†’ YES â”€â”€â†’ [Store in Cache] â”€â”€â†’ Return result
                              â”‚
                              NO
                              â†“
                    [Use Heuristic Fallback]
                              â†“
                         Return fast approximation
```

## 2. Budget Tier Decision Tree

```
                    System wants to use LLM
                              â†“
                    Check TokenBudget.get_status()
                              â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â†“               â†“               â†“
          GREEN          YELLOW            RED
        (0â€“70%)         (70â€“90%)        (90â€“100%)
              â†“               â†“               â†“
         All systems    Low-priority    Only critical
         use LLM       systems degrade  systems active
              â†“               â†“               â†“
        â”Œâ”€ Nova: LLM   â”œâ”€ Nova: Heuristic â”œâ”€ Nova: Heuristic
        â”œâ”€ Voxis: LLM  â”œâ”€ Voxis: Template â”œâ”€ Voxis: Template
        â”œâ”€ Evo: LLM    â”œâ”€ Evo: Skip/Fast  â”œâ”€ Evo: Skip/Fast
        â”œâ”€ Equor: LLM  â”œâ”€ Equor: LLM âœ“    â”œâ”€ Equor: LLM âœ“
        â””â”€ Others: LLM â””â”€ Others: Mixed   â””â”€ Others: Heuristic
```

## 3. Token Budget Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hour Begins: budget = 600,000 tokens                â”‚
â”‚ Cycle Period = 150ms                                â”‚
â”‚ ~4,000 cycles/hour in steady state                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Cycle 1: 150 tokens    â”‚
            â”‚ Remaining: 599,850     â”‚ â”€â”€â†’ GREEN (0.025%)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Cycle 100: 200 tokens  â”‚
            â”‚ Remaining: 420,000     â”‚ â”€â”€â†’ GREEN (30%)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Cycle 2,500: 300 tokensâ”‚
            â”‚ Remaining: 180,000     â”‚ â”€â”€â†’ YELLOW (70%)
            â”‚                         â”‚
            â”‚ Log: "Budget tier      â”‚
            â”‚ YELLOW. Low-priority   â”‚
            â”‚ systems degrade."      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Cycle 3,000: 100 tokensâ”‚
            â”‚ Remaining: 54,000      â”‚ â”€â”€â†’ RED (91%)
            â”‚                         â”‚
            â”‚ Log: "Budget tier RED. â”‚
            â”‚ Only critical systems  â”‚
            â”‚ active."               â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Rest of hour: Heuristicsâ”‚
            â”‚ only, no LLM calls     â”‚
            â”‚ Graceful degradation âœ“ â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 4. Prompt Cache Hit Scenario

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Voxis Renderer                                      â”‚
â”‚ Trigger: User asks "What are you thinking?"         â”‚
â”‚ Audience: Parent, morning context                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
    Construct prompt:
    "System: You are Ecodia..."
    "Messages: [user query about thinking]"
    "Context: parent, morning, 8:30am"
                         â†“
    Hash: SHA256("voxis:render:" + prompt) = "a1b2c3..."
    Redis key: "eos:cache:voxis:render:a1b2c3"
                         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Redis lookup: GET "eos:cache..." â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â†“                     â†“
        VALUE FOUND!          Miss (first time)
              â†“                     â†“
    â”Œâ”€ Return cached      â”Œâ”€ Call LLM
    â”‚   result immediatelyâ”‚   (charges budget)
    â”‚   (0ms latency)    â”‚
    â”‚                     â”œâ”€ Get response
    â”‚ Hit count: 1,534    â”‚
    â”‚ Hit rate: 87%  âœ“   â”‚   â”œâ”€ Validate JSON
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚   "outline": ["..."],
                          â”‚   â”‚   "tone": "warm"
                          â”‚
                          â”œâ”€ Store in cache
                          â”‚   SET "eos:cache..."
                          â”‚   + value
                          â”‚   + EX 60  (1 minute TTL)
                          â”‚
                          â””â”€ Return result
                              (200ms latency)

Result over 1 hour:
  - Cache hits:  1,537 (87%)
  - Cache misses:   225 (13%)
  - Token savings: 1,537 Ã— 150 tokens = 230,550 tokens saved
  - Cost savings: $2.30+ (if 1M tokens = $15)
```

## 5. Output Validation Flow

```
LLM Response received:
"```json
{\\"score\\": 0.7, \\"reasoning\\": \\"Good...
```"
                â†“
        [Try JSON.parse()]
                â†“
        Parse failed (unclosed string)
                â†“
        [Extract JSON]
        Find { and } boundaries
        Extract: {"score": 0.7, "reasoning": "Good...
                â†“
        [Try JSON.parse() again]
                â†“
        Still failed (truncated)
                â†“
        [Auto-fix]
        â”œâ”€ Truncate after last }
        â””â”€ Result: {"score": 0.7}
                â†“
        [Validate keys]
        Required: ["score", "reasoning"]
        Missing: ["reasoning"]
                â†“
        [Auto-correct with defaults]
        â”œâ”€ Add "reasoning": ""
        â””â”€ Result: {"score": 0.7, "reasoning": ""}
                â†“
        âœ… Valid! Use this result.
           (No LLM retry needed!)
                â†“
        Cost saved: ~500 tokens (avoided retry)
```

## 6. Heuristic Fallback Cascade

```
Budget enters RED tier (>90% usage)
                â†“
Nova EFE Evaluator needs pragmatic value
                â†“
    [Check can_use_llm()]
    â†’ False (budget exhausted)
                â†“
    [Call heuristic]
    EFEHeuristics.estimate_pragmatic_value_heuristic()
    â”œâ”€ Policy type: "express"
    â”œâ”€ Goal: "connect with child"
    â”œâ”€ Base score: 0.6 (for "express")
    â”œâ”€ Check for opposition: none found
    â””â”€ Return: 0.6 (~0 tokens, <1ms)
                â†“
    Use 0.6 instead of LLM
    System continues operating âœ“
                â†“
Result: Graceful degradation
  - Continue operating at reduced fidelity
  - No crash, no rejection
  - Fast approximations use known distributions
```

## 7. System Integration: Nova Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Nova EFE Evaluator in GREEN tier                                 â”‚
â”‚ evaluate(policy, goal, beliefs, affect, drive_weights)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                                  â†“
[Pragmatic Value]              [Epistemic Value]
        â†“                                  â†“
1. Check cache                 1. Check cache
   â”œâ”€ HIT? (80%) â”€â†’ Return     â”‚   â”œâ”€ HIT? (70%) â”€â†’ Return
   â”‚ cached (0ms)             â”‚   â”‚ cached (0ms)
   â”‚                           â”‚   â”‚
   â””â”€ MISS (20%)              â””â”€ MISS (30%)
        â†“                                  â†“
2. Check budget                2. Check budget
   â”œâ”€ Can use? YES             â”‚   â”œâ”€ Can use? YES
   â”‚                           â”‚   â”‚
   â””â”€ Call LLM                 â””â”€ Call LLM
        â”œâ”€ Prompt: "Rate       â”‚   â”œâ”€ Prompt: "Info gain
        â”‚   likelihood of      â”‚   â”‚   if we observe X?"
        â”‚   goal under policy" â”‚   â”‚
        â”œâ”€ Response: "0.7"     â”‚   â”œâ”€ Response: "0.5"
        â”œâ”€ Validate: 0.7 âœ“     â”‚   â”œâ”€ Validate: 0.5 âœ“
        â”œâ”€ Charge: 200 tokens  â”‚   â”œâ”€ Charge: 180 tokens
        â”‚                       â”‚   â”‚
        â””â”€ Cache 5min           â””â”€ Cache 5min
                â†“                                  â†“
        Pragmatic = 0.7                    Epistemic = 0.5
                                                  â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
        [Compute EFE Score]
        â”œâ”€ Pragmatic: -0.35 (weighted)
        â”œâ”€ Epistemic: -0.10 (weighted)
        â”œâ”€ Constitutional: -0.18 (weighted)
        â”œâ”€ Feasibility: +0.05 (penalty for complex)
        â””â”€ Risk: +0.02 (penalty)
                        â†“
        EFE = -(0.35 + 0.10 + 0.18 - 0.05 - 0.02)
            = -0.56
                        â†“
        Lower EFE = preferred policy âœ“
        (Used 380 tokens, cached next identical
         prompt, 40% faster on second identical
         belief state)
```

## 8. Metrics Dashboard Data Flow

```
System makes LLM call
        â†“
LLM Provider charges budget
        â†“
record_llm_call(
    system="nova.efe",
    input_tokens=150,
    output_tokens=50,
    latency_ms=180,
    cache_hit=False
)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLMMetricsCollector       â”‚
â”‚                           â”‚
â”‚ nova.efe:                 â”‚
â”‚   calls: 1,234            â”‚
â”‚   tokens: 300,000         â”‚
â”‚   cost: $4.50             â”‚
â”‚   latency_avg: 180ms      â”‚
â”‚   cache_hits: 900         â”‚
â”‚   cache_hit_rate: 73%     â”‚
â”‚                           â”‚
â”‚ voxis.render:             â”‚
â”‚   calls: 5,600            â”‚
â”‚   tokens: 420,000         â”‚
â”‚   cost: $6.30             â”‚
â”‚   latency_avg: 200ms      â”‚
â”‚   cache_hits: 4,500       â”‚
â”‚   cache_hit_rate: 80%     â”‚
â”‚                           â”‚
â”‚ TOTAL:                    â”‚
â”‚   calls: 8,900            â”‚
â”‚   tokens: 720,000 (used)  â”‚
â”‚   cost: $10.80            â”‚
â”‚   cache_hit_rate: 77%     â”‚
â”‚   budget_tier: GREEN âœ“    â”‚
â”‚   projected_cost: $14.20  â”‚
â”‚   (based on hour burn)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
GET /metrics/llm
        â†“
Dashboard displays
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ’š GREEN TIER              â”‚
â”‚ Tokens: 720k / 600k (77%)  â”‚
â”‚ Cost: $10.80               â”‚
â”‚ Cache hit rate: 77% âœ“âœ“âœ“    â”‚
â”‚ Avg latency: 190ms         â”‚
â”‚ Projected hourly: $14.20   â”‚
â”‚                            â”‚
â”‚ Top systems by cost:       â”‚
â”‚  1. voxis.render $6.30     â”‚
â”‚  2. nova.efe $4.50         â”‚
â”‚  3. equor.check $0.80      â”‚
â”‚                            â”‚
â”‚ Actions:                   â”‚
â”‚ â€¢ All systems active       â”‚
â”‚ â€¢ Continue monitoring      â”‚
â”‚ â€¢ (none needed)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 9. Phased Rollout Timeline

```
Week 1: Foundation (âœ… DONE)
â”œâ”€ Token budget system
â”œâ”€ Prompt cache infrastructure
â”œâ”€ Output validation
â”œâ”€ Heuristic fallbacks
â””â”€ Metrics collection

Week 2: Nova Integration (â†’ 50â€“70% reduction)
â”œâ”€ EFE pragmatic evaluation caching
â”œâ”€ EFE epistemic evaluation caching
â”œâ”€ Heuristic fallback in Yellow/Red
â”œâ”€ Measure 30% cache hit rate
â””â”€ Projected: $X.XX/hour â†’ $Y.YY/hour (70% savings)

Week 3: Voxis Integration (â†’ 40â€“50% reduction)
â”œâ”€ Expression generation caching
â”œâ”€ Template fallback
â”œâ”€ Output validation
â”œâ”€ Measure 40% cache hit rate
â””â”€ System-wide: (Week1 + Week2 + Week3 = 50â€“60% reduction)

Week 4: System-Wide Rollout (â†’ 60â€“70% reduction)
â”œâ”€ Evo hypothesis integration (70% cache)
â”œâ”€ Thread synthesis integration (80% cache)
â”œâ”€ Equor check tuning (identity check)
â”œâ”€ Oneiros reflection (off-cycle consolidation)
â””â”€ Final: 60â€“70% total cost reduction

Week 5: Auto-Tuning & Optimization
â”œâ”€ Enable Evo weight learning under constraints
â”œâ”€ Adaptive model tier selection
â”œâ”€ Cache TTL auto-tuning
â”œâ”€ Observability dashboards live
â””â”€ Monitoring: continuous optimization
```

---

**Key Insight:** The system maintains full capability while dramatically reducing cost. Heuristics are used only when needed (budget constraints), and cache hits avoid LLM calls entirely.
