# EcodiaOS LLM Cost Optimization â€” Distilled Summary

## Executive Overview

**Goal:** Reduce LLM token costs by 60â€“70% without sacrificing capability.

**Current state:** 31 LLM calls across 16 EcodiaOS systems. Budget is too tight (60k tokens/hour â†’ need 600k/hour for steady state).

**Solution:** Five core strategies implemented in Phase 1, ready for per-system integration (Phase 2â€“5).

---

## Five Core Strategies

### 1. **Token Budget System** (âœ… Complete)
- **What:** Tracks cumulative tokens/hour and tokens/cycle
- **How:** Three-tier degradation: Green (normal) â†’ Yellow (careful) â†’ Red (critical)
- **File:** `ecodiaos/clients/token_budget.py`
- **Impact:** Graceful degradation when approaching limits

### 2. **Prompt Caching** (âœ… Complete)
- **What:** Redis-backed semantic cache (key = SHA256(system + method + prompt))
- **Expected hit rate:** 30â€“80% depending on system
- **File:** `ecodiaos/clients/prompt_cache.py`
- **Impact:** 25â€“40% cost reduction from cache hits alone

### 3. **Output Validation** (âœ… Complete)
- **What:** Parse JSON/numbers/enums from LLM responses with auto-correction
- **Key:** No retry loops on parse failure â†’ fall back to heuristic immediately
- **File:** `ecodiaos/clients/output_validator.py`
- **Impact:** Eliminate token waste from malformed responses

### 4. **Heuristic Fallbacks** (âœ… Complete)
- **What:** Fast (<10ms) approximations for common LLM tasks
- **Use case:** Yellow/Red budget tiers or validation failures
- **File:** `ecodiaos/systems/nova/efe_heuristics.py`
- **Impact:** 20â€“30% additional cost reduction

### 5. **Metrics & Telemetry** (âœ… Complete)
- **What:** Track tokens, cost (USD), latency, cache hit rate per system
- **File:** `ecodiaos/telemetry/llm_metrics.py`
- **Endpoint:** `GET /metrics/llm` for dashboard data
- **Impact:** Visibility + auto-tuning potential

---

## Implementation Status

| Phase | What | Status | Impact |
|-------|------|--------|--------|
| **Phase 1** | Foundation (all 5 strategies) | âœ… Done | Ready for integration |
| **Phase 2** | Nova EFE integration | ðŸ”„ Pending | 50â€“70% reduction in Nova |
| **Phase 3** | Voxis renderer integration | ðŸ”„ Pending | 40â€“50% reduction in Voxis |
| **Phase 4** | System-wide rollout | ðŸ”„ Pending | 60â€“70% total reduction |
| **Phase 5** | Auto-tuning (Evo weights) | ðŸ”„ Pending | Continuous optimization |

---

## Quick Integration Pattern

All systems follow this 3-step pattern:

```python
# Step 1: Check budget
if not token_budget.can_use_llm(estimated_tokens=500):
    return await fallback_heuristic()

# Step 2: Try cache
cached = await cache.get("system", "method", prompt)
if cached:
    return cached

# Step 3: Call LLM, validate, cache
response = await llm.generate(prompt)
result = OutputValidator.extract_json(response.text)
if result is None:
    return await fallback_heuristic()

await cache.set("system", "method", prompt, result, ttl_seconds=300)
return result
```

---

## Budget Tiers & System Behavior

### Green Tier (0â€“70% usage)
- All systems use LLM
- Full capability

### Yellow Tier (70â€“90% usage)
- Low-priority systems degrade
- Nova: Use heuristics
- Voxis: Use templates
- Evo: Skip or use fast path
- Equor: **Always LLM** (critical)

### Red Tier (90â€“100% usage)
- Only critical systems active
- Everything else uses heuristics
- Equor still has precedence

---

## Cache TTL Guidelines

| System | TTL | Hit Rate Target |
|--------|-----|-----------------|
| Nova EFE | 5 min | 30% |
| Voxis Renderer | 1 min | 40% |
| Evo Hypothesis | 1 hour | 60% |
| Thread Identity | 6 hours | 70% |
| Equor Checks | 30 min | 10% (rare) |

---

## Configuration

### config/default.yaml

```yaml
llm:
  budget:
    max_tokens_per_hour: 600_000    # Was: 60,000 (too tight)
    max_calls_per_hour: 1_000
    hard_limit: false               # Graceful degradation

nova:
  efe_cache_ttl_s: 300

voxis:
  expression_cache_ttl_s: 60

evo:
  hypothesis_cache_ttl_s: 3600
```

---

## Monitoring

### Key Metrics

```
llm_tokens_charged          â€” Tokens used this hour
llm_cost_estimate           â€” USD cost estimate
llm_cache_hit_rate          â€” % calls from cache
llm_budget_tier             â€” Green/Yellow/Red
llm_latency_p99             â€” 99th percentile latency
```

### Dashboard Endpoint

```
GET /metrics/llm
â†’ Returns total cost, per-system breakdown, cache hit rate, budget tier
```

### Alert Rules

| Condition | Action |
|-----------|--------|
| Budget â†’ Yellow | Warn: low-priority systems degrade |
| Budget â†’ Red | Alert: critical-only mode |
| Cache hit < 10% | Adjust TTL or caching strategy |
| Latency p99 > 500ms | Investigate provider/network |

---

## Expected Outcomes

| Strategy | Individual | Combined |
|----------|-----------|----------|
| Caching | 25â€“40% | 60â€“70% |
| Heuristics | 20â€“30% | across all |
| Output validation | 10â€“15% | systems |

**Total expected reduction: 60â€“70% cost with same capability.**

---

## Common Integration Questions

### "Will heuristics produce worse decisions?"
No. Used only during:
1. Token budget exhaustion (graceful degradation)
2. Parse validation failures (fallback only)
3. Latency exceeds timeout (safety-critical)

In normal operation (Green tier, cache hits), LLM decisions are used.

### "How do I tune cache TTL?"
1. Monitor hit rate per system
2. If < target: reduce TTL (more misses â†’ more accurate)
3. If >> target: increase TTL (fewer misses â†’ faster)

### "What if I need precise decisions under budget constraints?"
- Configure `hard_limit=false` to allow overage
- Pre-allocate higher budget
- Equor (constitutional checks) always runs regardless of budget tier

---

## Files Reference

```
ecodiaos/
â”œâ”€â”€ clients/
â”‚   â”œâ”€â”€ token_budget.py          # Budget tracking
â”‚   â”œâ”€â”€ prompt_cache.py          # Semantic cache
â”‚   â”œâ”€â”€ output_validator.py      # Response validation
â”‚   â””â”€â”€ llm.py                   # Modified: budget integration
â”œâ”€â”€ systems/nova/
â”‚   â””â”€â”€ efe_heuristics.py        # Fast fallbacks
â”œâ”€â”€ telemetry/
â”‚   â””â”€â”€ llm_metrics.py           # Metrics collection
â””â”€â”€ config.py                    # Modified: budget config

docs/
â”œâ”€â”€ LLM_COST_OPTIMIZATION.md     # Strategy & framework
â”œâ”€â”€ LLM_INTEGRATION_GUIDE.md     # Integration details
â”œâ”€â”€ LLM_OPTIMIZATION_README.md   # Implementation summary
â”œâ”€â”€ OPTIMIZATION_FLOW_DIAGRAM.md # Visual flows
â””â”€â”€ DISTILLED_OPTIMIZATION_SUMMARY.md  # This file
```

---

## Next Steps

1. **Phase 2:** Integrate Nova EFE (start here)
   - Cache pragmatic/epistemic evaluations
   - Add budget checks before LLM calls
   - Expect 50â€“70% cost reduction in Nova

2. **Phase 3:** Integrate Voxis renderer
   - Cache expression generation
   - Template fallback in Red tier
   - Expect 40â€“50% cost reduction in Voxis

3. **Phase 4:** System-wide rollout
   - Integrate Evo, Equor, Thread, Oneiros
   - Tune cache TTLs per system
   - Enable monitoring dashboard

4. **Phase 5:** Auto-tuning
   - Evo learns EFE weights under budget constraints
   - Adaptive model tier selection
   - Cache TTL rebalancing

---

**Status:** Phase 1 foundation complete. Ready for system integration.
