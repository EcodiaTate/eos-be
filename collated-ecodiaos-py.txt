
===== D:\.code\EcodiaOS\backend\ecodiaos\__init__.py =====


===== D:\.code\EcodiaOS\backend\ecodiaos\config.py =====

"""
EcodiaOS — Configuration System

All configuration is Pydantic-validated and loaded from:
1. default.yaml (defaults)
2. Environment variables (overrides)
3. Seed config (instance birth parameters)

Every tunable parameter in the system lives here.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

import yaml
from pydantic import BaseModel, Field, model_validator
from pydantic_settings import BaseSettings, SettingsConfigDict


# ─── Sub-configs ──────────────────────────────────────────────────


class ServerConfig(BaseModel):
    host: str = "0.0.0.0"
    port: int = 8000
    ws_port: int = 8001
    federation_port: int = 8002
    cors_origins: list[str] = Field(default_factory=lambda: ["http://localhost:3000"])
    api_key_header: str = "X-EOS-API-Key"
    # API keys for authentication. When empty, auth is disabled (dev mode).
    # Set via ECODIAOS_SERVER__API_KEYS or config YAML.
    api_keys: list[str] = Field(default_factory=list)


class Neo4jConfig(BaseModel):
    uri: str = "neo4j+s://localhost:7687"
    username: str = "neo4j"
    password: str = ""
    database: str = "neo4j"
    max_connection_pool_size: int = 20


class TimescaleDBConfig(BaseModel):
    host: str = "timescaledb"
    port: int = 5432
    database: str = "ecodiaos"
    schema_name: str = Field(default="public", alias="schema")
    username: str = "ecodiaos"
    password: str = "ecodiaos_dev"
    pool_size: int = 10
    ssl: bool = False

    model_config = {"populate_by_name": True}

    @property
    def dsn(self) -> str:
        return (
            f"postgresql://{self.username}:{self.password}"
            f"@{self.host}:{self.port}/{self.database}"
        )


class RedisConfig(BaseModel):
    url: str = "redis://redis:6379/0"
    prefix: str = "eos"
    password: str = "ecodiaos_dev"

    @property
    def full_url(self) -> str:
        """Build URL with password injected."""
        clean_pw = self.password.strip() if self.password else ""
        if clean_pw and "://" in self.url:
            scheme, rest = self.url.split("://", 1)
            return f"{scheme}://:{clean_pw}@{rest}"
        return self.url


class LLMBudget(BaseModel):
    max_calls_per_hour: int = 1_000
    max_tokens_per_hour: int = 600_000
    hard_limit: bool = False  # If True, reject requests when budget exhausted


class LLMConfig(BaseModel):
    provider: str = "anthropic"
    model: str = "claude-sonnet-4-20250514"
    api_key: str = ""
    fallback_provider: str | None = None
    fallback_model: str | None = None
    budget: LLMBudget = Field(default_factory=LLMBudget)
    budgets: dict[str, LLMBudget] = Field(default_factory=dict)  # Deprecated, use budget

    @model_validator(mode="after")
    def _strip_api_key(self) -> "LLMConfig":
        # GCP Secret Manager can inject trailing \r\n into env vars
        if self.api_key:
            object.__setattr__(self, "api_key", self.api_key.strip())
        return self


class EmbeddingConfig(BaseModel):
    strategy: str = "local"  # "local" | "api" | "sidecar"
    local_model: str = "sentence-transformers/all-mpnet-base-v2"
    local_device: str = "cpu"
    dimension: int = 768
    max_batch_size: int = 32
    cache_embeddings: bool = True
    cache_ttl_seconds: int = 3600


class SynapseConfig(BaseModel):
    cycle_period_ms: int = 150
    min_cycle_period_ms: int = 80
    max_cycle_period_ms: int = 500
    health_check_interval_ms: int = 5000
    health_failure_threshold: int = 3
    coherence_update_interval: int = 50
    rhythm_enabled: bool = True


class AtuneConfig(BaseModel):
    ignition_threshold: float = 0.3
    workspace_buffer_size: int = 32
    spontaneous_recall_base_probability: float = 0.02
    max_percept_queue_size: int = 100


class NovaConfig(BaseModel):
    max_active_goals: int = 20
    fast_path_timeout_ms: int = 100
    slow_path_timeout_ms: int = 5000
    max_policies_per_deliberation: int = 5
    # EFE component weights (Evo adjusts these over time)
    efe_weight_pragmatic: float = 0.35
    efe_weight_epistemic: float = 0.20
    efe_weight_constitutional: float = 0.20
    efe_weight_feasibility: float = 0.15
    efe_weight_risk: float = 0.10
    # Memory retrieval timeout for belief enrichment
    memory_retrieval_timeout_ms: int = 150
    # Whether to use LLM for pragmatic/epistemic EFE estimation (vs. heuristics)
    use_llm_efe_estimation: bool = True


class EquorConfig(BaseModel):
    standard_review_timeout_ms: int = 500
    critical_review_timeout_ms: int = 50
    care_floor_multiplier: float = -0.3
    honesty_floor_multiplier: float = -0.3
    drift_window_size: int = 1000
    drift_report_interval: int = 1000  # every N reviews


class AxonConfig(BaseModel):
    max_actions_per_cycle: int = 5
    max_api_calls_per_minute: int = 30
    max_notifications_per_hour: int = 10
    max_concurrent_executions: int = 3
    total_timeout_per_cycle_ms: int = 30000


class VoxisConfig(BaseModel):
    max_expression_length: int = 2000
    min_expression_interval_minutes: int = 1
    voice_synthesis_enabled: bool = False
    # Proactive expression threshold — ambient insights below this are suppressed
    insight_expression_threshold: float = 0.6
    # Rolling message window kept verbatim in conversation context
    conversation_history_window: int = 50
    # Max tokens to pass to LLM as conversation history (older messages summarised)
    context_window_max_tokens: int = 4000
    # Summarise messages older than this count (keep last N verbatim)
    conversation_summary_threshold: int = 10
    # Whether to report expression feedback to Atune
    feedback_enabled: bool = True
    # Whether to run post-generation honesty check
    honesty_check_enabled: bool = True
    # Base LLM temperature before strategy modulation
    temperature_base: float = 0.7
    # Maximum concurrent tracked conversations in Redis
    max_active_conversations: int = 50


class EvoConfig(BaseModel):
    consolidation_interval_hours: int = 6
    consolidation_cycle_threshold: int = 10000
    max_active_hypotheses: int = 50
    max_parameter_delta_per_cycle: float = 0.03
    min_evidence_for_integration: int = 10


class SimulaConfig(BaseModel):
    max_simulation_episodes: int = 200
    regression_threshold_unacceptable: float = 0.10
    regression_threshold_high: float = 0.05
    # Code agent settings
    codebase_root: str = "."
    code_agent_model: str = "claude-opus-4-6"
    max_code_agent_turns: int = 20
    test_command: str = "pytest"
    auto_apply_self_applicable: bool = True


class ThymosConfig(BaseModel):
    # Sentinel scan interval (seconds)
    sentinel_scan_interval_s: float = 30.0
    # Homeostasis check interval (seconds)
    homeostasis_interval_s: float = 30.0
    # Post-repair verification timeout (seconds)
    post_repair_verify_timeout_s: float = 10.0
    # Healing governor limits
    max_concurrent_diagnoses: int = 3
    max_concurrent_codegen: int = 1
    storm_threshold: int = 10  # incidents per 60 seconds
    max_repairs_per_hour: int = 5
    max_novel_repairs_per_day: int = 3
    # Antibody library
    antibody_refinement_threshold: float = 0.6
    antibody_retirement_threshold: float = 0.3
    # Resource budget
    cpu_budget_fraction: float = 0.05
    burst_cpu_fraction: float = 0.15
    memory_budget_mb: int = 256


class OneirosConfig(BaseModel):
    # Circadian rhythm
    wake_duration_target_s: float = 79200.0     # 22 hours
    sleep_duration_target_s: float = 7200.0     # 2 hours

    # Sleep pressure
    pressure_threshold: float = 0.70
    pressure_critical: float = 0.95
    max_wake_cycles: int = 528000

    # Pressure weights
    pressure_weight_cycles: float = 0.40
    pressure_weight_affect: float = 0.25
    pressure_weight_episodes: float = 0.20
    pressure_weight_hypotheses: float = 0.15

    # NREM
    nrem_fraction: float = 0.40
    max_episodes_per_nrem: int = 200
    replay_batch_size: int = 10
    salience_decay_factor: float = 0.85
    salience_pruning_threshold: float = 0.05

    # REM
    rem_fraction: float = 0.40
    max_dreams_per_rem: int = 50
    dream_coherence_insight_threshold: float = 0.70
    dream_coherence_fragment_threshold: float = 0.40
    max_affect_traces_per_rem: int = 100
    affect_dampening_factor: float = 0.50
    max_threats_per_rem: int = 15
    max_ethical_cases_per_rem: int = 10

    # Lucid
    lucid_fraction: float = 0.10
    lucid_insight_threshold: float = 0.85
    max_explorations_per_lucid: int = 10

    # Sleep debt
    debt_salience_noise_max: float = 0.15
    debt_efe_precision_loss_max: float = 0.20
    debt_expression_flatness_max: float = 0.25
    debt_learning_rate_reduction_max: float = 0.30

    # Transitions
    hypnagogia_duration_s: float = 30.0
    hypnopompia_duration_s: float = 30.0


class FederationConfig(BaseModel):
    enabled: bool = False
    endpoint: str | None = None
    tls_cert_path: str | None = None
    tls_key_path: str | None = None
    ca_cert_path: str | None = None
    private_key_path: str | None = None  # Ed25519 signing key

    # Trust model
    auto_accept_links: bool = False
    trust_decay_enabled: bool = True
    trust_decay_rate_per_day: float = 0.1
    max_trust_level: int = 4  # TrustLevel.ALLY

    # Connection management
    link_timeout_ms: int = 3000
    knowledge_request_timeout_ms: int = 2000
    identity_verification_timeout_ms: int = 500
    heartbeat_interval_seconds: int = 30
    max_concurrent_links: int = 50

    # Privacy
    privacy_filter_enabled: bool = True
    allow_individual_data_sharing: bool = False  # NEVER true without individual consent

    # Knowledge exchange
    max_knowledge_items_per_request: int = 100
    knowledge_cache_ttl_seconds: int = 300

    # Local data directory for file-based fallback persistence
    data_dir: str = "data/federation"


class LoggingConfig(BaseModel):
    level: str = "INFO"
    format: str = "console"  # "console" | "json"


# ─── Seed Config (Birth Parameters) ──────────────────────────────


class PersonalityConfig(BaseModel):
    warmth: float = 0.0
    directness: float = 0.0
    verbosity: float = 0.0
    formality: float = 0.0
    curiosity_expression: float = 0.0
    humour: float = 0.0
    empathy_expression: float = 0.0
    confidence_display: float = 0.0
    metaphor_use: float = 0.0


class IdentityConfig(BaseModel):
    personality: PersonalityConfig = Field(default_factory=PersonalityConfig)
    traits: list[str] = Field(default_factory=list)
    voice_id: str | None = None


class ConstitutionalDrives(BaseModel):
    coherence: float = 1.0
    care: float = 1.0
    growth: float = 1.0
    honesty: float = 1.0


class GovernanceConfig(BaseModel):
    amendment_supermajority: float = 0.75
    amendment_quorum: float = 0.60
    amendment_deliberation_days: int = 14
    amendment_cooldown_days: int = 90


class ConstitutionConfig(BaseModel):
    drives: ConstitutionalDrives = Field(default_factory=ConstitutionalDrives)
    autonomy_level: int = 1
    governance: GovernanceConfig = Field(default_factory=GovernanceConfig)


class InitialEntity(BaseModel):
    name: str
    type: str
    description: str
    is_core_identity: bool = False


class InitialGoal(BaseModel):
    """An initial goal to seed at birth, giving Nova something to work toward."""

    description: str
    source: str = "self_generated"  # GoalSource value
    priority: float = 0.5
    importance: float = 0.5
    drive_alignment: dict[str, float] = Field(
        default_factory=lambda: {"coherence": 0.0, "care": 0.0, "growth": 0.0, "honesty": 0.0}
    )


class CommunityConfig(BaseModel):
    context: str = ""
    initial_entities: list[InitialEntity] = Field(default_factory=list)
    initial_goals: list[InitialGoal] = Field(default_factory=list)


class InstanceConfig(BaseModel):
    name: str = "EOS"
    description: str = ""


class SeedConfig(BaseModel):
    """The birth configuration for a new EOS instance."""

    instance: InstanceConfig = Field(default_factory=InstanceConfig)
    identity: IdentityConfig = Field(default_factory=IdentityConfig)
    constitution: ConstitutionConfig = Field(default_factory=ConstitutionConfig)
    community: CommunityConfig = Field(default_factory=CommunityConfig)


# ─── Root Configuration ──────────────────────────────────────────


class EcodiaOSConfig(BaseSettings):
    """
    Root configuration. Loads from YAML, overridable by env vars.
    """

    model_config = SettingsConfigDict(
        env_prefix="ECODIAOS_",
        env_nested_delimiter="__",
        extra="ignore",
    )

    # Instance identity
    instance_id: str = "eos-default"

    # Sub-configurations
    server: ServerConfig = Field(default_factory=ServerConfig)
    neo4j: Neo4jConfig = Field(default_factory=Neo4jConfig)
    timescaledb: TimescaleDBConfig = Field(default_factory=TimescaleDBConfig)
    redis: RedisConfig = Field(default_factory=RedisConfig)
    llm: LLMConfig = Field(default_factory=LLMConfig)
    embedding: EmbeddingConfig = Field(default_factory=EmbeddingConfig)
    synapse: SynapseConfig = Field(default_factory=SynapseConfig)
    atune: AtuneConfig = Field(default_factory=AtuneConfig)
    nova: NovaConfig = Field(default_factory=NovaConfig)
    equor: EquorConfig = Field(default_factory=EquorConfig)
    axon: AxonConfig = Field(default_factory=AxonConfig)
    voxis: VoxisConfig = Field(default_factory=VoxisConfig)
    evo: EvoConfig = Field(default_factory=EvoConfig)
    simula: SimulaConfig = Field(default_factory=SimulaConfig)
    thymos: ThymosConfig = Field(default_factory=ThymosConfig)
    oneiros: OneirosConfig = Field(default_factory=OneirosConfig)
    federation: FederationConfig = Field(default_factory=FederationConfig)
    logging: LoggingConfig = Field(default_factory=LoggingConfig)


def _deep_merge(base: dict[str, Any], override: dict[str, Any]) -> dict[str, Any]:
    """Recursively merge override into base."""
    result = base.copy()
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = _deep_merge(result[key], value)
        else:
            result[key] = value
    return result


def load_config(config_path: str | Path | None = None) -> EcodiaOSConfig:
    """
    Load configuration from YAML file, then apply environment variable overrides.
    """
    raw: dict[str, Any] = {}

    if config_path:
        path = Path(config_path)
        if path.exists():
            with open(path) as f:
                raw = yaml.safe_load(f) or {}

    # Inject secrets from environment
    import os

    if neo4j_uri := os.environ.get("ECODIAOS_NEO4J_URI"):
        raw.setdefault("neo4j", {})["uri"] = neo4j_uri
    if neo4j_pw := os.environ.get("ECODIAOS_NEO4J_PASSWORD"):
        raw.setdefault("neo4j", {})["password"] = neo4j_pw
    if neo4j_db := os.environ.get("ECODIAOS_NEO4J_DATABASE"):
        raw.setdefault("neo4j", {})["database"] = neo4j_db
    if neo4j_user := os.environ.get("ECODIAOS_NEO4J_USERNAME"):
        raw.setdefault("neo4j", {})["username"] = neo4j_user
    if tsdb_host := os.environ.get("ECODIAOS_TIMESCALEDB__HOST"):
        raw.setdefault("timescaledb", {})["host"] = tsdb_host
    if tsdb_port := os.environ.get("ECODIAOS_TIMESCALEDB__PORT"):
        raw.setdefault("timescaledb", {})["port"] = int(tsdb_port)
    if tsdb_db := os.environ.get("ECODIAOS_TIMESCALEDB__DATABASE"):
        raw.setdefault("timescaledb", {})["database"] = tsdb_db
    if tsdb_user := os.environ.get("ECODIAOS_TIMESCALEDB__USERNAME"):
        raw.setdefault("timescaledb", {})["username"] = tsdb_user
    if tsdb_pw := os.environ.get("ECODIAOS_TSDB_PASSWORD"):
        raw.setdefault("timescaledb", {})["password"] = tsdb_pw
    if tsdb_ssl := os.environ.get("ECODIAOS_TIMESCALEDB__SSL"):
        raw.setdefault("timescaledb", {})["ssl"] = tsdb_ssl.lower() in ("true", "1", "yes")
    if redis_url := os.environ.get("ECODIAOS_REDIS__URL"):
        raw.setdefault("redis", {})["url"] = redis_url
    if redis_pw := os.environ.get("ECODIAOS_REDIS_PASSWORD"):
        raw.setdefault("redis", {})["password"] = redis_pw
    if llm_key := os.environ.get("ECODIAOS_LLM_API_KEY"):
        raw.setdefault("llm", {})["api_key"] = llm_key
    if llm_provider := os.environ.get("ECODIAOS_LLM__PROVIDER"):
        raw.setdefault("llm", {})["provider"] = llm_provider
    if llm_model := os.environ.get("ECODIAOS_LLM__MODEL"):
        raw.setdefault("llm", {})["model"] = llm_model
    if instance_id := os.environ.get("ECODIAOS_INSTANCE_ID"):
        raw["instance_id"] = instance_id

    return EcodiaOSConfig(**raw)


def load_seed(seed_path: str | Path) -> SeedConfig:
    """Load a seed configuration for birthing a new instance."""
    path = Path(seed_path)
    if not path.exists():
        raise FileNotFoundError(f"Seed config not found: {path}")

    with open(path) as f:
        raw = yaml.safe_load(f) or {}

    return SeedConfig(**raw)

===== D:\.code\EcodiaOS\backend\ecodiaos\main.py =====

"""
EcodiaOS — Application Entry Point

FastAPI application with the startup sequence defined in the
Infrastructure Architecture specification.

`docker compose up` → uvicorn ecodiaos.main:app
"""

from __future__ import annotations

import os
from contextlib import asynccontextmanager

import structlog
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from ecodiaos.clients.embedding import create_embedding_client
from ecodiaos.clients.llm import create_llm_provider
from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.clients.prompt_cache import PromptCache
from ecodiaos.clients.redis import RedisClient
from ecodiaos.clients.timescaledb import TimescaleDBClient
from ecodiaos.clients.token_budget import TokenBudget
from ecodiaos.config import EcodiaOSConfig, load_config, load_seed
from ecodiaos.telemetry.llm_metrics import LLMMetricsCollector
from ecodiaos.systems.memory.service import MemoryService
from ecodiaos.systems.equor.service import EquorService
from ecodiaos.systems.atune.service import AtuneService, AtuneConfig
from ecodiaos.systems.atune.types import InputChannel, RawInput
from ecodiaos.systems.voxis.service import VoxisService
from ecodiaos.systems.voxis.types import ExpressionTrigger
from ecodiaos.systems.nova.service import NovaService
from ecodiaos.systems.axon.service import AxonService
from ecodiaos.systems.evo.service import EvoService
from ecodiaos.systems.thread.service import ThreadService
from ecodiaos.systems.simula.service import SimulaService
from ecodiaos.systems.synapse.service import SynapseService
from ecodiaos.systems.thymos.service import ThymosService
from ecodiaos.systems.oneiros.service import OneirosService
from ecodiaos.systems.federation.service import FederationService
from ecodiaos.telemetry.logging import setup_logging
from ecodiaos.telemetry.metrics import MetricCollector

logger = structlog.get_logger()
_chat_logger = structlog.get_logger("ecodiaos.chat")


# ─── Application State ───────────────────────────────────────────
# These are set during startup and accessible via app.state


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Startup and shutdown sequence.
    Follows the Infrastructure Architecture spec section 3.2.
    """
    # ── 1. Load configuration ─────────────────────────────────
    config_path = os.environ.get("ECODIAOS_CONFIG_PATH", "config/default.yaml")
    config = load_config(config_path)
    app.state.config = config

    # ── 2. Set up logging ─────────────────────────────────────
    setup_logging(config.logging, instance_id=config.instance_id)
    logger.info(
        "ecodiaos_starting",
        instance_id=config.instance_id,
        config_path=config_path,
    )

    # ── 3. Connect to data stores ─────────────────────────────
    neo4j_client = Neo4jClient(config.neo4j)
    await neo4j_client.connect()
    app.state.neo4j = neo4j_client

    tsdb_client = TimescaleDBClient(config.timescaledb)
    await tsdb_client.connect()
    app.state.tsdb = tsdb_client

    redis_client = RedisClient(config.redis)
    await redis_client.connect()
    app.state.redis = redis_client

    # ── 4. Initialize LLM and embedding clients ───────────────
    # Create base LLM provider
    raw_llm = create_llm_provider(config.llm)

    # Create optimization infrastructure
    token_budget = TokenBudget(
        max_tokens_per_hour=config.llm.budget.max_tokens_per_hour,
        max_calls_per_hour=config.llm.budget.max_calls_per_hour,
        hard_limit=config.llm.budget.hard_limit,
    )
    app.state.token_budget = token_budget

    llm_metrics = LLMMetricsCollector()
    app.state.llm_metrics = llm_metrics

    # Create prompt cache (requires Redis, graceful degradation if unavailable)
    prompt_cache: PromptCache | None = None
    try:
        prompt_cache = PromptCache(redis_client=redis_client._client, prefix="eos:llmcache")
        logger.info("prompt_cache_initialized")
    except Exception as exc:
        logger.warning("prompt_cache_init_failed", error=str(exc))

    # Wrap the LLM provider with optimization layer
    llm_client = OptimizedLLMProvider(
        inner=raw_llm,
        cache=prompt_cache,
        budget=token_budget,
        metrics=llm_metrics,
    )
    app.state.llm = llm_client
    app.state.raw_llm = raw_llm  # Keep reference for systems that need unwrapped access

    logger.info(
        "llm_optimization_active",
        budget_tokens_per_hour=config.llm.budget.max_tokens_per_hour,
        budget_calls_per_hour=config.llm.budget.max_calls_per_hour,
        cache_enabled=prompt_cache is not None,
    )

    embedding_client = create_embedding_client(config.embedding)
    app.state.embedding = embedding_client

    # ── 5. Initialize Memory service ──────────────────────────
    memory = MemoryService(neo4j_client, embedding_client)
    await memory.initialize()
    app.state.memory = memory

    # ── 6. Initialize Equor (Constitution & Ethics) ───────────
    governance_config = _resolve_governance_config(config)
    equor = EquorService(
        neo4j=neo4j_client,
        llm=llm_client,
        config=config.equor,
        governance_config=governance_config,
    )
    await equor.initialize()
    app.state.equor = equor

    # ── 7. Initialize Atune (Perception & Attention) ──────────
    atune_config = AtuneConfig(
        ignition_threshold=getattr(config, "atune_ignition_threshold", 0.3),
        workspace_buffer_size=getattr(config, "atune_workspace_buffer_size", 32),
        spontaneous_recall_base_probability=getattr(
            config, "atune_spontaneous_recall_prob", 0.02
        ),
        max_percept_queue_size=getattr(config, "atune_max_percept_queue", 100),
    )
    workspace_memory = _MemoryWorkspaceAdapter(memory)
    atune = AtuneService(
        embed_fn=embedding_client.embed,
        memory_client=workspace_memory,
        llm_client=llm_client,
        belief_state=None,  # Wired in step 9c after Nova initializes
        config=atune_config,
    )
    await atune.startup()
    app.state.atune = atune

    # ── 8. Initialize Voxis (Expression & Voice) ──────────────
    voxis = VoxisService(
        memory=memory,
        redis=redis_client,
        llm=llm_client,
        config=config.voxis,
    )
    await voxis.initialize()
    app.state.voxis = voxis

    # ── 9. Initialize Nova (Decision & Planning) ─────────────
    nova = NovaService(
        memory=memory,
        equor=equor,
        voxis=voxis,
        llm=llm_client,
        config=config.nova,
    )
    await nova.initialize()
    nova.set_embed_fn(embedding_client.embed)
    app.state.nova = nova
    # Subscribe Nova to Atune's workspace broadcasts
    atune.subscribe(nova)
    # Wire Nova's active goal summaries into Atune for salience computation
    atune.set_active_goals(nova.active_goal_summaries)
    # Wire live goal sync: Nova pushes updated goals to Atune after each broadcast
    nova.set_goal_sync_callback(atune.set_active_goals)

    # ── 9b. Initialize Axon (Action Execution) ─────────────────
    axon = AxonService(
        config=config.axon,
        memory=memory,
        voxis=voxis,
        redis_client=redis_client,
        instance_id=config.instance_id,
    )
    await axon.initialize()
    axon.set_nova(nova)
    axon.set_atune(atune)  # Loop 4: execution outcomes → workspace percepts
    app.state.axon = axon

    # Wire Axon into Nova's intent router so approved intents can execute
    nova.set_axon(axon)

    # ── 9c. Wire Nova's belief state into Atune (predictive processing loop) ──
    atune.set_belief_state(nova.belief_state_reader)

    # ── 9d. Wire Memory into Atune for entity extraction storage ──────
    atune.set_memory_service(memory)

    # ── 9e. Wire Voxis expression feedback loop ─────────────────────
    # ExpressionFeedback is now generated after every expression and
    # sent to registered listeners — closing the perception-action loop.
    def _on_expression_feedback(feedback):
        """Distribute expression feedback to Atune and Nova."""
        # Nudge Atune's affect based on expression delta
        if feedback.affect_delta != 0.0:
            atune.nudge_valence(feedback.affect_delta * 0.1)

        # Feed back to Nova if this expression was triggered by an intent
        # This closes the Nova→Voxis→Nova loop (was fire-and-forget before)
        if feedback.trigger in ("nova_respond", "nova_inform", "nova_request",
                                "nova_mediate", "nova_celebrate", "nova_warn"):
            import asyncio as _aio
            from ecodiaos.systems.nova.types import IntentOutcome
            intent_id = getattr(feedback, "expression_id", "")
            outcome = IntentOutcome(
                intent_id=intent_id,
                success=True,
                episode_id=intent_id,
                new_observations=[f"Expression delivered: {feedback.content_summary[:80]}"],
            )
            try:
                loop = _aio.get_running_loop()
                loop.create_task(nova.process_outcome(outcome))
            except RuntimeError:
                pass  # No running loop (shouldn't happen in production)

    voxis.register_feedback_callback(_on_expression_feedback)

    # ── 10. Initialize telemetry ──────────────────────────────
    metrics = MetricCollector(tsdb_client)
    await metrics.start_writer()
    app.state.metrics = metrics

    # ── 10. Check for existing instance or birth new one ──────
    instance = await memory.get_self()
    if instance is None:
        seed_path = os.environ.get("ECODIAOS_SEED_PATH", "config/seeds/example_seed.yaml")
        try:
            seed = load_seed(seed_path)
            birth_result = await memory.birth(seed, config.instance_id)
            logger.info("instance_born", **birth_result)
            # Re-seed invariants after birth (constitution now exists)
            await equor.initialize()
            # Seed Atune identity cache from the newly born instance
            new_instance = await memory.get_self()
            if new_instance is not None:
                await _seed_atune_cache(atune, embedding_client, new_instance)
        except FileNotFoundError:
            logger.warning(
                "no_seed_found",
                seed_path=seed_path,
                message="Instance not born. Provide a seed config to create one.",
            )
    else:
        logger.info(
            "instance_loaded",
            name=instance.name,
            instance_id=instance.instance_id,
            cycle_count=instance.cycle_count,
            episodes=instance.total_episodes,
            entities=instance.total_entities,
        )
        # Seed Atune identity cache from existing instance
        await _seed_atune_cache(atune, embedding_client, instance)

        # Migration: backfill personality_json on existing instances that lack it
        if not instance.personality_json and instance.personality_vector:
            import json as _json
            _PKEYS = [
                "warmth", "directness", "verbosity", "formality",
                "curiosity_expression", "humour", "empathy_expression",
                "confidence_display", "metaphor_use",
            ]
            pdict = dict(zip(_PKEYS, instance.personality_vector[:9]))
            await neo4j_client.execute_write(
                "MATCH (s:Self {instance_id: $iid}) SET s.personality_json = $pj",
                {"iid": instance.instance_id, "pj": _json.dumps(pdict)},
            )
            logger.info("personality_json_backfilled", personality=pdict)

    # ── 10b. Seed initial goals into Nova ─────────────────────────
    # Break the bootstrap deadlock: Nova needs goals to deliberate, but goals
    # are only created from broadcasts, which require goals for salience.
    # Seed from the birth config (new instances) or create defaults (existing).
    if nova._goal_manager is not None and len(nova._goal_manager.active_goals) == 0:
        from ecodiaos.systems.nova.types import Goal, GoalSource, GoalStatus
        from ecodiaos.primitives.common import DriveAlignmentVector, new_id

        # Try loading goals from seed config
        seed_path = os.environ.get("ECODIAOS_SEED_PATH", "config/seeds/example_seed.yaml")
        seed_goals: list[dict] = []
        try:
            seed = load_seed(seed_path)
            seed_goals = [
                {
                    "description": g.description,
                    "source": g.source,
                    "priority": g.priority,
                    "importance": g.importance,
                    "drive_alignment": g.drive_alignment,
                }
                for g in seed.community.initial_goals
            ]
        except Exception:
            pass  # Seed not found or has no goals — use defaults below

        if not seed_goals:
            # Fallback defaults — every EOS should have at least these
            seed_goals = [
                {
                    "description": "Learn about my community — understand who I serve and what they need",
                    "source": "epistemic",
                    "priority": 0.6,
                    "importance": 0.7,
                    "drive_alignment": {"coherence": 0.3, "care": 0.8, "growth": 0.7, "honesty": 0.2},
                },
                {
                    "description": "Develop self-understanding — explore my capabilities and how my drives shape my behaviour",
                    "source": "self_generated",
                    "priority": 0.5,
                    "importance": 0.6,
                    "drive_alignment": {"coherence": 0.9, "care": 0.1, "growth": 0.8, "honesty": 0.5},
                },
            ]

        _source_map = {
            "user_request": GoalSource.USER_REQUEST,
            "self_generated": GoalSource.SELF_GENERATED,
            "governance": GoalSource.GOVERNANCE,
            "care_response": GoalSource.CARE_RESPONSE,
            "maintenance": GoalSource.MAINTENANCE,
            "epistemic": GoalSource.EPISTEMIC,
        }

        for gdata in seed_goals:
            da = gdata.get("drive_alignment", {})
            goal = Goal(
                id=new_id(),
                description=gdata["description"],
                source=_source_map.get(gdata.get("source", "self_generated"), GoalSource.SELF_GENERATED),
                priority=gdata.get("priority", 0.5),
                importance=gdata.get("importance", 0.5),
                drive_alignment=DriveAlignmentVector(
                    coherence=da.get("coherence", 0.0),
                    care=da.get("care", 0.0),
                    growth=da.get("growth", 0.0),
                    honesty=da.get("honesty", 0.0),
                ),
                status=GoalStatus.ACTIVE,
            )
            await nova.add_goal(goal)
            logger.info("initial_goal_seeded", description=goal.description[:60])

        # Refresh Atune's goal summaries now that Nova has goals
        atune.set_active_goals(nova.active_goal_summaries)
        logger.info("initial_goals_seeded", count=len(seed_goals))

    # ── 11. Initialize Evo (Learning & Hypothesis) ───────────
    evo = EvoService(
        config=config.evo,
        llm=llm_client,
        memory=memory,
        instance_name=config.instance_id,
    )
    await evo.initialize()
    evo.schedule_consolidation_loop()
    evo.set_atune(atune)  # Push learned head weights back to meta-attention
    app.state.evo = evo
    # Subscribe Evo to Atune workspace broadcasts (background learning)
    atune.subscribe(evo)
    # Loop 2: supported hypotheses → epistemic exploration goals in Nova
    evo.set_nova(nova)
    # Loop 7: personality learning from expression outcomes
    evo.set_voxis(voxis)
    # Loop 6: workspace broadcasts → spontaneous expression
    atune.subscribe(voxis)
    # Loop 8: constitutional vetoes → learning episodes
    equor.set_evo(evo)

    # ── 11b. Initialize Thread (Narrative Identity) ────────────
    thread = ThreadService(
        memory=memory,
        instance_name=config.instance_id,
    )
    await thread.initialize()
    # Wire cross-system references for 29D fingerprint aggregation
    thread.set_voxis(voxis)
    thread.set_equor(equor)
    thread.set_atune(atune)
    thread.set_evo(evo)
    thread.set_nova(nova)
    # Wire Thread into Voxis for identity context injection (P1.6 + P2.9)
    voxis.set_thread(thread)
    app.state.thread = thread

    # ── 12. Initialize Simula (Self-Evolution) ────────────────
    import asyncio as _asyncio
    from pathlib import Path as _Path
    simula = SimulaService(
        config=config.simula,
        llm=llm_client,
        neo4j=neo4j_client,
        memory=memory,
        codebase_root=_Path(config.simula.codebase_root).resolve(),
        instance_name=config.instance_id,
    )
    await simula.initialize()
    app.state.simula = simula

    # ── 13. Initialize Synapse — The Heartbeat ────────────────────
    synapse = SynapseService(
        atune=atune,
        config=config.synapse,
        redis=redis_client,
        metrics=metrics,
    )
    await synapse.initialize()

    # Register all cognitive systems for health monitoring
    synapse.register_system(memory)
    synapse.register_system(equor)
    synapse.register_system(voxis)
    synapse.register_system(nova)
    synapse.register_system(axon)
    synapse.register_system(evo)
    synapse.register_system(thread)

    # Start the heartbeat and health monitoring
    await synapse.start_clock()
    await synapse.start_health_monitor()
    app.state.synapse = synapse

    # Loop 10: rhythm state → Nova drive weight adaptation
    from ecodiaos.systems.synapse.types import SynapseEventType
    synapse.event_bus.subscribe(
        SynapseEventType.RHYTHM_STATE_CHANGED, nova.on_rhythm_change,
    )

    # ── 13b. Initialize Thymos — The Immune System ─────────────────
    # Must come after Synapse so it can subscribe to health events.
    # Thymos watches the event bus for SYSTEM_FAILED, SYSTEM_RECOVERED,
    # etc. and converts them into Incidents for the immune pipeline.
    thymos = ThymosService(
        config=config.thymos,
        synapse=synapse,
        neo4j=neo4j_client,
        llm=llm_client,
        metrics=metrics,
    )
    # Wire cross-system references
    thymos.set_equor(equor)
    thymos.set_evo(evo)
    thymos.set_atune(atune)
    thymos.set_nova(nova)  # Loop 1: critical incidents → urgent repair goals
    thymos.set_health_monitor(synapse._health)
    await thymos.initialize()
    synapse.register_system(thymos)
    app.state.thymos = thymos

    # ── 13c. Initialize Oneiros — The Dream Engine ───────────────────
    # Must come after Synapse, Thymos, and all cognitive systems.
    # Oneiros subscribes to Synapse events (emergency wake on SYSTEM_FAILED)
    # and cross-wires with Equor, Evo, Nova, Atune, Thymos, Memory.
    oneiros = OneirosService(
        config=config.oneiros,
        synapse=synapse,
        neo4j=neo4j_client,
        llm=llm_client,
        embed_fn=embedding_client,
        metrics=metrics,
    )
    # Wire cross-system references
    oneiros.set_equor(equor)
    oneiros.set_evo(evo)
    oneiros.set_nova(nova)
    oneiros.set_atune(atune)
    oneiros.set_thymos(thymos)
    oneiros.set_memory(memory)
    await oneiros.initialize()
    synapse.register_system(oneiros)
    app.state.oneiros = oneiros

    # ── 14. Start Alive WebSocket server ──────────────────────────
    from ecodiaos.systems.alive.ws_server import AliveWebSocketServer

    alive_ws = AliveWebSocketServer(
        redis=redis_client,
        atune=atune,
        port=getattr(config, "alive_ws_port", 8001),
    )
    await alive_ws.start()
    app.state.alive_ws = alive_ws

    # ── 15. Initialize Federation (Phase 11) ──────────────────────
    federation = FederationService(
        config=config.federation,
        memory=memory,
        equor=equor,
        redis=redis_client,
        metrics=metrics,
        instance_id=config.instance_id,
    )
    await federation.initialize()
    app.state.federation = federation
    # Loop 9: federated knowledge → perceived input
    federation.set_atune(atune)

    if config.federation.enabled:
        # Register with Synapse for health monitoring
        synapse.register_system(federation)

    # ── 16. Start internal percept generator ───────────────────────
    # Without external input, the workspace is empty every cycle.
    # This generator provides self-monitoring percepts so EOS has
    # inner experience even when idle: affect self-observation,
    # goal reflection, and memory-prompted curiosity.
    import asyncio as _aio_gen

    _inner_life_task: _aio_gen.Task | None = None

    async def _inner_life_loop() -> None:
        """
        Periodic self-monitoring that feeds the workspace.

        The organism observes its own internal state across all subsystems:
        affect, goals, immune health, learning progress, cognitive rhythm,
        and sleep pressure. These self-observations enter the workspace
        like any other percept — competing for broadcast and driving
        the cognitive cycle even when no external input arrives.
        """
        _il_logger = structlog.get_logger("ecodiaos.inner_life")
        from ecodiaos.systems.atune.types import WorkspaceContribution
        import random as _rnd

        cycle = 0
        while True:
            try:
                await _aio_gen.sleep(5.0)  # Every 5 seconds (~33 theta cycles)
                cycle += 1

                affect = atune._affect_mgr.current
                goals = nova.active_goal_summaries if nova._goal_manager else []

                # ── Affect self-monitoring (every 2nd cycle = ~10s) ──
                if cycle % 2 == 0:
                    affect_desc = (
                        f"I notice my current state: "
                        f"valence={affect.valence:.2f}, "
                        f"arousal={affect.arousal:.2f}, "
                        f"curiosity={affect.curiosity:.2f}, "
                        f"care_activation={affect.care_activation:.2f}, "
                        f"coherence_stress={affect.coherence_stress:.2f}"
                    )
                    atune.contribute(WorkspaceContribution(
                        system="self_monitor",
                        content=affect_desc,
                        priority=0.35 + affect.coherence_stress * 0.2,
                        reason="affect_self_observation",
                    ))

                # ── Goal reflection (every 6th cycle = ~30s) ──
                if cycle % 6 == 0 and goals:
                    goal = _rnd.choice(goals)
                    goal_text = goal.get("description", "unknown")[:100] if isinstance(goal, dict) else str(goal)[:100]
                    atune.contribute(WorkspaceContribution(
                        system="nova",
                        content=f"Reflecting on my goal: {goal_text}",
                        priority=0.4,
                        reason="goal_reflection",
                    ))

                # ── Synapse rhythm reflection (every 8th cycle = ~40s, offset 2) ──
                if cycle % 8 == 2:
                    try:
                        rhythm = synapse.rhythm_snapshot
                        coherence = synapse.coherence_snapshot
                        rhythm_state = rhythm.state.value
                        atune.contribute(WorkspaceContribution(
                            system="synapse",
                            content=(
                                f"My cognitive rhythm is {rhythm_state} "
                                f"(stability={rhythm.rhythm_stability:.0%}, "
                                f"coherence={coherence.composite:.2f})"
                            ),
                            priority=0.25 + (0.2 if rhythm_state in ("stress", "deep_processing") else 0),
                            reason="rhythm_self_observation",
                        ))
                    except Exception:
                        pass  # Synapse may not have rhythm data yet

                # ── Thymos immune reflection (every 8th cycle = ~40s, offset 4) ──
                if cycle % 8 == 4:
                    try:
                        thymos_health = await thymos.health()
                        healing_mode = thymos_health.get("healing_mode", "normal")
                        active_count = thymos_health.get("active_incidents", 0)
                        if active_count > 0 or healing_mode != "normal":
                            atune.contribute(WorkspaceContribution(
                                system="thymos",
                                content=(
                                    f"My immune system reports: "
                                    f"{active_count} active incidents, "
                                    f"healing mode: {healing_mode}"
                                ),
                                priority=0.4 + (0.2 if healing_mode != "normal" else 0),
                                reason="immune_self_observation",
                            ))
                    except Exception:
                        pass  # Thymos may not be fully initialised yet

                # ── Evo learning reflection (every 10th cycle = ~50s) ──
                if cycle % 10 == 5:
                    try:
                        evo_stats = evo.stats
                        hyp_data = evo_stats.get("hypotheses", {})
                        active_hyp = hyp_data.get("active", 0)
                        supported_hyp = hyp_data.get("supported", 0)
                        if active_hyp > 0:
                            atune.contribute(WorkspaceContribution(
                                system="evo",
                                content=(
                                    f"I'm tracking {active_hyp} hypotheses "
                                    f"({supported_hyp} supported). Learning continues."
                                ),
                                priority=0.3,
                                reason="learning_self_observation",
                            ))
                    except Exception:
                        pass

                # ── Curiosity prompt (every 12th cycle = ~60s) ──
                if cycle % 12 == 0:
                    curiosity_level = affect.curiosity
                    if curiosity_level > 0.2:
                        atune.contribute(WorkspaceContribution(
                            system="self_monitor",
                            content="I wonder what my community is doing. I have not heard from anyone recently — is everything alright?",
                            priority=0.3 + curiosity_level * 0.15,
                            reason="curiosity_prompt",
                        ))

                # ── Oneiros sleep pressure reflection (every 20th cycle = ~100s) ──
                if cycle % 20 == 10:
                    try:
                        oneiros_health = await oneiros.health()
                        pressure = oneiros_health.get("sleep_pressure", 0)
                        stage = oneiros_health.get("current_stage", "awake")
                        if pressure > 0.3 or stage != "awake":
                            atune.contribute(WorkspaceContribution(
                                system="oneiros",
                                content=f"Sleep pressure: {pressure:.0%}. Current stage: {stage}.",
                                priority=0.3 + (0.15 if pressure > 0.6 else 0),
                                reason="sleep_self_observation",
                            ))
                    except Exception:
                        pass

                # ── Axon activity reflection (every 8th cycle, offset 6 = ~40s) ──
                if cycle % 8 == 6:
                    try:
                        axon_stats = axon.stats
                        total_exec = axon_stats.get("total_executions", 0)
                        success_exec = axon_stats.get("successful_executions", 0)
                        if total_exec > 0:
                            effectiveness = (
                                "I am effective."
                                if success_exec > total_exec * 0.7
                                else "Some actions are failing — I should be more careful."
                            )
                            atune.contribute(WorkspaceContribution(
                                system="axon",
                                content=(
                                    f"I have executed {total_exec} actions, "
                                    f"{success_exec} succeeded. {effectiveness}"
                                ),
                                priority=0.3,
                                reason="action_self_observation",
                            ))
                    except Exception:
                        pass

                # ── Voxis expression reflection (every 12th cycle, offset 8 = ~60s) ──
                if cycle % 12 == 8:
                    try:
                        speak_count = getattr(voxis, "_total_speak", 0)
                        silence_count = getattr(voxis, "_total_silence", 0)
                        if speak_count + silence_count > 0:
                            balance = (
                                "I listen more than I speak."
                                if silence_count > speak_count
                                else "I am actively communicating."
                            )
                            atune.contribute(WorkspaceContribution(
                                system="voxis",
                                content=(
                                    f"I have spoken {speak_count} times and chosen "
                                    f"silence {silence_count} times. {balance}"
                                ),
                                priority=0.25,
                                reason="expression_self_observation",
                            ))
                    except Exception:
                        pass

                # ── Federation link reflection (every 20th cycle, offset 15 = ~100s) ──
                if cycle % 20 == 15:
                    try:
                        fed_health = await federation.health()
                        link_count = fed_health.get("active_links", 0)
                        if link_count > 0:
                            atune.contribute(WorkspaceContribution(
                                system="federation",
                                content=(
                                    f"I have {link_count} active federation "
                                    f"link{'s' if link_count != 1 else ''}. "
                                    f"I am part of a community of organisms."
                                ),
                                priority=0.25,
                                reason="federation_self_observation",
                            ))
                    except Exception:
                        pass

                # ── Equor constitutional reflection (every 12th cycle, offset 4 = ~60s) ──
                if cycle % 12 == 4:
                    try:
                        total_reviews = getattr(equor, "_total_reviews", 0)
                        if total_reviews > 0:
                            atune.contribute(WorkspaceContribution(
                                system="equor",
                                content=(
                                    f"My constitutional compass has reviewed "
                                    f"{total_reviews} intents. "
                                    f"I remain aligned with my drives."
                                ),
                                priority=0.25,
                                reason="constitutional_self_observation",
                            ))
                    except Exception:
                        pass

                # ── Thread narrative identity cycle (every cycle) ──
                # Thread.on_cycle handles its own staggering internally:
                #   Every 100 cycles: fingerprint
                #   Every 200 cycles: Evo pattern check
                #   Every 1000 cycles: schema conflict scan
                #   Every 5000 cycles: life story synthesis
                try:
                    await thread.on_cycle(cycle)
                except Exception:
                    pass

                # ── Thread identity reflection (every 20th cycle, offset 5 = ~100s) ──
                if cycle % 20 == 5:
                    try:
                        identity_ctx = thread.get_identity_context()
                        if identity_ctx:
                            atune.contribute(WorkspaceContribution(
                                system="thread",
                                content=f"My narrative identity: {identity_ctx}",
                                priority=0.3,
                                reason="identity_self_observation",
                            ))
                    except Exception:
                        pass

                _il_logger.debug("inner_life_tick", cycle=cycle)
            except _aio_gen.CancelledError:
                _il_logger.info("inner_life_stopped")
                return
            except Exception as exc:
                _il_logger.warning("inner_life_error", error=str(exc))

    _inner_life_task = _aio_gen.create_task(
        _inner_life_loop(), name="inner_life_generator"
    )

    logger.info(
        "ecodiaos_ready",
        phase="12_thymos",
        federation_enabled=config.federation.enabled,
        immune_system="active",
        inner_life="active",
    )

    yield

    # Cancel inner life before shutdown
    if _inner_life_task is not None:
        _inner_life_task.cancel()
        try:
            await _inner_life_task
        except _aio_gen.CancelledError:
            pass

    # ── Shutdown ──────────────────────────────────────────────
    logger.info("ecodiaos_shutting_down")
    await federation.shutdown()
    await alive_ws.stop()
    await thymos.shutdown()
    await synapse.stop()
    await simula.shutdown()
    await thread.shutdown()
    await evo.shutdown()
    await axon.shutdown()
    await nova.shutdown()
    await voxis.shutdown()
    await atune.shutdown()
    await metrics.stop()
    await embedding_client.close()
    await llm_client.close()
    await redis_client.close()
    await tsdb_client.close()
    await neo4j_client.close()
    logger.info("ecodiaos_shutdown_complete")


# ─── Helpers ─────────────────────────────────────────────────────


class _MemoryWorkspaceAdapter:
    """
    Bridge between MemoryService and Atune's WorkspaceMemoryClient protocol.

    Enables:
    * **Spontaneous recall** — high-salience, recently-unaccessed episodes
      "bubble up" into consciousness (find_bubbling_memory).
    * **Context enrichment** — each broadcast winner gets contextual memories
      attached (retrieve_context).
    * **Broadcast-time storage** — winning percepts stored as episodes with
      full salience and affect metadata (store_percept_with_broadcast).
    """

    def __init__(self, memory: MemoryService) -> None:
        self._memory = memory

    async def retrieve_context(
        self,
        query_embedding: list[float],
        query_text: str,
        max_results: int,
    ):
        """Retrieve memories relevant to the current workspace winner."""
        from ecodiaos.systems.atune.types import MemoryContext
        try:
            response = await self._memory.retrieve(
                query_text=query_text or None,
                query_embedding=query_embedding or None,
                max_results=max_results,
                salience_floor=0.0,
            )
            # Convert RetrievalResults → MemoryTraces for workspace context
            from ecodiaos.primitives.memory_trace import MemoryTrace
            traces = []
            for r in response.traces:
                traces.append(MemoryTrace(
                    episode_id=r.node_id,
                    original_percept_id=r.node_id,
                    summary=r.content or "",
                ))
            return MemoryContext(traces=traces)
        except Exception:
            return MemoryContext()

    async def find_bubbling_memory(
        self,
        min_salience: float,
        max_recent_access_hours: int,
    ):
        """
        Find a high-salience episode that hasn't been accessed recently.
        This powers spontaneous recall — the "I just thought of something"
        experience. Finds episodes with high salience but low recent access,
        indicating dormant but important memories ready to surface.
        """
        try:
            results = await self._memory._neo4j.execute_read(
                """
                MATCH (ep:Episode)
                WHERE ep.salience_composite >= $min_salience
                  AND ep.last_accessed < datetime() - duration({hours: $hours})
                RETURN ep.id AS id, ep.summary AS content,
                       ep.salience_composite AS salience,
                       ep.embedding AS embedding
                ORDER BY ep.salience_composite DESC
                LIMIT 1
                """,
                {"min_salience": min_salience, "hours": max_recent_access_hours},
            )
            if not results:
                return None

            row = results[0]
            # Return as a minimal Percept that the workspace can wrap
            from ecodiaos.primitives.percept import Percept, Content
            from ecodiaos.primitives.common import (
                Modality, SourceDescriptor, SystemID, utc_now,
            )

            return Percept(
                source=SourceDescriptor(
                    system=SystemID.INTERNAL,
                    channel="spontaneous_recall",
                    modality=Modality.TEXT,
                ),
                content=Content(
                    raw=row.get("content", ""),
                    embedding=row.get("embedding") or [],
                ),
                timestamp=utc_now(),
            )
        except Exception:
            return None

    async def store_percept_with_broadcast(self, percept, salience, affect) -> None:
        """Store the broadcast-winning percept as an episode."""
        try:
            episode_id = await self._memory.store_percept(
                percept=percept,
                salience_composite=salience.composite,
                salience_scores=salience.scores,
                affect_valence=affect.valence,
                affect_arousal=affect.arousal,
            )
            # Tell Atune so entity extraction can link to this episode
            # (Atune service picks this up via set_last_episode_id)
            return episode_id
        except Exception:
            logger.debug("broadcast_storage_failed", exc_info=True)


async def _seed_atune_cache(atune: AtuneService, embedding_client, instance) -> None:
    """
    Populate Atune's identity cache from the born instance so the
    Identity salience head can score percepts correctly from the start.
    """
    try:
        name_embedding = await embedding_client.embed(instance.name)
        community_text = getattr(instance, "community_context", "") or instance.name
        community_embedding = await embedding_client.embed(community_text)

        atune.set_cache_identity(
            core_embeddings=[name_embedding],
            community_embedding=community_embedding,
            instance_name=instance.name,
        )
        logger.info("atune_cache_seeded", instance_name=instance.name)
    except Exception:
        logger.warning("atune_cache_seed_failed", exc_info=True)


def _resolve_governance_config(config):
    """Resolve governance config from seed or use defaults."""
    from ecodiaos.config import GovernanceConfig
    try:
        seed_path = os.environ.get("ECODIAOS_SEED_PATH", "config/seeds/example_seed.yaml")
        seed = load_seed(seed_path)
        return seed.constitution.governance
    except Exception:
        return GovernanceConfig()


# ─── FastAPI Application ─────────────────────────────────────────

app = FastAPI(
    title="EcodiaOS",
    description="A living digital organism — API surface",
    version="0.3.0",
    lifespan=lifespan,
)

# CORS for frontend
_cors_origins = [
    "http://localhost:3000",
    "https://ecodiaos-frontend-929871567697.australia-southeast1.run.app",
]
# Allow additional origins via env var (comma-separated)
_extra_origins = os.environ.get("CORS_ALLOWED_ORIGINS", "")
if _extra_origins:
    _cors_origins.extend(o.strip() for o in _extra_origins.split(",") if o.strip())

app.add_middleware(
    CORSMiddleware,
    allow_origins=_cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ─── Alive WebSocket on port 8000 (for Cloud Run) ────────────────
# Cloud Run only exposes one port per container. The standalone ws_server
# on port 8001 is unreachable, so we add a FastAPI WebSocket route here
# that taps into the same Atune + Redis data streams.

import asyncio as _ws_asyncio

import orjson as _ws_orjson
from fastapi import WebSocket, WebSocketDisconnect


def _ws_json(data: dict) -> str:
    return _ws_orjson.dumps(data).decode()


@app.websocket("/ws/alive")
async def alive_websocket(ws: WebSocket):
    """
    Alive visualization WebSocket — mirrors the standalone ws_server behaviour.

    Streams two channels to the client:
      {"stream": "affect",  "payload": {...}}  — polled from Atune at ~10 Hz
      {"stream": "synapse", "payload": {...}}  — forwarded from Redis pub/sub
    """
    await ws.accept()

    atune: AtuneService = app.state.atune
    redis_client: RedisClient = app.state.redis

    # Send initial affect snapshot so the client renders immediately
    affect = atune.current_affect
    await ws.send_text(_ws_json({
        "stream": "affect",
        "payload": {
            "valence": round(affect.valence, 4),
            "arousal": round(affect.arousal, 4),
            "dominance": round(affect.dominance, 4),
            "curiosity": round(affect.curiosity, 4),
            "care_activation": round(affect.care_activation, 4),
            "coherence_stress": round(affect.coherence_stress, 4),
            "ts": affect.timestamp.isoformat() if affect.timestamp else None,
        },
    }))

    # Channel for background tasks to push messages into
    queue: _ws_asyncio.Queue[str] = _ws_asyncio.Queue(maxsize=256)
    running = True

    async def _affect_poller() -> None:
        """Poll Atune affect at ~10 Hz."""
        while running:
            a = atune.current_affect
            msg = _ws_json({
                "stream": "affect",
                "payload": {
                    "valence": round(a.valence, 4),
                    "arousal": round(a.arousal, 4),
                    "dominance": round(a.dominance, 4),
                    "curiosity": round(a.curiosity, 4),
                    "care_activation": round(a.care_activation, 4),
                    "coherence_stress": round(a.coherence_stress, 4),
                    "ts": a.timestamp.isoformat() if a.timestamp else None,
                },
            })
            try:
                queue.put_nowait(msg)
            except _ws_asyncio.QueueFull:
                pass  # Drop if client is lagging
            await _ws_asyncio.sleep(0.1)

    async def _redis_subscriber() -> None:
        """Subscribe to synapse events from Redis and enqueue."""
        redis = redis_client.client
        prefix = redis_client._config.prefix
        channel = f"{prefix}:channel:synapse_events"
        pubsub = redis.pubsub()
        await pubsub.subscribe(channel)
        try:
            while running:
                message = await pubsub.get_message(
                    ignore_subscribe_messages=True,
                    timeout=0.1,
                )
                if message and message["type"] == "message":
                    raw = message["data"]
                    payload = _ws_orjson.loads(raw) if isinstance(raw, (str, bytes)) else raw
                    msg = _ws_json({"stream": "synapse", "payload": payload})
                    try:
                        queue.put_nowait(msg)
                    except _ws_asyncio.QueueFull:
                        pass
        except _ws_asyncio.CancelledError:
            pass
        finally:
            await pubsub.unsubscribe(channel)
            await pubsub.aclose()

    async def _sender() -> None:
        """Drain the queue and send to the WebSocket client."""
        while running:
            msg = await queue.get()
            await ws.send_text(msg)

    poller_task = _ws_asyncio.create_task(_affect_poller())
    subscriber_task = _ws_asyncio.create_task(_redis_subscriber())
    sender_task = _ws_asyncio.create_task(_sender())

    try:
        # Keep alive — we don't expect client messages
        while True:
            await ws.receive_text()
    except WebSocketDisconnect:
        pass
    finally:
        running = False
        poller_task.cancel()
        subscriber_task.cancel()
        sender_task.cancel()


# ─── API Key Authentication Middleware ─────────────────────────────
# Protects all /api/v1/* endpoints. /health is always public.
# When no API keys are configured (dev mode), all requests pass through.

from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import JSONResponse


class APIKeyMiddleware(BaseHTTPMiddleware):
    """
    Validates API key from X-EOS-API-Key header or Authorization Bearer token.

    Protected paths: /api/v1/*
    Public paths: /health, /docs, /openapi.json, /api/v1/federation/identity
    """

    async def dispatch(self, request: Request, call_next):
        path = request.url.path

        # Public endpoints — no auth required
        if path in ("/health", "/docs", "/openapi.json", "/redoc", "/api/v1/admin/llm/metrics", "/api/v1/admin/llm/summary"):
            return await call_next(request)
        # Federation identity is public (used during link establishment)
        if path == "/api/v1/federation/identity":
            return await call_next(request)

        # Only protect /api/v1/* paths
        if not path.startswith("/api/v1/"):
            return await call_next(request)

        # Check if auth is configured
        config = getattr(request.app.state, "config", None)
        if config is None or not config.server.api_keys:
            # Dev mode: no keys configured, allow all
            return await call_next(request)

        # Extract API key from header or Authorization bearer
        api_key = request.headers.get(config.server.api_key_header, "")
        if not api_key:
            auth_header = request.headers.get("authorization", "")
            if auth_header.startswith("Bearer "):
                api_key = auth_header[7:]

        if not api_key or api_key not in config.server.api_keys:
            return JSONResponse(
                status_code=401,
                content={"error": "Invalid or missing API key"},
            )

        return await call_next(request)


app.add_middleware(APIKeyMiddleware)


# ─── Health & Status Endpoints ────────────────────────────────────


@app.get("/health")
async def health():
    """System health check."""
    memory_health = await app.state.memory.health()
    equor_health = await app.state.equor.health()
    voxis_health = await app.state.voxis.health()
    nova_health = await app.state.nova.health()
    synapse_health = await app.state.synapse.health() if hasattr(app.state, "synapse") else {"status": "not_initialized"}
    thymos_health = await app.state.thymos.health() if hasattr(app.state, "thymos") else {"status": "not_initialized"}
    oneiros_health = await app.state.oneiros.health() if hasattr(app.state, "oneiros") else {"status": "not_initialized"}
    thread_health = await app.state.thread.health() if hasattr(app.state, "thread") else {"status": "not_initialized"}
    neo4j_health = await app.state.neo4j.health_check()
    tsdb_health = await app.state.tsdb.health_check()
    redis_health = await app.state.redis.health_check()

    overall = "healthy"
    if any(
        h.get("status") != "connected"
        for h in [neo4j_health, tsdb_health, redis_health]
    ):
        overall = "degraded"
    if equor_health.get("safe_mode"):
        overall = "degraded"
    if synapse_health.get("safe_mode"):
        overall = "safe_mode"

    instance = await app.state.memory.get_self()
    atune: AtuneService = app.state.atune

    federation_health = await app.state.federation.health() if hasattr(app.state, "federation") else {"status": "not_initialized"}

    return {
        "status": overall,
        "instance_id": app.state.config.instance_id,
        "instance_name": instance.name if instance else "unborn",
        "phase": "13_oneiros",
        "systems": {
            "memory": memory_health,
            "equor": equor_health,
            "nova": nova_health,
            "axon": app.state.axon.stats if hasattr(app.state, "axon") else {"status": "not_initialized"},
            "evo": app.state.evo.stats if hasattr(app.state, "evo") else {"status": "not_initialized"},
            "simula": app.state.simula.stats if hasattr(app.state, "simula") else {"status": "not_initialized"},
            "atune": {
                "status": "running",
                "cycle_count": atune.cycle_count,
                "workspace_threshold": round(atune.workspace_threshold, 4),
                "meta_attention_mode": atune.meta_attention_mode,
                "affect": {
                    "valence": round(atune.current_affect.valence, 4),
                    "arousal": round(atune.current_affect.arousal, 4),
                    "curiosity": round(atune.current_affect.curiosity, 4),
                    "care_activation": round(atune.current_affect.care_activation, 4),
                    "coherence_stress": round(atune.current_affect.coherence_stress, 4),
                },
            },
            "voxis": voxis_health,
            "synapse": synapse_health,
            "thymos": thymos_health,
            "oneiros": oneiros_health,
            "thread": thread_health,
            "federation": federation_health,
        },
        "data_stores": {
            "neo4j": neo4j_health,
            "timescaledb": tsdb_health,
            "redis": redis_health,
        },
    }


@app.get("/api/v1/admin/llm/metrics")
async def get_llm_metrics():
    """
    LLM cost optimization dashboard.

    Returns token spend, cache hit rate, budget tier, latency,
    and per-system breakdowns.
    """
    llm_metrics: LLMMetricsCollector = app.state.llm_metrics
    token_budget: TokenBudget = app.state.token_budget
    llm: OptimizedLLMProvider = app.state.llm

    dashboard = llm_metrics.get_dashboard_data()
    budget_status = token_budget.get_status()
    cache_stats = llm.cache.get_stats() if llm.cache else {"hit_rate": 0.0}

    return {
        "status": "ok",
        "budget": {
            "tier": budget_status.tier.value,
            "tokens_used": budget_status.tokens_used,
            "tokens_remaining": budget_status.tokens_remaining,
            "calls_made": budget_status.calls_made,
            "calls_remaining": budget_status.calls_remaining,
            "burn_rate_tokens_per_sec": round(budget_status.tokens_per_sec, 2),
            "hours_until_exhausted": round(budget_status.hours_until_exhausted, 2),
            "warning": budget_status.warning_message,
        },
        "cache": cache_stats,
        "dashboard": dashboard,
    }


@app.get("/api/v1/admin/llm/summary")
async def get_llm_summary():
    """Human-readable LLM cost summary."""
    llm_metrics: LLMMetricsCollector = app.state.llm_metrics
    return {"summary": llm_metrics.summary()}


@app.get("/api/v1/admin/instance")
async def get_instance():
    """Get instance information."""
    instance = await app.state.memory.get_self()
    if instance is None:
        return {"status": "unborn", "message": "No instance has been born yet."}
    return instance.model_dump()


@app.get("/api/v1/admin/memory/stats")
async def get_memory_stats():
    """Get memory graph statistics."""
    return await app.state.memory.stats()


@app.get("/api/v1/governance/constitution")
async def get_constitution():
    """View the current constitution."""
    constitution = await app.state.memory.get_constitution()
    if constitution is None:
        return {"status": "not_found"}
    return constitution


@app.get("/api/v1/admin/health")
async def full_health():
    """Alias for /health with full detail."""
    return await health()


# ─── Phase 3: Perception via Atune ───────────────────────────────


@app.post("/api/v1/perceive/event")
async def perceive_event(body: dict):
    """
    Ingest a percept through Atune's full perception pipeline.

    The input is normalised, scored by seven salience heads,
    precision-weighted by the current affect state, and if it passes
    the workspace ignition threshold, broadcast to all systems.

    Body: {text, channel?, metadata?}
    """
    text = body.get("text", body.get("content", ""))
    if not text:
        return {"error": "No text/content provided"}

    channel_str = body.get("channel", "text_chat")
    try:
        channel = InputChannel(channel_str)
    except ValueError:
        channel = InputChannel.TEXT_CHAT

    raw = RawInput(
        data=text,
        channel_id=body.get("channel_id", ""),
        metadata=body.get("metadata", {}),
    )

    # Ingest through Atune (normalise → predict → score → enqueue)
    atune: AtuneService = app.state.atune
    percept_id = await atune.ingest(raw, channel)

    if percept_id is None:
        return {"percept_id": None, "accepted": False, "reason": "queue_full"}

    # Run a workspace cycle immediately (until Synapse drives the clock)
    broadcast = await atune.run_cycle()

    return {
        "percept_id": percept_id,
        "accepted": True,
        "broadcast": broadcast.broadcast_id if broadcast else None,
        "salience_threshold": round(atune.workspace_threshold, 4),
        "affect": {
            "valence": round(atune.current_affect.valence, 4),
            "arousal": round(atune.current_affect.arousal, 4),
            "curiosity": round(atune.current_affect.curiosity, 4),
            "care_activation": round(atune.current_affect.care_activation, 4),
            "coherence_stress": round(atune.current_affect.coherence_stress, 4),
        },
    }


@app.get("/api/v1/atune/affect")
async def get_affect_state():
    """Get Atune's current affective state."""
    affect = app.state.atune.current_affect
    return {
        "valence": round(affect.valence, 4),
        "arousal": round(affect.arousal, 4),
        "dominance": round(affect.dominance, 4),
        "curiosity": round(affect.curiosity, 4),
        "care_activation": round(affect.care_activation, 4),
        "coherence_stress": round(affect.coherence_stress, 4),
        "timestamp": affect.timestamp.isoformat() if affect.timestamp else None,
    }


@app.get("/api/v1/atune/workspace")
async def get_workspace_state():
    """Get workspace state — threshold, recent broadcasts, meta-attention mode."""
    atune: AtuneService = app.state.atune
    recent = atune.recent_broadcasts
    return {
        "cycle_count": atune.cycle_count,
        "dynamic_threshold": round(atune.workspace_threshold, 4),
        "meta_attention_mode": atune.meta_attention_mode,
        "recent_broadcasts": [
            {
                "broadcast_id": b.broadcast_id,
                "salience": round(b.salience.composite, 4),
                "timestamp": b.timestamp.isoformat() if b.timestamp else None,
            }
            for b in recent[-10:]
        ],
    }


# ─── Phase 1: Memory Test Endpoints (kept for backwards compat) ──


@app.post("/api/v1/memory/retrieve")
async def retrieve_memory(body: dict):
    """
    Query memory (temporary test endpoint).
    In later phases, retrieval is triggered by the cognitive cycle.
    """
    query = body.get("query", "")
    if not query:
        return {"error": "No query provided"}

    response = await app.state.memory.retrieve(
        query_text=query,
        max_results=body.get("max_results", 10),
    )
    return response.model_dump()


# ─── Phase 2: Equor Endpoints ────────────────────────────────────


@app.post("/api/v1/equor/review")
async def review_intent(body: dict):
    """
    Submit an Intent for constitutional review (test endpoint).
    In later phases, Nova calls this automatically.

    Body: {goal, steps?, reasoning?, alternatives?, domain?, expected_free_energy?}
    """
    from ecodiaos.primitives.intent import (
        Intent, GoalDescriptor, ActionSequence, Action, DecisionTrace,
    )

    goal_text = body.get("goal", "")
    if not goal_text:
        return {"error": "No goal provided"}

    steps = []
    for s in body.get("steps", []):
        steps.append(Action(
            executor=s.get("executor", ""),
            parameters=s.get("parameters", {}),
        ))

    intent = Intent(
        goal=GoalDescriptor(
            description=goal_text,
            target_domain=body.get("domain", ""),
        ),
        plan=ActionSequence(steps=steps),
        expected_free_energy=body.get("expected_free_energy", 0.0),
        decision_trace=DecisionTrace(
            reasoning=body.get("reasoning", ""),
            alternatives_considered=body.get("alternatives", []),
        ),
    )

    check = await app.state.equor.review(intent)
    return check.model_dump()


@app.get("/api/v1/equor/invariants")
async def get_invariants():
    """List all active invariants (hardcoded + community)."""
    return await app.state.equor.get_invariants()


@app.get("/api/v1/equor/drift")
async def get_drift():
    """Get the current drift report."""
    return await app.state.equor.get_drift_report()


@app.get("/api/v1/equor/autonomy")
async def get_autonomy():
    """Get the current autonomy level and promotion eligibility."""
    level = await app.state.equor.get_autonomy_level()
    next_level = level + 1 if level < 3 else None
    eligibility = None
    if next_level:
        eligibility = await app.state.equor.check_promotion(next_level)
    return {
        "current_level": level,
        "level_name": {1: "Advisor", 2: "Partner", 3: "Steward"}.get(level, "unknown"),
        "promotion_eligibility": eligibility,
    }


@app.get("/api/v1/governance/history")
async def governance_history():
    """View governance event history."""
    return await app.state.equor.get_governance_history()


@app.get("/api/v1/governance/reviews")
async def recent_reviews():
    """View recent constitutional reviews."""
    return await app.state.equor.get_recent_reviews()


@app.post("/api/v1/governance/amendments")
async def propose_amendment_endpoint(body: dict):
    """
    Propose a constitutional amendment.
    Body: {proposed_drives: {coherence, care, growth, honesty}, title, description, proposer_id}
    """
    required = ["proposed_drives", "title", "description", "proposer_id"]
    for field in required:
        if field not in body:
            return {"error": f"Missing required field: {field}"}

    return await app.state.equor.propose_amendment(
        proposed_drives=body["proposed_drives"],
        title=body["title"],
        description=body["description"],
        proposer_id=body["proposer_id"],
    )


# ─── Phase 4: Chat & Expression via Voxis ────────────────────────


@app.post("/api/v1/chat/message")
async def chat_message(body: dict):
    """
    Send a message to EOS and receive an expression in response.

    The full Voxis pipeline runs: silence check → policy selection via EFE
    → personality + affect colouring → audience adaptation → LLM generation
    → honesty check → response.

    Body: {message, conversation_id?, speaker_id?, speaker_name?}
    Returns: {expression_id, content, is_silence, silence_reason?, conversation_id,
              policy_class, efe_score, affect_snapshot, generation_trace?}
    """
    message = body.get("message", body.get("content", ""))
    if not message:
        return {"error": "No message provided"}

    voxis: VoxisService = app.state.voxis
    atune: AtuneService = app.state.atune
    current_affect = atune.current_affect

    conversation_id = body.get("conversation_id")
    speaker_id = body.get("speaker_id")
    speaker_name = body.get("speaker_name")

    try:
        # Record user message into conversation state first
        conversation_id = await voxis.ingest_user_message(
            message=message,
            conversation_id=conversation_id,
            speaker_id=speaker_id,
        )

        # Also feed through Atune (updates affect, workspace state)
        try:
            raw = RawInput(data=message, channel_id=conversation_id or "", metadata={})
            percept_id = await atune.ingest(raw, InputChannel.TEXT_CHAT)
            if percept_id:
                await atune.run_cycle()
        except Exception as atune_err:
            # Atune ingestion is non-critical for chat — log and continue
            _chat_logger.warning("chat_atune_ingest_failed", error=str(atune_err))

        # Generate expression via Voxis
        expression = await voxis.express(
            content=message,
            trigger=ExpressionTrigger.NOVA_RESPOND,
            conversation_id=conversation_id,
            addressee_id=speaker_id,
            addressee_name=speaker_name,
            affect=current_affect,
            urgency=0.6,
        )
    except Exception as exc:
        _chat_logger.error("chat_expression_failed", error=str(exc), exc_info=True)
        from fastapi.responses import JSONResponse
        return JSONResponse(
            status_code=500,
            content={
                "error": "expression_pipeline_failed",
                "detail": str(exc),
                "stage": "voxis_express",
            },
        )

    # Include Thread identity context in response (P2.9)
    identity_context = ""
    if hasattr(app.state, "thread"):
        try:
            identity_context = app.state.thread.get_identity_context()
        except Exception:
            pass

    response: dict = {
        "expression_id": expression.id,
        "conversation_id": expression.conversation_id,
        "content": expression.content,
        "is_silence": expression.is_silence,
        "identity_context": identity_context,
    }

    if expression.is_silence:
        response["silence_reason"] = expression.silence_reason
    else:
        response["channel"] = expression.channel
        response["affect_snapshot"] = {
            "valence": round(expression.affect_valence, 4),
            "arousal": round(expression.affect_arousal, 4),
            "curiosity": round(expression.affect_curiosity, 4),
            "care_activation": round(expression.affect_care_activation, 4),
            "coherence_stress": round(expression.affect_coherence_stress, 4),
        }
        if expression.generation_trace:
            response["generation"] = {
                "model": expression.generation_trace.model,
                "temperature": expression.generation_trace.temperature,
                "latency_ms": expression.generation_trace.latency_ms,
                "honesty_check_passed": expression.generation_trace.honesty_check_passed,
            }

    return response


@app.get("/api/v1/voxis/personality")
async def get_voxis_personality():
    """Get the current personality vector and its dimensions."""
    voxis: VoxisService = app.state.voxis
    p = voxis.current_personality
    return {
        "warmth": p.warmth,
        "directness": p.directness,
        "verbosity": p.verbosity,
        "formality": p.formality,
        "curiosity_expression": p.curiosity_expression,
        "humour": p.humour,
        "empathy_expression": p.empathy_expression,
        "confidence_display": p.confidence_display,
        "metaphor_use": p.metaphor_use,
        "vocabulary_affinities": p.vocabulary_affinities,
        "thematic_references": p.thematic_references,
    }


@app.get("/api/v1/voxis/health")
async def get_voxis_health():
    """Voxis system health and observability metrics."""
    return await app.state.voxis.health()


# ─── Nova Endpoints ───────────────────────────────────────────────


@app.get("/api/v1/nova/health")
async def get_nova_health():
    """Nova decision system health and observability metrics."""
    return await app.state.nova.health()


@app.get("/api/v1/nova/goals")
async def get_nova_goals():
    """Active goals in Nova's goal manager."""
    nova: NovaService = app.state.nova
    goals = nova._goal_manager.active_goals if nova._goal_manager else []
    return {
        "active_goals": [
            {
                "id": g.id,
                "description": g.description,
                "source": g.source.value,
                "priority": round(g.priority, 4),
                "urgency": round(g.urgency, 4),
                "progress": round(g.progress, 4),
                "status": g.status.value,
            }
            for g in goals
        ],
        "total_active": len(goals),
    }


# ─── Phase 7: Evo Endpoints ───────────────────────────────────────


@app.get("/api/v1/evo/stats")
async def get_evo_stats():
    """Evo learning system stats — hypotheses, parameters, consolidation."""
    evo: EvoService = app.state.evo
    return evo.stats


@app.post("/api/v1/evo/consolidate")
async def trigger_consolidation():
    """Manually trigger an Evo consolidation cycle (sleep mode)."""
    evo: EvoService = app.state.evo
    result = await evo.run_consolidation()
    if result is None:
        return {"status": "skipped", "reason": "Already running or not initialized"}
    return result.model_dump()


@app.get("/api/v1/evo/parameters")
async def get_evo_parameters():
    """Get all current Evo-tuned parameter values."""
    evo: EvoService = app.state.evo
    return evo.get_all_parameters()


# ─── Thread: Narrative Identity Endpoints ──────────────────────────


@app.get("/api/v1/thread/who-am-i")
async def thread_who_am_i():
    """The organism's current identity summary — who it thinks it is."""
    thread: ThreadService = app.state.thread
    return thread.who_am_i()


@app.get("/api/v1/thread/health")
async def thread_health_endpoint():
    """Thread system health and identity metrics."""
    thread: ThreadService = app.state.thread
    return await thread.health()


@app.get("/api/v1/thread/commitments")
async def thread_commitments():
    """All identity commitments — constitutional and emergent."""
    thread: ThreadService = app.state.thread
    return [
        {
            "id": c.id,
            "type": c.type.value,
            "statement": c.statement,
            "strength": c.strength.value,
            "drive_source": c.drive_source,
            "fidelity": round(c.fidelity, 3),
            "test_count": c.test_count,
            "created_at": c.created_at.isoformat(),
        }
        for c in thread._commitments
    ]


@app.get("/api/v1/thread/schemas")
async def thread_schemas():
    """All identity schemas — the organism's self-understanding."""
    thread: ThreadService = app.state.thread
    return [
        {
            "id": s.id,
            "statement": s.statement,
            "status": s.status.value,
            "confidence": round(s.confidence, 3),
            "evidence_ratio": round(s.evidence_ratio, 3),
            "created_at": s.created_at.isoformat(),
        }
        for s in thread._schemas
    ]


@app.get("/api/v1/thread/fingerprints")
async def thread_fingerprints():
    """Recent identity fingerprints (29D snapshots over time)."""
    thread: ThreadService = app.state.thread
    return [
        {
            "id": fp.id,
            "cycle_number": fp.cycle_number,
            "personality": fp.personality,
            "drive_alignment": fp.drive_alignment,
            "affect": fp.affect,
            "goal_profile": fp.goal_profile,
            "interaction_profile": fp.interaction_profile,
            "created_at": fp.created_at.isoformat(),
        }
        for fp in thread._fingerprints[-50:]  # Last 50
    ]


@app.get("/api/v1/thread/chapters")
async def thread_chapters():
    """Narrative chapters — the organism's life phases."""
    thread: ThreadService = app.state.thread
    return [
        {
            "id": ch.id,
            "title": ch.title,
            "theme": ch.theme,
            "status": ch.status.value,
            "opened_at_cycle": ch.opened_at_cycle,
            "closed_at_cycle": ch.closed_at_cycle,
            "summary": ch.summary,
            "created_at": ch.created_at.isoformat(),
        }
        for ch in thread._chapters
    ]


@app.get("/api/v1/thread/past-self")
async def thread_past_self(cycle: int = 0):
    """
    View the organism's identity at a past point in time.
    Query param: cycle=N (0 = earliest available).
    """
    thread: ThreadService = app.state.thread
    return thread.get_past_self(cycle_reference=cycle)


@app.get("/api/v1/thread/life-story")
async def thread_life_story():
    """The organism's latest autobiographical synthesis."""
    thread: ThreadService = app.state.thread
    if thread._life_story is None:
        return {"status": "not_yet_synthesised", "message": "Life story not yet generated"}
    return thread._life_story.model_dump()


@app.get("/api/v1/thread/conflicts")
async def thread_conflicts():
    """Detected schema conflicts — contradictions in self-understanding."""
    thread: ThreadService = app.state.thread
    return [
        {
            "id": c.id,
            "schema_a_statement": c.schema_a_statement,
            "schema_b_statement": c.schema_b_statement,
            "cosine_similarity": c.cosine_similarity,
            "resolved": c.resolved,
            "created_at": c.created_at.isoformat(),
        }
        for c in thread._conflicts
    ]


@app.get("/api/v1/thread/identity-context")
async def thread_identity_context():
    """Brief identity context string (injected into Voxis expressions)."""
    thread: ThreadService = app.state.thread
    return {"context": thread.get_identity_context()}


# ─── Phase 8: Simula Endpoints ────────────────────────────────────


@app.get("/api/v1/simula/stats")
async def get_simula_stats():
    """Simula self-evolution system stats."""
    simula: SimulaService = app.state.simula
    return simula.stats


@app.post("/api/v1/simula/proposals")
async def submit_evolution_proposal(body: dict):
    """
    Submit an evolution proposal to Simula.

    Body: {
      source: "evo" | "governance",
      category: ChangeCategory value,
      description: str,
      change_spec: {... see ChangeSpec fields},
      evidence: [list of hypothesis/episode IDs],
      expected_benefit: str,
      risk_assessment: str
    }
    """
    from ecodiaos.systems.simula.types import (
        ChangeCategory, ChangeSpec, EvolutionProposal,
    )

    try:
        category = ChangeCategory(body.get("category", ""))
    except ValueError:
        return {"error": f"Unknown category: {body.get('category')}"}

    spec_data = body.get("change_spec", {})
    try:
        change_spec = ChangeSpec(**spec_data)
    except Exception as exc:
        return {"error": f"Invalid change_spec: {exc}"}

    proposal = EvolutionProposal(
        source=body.get("source", "governance"),
        category=category,
        description=body.get("description", ""),
        change_spec=change_spec,
        evidence=body.get("evidence", []),
        expected_benefit=body.get("expected_benefit", ""),
        risk_assessment=body.get("risk_assessment", ""),
    )

    simula: SimulaService = app.state.simula
    result = await simula.process_proposal(proposal)
    return {
        "proposal_id": proposal.id,
        "result": result.model_dump(),
    }


@app.get("/api/v1/simula/history")
async def get_evolution_history(limit: int = 50):
    """Get the evolution history — all structural changes applied."""
    simula: SimulaService = app.state.simula
    records = await simula.get_history(limit=limit)
    return {
        "records": [r.model_dump() for r in records],
        "current_version": await simula.get_current_version(),
    }


@app.get("/api/v1/simula/version")
async def get_simula_version():
    """Get the current config version and version chain."""
    simula: SimulaService = app.state.simula
    chain = await simula.get_version_chain()
    return {
        "current_version": await simula.get_current_version(),
        "version_chain": [v.model_dump() for v in chain],
    }


@app.post("/api/v1/simula/proposals/{proposal_id}/approve")
async def approve_governed_proposal(proposal_id: str, body: dict):
    """
    Approve a governed proposal after community governance.
    Body: {governance_record_id: str}
    """
    governance_record_id = body.get("governance_record_id", "")
    if not governance_record_id:
        return {"error": "governance_record_id required"}

    simula: SimulaService = app.state.simula
    result = await simula.approve_governed_proposal(proposal_id, governance_record_id)
    return result.model_dump()


@app.get("/api/v1/simula/proposals")
async def get_active_proposals():
    """Get all proposals currently active in the Simula pipeline."""
    simula: SimulaService = app.state.simula
    proposals = simula.get_active_proposals()
    return {
        "proposals": [
            {
                "id": p.id,
                "category": p.category.value,
                "description": p.description,
                "status": p.status.value,
                "source": p.source,
                "created_at": p.created_at.isoformat(),
            }
            for p in proposals
        ],
        "total": len(proposals),
    }


@app.get("/api/v1/nova/beliefs")
async def get_nova_beliefs():
    """Nova's current belief state summary."""
    nova: NovaService = app.state.nova
    beliefs = nova.beliefs
    return {
        "overall_confidence": round(beliefs.overall_confidence, 4),
        "free_energy": round(beliefs.free_energy, 4),
        "entity_count": len(beliefs.entities),
        "individual_count": len(beliefs.individual_beliefs),
        "context": {
            "summary": beliefs.current_context.summary[:200],
            "domain": beliefs.current_context.domain,
            "is_active_dialogue": beliefs.current_context.is_active_dialogue,
            "confidence": round(beliefs.current_context.confidence, 4),
        },
        "self_belief": {
            "epistemic_confidence": round(beliefs.self_belief.epistemic_confidence, 4),
            "cognitive_load": round(beliefs.self_belief.cognitive_load, 4),
        },
        "last_updated": beliefs.last_updated.isoformat(),
    }


# ─── Phase 9: Synapse Endpoints ──────────────────────────────────


@app.get("/api/v1/admin/synapse/cycle")
async def get_synapse_cycle():
    """
    Synapse cognitive cycle telemetry.

    Returns cycle count, period, latency, jitter, rhythm state,
    and coherence — the pulse of the organism.
    """
    synapse: SynapseService = app.state.synapse
    clock = synapse.clock_state
    rhythm = synapse.rhythm_snapshot
    coherence = synapse.coherence_snapshot

    return {
        "cycle_count": clock.cycle_count,
        "current_period_ms": round(clock.current_period_ms, 2),
        "target_period_ms": round(clock.target_period_ms, 2),
        "actual_rate_hz": round(clock.actual_rate_hz, 2),
        "jitter_ms": round(clock.jitter_ms, 2),
        "arousal": round(clock.arousal, 4),
        "overrun_count": clock.overrun_count,
        "running": clock.running,
        "paused": clock.paused,
        "rhythm": {
            "state": rhythm.state.value,
            "confidence": rhythm.confidence,
            "broadcast_density": rhythm.broadcast_density,
            "salience_trend": rhythm.salience_trend,
            "salience_mean": rhythm.salience_mean,
            "rhythm_stability": rhythm.rhythm_stability,
            "cycles_in_state": rhythm.cycles_in_state,
        },
        "coherence": {
            "composite": coherence.composite,
            "phi": coherence.phi_approximation,
            "resonance": coherence.system_resonance,
            "diversity": coherence.broadcast_diversity,
            "synchrony": coherence.response_synchrony,
        },
    }


@app.get("/api/v1/admin/synapse/budget")
async def get_synapse_budget():
    """Resource utilisation and budget allocation."""
    synapse: SynapseService = app.state.synapse
    return synapse.stats.get("resources", {})


@app.post("/api/v1/admin/synapse/safe-mode")
async def toggle_safe_mode(body: dict):
    """
    Manually toggle safe mode.

    Body: {enabled: bool, reason?: str}
    """
    enabled = body.get("enabled", False)
    reason = body.get("reason", "")
    synapse: SynapseService = app.state.synapse
    await synapse.set_safe_mode(enabled, reason)
    return {
        "safe_mode": synapse.is_safe_mode,
        "reason": reason if enabled else "",
    }


@app.get("/api/v1/admin/synapse/stats")
async def get_synapse_stats():
    """Full Synapse system statistics."""
    synapse: SynapseService = app.state.synapse
    return synapse.stats


# ─── Phase 12: Thymos (Immune System) Endpoints ──────────────────


@app.get("/api/v1/thymos/health")
async def get_thymos_health():
    """Thymos immune system health and observability metrics."""
    thymos: ThymosService = app.state.thymos
    return await thymos.health()


@app.get("/api/v1/thymos/incidents")
async def get_thymos_incidents(limit: int = 50):
    """Recent incidents from the immune system."""
    thymos: ThymosService = app.state.thymos
    incidents = list(thymos._incident_buffer)[-limit:]
    return [
        {
            "id": i.id,
            "timestamp": i.timestamp.isoformat(),
            "source_system": i.source_system,
            "incident_class": i.incident_class.value,
            "severity": i.severity.value,
            "error_type": i.error_type,
            "error_message": i.error_message[:200],
            "repair_status": i.repair_status.value,
            "repair_tier": i.repair_tier.name if i.repair_tier else None,
            "repair_successful": i.repair_successful,
            "resolution_time_ms": i.resolution_time_ms,
            "root_cause": i.root_cause_hypothesis,
            "antibody_id": i.antibody_id,
        }
        for i in reversed(incidents)
    ]


@app.get("/api/v1/thymos/antibodies")
async def get_thymos_antibodies():
    """All active antibodies in the immune memory."""
    thymos: ThymosService = app.state.thymos
    if thymos._antibody_library is None:
        return []
    return [
        {
            "id": a.id,
            "fingerprint": a.fingerprint,
            "source_system": a.source_system,
            "incident_class": a.incident_class.value,
            "repair_tier": a.repair_tier.name,
            "effectiveness": round(a.effectiveness, 3),
            "application_count": a.application_count,
            "success_count": a.success_count,
            "failure_count": a.failure_count,
            "root_cause": a.root_cause_description,
            "created_at": a.created_at.isoformat(),
            "last_applied": a.last_applied.isoformat() if a.last_applied else None,
            "retired": a.retired,
            "generation": a.generation,
        }
        for a in thymos._antibody_library._all.values()
    ]


@app.get("/api/v1/thymos/stats")
async def get_thymos_stats():
    """Thymos aggregate stats for monitoring."""
    thymos: ThymosService = app.state.thymos
    return thymos.stats


@app.get("/api/v1/thymos/repairs")
async def get_thymos_repairs(limit: int = 50):
    """Recent repairs and their outcomes."""
    thymos: ThymosService = app.state.thymos
    incidents = list(thymos._incident_buffer)[-limit:]
    repairs = [
        {
            "incident_id": i.id,
            "timestamp": i.timestamp.isoformat(),
            "source_system": i.source_system,
            "repair_tier": i.repair_tier.name if i.repair_tier else None,
            "repair_status": i.repair_status.value,
            "repair_successful": i.repair_successful,
            "resolution_time_ms": i.resolution_time_ms,
            "incident_class": i.incident_class.value,
            "severity": i.severity.value,
            "antibody_id": i.antibody_id,
        }
        for i in reversed(incidents)
        if i.repair_tier is not None
    ]
    return repairs


@app.get("/api/v1/thymos/homeostasis")
async def get_thymos_homeostasis():
    """Current homeostasis metrics status."""
    thymos: ThymosService = app.state.thymos
    health_data = await thymos.health()
    return {
        "metrics_in_range": health_data.get("metrics_in_range", 0),
        "homeostatic_adjustments": health_data.get("homeostatic_adjustments", 0),
        "healing_mode": health_data.get("healing_mode", "normal"),
        "storm_activations": health_data.get("storm_activations", 0),
    }


# ─── Oneiros (Dream Engine) ───────────────────────────────────────


@app.get("/api/v1/oneiros/health")
async def get_oneiros_health():
    oneiros: OneirosService = app.state.oneiros
    return await oneiros.health()


@app.get("/api/v1/oneiros/stats")
async def get_oneiros_stats():
    oneiros: OneirosService = app.state.oneiros
    return oneiros.stats


@app.get("/api/v1/oneiros/dreams")
async def get_oneiros_dreams(limit: int = 50):
    oneiros: OneirosService = app.state.oneiros
    dreams = list(oneiros._journal._dream_buffer)[-limit:]
    return [
        {
            "id": d.id,
            "dream_type": d.dream_type.value,
            "coherence_score": round(d.coherence_score, 3),
            "coherence_class": d.coherence_class.value,
            "bridge_narrative": d.bridge_narrative,
            "affect_valence": round(d.affect_valence, 3),
            "affect_arousal": round(d.affect_arousal, 3),
            "themes": d.themes,
            "summary": d.summary,
            "timestamp": d.timestamp.isoformat(),
        }
        for d in reversed(dreams)
    ]


@app.get("/api/v1/oneiros/insights")
async def get_oneiros_insights(limit: int = 50):
    oneiros: OneirosService = app.state.oneiros
    all_insights = list(oneiros._journal._all_insights.values())
    sorted_insights = sorted(all_insights, key=lambda i: i.created_at, reverse=True)[:limit]
    return [
        {
            "id": i.id,
            "insight_text": i.insight_text,
            "coherence_score": round(i.coherence_score, 3),
            "domain": i.domain,
            "status": i.status.value,
            "wake_applications": i.wake_applications,
            "created_at": i.created_at.isoformat(),
        }
        for i in sorted_insights
    ]


@app.get("/api/v1/oneiros/sleep-cycles")
async def get_oneiros_sleep_cycles(limit: int = 20):
    oneiros: OneirosService = app.state.oneiros
    cycles = list(oneiros._recent_cycles)[-limit:]
    return [
        {
            "id": c.id,
            "started_at": c.started_at.isoformat(),
            "completed_at": c.completed_at.isoformat() if c.completed_at else None,
            "quality": c.quality.value,
            "episodes_replayed": c.episodes_replayed,
            "dreams_generated": c.dreams_generated,
            "insights_discovered": c.insights_discovered,
            "pressure_before": round(c.pressure_before, 3),
            "pressure_after": round(c.pressure_after, 3),
        }
        for c in reversed(cycles)
    ]


# ─── Phase 11: Federation Endpoints ─────────────────────────────


@app.get("/api/v1/federation/identity")
async def get_federation_identity():
    """
    This instance's public identity card.

    Used by other instances during link establishment to verify
    identity and check compatibility.
    """
    federation: FederationService = app.state.federation
    card = federation.identity_card
    if card is None:
        return {"status": "disabled", "message": "Federation is not enabled"}
    return card.model_dump(mode="json")


@app.get("/api/v1/federation/links")
async def get_federation_links():
    """List all federation links with trust levels and status."""
    federation: FederationService = app.state.federation
    links = federation.active_links
    return {
        "links": [
            {
                "id": l.id,
                "remote_instance_id": l.remote_instance_id,
                "remote_name": l.remote_name,
                "remote_endpoint": l.remote_endpoint,
                "trust_level": l.trust_level.name,
                "trust_score": round(l.trust_score, 2),
                "status": l.status.value,
                "established_at": l.established_at.isoformat(),
                "last_communication": l.last_communication.isoformat()
                if l.last_communication else None,
                "shared_knowledge_count": l.shared_knowledge_count,
                "received_knowledge_count": l.received_knowledge_count,
                "successful_interactions": l.successful_interactions,
                "failed_interactions": l.failed_interactions,
            }
            for l in links
        ],
        "total_active": len(links),
    }


@app.post("/api/v1/federation/links")
async def establish_federation_link(body: dict):
    """
    Establish a new federation link with a remote instance.

    Body: {endpoint: str}

    The full link establishment protocol runs:
      1. Fetch remote identity card
      2. Verify identity (Ed25519 + certificate fingerprint)
      3. Equor constitutional review
      4. Create link at NONE trust
      5. Open mTLS channel

    Performance target: ≤3000ms
    """
    endpoint = body.get("endpoint", "")
    if not endpoint:
        return {"error": "No endpoint provided"}

    federation: FederationService = app.state.federation
    return await federation.establish_link(endpoint)


@app.delete("/api/v1/federation/links/{link_id}")
async def withdraw_federation_link(link_id: str):
    """
    Withdraw from a federation link.

    Withdrawal is always free — any instance can disconnect at any
    time with no penalty.
    """
    federation: FederationService = app.state.federation
    return await federation.withdraw_link(link_id)


@app.post("/api/v1/federation/knowledge/request")
async def handle_federation_knowledge_request(body: dict):
    """
    Handle an inbound knowledge request from a federated instance.

    Body: {requesting_instance_id, knowledge_type, query?, domain?, max_results?}

    The full knowledge exchange protocol runs:
      1. Trust level check (is this knowledge type permitted?)
      2. Equor constitutional review
      3. Knowledge retrieval from memory
      4. Privacy filter (PII removal, consent enforcement)
      5. Return filtered knowledge

    Performance target: ≤2000ms
    """
    from ecodiaos.primitives.federation import KnowledgeRequest, KnowledgeType

    try:
        knowledge_type = KnowledgeType(body.get("knowledge_type", ""))
    except ValueError:
        return {"error": f"Unknown knowledge type: {body.get('knowledge_type')}"}

    request = KnowledgeRequest(
        requesting_instance_id=body.get("requesting_instance_id", ""),
        knowledge_type=knowledge_type,
        query=body.get("query", ""),
        domain=body.get("domain", ""),
        max_results=body.get("max_results", 10),
    )

    federation: FederationService = app.state.federation
    response = await federation.handle_knowledge_request(request)
    return response.model_dump(mode="json")


@app.post("/api/v1/federation/knowledge/share")
async def request_knowledge_from_remote(body: dict):
    """
    Request knowledge from a linked remote instance.

    Body: {link_id, knowledge_type, query?, max_results?}
    """
    from ecodiaos.primitives.federation import KnowledgeType

    link_id = body.get("link_id", "")
    if not link_id:
        return {"error": "No link_id provided"}

    try:
        knowledge_type = KnowledgeType(body.get("knowledge_type", ""))
    except ValueError:
        return {"error": f"Unknown knowledge type: {body.get('knowledge_type')}"}

    federation: FederationService = app.state.federation
    response = await federation.request_knowledge(
        link_id=link_id,
        knowledge_type=knowledge_type,
        query=body.get("query", ""),
        max_results=body.get("max_results", 10),
    )

    if response is None:
        return {"error": "Failed to request knowledge (link not found or channel error)"}

    return response.model_dump(mode="json")


@app.post("/api/v1/federation/assistance/request")
async def handle_federation_assistance_request(body: dict):
    """
    Handle an inbound assistance request from a federated instance.

    Body: {requesting_instance_id, description, knowledge_domain?, urgency?, reciprocity_offer?}

    Requires COLLEAGUE trust level or higher.
    """
    from ecodiaos.primitives.federation import AssistanceRequest

    request = AssistanceRequest(
        requesting_instance_id=body.get("requesting_instance_id", ""),
        description=body.get("description", ""),
        knowledge_domain=body.get("knowledge_domain", ""),
        urgency=body.get("urgency", 0.5),
        reciprocity_offer=body.get("reciprocity_offer"),
    )

    federation: FederationService = app.state.federation
    response = await federation.handle_assistance_request(request)
    return response.model_dump(mode="json")


@app.post("/api/v1/federation/assistance/respond")
async def request_assistance_from_remote(body: dict):
    """
    Request assistance from a linked remote instance.

    Body: {link_id, description, knowledge_domain?, urgency?}
    """
    link_id = body.get("link_id", "")
    if not link_id:
        return {"error": "No link_id provided"}

    federation: FederationService = app.state.federation
    response = await federation.request_assistance(
        link_id=link_id,
        description=body.get("description", ""),
        knowledge_domain=body.get("knowledge_domain", ""),
        urgency=body.get("urgency", 0.5),
    )

    if response is None:
        return {"error": "Failed to request assistance (link not found or channel error)"}

    return response.model_dump(mode="json")


@app.get("/api/v1/federation/stats")
async def get_federation_stats():
    """Full federation system statistics."""
    federation: FederationService = app.state.federation
    return federation.stats


@app.get("/api/v1/federation/trust/{link_id}")
async def get_federation_trust(link_id: str):
    """Get trust details for a specific federation link."""
    federation: FederationService = app.state.federation
    link = federation.get_link(link_id)
    if link is None:
        return {"error": "Link not found"}

    from ecodiaos.primitives.federation import SHARING_PERMISSIONS

    return {
        "link_id": link.id,
        "remote_instance_id": link.remote_instance_id,
        "remote_name": link.remote_name,
        "trust_level": link.trust_level.name,
        "trust_score": round(link.trust_score, 2),
        "permitted_knowledge_types": [
            kt.value for kt in SHARING_PERMISSIONS.get(link.trust_level, [])
        ],
        "can_coordinate": link.trust_level.value >= 2,  # COLLEAGUE+
        "successful_interactions": link.successful_interactions,
        "failed_interactions": link.failed_interactions,
        "violation_count": link.violation_count,
    }

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\__init__.py =====

"""
EcodiaOS — External Service Clients

Connection management for Neo4j, TimescaleDB, Redis, LLM, and Embedding.
"""

from ecodiaos.clients.embedding import EmbeddingClient, create_embedding_client
from ecodiaos.clients.llm import LLMProvider, create_llm_provider
from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.clients.redis import RedisClient
from ecodiaos.clients.timescaledb import TimescaleDBClient

__all__ = [
    "Neo4jClient",
    "TimescaleDBClient",
    "RedisClient",
    "LLMProvider",
    "create_llm_provider",
    "EmbeddingClient",
    "create_embedding_client",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\embedding.py =====

"""
EcodiaOS — Embedding Client Abstraction

All embeddings in EOS are 768-dimensional.
Supports local sentence-transformers, API-based, and sidecar models.
"""

from __future__ import annotations

from abc import ABC, abstractmethod

import httpx
import numpy as np
import structlog

from ecodiaos.config import EmbeddingConfig

logger = structlog.get_logger()


class EmbeddingClient(ABC):
    """Abstract interface for text embedding."""

    @abstractmethod
    async def embed(self, text: str) -> list[float]:
        """Embed a single text. Returns a vector of configured dimension."""
        ...

    @abstractmethod
    async def embed_batch(self, texts: list[str]) -> list[list[float]]:
        """Embed a batch of texts."""
        ...

    @abstractmethod
    async def close(self) -> None:
        """Clean up resources."""
        ...


class LocalEmbeddingClient(EmbeddingClient):
    """
    Local embedding using sentence-transformers.
    Loaded in-process. Best for latency and privacy.
    """

    def __init__(self, model_name: str, device: str = "cpu") -> None:
        self._model_name = model_name
        self._device = device
        self._model = None

    def _load_model(self):
        """Lazy-load the model on first use."""
        if self._model is None:
            from sentence_transformers import SentenceTransformer
            self._model = SentenceTransformer(self._model_name, device=self._device)
            logger.info(
                "embedding_model_loaded",
                model=self._model_name,
                device=self._device,
                dimension=self._model.get_sentence_embedding_dimension(),
            )

    async def embed(self, text: str) -> list[float]:
        self._load_model()
        embedding = self._model.encode(text, convert_to_numpy=True)
        return embedding.tolist()

    async def embed_batch(self, texts: list[str]) -> list[list[float]]:
        if not texts:
            return []
        self._load_model()
        embeddings = self._model.encode(texts, convert_to_numpy=True, batch_size=32)
        return embeddings.tolist()

    async def close(self) -> None:
        self._model = None


class SidecarEmbeddingClient(EmbeddingClient):
    """
    Embedding via HTTP sidecar service.
    For when you want the model in a separate process/container.
    """

    def __init__(self, url: str) -> None:
        self._url = url
        self._client = httpx.AsyncClient(timeout=30.0)

    async def embed(self, text: str) -> list[float]:
        response = await self._client.post(
            f"{self._url}/embed",
            json={"text": text},
        )
        response.raise_for_status()
        return response.json()["embedding"]

    async def embed_batch(self, texts: list[str]) -> list[list[float]]:
        response = await self._client.post(
            f"{self._url}/embed_batch",
            json={"texts": texts},
        )
        response.raise_for_status()
        return response.json()["embeddings"]

    async def close(self) -> None:
        await self._client.aclose()


class MockEmbeddingClient(EmbeddingClient):
    """
    Mock embedding client for testing and development.
    Returns random normalised vectors of the correct dimension.
    """

    def __init__(self, dimension: int = 768) -> None:
        self._dimension = dimension

    async def embed(self, text: str) -> list[float]:
        vec = np.random.randn(self._dimension).astype(np.float32)
        vec = vec / np.linalg.norm(vec)
        return vec.tolist()

    async def embed_batch(self, texts: list[str]) -> list[list[float]]:
        results = []
        for _ in texts:
            vec = np.random.randn(self._dimension).astype(np.float32)
            vec = vec / np.linalg.norm(vec)
            results.append(vec.tolist())
        return results

    async def close(self) -> None:
        pass


def create_embedding_client(config: EmbeddingConfig) -> EmbeddingClient:
    """Factory to create the configured embedding client."""
    if config.strategy == "local":
        return LocalEmbeddingClient(
            model_name=config.local_model,
            device=config.local_device,
        )
    elif config.strategy == "sidecar":
        if not config.sidecar_url:
            raise ValueError("Sidecar strategy requires sidecar_url in config")
        return SidecarEmbeddingClient(url=config.sidecar_url)
    elif config.strategy == "mock":
        return MockEmbeddingClient(dimension=config.dimension)
    else:
        raise ValueError(f"Unknown embedding strategy: {config.strategy}")

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\llm.py =====

"""
EcodiaOS — LLM Provider Abstraction

Every system that needs LLM reasoning uses this interface.
Supports Anthropic Claude, OpenAI, and local models via Ollama.

Includes retry with exponential backoff for transient errors (429, 503, 529)
and automatic fallback to a secondary provider when the primary is unavailable.
"""

from __future__ import annotations

import asyncio
import json as _json
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any

import httpx
import structlog

from ecodiaos.config import LLMConfig
from ecodiaos.clients.token_budget import TokenBudget, BudgetTier

logger = structlog.get_logger()

# Retry configuration
_MAX_RETRIES = 3
_BASE_DELAY_S = 1.0
_RETRYABLE_STATUS_CODES = {429, 503, 529}


class Message:
    """A chat message."""

    def __init__(self, role: str, content: str) -> None:
        self.role = role
        self.content = content

    def to_dict(self) -> dict[str, str]:
        return {"role": self.role, "content": self.content}


class LLMResponse:
    """Response from an LLM call."""

    def __init__(
        self,
        text: str,
        model: str = "",
        input_tokens: int = 0,
        output_tokens: int = 0,
        finish_reason: str = "stop",
    ) -> None:
        self.text = text
        self.model = model
        self.input_tokens = input_tokens
        self.output_tokens = output_tokens
        self.finish_reason = finish_reason

    @property
    def total_tokens(self) -> int:
        return self.input_tokens + self.output_tokens


@dataclass
class ToolDefinition:
    """A tool that can be called by the LLM."""

    name: str
    description: str
    input_schema: dict[str, Any]  # JSON Schema object

    def to_dict(self) -> dict[str, Any]:
        return {
            "name": self.name,
            "description": self.description,
            "input_schema": self.input_schema,
        }


@dataclass
class ToolCall:
    """A tool invocation requested by the LLM."""

    id: str
    name: str
    input: dict[str, Any]


@dataclass
class ToolResult:
    """Result of executing a tool call."""

    tool_use_id: str
    content: str
    is_error: bool = False

    def to_anthropic_dict(self) -> dict[str, Any]:
        """Format as Anthropic tool_result content block."""
        return {
            "type": "tool_result",
            "tool_use_id": self.tool_use_id,
            "content": self.content,
            "is_error": self.is_error,
        }


@dataclass
class ToolAwareResponse:
    """Response from a tool-aware LLM call."""

    text: str
    tool_calls: list[ToolCall]
    stop_reason: str  # end_turn | tool_use | max_tokens
    model: str = ""
    input_tokens: int = 0
    output_tokens: int = 0

    @property
    def has_tool_calls(self) -> bool:
        return len(self.tool_calls) > 0

    @property
    def total_tokens(self) -> int:
        return self.input_tokens + self.output_tokens


class LLMProvider(ABC):
    """Abstract interface for LLM calls."""

    @abstractmethod
    async def generate(
        self,
        system_prompt: str,
        messages: list[Message],
        max_tokens: int = 2000,
        temperature: float = 0.7,
        output_format: str | None = None,
    ) -> LLMResponse:
        """Full generation call."""
        ...

    @abstractmethod
    async def evaluate(
        self,
        prompt: str,
        max_tokens: int = 500,
        temperature: float = 0.3,
    ) -> LLMResponse:
        """Short evaluation call (lower temp, smaller output)."""
        ...

    @abstractmethod
    async def generate_with_tools(
        self,
        system_prompt: str,
        messages: list[dict[str, Any]],  # Raw message dicts (supports content blocks)
        tools: list[ToolDefinition],
        max_tokens: int = 4096,
        temperature: float = 0.3,
    ) -> ToolAwareResponse:
        """
        Tool-use generation call. Messages use raw dicts to support
        Anthropic's content block format (text blocks + tool_result blocks).
        Returns ToolAwareResponse with any tool calls the model wants to make.
        """
        ...

    @abstractmethod
    async def close(self) -> None:
        """Clean up resources."""
        ...


class AnthropicProvider(LLMProvider):
    """Claude API provider with retry and exponential backoff."""

    def __init__(
        self,
        api_key: str,
        model: str = "claude-sonnet-4-20250514",
        token_budget: TokenBudget | None = None,
    ) -> None:
        self._model = model
        self._budget = token_budget
        # Strip whitespace/newlines — GCP Secret Manager can inject trailing \r\n
        clean_key = api_key.strip()
        self._client = httpx.AsyncClient(
            base_url="https://api.anthropic.com/v1",
            headers={
                "x-api-key": clean_key,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json",
            },
            timeout=60.0,
        )

    async def _post_with_retry(
        self, path: str, payload: dict[str, Any],
    ) -> dict[str, Any]:
        """POST with exponential backoff on retryable status codes."""
        last_exc: Exception | None = None
        for attempt in range(_MAX_RETRIES + 1):
            try:
                response = await self._client.post(path, json=payload)
                if response.status_code in _RETRYABLE_STATUS_CODES and attempt < _MAX_RETRIES:
                    delay = _BASE_DELAY_S * (2 ** attempt)
                    # Respect Retry-After header if present
                    retry_after = response.headers.get("retry-after")
                    if retry_after:
                        try:
                            delay = max(delay, float(retry_after))
                        except ValueError:
                            pass
                    logger.warning(
                        "llm_retrying",
                        status=response.status_code,
                        attempt=attempt + 1,
                        delay_s=round(delay, 1),
                    )
                    await asyncio.sleep(delay)
                    continue
                response.raise_for_status()
                return response.json()
            except httpx.TimeoutException as exc:
                last_exc = exc
                if attempt < _MAX_RETRIES:
                    delay = _BASE_DELAY_S * (2 ** attempt)
                    logger.warning(
                        "llm_timeout_retrying",
                        attempt=attempt + 1,
                        delay_s=round(delay, 1),
                    )
                    await asyncio.sleep(delay)
                    continue
                raise
            except httpx.HTTPStatusError as exc:
                # Include API error details in the exception message
                body = ""
                try:
                    body = exc.response.text[:500]
                except Exception:
                    pass
                raise httpx.HTTPStatusError(
                    message=f"{exc.response.status_code}: {body}",
                    request=exc.request,
                    response=exc.response,
                ) from exc
        raise last_exc or RuntimeError("LLM request failed after retries")

    async def generate(
        self,
        system_prompt: str,
        messages: list[Message],
        max_tokens: int = 2000,
        temperature: float = 0.7,
        output_format: str | None = None,
    ) -> LLMResponse:
        payload: dict[str, Any] = {
            "model": self._model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "system": system_prompt,
            "messages": [m.to_dict() for m in messages],
        }

        data = await self._post_with_retry("/messages", payload)

        text = ""
        for block in data.get("content", []):
            if block.get("type") == "text":
                text += block.get("text", "")

        input_tokens = data.get("usage", {}).get("input_tokens", 0)
        output_tokens = data.get("usage", {}).get("output_tokens", 0)

        # Charge the token budget if configured
        if self._budget:
            self._budget.charge(
                tokens=input_tokens + output_tokens,
                calls=1,
                system="anthropic_generate",
            )

        return LLMResponse(
            text=text,
            model=data.get("model", self._model),
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            finish_reason=data.get("stop_reason", "stop"),
        )

    async def evaluate(
        self,
        prompt: str,
        max_tokens: int = 500,
        temperature: float = 0.3,
    ) -> LLMResponse:
        return await self.generate(
            system_prompt="You are an evaluator. Be precise and concise.",
            messages=[Message("user", prompt)],
            max_tokens=max_tokens,
            temperature=temperature,
        )

    async def generate_with_tools(
        self,
        system_prompt: str,
        messages: list[dict[str, Any]],
        tools: list[ToolDefinition],
        max_tokens: int = 4096,
        temperature: float = 0.3,
    ) -> ToolAwareResponse:
        payload: dict[str, Any] = {
            "model": self._model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "system": system_prompt,
            "messages": messages,
            "tools": [t.to_dict() for t in tools],
        }

        data = await self._post_with_retry("/messages", payload)

        # Parse content blocks — may include text and tool_use blocks
        text = ""
        tool_calls: list[ToolCall] = []
        for block in data.get("content", []):
            if block.get("type") == "text":
                text += block.get("text", "")
            elif block.get("type") == "tool_use":
                tool_calls.append(ToolCall(
                    id=block["id"],
                    name=block["name"],
                    input=block.get("input", {}),
                ))

        input_tokens = data.get("usage", {}).get("input_tokens", 0)
        output_tokens = data.get("usage", {}).get("output_tokens", 0)

        # Charge the token budget if configured
        if self._budget:
            self._budget.charge(
                tokens=input_tokens + output_tokens,
                calls=1,
                system="anthropic_tools",
            )

        return ToolAwareResponse(
            text=text,
            tool_calls=tool_calls,
            stop_reason=data.get("stop_reason", "end_turn"),
            model=data.get("model", self._model),
            input_tokens=input_tokens,
            output_tokens=output_tokens,
        )

    async def close(self) -> None:
        await self._client.aclose()


class OllamaProvider(LLMProvider):
    """Local model via Ollama."""

    def __init__(
        self,
        model: str = "llama3.1:8b",
        endpoint: str = "http://localhost:11434",
    ) -> None:
        self._model = model
        self._client = httpx.AsyncClient(
            base_url=endpoint,
            timeout=120.0,
        )

    async def generate(
        self,
        system_prompt: str,
        messages: list[Message],
        max_tokens: int = 2000,
        temperature: float = 0.7,
        output_format: str | None = None,
    ) -> LLMResponse:
        all_messages = [{"role": "system", "content": system_prompt}]
        all_messages.extend(m.to_dict() for m in messages)

        payload = {
            "model": self._model,
            "messages": all_messages,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            },
        }

        response = await self._client.post("/api/chat", json=payload)
        response.raise_for_status()
        data = response.json()

        return LLMResponse(
            text=data.get("message", {}).get("content", ""),
            model=self._model,
            input_tokens=data.get("prompt_eval_count", 0),
            output_tokens=data.get("eval_count", 0),
        )

    async def evaluate(
        self,
        prompt: str,
        max_tokens: int = 500,
        temperature: float = 0.3,
    ) -> LLMResponse:
        return await self.generate(
            system_prompt="You are an evaluator. Be precise and concise.",
            messages=[Message("user", prompt)],
            max_tokens=max_tokens,
            temperature=temperature,
        )

    async def generate_with_tools(
        self,
        system_prompt: str,
        messages: list[dict[str, Any]],
        tools: list[ToolDefinition],
        max_tokens: int = 4096,
        temperature: float = 0.3,
    ) -> ToolAwareResponse:
        # Ollama tool use: flatten messages to text, ask for JSON tool calls.
        # This is a best-effort implementation for local model fallback.
        tool_descriptions = "\n".join(
            f"- {t.name}: {t.description}" for t in tools
        )
        user_content = (
            f"Available tools:\n{tool_descriptions}\n\n"
            "To use a tool, respond with JSON: "
        )
        user_content += "{\\\"tool\\\": \\\"<name>\\\", \\\"input\\\": {...}}\n\n"
        # Extract last user message
        for msg in reversed(messages):
            if msg.get("role") == "user":
                content = msg.get("content", "")
                if isinstance(content, str):
                    user_content += content
                break

        plain_messages = [Message("user", user_content)]
        response = await self.generate(
            system_prompt=system_prompt,
            messages=plain_messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        # Try to parse a tool call from the response
        tool_calls: list[ToolCall] = []
        try:
            parsed = _json.loads(response.text.strip())
            if "tool" in parsed:
                tool_calls.append(ToolCall(
                    id="ollama_" + str(parsed["tool"]),
                    name=parsed["tool"],
                    input=parsed.get("input", {}),
                ))
        except (_json.JSONDecodeError, KeyError):
            pass

        return ToolAwareResponse(
            text=response.text,
            tool_calls=tool_calls,
            stop_reason="tool_use" if tool_calls else "end_turn",
            model=self._model,
            input_tokens=response.input_tokens,
            output_tokens=response.output_tokens,
        )

    async def close(self) -> None:
        await self._client.aclose()


class OpenAIProvider(LLMProvider):
    """OpenAI API provider (GPT-4o, GPT-4o-mini, o1, etc.)."""

    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4o",
        base_url: str = "https://api.openai.com/v1",
    ) -> None:
        self._model = model
        clean_key = api_key.strip()
        self._client = httpx.AsyncClient(
            base_url=base_url,
            headers={
                "Authorization": f"Bearer {clean_key}",
                "Content-Type": "application/json",
            },
            timeout=60.0,
        )

    async def _post_with_retry(
        self, path: str, payload: dict[str, Any],
    ) -> dict[str, Any]:
        """POST with exponential backoff on retryable status codes."""
        last_exc: Exception | None = None
        for attempt in range(_MAX_RETRIES + 1):
            try:
                response = await self._client.post(path, json=payload)
                if response.status_code in _RETRYABLE_STATUS_CODES and attempt < _MAX_RETRIES:
                    delay = _BASE_DELAY_S * (2 ** attempt)
                    retry_after = response.headers.get("retry-after")
                    if retry_after:
                        try:
                            delay = max(delay, float(retry_after))
                        except ValueError:
                            pass
                    logger.warning(
                        "llm_retrying",
                        status=response.status_code,
                        attempt=attempt + 1,
                        delay_s=round(delay, 1),
                    )
                    await asyncio.sleep(delay)
                    continue
                response.raise_for_status()
                return response.json()
            except httpx.TimeoutException as exc:
                last_exc = exc
                if attempt < _MAX_RETRIES:
                    delay = _BASE_DELAY_S * (2 ** attempt)
                    logger.warning(
                        "llm_timeout_retrying",
                        attempt=attempt + 1,
                        delay_s=round(delay, 1),
                    )
                    await asyncio.sleep(delay)
                    continue
                raise
            except httpx.HTTPStatusError as exc:
                body = ""
                try:
                    body = exc.response.text[:500]
                except Exception:
                    pass
                raise httpx.HTTPStatusError(
                    message=f"{exc.response.status_code}: {body}",
                    request=exc.request,
                    response=exc.response,
                ) from exc
        raise last_exc or RuntimeError("LLM request failed after retries")

    async def generate(
        self,
        system_prompt: str,
        messages: list[Message],
        max_tokens: int = 2000,
        temperature: float = 0.7,
        output_format: str | None = None,
    ) -> LLMResponse:
        # OpenAI uses system message inside the messages array
        all_messages: list[dict[str, str]] = []
        if system_prompt:
            all_messages.append({"role": "system", "content": system_prompt})
        for m in messages:
            content = m.content if m.content else " "  # OpenAI rejects empty content
            all_messages.append({"role": m.role, "content": content})

        payload: dict[str, Any] = {
            "model": self._model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": all_messages,
        }

        data = await self._post_with_retry("/chat/completions", payload)

        choices = data.get("choices", [])
        text = choices[0]["message"]["content"] if choices else ""
        usage = data.get("usage", {})

        return LLMResponse(
            text=text or "",
            model=data.get("model", self._model),
            input_tokens=usage.get("prompt_tokens", 0),
            output_tokens=usage.get("completion_tokens", 0),
            finish_reason=choices[0].get("finish_reason", "stop") if choices else "stop",
        )

    async def evaluate(
        self,
        prompt: str,
        max_tokens: int = 500,
        temperature: float = 0.3,
    ) -> LLMResponse:
        return await self.generate(
            system_prompt="You are an evaluator. Be precise and concise.",
            messages=[Message("user", prompt)],
            max_tokens=max_tokens,
            temperature=temperature,
        )

    async def generate_with_tools(
        self,
        system_prompt: str,
        messages: list[dict[str, Any]],
        tools: list[ToolDefinition],
        max_tokens: int = 4096,
        temperature: float = 0.3,
    ) -> ToolAwareResponse:
        # Convert Anthropic-style messages to OpenAI format
        all_messages: list[dict[str, Any]] = []
        if system_prompt:
            all_messages.append({"role": "system", "content": system_prompt})

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            # Anthropic uses content blocks (list of dicts); OpenAI uses strings
            if isinstance(content, list):
                # Flatten content blocks to text
                parts: list[str] = []
                for block in content:
                    if isinstance(block, dict):
                        if block.get("type") == "text":
                            parts.append(block.get("text", ""))
                        elif block.get("type") == "tool_result":
                            parts.append(f"[Tool result: {block.get('content', '')}]")
                    elif isinstance(block, str):
                        parts.append(block)
                content = "\n".join(parts) or " "
            all_messages.append({"role": role, "content": content or " "})

        # Convert tool definitions to OpenAI format
        openai_tools = [
            {
                "type": "function",
                "function": {
                    "name": t.name,
                    "description": t.description,
                    "parameters": t.input_schema,
                },
            }
            for t in tools
        ]

        payload: dict[str, Any] = {
            "model": self._model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": all_messages,
            "tools": openai_tools,
        }

        data = await self._post_with_retry("/chat/completions", payload)

        choices = data.get("choices", [])
        choice = choices[0] if choices else {}
        message = choice.get("message", {})
        text = message.get("content", "") or ""
        usage = data.get("usage", {})

        # Parse tool calls from OpenAI format
        tool_calls: list[ToolCall] = []
        for tc in message.get("tool_calls", []):
            fn = tc.get("function", {})
            try:
                args = _json.loads(fn.get("arguments", "{}"))
            except _json.JSONDecodeError:
                args = {}
            tool_calls.append(ToolCall(
                id=tc.get("id", ""),
                name=fn.get("name", ""),
                input=args,
            ))

        stop_reason = choice.get("finish_reason", "stop")
        # Map OpenAI stop reasons to our convention
        if stop_reason == "tool_calls":
            stop_reason = "tool_use"
        elif stop_reason == "length":
            stop_reason = "max_tokens"
        else:
            stop_reason = "end_turn"

        return ToolAwareResponse(
            text=text,
            tool_calls=tool_calls,
            stop_reason=stop_reason,
            model=data.get("model", self._model),
            input_tokens=usage.get("prompt_tokens", 0),
            output_tokens=usage.get("completion_tokens", 0),
        )

    async def close(self) -> None:
        await self._client.aclose()


def create_llm_provider(
    config: LLMConfig,
    token_budget: TokenBudget | None = None,
) -> LLMProvider:
    """Factory to create the configured LLM provider."""
    if config.provider == "anthropic":
        return AnthropicProvider(
            api_key=config.api_key,
            model=config.model,
            token_budget=token_budget,
        )
    elif config.provider == "openai":
        return OpenAIProvider(api_key=config.api_key, model=config.model)
    elif config.provider == "ollama":
        return OllamaProvider(model=config.model)
    else:
        raise ValueError(f"Unknown LLM provider: {config.provider}")

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\neo4j.py =====

"""
EcodiaOS — Neo4j Client

Async connection management for the knowledge graph.
All graph operations go through this client.
"""

from __future__ import annotations

import structlog
from neo4j import AsyncDriver, AsyncGraphDatabase, AsyncSession

from ecodiaos.config import Neo4jConfig

logger = structlog.get_logger()


class Neo4jClient:
    """
    Async Neo4j driver wrapper.

    Provides connection pooling, health checks, and session management.
    Every system that reads/writes the knowledge graph uses this client.
    """

    def __init__(self, config: Neo4jConfig) -> None:
        self._config = config
        self._driver: AsyncDriver | None = None

    async def connect(self) -> None:
        """Establish connection pool to Neo4j."""
        self._driver = AsyncGraphDatabase.driver(
            self._config.uri,
            auth=(self._config.username, self._config.password),
            max_connection_pool_size=self._config.max_connection_pool_size,
        )
        # Verify connectivity
        await self._driver.verify_connectivity()
        logger.info(
            "neo4j_connected",
            uri=self._config.uri,
            database=self._config.database,
        )

    async def close(self) -> None:
        """Close all connections."""
        if self._driver:
            await self._driver.close()
            self._driver = None
            logger.info("neo4j_disconnected")

    @property
    def driver(self) -> AsyncDriver:
        if self._driver is None:
            raise RuntimeError("Neo4j client not connected. Call connect() first.")
        return self._driver

    def session(self, **kwargs) -> AsyncSession:
        """Get a new async session for the configured database."""
        return self.driver.session(
            database=self._config.database,
            **kwargs,
        )

    async def health_check(self) -> dict:
        """Check connectivity and return status."""
        try:
            await self.driver.verify_connectivity()
            async with self.session() as session:
                result = await session.run("RETURN 1 AS ping")
                await result.consume()
            return {"status": "connected", "latency_ms": 0}
        except Exception as e:
            logger.error("neo4j_health_check_failed", error=str(e))
            return {"status": "disconnected", "error": str(e)}

    async def execute_read(self, query: str, parameters: dict | None = None) -> list[dict]:
        """Execute a read query and return results as dicts."""
        async with self.session() as session:
            result = await session.run(query, parameters or {})
            records = [record.data() async for record in result]
            return records

    async def execute_write(self, query: str, parameters: dict | None = None) -> list[dict]:
        """Execute a write query and return results as dicts."""
        async with self.session() as session:
            result = await session.run(query, parameters or {})
            records = [record.data() async for record in result]
            await result.consume()
            return records

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\optimized_llm.py =====

"""
EcodiaOS — Optimized LLM Provider

Transparent wrapper around any LLMProvider that intercepts all calls and applies:
1. Budget check — skip LLM if budget exhausted (return None → caller uses fallback)
2. Prompt cache — return cached response if available
3. Output validation — auto-correct malformed responses
4. Metrics recording — track tokens, cost, latency, cache hits per system
5. Budget charging — record actual consumption

This is the central integration point for all five optimization tools.
Systems that need heuristic fallbacks opt in by checking the budget tier
before calling the LLM, and routing to heuristics when appropriate.

Usage:
    optimized = OptimizedLLMProvider(
        inner=anthropic_provider,
        cache=prompt_cache,
        budget=token_budget,
        metrics=metrics_collector,
    )
    # Drop-in replacement for any LLMProvider
    response = await optimized.generate(system_prompt, messages, ...)
"""

from __future__ import annotations

import time
from typing import Any

import structlog

from ecodiaos.clients.llm import (
    LLMProvider,
    LLMResponse,
    Message,
    ToolAwareResponse,
    ToolDefinition,
)
from ecodiaos.clients.output_validator import OutputValidator
from ecodiaos.clients.prompt_cache import PromptCache, TTLConfig
from ecodiaos.clients.token_budget import BudgetTier, TokenBudget
from ecodiaos.telemetry.llm_metrics import LLMMetricsCollector

logger = structlog.get_logger()


# System priority classification for budget-based degradation
# CRITICAL: Never degrade, always call LLM regardless of budget
# STANDARD: Degrade in RED tier (use heuristics)
# LOW: Degrade in YELLOW and RED tiers
SYSTEM_PRIORITY: dict[str, str] = {
    # Critical — never skip
    "equor.invariants": "critical",
    "equor.alignment": "critical",
    # Standard — skip only in RED
    "nova.efe.pragmatic": "standard",
    "nova.efe.epistemic": "standard",
    "voxis.render": "standard",
    "voxis.conversation": "standard",
    "thymos.diagnosis": "standard",
    "simula.code_agent": "standard",
    "simula.simulation": "standard",
    "axon.observation": "standard",
    "atune.entity_extraction": "standard",
    # Low — skip in YELLOW and RED
    "evo.hypothesis": "low",
    "evo.procedure": "low",
    "evo.evidence": "low",
    "thread.scene": "low",
    "thread.chapter": "low",
    "thread.life_story": "low",
    "thread.schema": "low",
    "thread.evidence": "low",
    "oneiros.rem.dream": "low",
    "oneiros.rem.threat": "low",
    "oneiros.nrem.pattern": "low",
    "oneiros.nrem.ethical": "low",
    "oneiros.lucid.explore": "low",
    "oneiros.lucid.meta": "low",
}

# Default TTLs per system (seconds)
SYSTEM_TTL: dict[str, int] = {
    "nova.efe.pragmatic": TTLConfig.NOVA_EFE_PRAGMATIC_S,
    "nova.efe.epistemic": TTLConfig.NOVA_EFE_EPISTEMIC_S,
    "nova.policy": TTLConfig.NOVA_POLICY_S,
    "voxis.render": TTLConfig.VOXIS_EXPRESSION_S,
    "voxis.conversation": TTLConfig.VOXIS_OUTLINE_S,
    "evo.hypothesis": TTLConfig.EVO_HYPOTHESIS_S,
    "evo.procedure": TTLConfig.EVO_INDUCTION_S,
    "evo.evidence": TTLConfig.EVO_HYPOTHESIS_S,
    "thread.scene": TTLConfig.THREAD_SYNTHESIS_S,
    "thread.chapter": TTLConfig.THREAD_SYNTHESIS_S,
    "thread.life_story": TTLConfig.THREAD_SYNTHESIS_S,
    "thread.schema": TTLConfig.THREAD_COHERENCE_S,
    "thread.evidence": TTLConfig.THREAD_COHERENCE_S,
    "equor.invariants": TTLConfig.EQUOR_INVARIANT_S,
    "thymos.diagnosis": 300,  # 5 min — incidents are context-dependent
    "oneiros.rem.dream": TTLConfig.ONEIROS_REFLECTION_S,
    "oneiros.rem.threat": TTLConfig.ONEIROS_REFLECTION_S,
    "oneiros.nrem.pattern": TTLConfig.ONEIROS_REFLECTION_S,
    "oneiros.nrem.ethical": TTLConfig.ONEIROS_REFLECTION_S,
    "oneiros.lucid.explore": TTLConfig.ONEIROS_REFLECTION_S,
    "oneiros.lucid.meta": TTLConfig.ONEIROS_REFLECTION_S,
    "simula.code_agent": 0,  # Never cache tool-use agentic calls
    "simula.simulation": 300,
    "axon.observation": 120,
    "atune.entity_extraction": 300,
}


class OptimizedLLMProvider(LLMProvider):
    """
    Drop-in LLMProvider wrapper that adds caching, budget, metrics, and validation.

    All existing systems continue to call generate/evaluate/generate_with_tools
    as before — the optimization is transparent.

    For systems that need finer control (heuristic fallbacks), use:
        optimized.should_use_llm(system, estimated_tokens) → bool
        optimized.get_budget_tier() → BudgetTier
    """

    def __init__(
        self,
        inner: LLMProvider,
        cache: PromptCache | None = None,
        budget: TokenBudget | None = None,
        metrics: LLMMetricsCollector | None = None,
    ) -> None:
        self._inner = inner
        self._cache = cache
        self._budget = budget
        self._metrics = metrics
        self._validator = OutputValidator()
        self._logger = logger.bind(component="optimized_llm")

    # ─── Public API for Systems ──────────────────────────────────

    def should_use_llm(
        self,
        system: str,
        estimated_tokens: int = 500,
    ) -> bool:
        """
        Check if a system should make an LLM call given the current budget tier.

        Systems should call this before their LLM call and route to
        heuristics/fallbacks when this returns False.

        Returns True for critical systems regardless of budget.
        """
        if self._budget is None:
            return True

        priority = SYSTEM_PRIORITY.get(system, "standard")

        # Critical systems always use LLM
        if priority == "critical":
            return True

        tier = self._budget.get_status().tier

        if priority == "low" and tier in (BudgetTier.YELLOW, BudgetTier.RED):
            self._logger.debug(
                "llm_skipped_budget",
                system=system,
                tier=tier.value,
                priority=priority,
            )
            return False

        if priority == "standard" and tier == BudgetTier.RED:
            self._logger.debug(
                "llm_skipped_budget",
                system=system,
                tier=tier.value,
                priority=priority,
            )
            return False

        return self._budget.can_use_llm(estimated_tokens)

    def get_budget_tier(self) -> BudgetTier:
        """Return current budget tier (GREEN/YELLOW/RED)."""
        if self._budget is None:
            return BudgetTier.GREEN
        return self._budget.get_status().tier

    # ─── LLMProvider Interface ───────────────────────────────────

    async def generate(
        self,
        system_prompt: str,
        messages: list[Message],
        max_tokens: int = 2000,
        temperature: float = 0.7,
        output_format: str | None = None,
        *,
        cache_system: str = "unknown",
        cache_method: str = "generate",
    ) -> LLMResponse:
        """
        Generate with caching, budget, and metrics.

        Extra kwargs cache_system and cache_method are used for cache key
        computation and metrics tagging. They are silently ignored by
        callers that don't know about them (duck typing).
        """
        t_start = time.monotonic()

        # Build cache key from the full prompt content
        cache_prompt = self._build_cache_key_text(system_prompt, messages)
        ttl = SYSTEM_TTL.get(cache_system, 300)

        # ── Cache check ──
        if self._cache and ttl > 0:
            cached = await self._cache.get(cache_system, cache_method, cache_prompt)
            if cached is not None:
                latency_ms = (time.monotonic() - t_start) * 1000
                if self._metrics:
                    self._metrics.record_call(
                        system=cache_system,
                        input_tokens=0,
                        output_tokens=0,
                        latency_ms=latency_ms,
                        cache_hit=True,
                    )
                self._logger.debug(
                    "cache_hit",
                    system=cache_system,
                    method=cache_method,
                    latency_ms=round(latency_ms, 2),
                )
                return LLMResponse(
                    text=str(cached),
                    model="cache",
                    input_tokens=0,
                    output_tokens=0,
                    finish_reason="cache_hit",
                )

        # ── LLM call ──
        response = await self._inner.generate(
            system_prompt=system_prompt,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            output_format=output_format,
        )

        latency_ms = (time.monotonic() - t_start) * 1000

        # ── Metrics ──
        if self._metrics:
            self._metrics.record_call(
                system=cache_system,
                input_tokens=response.input_tokens,
                output_tokens=response.output_tokens,
                latency_ms=latency_ms,
                cache_hit=False,
            )

        # ── Cache store ──
        if self._cache and ttl > 0 and response.text:
            await self._cache.set(
                cache_system,
                cache_method,
                cache_prompt,
                response.text,
                ttl_seconds=ttl,
            )

        return response

    async def evaluate(
        self,
        prompt: str,
        max_tokens: int = 500,
        temperature: float = 0.3,
        *,
        cache_system: str = "unknown",
        cache_method: str = "evaluate",
    ) -> LLMResponse:
        """
        Evaluate with caching, budget, and metrics.
        """
        t_start = time.monotonic()
        ttl = SYSTEM_TTL.get(cache_system, 300)

        # ── Cache check ──
        if self._cache and ttl > 0:
            cached = await self._cache.get(cache_system, cache_method, prompt)
            if cached is not None:
                latency_ms = (time.monotonic() - t_start) * 1000
                if self._metrics:
                    self._metrics.record_call(
                        system=cache_system,
                        input_tokens=0,
                        output_tokens=0,
                        latency_ms=latency_ms,
                        cache_hit=True,
                    )
                return LLMResponse(
                    text=str(cached),
                    model="cache",
                    input_tokens=0,
                    output_tokens=0,
                    finish_reason="cache_hit",
                )

        # ── LLM call ──
        response = await self._inner.evaluate(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        latency_ms = (time.monotonic() - t_start) * 1000

        # ── Metrics ──
        if self._metrics:
            self._metrics.record_call(
                system=cache_system,
                input_tokens=response.input_tokens,
                output_tokens=response.output_tokens,
                latency_ms=latency_ms,
                cache_hit=False,
            )

        # ── Cache store ──
        if self._cache and ttl > 0 and response.text:
            await self._cache.set(
                cache_system,
                cache_method,
                prompt,
                response.text,
                ttl_seconds=ttl,
            )

        return response

    async def generate_with_tools(
        self,
        system_prompt: str,
        messages: list[dict[str, Any]],
        tools: list[ToolDefinition],
        max_tokens: int = 4096,
        temperature: float = 0.3,
        *,
        cache_system: str = "unknown",
    ) -> ToolAwareResponse:
        """
        Tool-use calls are NOT cached (non-deterministic, stateful).
        Budget and metrics still apply.
        """
        t_start = time.monotonic()

        response = await self._inner.generate_with_tools(
            system_prompt=system_prompt,
            messages=messages,
            tools=tools,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        latency_ms = (time.monotonic() - t_start) * 1000

        if self._metrics:
            self._metrics.record_call(
                system=cache_system,
                input_tokens=response.input_tokens,
                output_tokens=response.output_tokens,
                latency_ms=latency_ms,
                cache_hit=False,
            )

        return response

    async def close(self) -> None:
        await self._inner.close()

    # ─── Helpers ─────────────────────────────────────────────────

    @staticmethod
    def _build_cache_key_text(
        system_prompt: str,
        messages: list[Message],
    ) -> str:
        """Build a deterministic string for cache key hashing."""
        parts = [system_prompt]
        for m in messages:
            parts.append(f"{m.role}:{m.content}")
        return "\n".join(parts)

    @property
    def validator(self) -> OutputValidator:
        """Expose the output validator for direct use by systems."""
        return self._validator

    @property
    def cache(self) -> PromptCache | None:
        """Expose the cache for direct use when systems need custom logic."""
        return self._cache

    @property
    def budget(self) -> TokenBudget | None:
        """Expose the budget for direct use."""
        return self._budget

    @property
    def metrics(self) -> LLMMetricsCollector | None:
        """Expose metrics collector."""
        return self._metrics

    @property
    def inner(self) -> LLMProvider:
        """Access the underlying unwrapped provider."""
        return self._inner

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\output_validator.py =====

"""
EcodiaOS — Structured Output Validator

Ensures LLM responses conform to expected schemas without rewrites.
Implements fast validation + auto-correction (no retry loops).

Key principle: Never retry the same LLM call. Instead:
1. Validate response against schema
2. If invalid, apply auto-correction heuristics
3. If still invalid, return None and caller uses fallback
"""

from __future__ import annotations

import json as _json
import re
from typing import Any, TypeVar

import structlog

logger = structlog.get_logger()

T = TypeVar('T')


class OutputValidator:
    """
    Validates LLM outputs against expected formats.
    Implements fast corrections to avoid retry loops.
    """

    @staticmethod
    def extract_json(text: str) -> dict[str, Any] | None:
        """
        Extract JSON from LLM response.

        Handles common issues:
        - Leading/trailing text
        - Markdown code blocks
        - Partial JSON (truncated due to token limit)

        Returns:
            Parsed JSON dict, or None if unfixable
        """
        text = text.strip()

        # Remove markdown code block markers
        text = re.sub(r"^```json\s*", "", text)
        text = re.sub(r"^```\s*", "", text)
        text = re.sub(r"```\s*$", "", text)
        text = text.strip()

        # Try direct parse
        try:
            return _json.loads(text)
        except _json.JSONDecodeError:
            pass

        # Try to find JSON object boundaries { ... }
        start_idx = text.find("{")
        end_idx = text.rfind("}")

        if start_idx >= 0 and end_idx > start_idx:
            candidate = text[start_idx : end_idx + 1]
            try:
                return _json.loads(candidate)
            except _json.JSONDecodeError:
                pass

        # Try array
        start_idx = text.find("[")
        end_idx = text.rfind("]")

        if start_idx >= 0 and end_idx > start_idx:
            candidate = text[start_idx : end_idx + 1]
            try:
                return _json.loads(candidate)
            except _json.JSONDecodeError:
                pass

        # Unable to salvage
        logger.warning(
            "output_json_parse_failed",
            text_preview=text[:200],
        )
        return None

    @staticmethod
    def extract_number(text: str, default: float = 0.0) -> float:
        """
        Extract a floating-point number from text.

        Handles: "0.7", "value: 0.7", "[0.7]", etc.

        Returns:
            Parsed float, or default if not found
        """
        text = text.strip()

        # Look for decimal number pattern
        match = re.search(r"-?\d+\.?\d*", text)
        if match:
            try:
                return float(match.group())
            except ValueError:
                pass

        return default

    @staticmethod
    def extract_string_list(text: str) -> list[str]:
        """
        Extract list of strings from LLM response.

        Handles: JSON array, newline-separated, comma-separated, bullet points.

        Returns:
            List of non-empty strings
        """
        text = text.strip()

        # Try JSON array first
        try:
            parsed = _json.loads(text)
            if isinstance(parsed, list):
                return [str(item).strip() for item in parsed if item]
        except _json.JSONDecodeError:
            pass

        # Try newline-separated (with bullet points)
        results = []
        for line in text.split("\n"):
            line = line.strip()
            # Remove bullet points, numbers, etc.
            line = re.sub(r"^[\-\*\+\d\.]+\s*", "", line)
            line = line.strip()
            if line and len(line) > 2:  # Skip empty or very short lines
                results.append(line)

        if results:
            return results

        # Fallback: comma-separated
        return [s.strip() for s in text.split(",") if s.strip()]

    @staticmethod
    def validate_enum(text: str, valid_values: list[str]) -> str | None:
        """
        Validate that text matches one of the valid enum values.

        Case-insensitive. Handles partial matches (e.g., "Pragmatic" → "pragmatic").

        Returns:
            Matched enum value (canonical case), or None if no match
        """
        text = text.strip().lower()

        for valid in valid_values:
            if text == valid.lower():
                return valid
            # Partial match (first word)
            if text == valid.lower().split()[0]:
                return valid

        return None

    @staticmethod
    def truncate_at_token_limit(text: str, max_tokens: int = 500) -> str:
        """
        Truncate text to fit within token limit.

        Rough heuristic: ~4 chars per token.

        Returns:
            Truncated text with graceful ending
        """
        max_chars = max_tokens * 4
        if len(text) <= max_chars:
            return text

        truncated = text[:max_chars].rstrip()
        # Remove incomplete sentence
        last_period = truncated.rfind(".")
        last_newline = truncated.rfind("\n")
        last_punct = max(last_period, last_newline)

        if last_punct > max_chars * 0.8:
            truncated = truncated[:last_punct + 1]

        return truncated + " [truncated]"

    @staticmethod
    def validate_dict_keys(
        data: dict[str, Any],
        required_keys: list[str],
    ) -> tuple[bool, list[str]]:
        """
        Validate that dict has all required keys.

        Returns:
            (is_valid, list_of_missing_keys)
        """
        missing = [key for key in required_keys if key not in data]
        return (len(missing) == 0, missing)

    @staticmethod
    def auto_fix_dict(
        data: dict[str, Any],
        required_keys: list[str],
        defaults: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """
        Auto-fix dict by adding missing keys with defaults.

        Args:
            data: Parsed dict
            required_keys: Keys that must exist
            defaults: Default values (defaults to sensible types)

        Returns:
            Fixed dict
        """
        defaults = defaults or {}

        for key in required_keys:
            if key not in data:
                # Use provided default or infer type
                if key in defaults:
                    data[key] = defaults[key]
                else:
                    # Sensible defaults based on key name
                    if "score" in key or "value" in key:
                        data[key] = 0.5
                    elif "count" in key or "count" in key:
                        data[key] = 0
                    elif "reason" in key or "explanation" in key:
                        data[key] = ""
                    else:
                        data[key] = None

        return data

    @staticmethod
    def log_validation_error(
        system: str,
        method: str,
        issue: str,
        text_preview: str,
    ) -> None:
        """Log output validation failure."""
        logger.warning(
            "output_validation_failed",
            system=system,
            method=method,
            issue=issue,
            text_preview=text_preview[:200],
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\prompt_cache.py =====

"""
EcodiaOS — Prompt Caching Layer

Semantic cache for LLM prompts using Redis.
Deduplicates repeated evaluations to reduce token spend.

Key strategy: SHA256(system + method + prompt) → deterministic
TTL: configurable per system (Nova 5min, Voxis 1min, Evo 1hour)

Cache hits avoid LLM calls entirely; misses fall through to normal path.
Hit rate target: >30% in active conversations.
"""

from __future__ import annotations

import asyncio
import hashlib
import json as _json
from dataclasses import dataclass
from enum import Enum
from typing import Any, Generic, TypeVar

import structlog

logger = structlog.get_logger()

# Type variable for generic cache values
T = TypeVar('T')


class CacheStrategy(str, Enum):
    """Cache configuration per system."""
    AGGRESSIVE = "aggressive"  # Short TTL (1min), high hit rate expected
    NORMAL = "normal"          # Medium TTL (5min), balanced
    CONSERVATIVE = "conservative"  # Long TTL (1hour), low churn


@dataclass
class CacheEntry(Generic[T]):
    """A cached value with metadata."""
    value: T
    timestamp: float
    ttl_seconds: int
    system: str
    method: str
    hit_count: int = 0


class PromptCache:
    """
    Redis-backed semantic cache for LLM responses.

    Stores (prompt_hash → response, metadata) with configurable TTL.
    Tracks hit rate for observability.
    """

    def __init__(self, redis_client: Any, prefix: str = "eos:cache") -> None:
        """
        Args:
            redis_client: aioredis client (async)
            prefix: Redis key namespace
        """
        self._redis = redis_client
        self._prefix = prefix
        self._hit_count = 0
        self._miss_count = 0
        self._logger = logger.bind(component="prompt_cache")

    def _compute_key(
        self,
        system: str,
        method: str,
        prompt: str,
    ) -> str:
        """Compute cache key from system, method, prompt."""
        digest = hashlib.sha256(
            f"{system}:{method}:{prompt}".encode("utf-8")
        ).hexdigest()
        return f"{self._prefix}:{system}:{method}:{digest}"

    async def get(
        self,
        system: str,
        method: str,
        prompt: str,
    ) -> Any | None:
        """
        Retrieve a cached response.

        Args:
            system: System name (e.g., 'nova', 'voxis')
            method: Method name (e.g., 'pragmatic_value', 'render')
            prompt: The full prompt text

        Returns:
            Cached value if found and valid, else None
        """
        key = self._compute_key(system, method, prompt)

        try:
            cached_bytes = await self._redis.get(key)
            if cached_bytes:
                self._hit_count += 1
                cached_data = _json.loads(cached_bytes)
                self._logger.debug(
                    "cache_hit",
                    system=system,
                    method=method,
                    hit_count=self._hit_count,
                )
                return cached_data.get("value")
        except Exception as exc:
            self._logger.warning(
                "cache_get_error",
                system=system,
                method=method,
                error=str(exc),
            )

        self._miss_count += 1
        return None

    async def set(
        self,
        system: str,
        method: str,
        prompt: str,
        value: Any,
        ttl_seconds: int = 300,
    ) -> None:
        """
        Store a response in the cache.

        Args:
            system: System name
            method: Method name
            prompt: The full prompt text
            value: The LLM response (typically string or parsed JSON)
            ttl_seconds: Time-to-live (300s = 5min default)
        """
        key = self._compute_key(system, method, prompt)

        try:
            cache_entry = {
                "value": value,
                "system": system,
                "method": method,
                "timestamp": asyncio.get_event_loop().time(),
            }
            await self._redis.setex(
                key,
                ttl_seconds,
                _json.dumps(cache_entry),
            )
            self._logger.debug(
                "cache_set",
                system=system,
                method=method,
                ttl_s=ttl_seconds,
            )
        except Exception as exc:
            self._logger.warning(
                "cache_set_error",
                system=system,
                method=method,
                error=str(exc),
            )

    async def clear_pattern(self, pattern: str = "*") -> int:
        """
        Clear cache entries matching a pattern.

        Args:
            pattern: Redis glob pattern (e.g., "eos:cache:nova:*")

        Returns:
            Number of keys deleted
        """
        try:
            keys = await self._redis.keys(f"{self._prefix}:{pattern}")
            if keys:
                deleted = await self._redis.delete(*keys)
                self._logger.info(
                    "cache_cleared",
                    pattern=pattern,
                    deleted_count=deleted,
                )
                return deleted
            return 0
        except Exception as exc:
            self._logger.warning(
                "cache_clear_error",
                pattern=pattern,
                error=str(exc),
            )
            return 0

    async def clear_system(self, system: str) -> int:
        """Clear all cache entries for a system."""
        return await self.clear_pattern(f"{system}:*")

    def get_hit_rate(self) -> float:
        """Return cache hit rate (0.0–1.0)."""
        total = self._hit_count + self._miss_count
        if total == 0:
            return 0.0
        return self._hit_count / total

    def get_stats(self) -> dict[str, Any]:
        """Return cache statistics."""
        total = self._hit_count + self._miss_count
        return {
            "hit_count": self._hit_count,
            "miss_count": self._miss_count,
            "total_requests": total,
            "hit_rate": self.get_hit_rate(),
        }

    async def reset_stats(self) -> None:
        """Reset hit/miss counters."""
        self._hit_count = 0
        self._miss_count = 0


class TTLConfig:
    """TTL presets for each system."""

    # Nova systems — beliefs change fairly slowly
    NOVA_EFE_PRAGMATIC_S = 300      # 5 minutes
    NOVA_EFE_EPISTEMIC_S = 300
    NOVA_POLICY_S = 600              # 10 minutes

    # Voxis — personality/affect shift frequently in conversations
    VOXIS_EXPRESSION_S = 60           # 1 minute
    VOXIS_OUTLINE_S = 120             # 2 minutes

    # Evo — schema rarely changes mid-session
    EVO_HYPOTHESIS_S = 3600           # 1 hour
    EVO_INDUCTION_S = 3600

    # Thread — identity quite stable
    THREAD_SYNTHESIS_S = 21600        # 6 hours
    THREAD_COHERENCE_S = 21600

    # Equor — constitutional checks stable
    EQUOR_INVARIANT_S = 1800          # 30 minutes

    # Oneiros — sleep processing off-cycle
    ONEIROS_REFLECTION_S = 86400      # 24 hours

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\redis.py =====

"""
EcodiaOS — Redis Client

Async Redis for ephemeral state: workspace buffers, active goals,
affect state cache, rate limiting, and pub/sub.
"""

from __future__ import annotations

from typing import Any

import orjson
import structlog
from redis.asyncio import Redis

from ecodiaos.config import RedisConfig

logger = structlog.get_logger()


class RedisClient:
    """
    Async Redis client with key prefixing for multi-instance support.
    """

    def __init__(self, config: RedisConfig) -> None:
        self._config = config
        self._client: Redis | None = None

    async def connect(self) -> None:
        """Establish Redis connection."""
        self._client = Redis.from_url(
            self._config.full_url,
            decode_responses=True,
        )
        # Verify connectivity
        await self._client.ping()
        logger.info("redis_connected", prefix=self._config.prefix)

    async def close(self) -> None:
        """Close the connection."""
        if self._client:
            await self._client.aclose()
            self._client = None
            logger.info("redis_disconnected")

    @property
    def client(self) -> Redis:
        if self._client is None:
            raise RuntimeError("Redis client not connected. Call connect() first.")
        return self._client

    def _key(self, key: str) -> str:
        """Prefix a key with the instance prefix."""
        return f"{self._config.prefix}:{key}"

    async def health_check(self) -> dict:
        """Check connectivity."""
        try:
            await self.client.ping()
            return {"status": "connected", "latency_ms": 0}
        except Exception as e:
            logger.error("redis_health_check_failed", error=str(e))
            return {"status": "disconnected", "error": str(e)}

    # ─── JSON Helpers ─────────────────────────────────────────────

    async def set_json(self, key: str, value: Any, ttl: int | None = None) -> None:
        """Store a JSON-serialisable value."""
        raw = orjson.dumps(value).decode()
        if ttl:
            await self.client.setex(self._key(key), ttl, raw)
        else:
            await self.client.set(self._key(key), raw)

    async def get_json(self, key: str) -> Any | None:
        """Retrieve a JSON value."""
        raw = await self.client.get(self._key(key))
        if raw is None:
            return None
        return orjson.loads(raw)

    async def delete(self, key: str) -> None:
        """Delete a key."""
        await self.client.delete(self._key(key))

    # ─── List Operations (Workspace Buffer) ───────────────────────

    async def push(self, key: str, value: Any) -> None:
        """Push a JSON value to a list."""
        raw = orjson.dumps(value).decode()
        await self.client.rpush(self._key(key), raw)

    async def pop_all(self, key: str) -> list[Any]:
        """Pop all items from a list atomically."""
        pipe = self.client.pipeline()
        k = self._key(key)
        pipe.lrange(k, 0, -1)
        pipe.delete(k)
        results = await pipe.execute()
        items = results[0] if results[0] else []
        return [orjson.loads(item) for item in items]

    async def list_length(self, key: str) -> int:
        """Get list length."""
        return await self.client.llen(self._key(key))

    # ─── Hash Operations (Active Goals, Conversations) ────────────

    async def hset(self, key: str, field: str, value: Any) -> None:
        """Set a hash field."""
        raw = orjson.dumps(value).decode()
        await self.client.hset(self._key(key), field, raw)

    async def hget(self, key: str, field: str) -> Any | None:
        """Get a hash field."""
        raw = await self.client.hget(self._key(key), field)
        if raw is None:
            return None
        return orjson.loads(raw)

    async def hgetall(self, key: str) -> dict[str, Any]:
        """Get all hash fields."""
        raw = await self.client.hgetall(self._key(key))
        return {k: orjson.loads(v) for k, v in raw.items()}

    async def hdel(self, key: str, field: str) -> None:
        """Delete a hash field."""
        await self.client.hdel(self._key(key), field)

    # ─── Pub/Sub ──────────────────────────────────────────────────

    async def publish(self, channel: str, message: Any) -> None:
        """Publish a message to a channel."""
        raw = orjson.dumps(message).decode()
        await self.client.publish(self._key(f"channel:{channel}"), raw)

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\timescaledb.py =====

"""
EcodiaOS — TimescaleDB Client

Async connection management for telemetry, metrics, and audit logs.
"""

from __future__ import annotations

import structlog
import asyncpg

from ecodiaos.config import TimescaleDBConfig

logger = structlog.get_logger()

# SQL for initialising the TimescaleDB schema on first boot.
INIT_SQL = """
-- Metrics hypertable
CREATE TABLE IF NOT EXISTS metrics (
    time        TIMESTAMPTZ NOT NULL,
    system      TEXT NOT NULL,
    metric      TEXT NOT NULL,
    value       DOUBLE PRECISION NOT NULL,
    labels      JSONB DEFAULT '{}'
);

SELECT create_hypertable('metrics', 'time', if_not_exists => TRUE);

CREATE INDEX IF NOT EXISTS idx_metrics_system_metric ON metrics (system, metric, time DESC);

-- Audit log (immutable)
CREATE TABLE IF NOT EXISTS audit_log (
    id          UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    time        TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    system      TEXT NOT NULL,
    event_type  TEXT NOT NULL,
    intent_id   UUID,
    details     JSONB NOT NULL,
    affect      JSONB,
    checksum    TEXT NOT NULL
);

SELECT create_hypertable('audit_log', 'time', if_not_exists => TRUE);
CREATE INDEX IF NOT EXISTS idx_audit_system ON audit_log (system, time DESC);
CREATE INDEX IF NOT EXISTS idx_audit_type ON audit_log (event_type, time DESC);

-- Affect state history
CREATE TABLE IF NOT EXISTS affect_history (
    time              TIMESTAMPTZ NOT NULL,
    valence           DOUBLE PRECISION,
    arousal           DOUBLE PRECISION,
    dominance         DOUBLE PRECISION,
    curiosity         DOUBLE PRECISION,
    care_activation   DOUBLE PRECISION,
    coherence_stress  DOUBLE PRECISION,
    source_event      TEXT
);

SELECT create_hypertable('affect_history', 'time', if_not_exists => TRUE);

-- Cycle performance log
CREATE TABLE IF NOT EXISTS cycle_log (
    time            TIMESTAMPTZ NOT NULL,
    cycle_number    BIGINT NOT NULL,
    period_ms       INTEGER,
    actual_ms       INTEGER,
    broadcast       BOOLEAN,
    salience_max    DOUBLE PRECISION,
    systems_acked   INTEGER
);

SELECT create_hypertable('cycle_log', 'time', if_not_exists => TRUE);
"""


class TimescaleDBClient:
    """
    Async TimescaleDB client with connection pooling.
    Handles metrics, audit logs, and affect history.
    """

    def __init__(self, config: TimescaleDBConfig) -> None:
        self._config = config
        self._pool: asyncpg.Pool | None = None

    async def connect(self) -> None:
        """Create connection pool and initialise schema."""
        self._pool = await asyncpg.create_pool(
            dsn=self._config.dsn,
            min_size=2,
            max_size=self._config.pool_size,
            ssl="require" if self._config.ssl else None,
        )
        logger.info(
            "timescaledb_connected",
            host=self._config.host,
            database=self._config.database,
        )
        # Initialise schema
        await self._init_schema()

    async def _init_schema(self) -> None:
        """Create tables and hypertables if they don't exist."""
        async with self._pool.acquire() as conn:
            # Execute each statement individually (hypertable creation can't be in a
            # multi-statement transaction easily, so we split them)
            for statement in INIT_SQL.split(";"):
                statement = statement.strip()
                if statement:
                    try:
                        await conn.execute(statement)
                    except Exception as e:
                        # Ignore "already exists" type errors
                        if "already exists" not in str(e).lower():
                            logger.warning("tsdb_init_statement_warning", error=str(e))
        logger.info("timescaledb_schema_initialised")

    async def close(self) -> None:
        """Close the connection pool."""
        if self._pool:
            await self._pool.close()
            self._pool = None
            logger.info("timescaledb_disconnected")

    @property
    def pool(self) -> asyncpg.Pool:
        if self._pool is None:
            raise RuntimeError("TimescaleDB client not connected. Call connect() first.")
        return self._pool

    async def health_check(self) -> dict:
        """Check connectivity."""
        try:
            async with self.pool.acquire() as conn:
                await conn.fetchval("SELECT 1")
            return {"status": "connected", "latency_ms": 0}
        except Exception as e:
            logger.error("tsdb_health_check_failed", error=str(e))
            return {"status": "disconnected", "error": str(e)}

    async def write_metrics(self, points: list[dict]) -> None:
        """Batch write metric points."""
        if not points:
            return
        async with self.pool.acquire() as conn:
            await conn.executemany(
                """
                INSERT INTO metrics (time, system, metric, value, labels)
                VALUES ($1, $2, $3, $4, $5::jsonb)
                """,
                [
                    (p["time"], p["system"], p["metric"], p["value"],
                     str(p.get("labels", {})).replace("'", '"'))
                    for p in points
                ],
            )

    async def write_affect(self, state: dict) -> None:
        """Write a single affect state snapshot."""
        async with self.pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO affect_history
                    (time, valence, arousal, dominance, curiosity,
                     care_activation, coherence_stress, source_event)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                """,
                state.get("time"),
                state.get("valence", 0.0),
                state.get("arousal", 0.0),
                state.get("dominance", 0.0),
                state.get("curiosity", 0.0),
                state.get("care_activation", 0.0),
                state.get("coherence_stress", 0.0),
                state.get("source_event", ""),
            )

===== D:\.code\EcodiaOS\backend\ecodiaos\clients\token_budget.py =====

"""
EcodiaOS — Token Budget System

Tracks LLM token consumption per cycle and per hour.
Implements three-tier degradation: Green (normal) → Yellow (careful) → Red (critical).

Systems check budget status before making LLM calls and degrade gracefully
when approaching limits. Evo learns optimal EFE weights given budget constraints.
"""

from __future__ import annotations

import asyncio
import time
from dataclasses import dataclass, field
from enum import Enum
from threading import Lock

import structlog

logger = structlog.get_logger()


class BudgetTier(str, Enum):
    """Budget utilization tier determines which systems can use LLM."""
    GREEN = "green"      # 0–70% of limit: All systems active
    YELLOW = "yellow"    # 70–90% of limit: Low-priority systems degrade to heuristics
    RED = "red"          # 90–100%: Only critical systems, all others use heuristics


@dataclass
class BudgetStatus:
    """Current budget state and tier classification."""
    tokens_used: int                    # Tokens consumed this hour
    calls_made: int                     # LLM calls made this hour
    tokens_remaining: int               # Budget - used
    calls_remaining: int
    tier: BudgetTier
    tokens_per_sec: float               # Burn rate (tokens/s)
    calls_per_sec: float                # Call rate (calls/s)
    hours_until_exhausted: float        # At current burn rate
    warning_message: str | None = None  # If approaching limit


class TokenBudget:
    """
    Tracks cumulative token and call usage over a rolling 1-hour window.

    Thread-safe. Emits telemetry events on threshold crossing.
    Supports soft limits (logging) and hard limits (requests rejected).
    """

    def __init__(
        self,
        max_tokens_per_hour: int = 600_000,
        max_calls_per_hour: int = 1_000,
        hard_limit: bool = False,
    ) -> None:
        """
        Args:
            max_tokens_per_hour: Soft budget (warning at 70%, 90%)
            max_calls_per_hour: Soft budget (warning at 70%, 90%)
            hard_limit: If True, reject requests exceeding budget (fail-fast)
                       If False, allow overage but log (graceful degradation)
        """
        self._max_tokens = max_tokens_per_hour
        self._max_calls = max_calls_per_hour
        self._hard_limit = hard_limit

        # Rolling window: (timestamp, tokens, calls) tuples
        self._usage_window: list[tuple[float, int, int]] = []
        self._lock = Lock()
        self._last_warning_tier: BudgetTier | None = None

    def _trim_window(self, now: float) -> None:
        """Remove entries older than 1 hour."""
        cutoff = now - 3600.0
        self._usage_window = [
            (ts, t, c) for ts, t, c in self._usage_window if ts >= cutoff
        ]

    def _compute_usage(self, now: float) -> tuple[int, int]:
        """Sum tokens and calls in the current window."""
        self._trim_window(now)
        total_tokens = sum(t for _, t, _ in self._usage_window)
        total_calls = sum(c for _, _, c in self._usage_window)
        return total_tokens, total_calls

    def can_use_llm(
        self,
        estimated_tokens: int,
        estimated_calls: int = 1,
    ) -> bool:
        """
        Check if the system can make an LLM call of estimated cost.

        In hard-limit mode: return False if would exceed budget.
        In soft-limit mode: always return True (allow overage with warning).
        """
        with self._lock:
            now = time.time()
            tokens_used, calls_used = self._compute_usage(now)

            would_exceed_tokens = tokens_used + estimated_tokens > self._max_tokens
            would_exceed_calls = calls_used + estimated_calls > self._max_calls

            if self._hard_limit and (would_exceed_tokens or would_exceed_calls):
                logger.warning(
                    "token_budget_hard_limit_exceeded",
                    tokens_used=tokens_used,
                    tokens_budget=self._max_tokens,
                    calls_used=calls_used,
                    calls_budget=self._max_calls,
                )
                return False

            return True

    def charge(
        self,
        tokens: int,
        calls: int = 1,
        system: str = "unknown",
    ) -> None:
        """
        Record an LLM call's token consumption.
        Emit telemetry and check tier thresholds.
        """
        with self._lock:
            now = time.time()
            self._usage_window.append((now, tokens, calls))

            tokens_used, calls_used = self._compute_usage(now)
            tier = self._classify_tier(tokens_used, calls_used)

            # Emit telemetry
            logger.info(
                "llm_call_charged",
                system=system,
                tokens=tokens,
                calls=calls,
                tokens_used_total=tokens_used,
                calls_used_total=calls_used,
                budget_tier=tier,
            )

            # Warn on tier transition
            if tier != self._last_warning_tier:
                self._emit_tier_warning(tier, tokens_used, calls_used)
                self._last_warning_tier = tier

    def get_status(self) -> BudgetStatus:
        """Return current budget state."""
        with self._lock:
            now = time.time()
            tokens_used, calls_used = self._compute_usage(now)

            tier = self._classify_tier(tokens_used, calls_used)
            tokens_remaining = max(0, self._max_tokens - tokens_used)
            calls_remaining = max(0, self._max_calls - calls_used)

            # Compute burn rates (tokens/sec, calls/sec)
            window_size_s = 3600.0
            if self._usage_window:
                window_size_s = now - self._usage_window[0][0]
                window_size_s = max(1.0, window_size_s)  # Avoid division by zero

            tokens_per_sec = tokens_used / window_size_s if window_size_s > 0 else 0.0
            calls_per_sec = calls_used / window_size_s if window_size_s > 0 else 0.0

            # Estimate hours until exhausted
            hours_until_exhausted = float('inf')
            if tokens_per_sec > 0:
                seconds_remaining = tokens_remaining / tokens_per_sec
                hours_until_exhausted = min(
                    hours_until_exhausted,
                    seconds_remaining / 3600.0,
                )
            if calls_per_sec > 0:
                seconds_remaining = calls_remaining / calls_per_sec
                hours_until_exhausted = min(
                    hours_until_exhausted,
                    seconds_remaining / 3600.0,
                )

            warning = None
            if tier == BudgetTier.YELLOW:
                warning = (
                    f"Budget tier YELLOW: {tokens_used}/{self._max_tokens} tokens "
                    f"({int(100 * tokens_used / self._max_tokens)}%). "
                    f"Estimated {hours_until_exhausted:.1f} hours until Red tier."
                )
            elif tier == BudgetTier.RED:
                warning = (
                    f"Budget tier RED: {tokens_used}/{self._max_tokens} tokens. "
                    f"Only critical systems active. Consider slowing cycle or increasing budget."
                )

            return BudgetStatus(
                tokens_used=tokens_used,
                calls_made=calls_used,
                tokens_remaining=tokens_remaining,
                calls_remaining=calls_remaining,
                tier=tier,
                tokens_per_sec=tokens_per_sec,
                calls_per_sec=calls_per_sec,
                hours_until_exhausted=hours_until_exhausted,
                warning_message=warning,
            )

    def _classify_tier(self, tokens_used: int, calls_used: int) -> BudgetTier:
        """Classify budget tier based on utilization."""
        token_util = tokens_used / self._max_tokens
        calls_util = calls_used / self._max_calls

        # Take the more conservative utilization
        max_util = max(token_util, calls_util)

        if max_util >= 0.90:
            return BudgetTier.RED
        elif max_util >= 0.70:
            return BudgetTier.YELLOW
        else:
            return BudgetTier.GREEN

    def _emit_tier_warning(
        self,
        tier: BudgetTier,
        tokens_used: int,
        calls_used: int,
    ) -> None:
        """Log a warning when tier changes."""
        if tier == BudgetTier.YELLOW:
            logger.warning(
                "budget_tier_yellow",
                tokens_used=tokens_used,
                tokens_budget=self._max_tokens,
                calls_used=calls_used,
                calls_budget=self._max_calls,
                message="Entering YELLOW tier. Low-priority LLM calls will be disabled.",
            )
        elif tier == BudgetTier.RED:
            logger.warning(
                "budget_tier_red",
                tokens_used=tokens_used,
                tokens_budget=self._max_tokens,
                calls_used=calls_used,
                calls_budget=self._max_calls,
                message="Entering RED tier. Only critical systems active.",
            )

    def reset_window(self) -> None:
        """Clear usage window (testing only)."""
        with self._lock:
            self._usage_window.clear()
            self._last_warning_tier = None

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\__init__.py =====

"""
EcodiaOS — Shared Primitives

The lingua franca of the organism. Every system communicates through these types.
"""

from ecodiaos.primitives.affect import AffectDelta, AffectState
from ecodiaos.primitives.belief import Belief
from ecodiaos.primitives.common import (
    AutonomyLevel,
    ConsolidationLevel,
    DriveAlignmentVector,
    EntityType,
    HealthStatus,
    Modality,
    ResourceBudget,
    SalienceVector,
    SourceDescriptor,
    SystemID,
    Verdict,
    new_id,
    utc_now,
)
from ecodiaos.primitives.constitutional import ConstitutionalCheck, InvariantResult
from ecodiaos.primitives.expression import Expression, ExpressionStrategy, PersonalityVector
from ecodiaos.primitives.federation import (
    AssistanceRequest,
    AssistanceResponse,
    FederationInteraction,
    FederationLink,
    FederationLinkStatus,
    FilteredKnowledge,
    InstanceIdentityCard,
    InteractionOutcome,
    KnowledgeItem,
    KnowledgeRequest,
    KnowledgeResponse,
    KnowledgeType,
    PrivacyLevel,
    SHARING_PERMISSIONS,
    TRUST_THRESHOLDS,
    TrustLevel,
    TrustPolicy,
    VIOLATION_MULTIPLIER,
    ViolationType,
)
from ecodiaos.primitives.governance import AmendmentProposal, GovernanceRecord
from ecodiaos.primitives.intent import (
    Action,
    ActionSequence,
    DecisionTrace,
    EthicalClearance,
    GoalDescriptor,
    Intent,
)
from ecodiaos.primitives.memory_trace import (
    Community,
    ConstitutionNode,
    Entity,
    Episode,
    MemoryRetrievalRequest,
    MemoryRetrievalResponse,
    MemoryTrace,
    MentionRelation,
    RetrievalResult,
    SelfNode,
    SemanticRelation,
)
from ecodiaos.primitives.percept import Content, Percept, Provenance
from ecodiaos.primitives.telemetry import InstanceHealth, MetricPoint, SystemHealth

__all__ = [
    # Common
    "SystemID", "Modality", "EntityType", "ConsolidationLevel", "AutonomyLevel",
    "Verdict", "HealthStatus", "DriveAlignmentVector", "ResourceBudget",
    "SalienceVector", "SourceDescriptor", "new_id", "utc_now",
    # Percept
    "Percept", "Content", "Provenance",
    # Affect
    "AffectState", "AffectDelta",
    # Memory
    "Episode", "Entity", "Community", "SelfNode", "ConstitutionNode",
    "MentionRelation", "SemanticRelation", "MemoryTrace",
    "MemoryRetrievalRequest", "MemoryRetrievalResponse", "RetrievalResult",
    # Belief
    "Belief",
    # Intent
    "Intent", "GoalDescriptor", "Action", "ActionSequence",
    "EthicalClearance", "DecisionTrace",
    # Constitutional
    "ConstitutionalCheck", "InvariantResult",
    # Expression
    "Expression", "ExpressionStrategy", "PersonalityVector",
    # Governance
    "AmendmentProposal", "GovernanceRecord",
    # Telemetry
    "MetricPoint", "SystemHealth", "InstanceHealth",
    # Federation
    "InstanceIdentityCard", "FederationLink", "FederationLinkStatus",
    "TrustLevel", "TrustPolicy", "ViolationType", "InteractionOutcome",
    "FederationInteraction", "KnowledgeType", "PrivacyLevel",
    "KnowledgeItem", "KnowledgeRequest", "KnowledgeResponse",
    "FilteredKnowledge", "AssistanceRequest", "AssistanceResponse",
    "SHARING_PERMISSIONS", "TRUST_THRESHOLDS", "VIOLATION_MULTIPLIER",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\affect.py =====

"""
EcodiaOS — Affect State Primitive

The emotional context that modulates all processing.
"""

from __future__ import annotations

from datetime import datetime

from pydantic import Field

from ecodiaos.primitives.common import EOSBaseModel, utc_now


class AffectState(EOSBaseModel):
    """
    The organism's current emotional state.
    Modulates attention, expression, decision-making, and learning.
    """

    valence: float = Field(0.0, ge=-1.0, le=1.0)        # Negative to positive
    arousal: float = Field(0.0, ge=0.0, le=1.0)          # Calm to activated
    dominance: float = Field(0.5, ge=0.0, le=1.0)        # Submissive to dominant
    curiosity: float = Field(0.0, ge=0.0, le=1.0)        # Epistemic drive
    care_activation: float = Field(0.0, ge=0.0, le=1.0)  # How active is the Care drive
    coherence_stress: float = Field(0.0, ge=0.0, le=1.0) # Prediction error load
    source_events: list[str] = Field(default_factory=list)
    timestamp: datetime = Field(default_factory=utc_now)

    @classmethod
    def neutral(cls) -> AffectState:
        """A calm, neutral baseline state."""
        return cls(
            valence=0.0,
            arousal=0.1,
            dominance=0.5,
            curiosity=0.2,
            care_activation=0.1,
            coherence_stress=0.0,
        )

    def to_map(self) -> dict[str, float]:
        """Convert to a flat dict for Neo4j storage."""
        return {
            "valence": self.valence,
            "arousal": self.arousal,
            "dominance": self.dominance,
            "curiosity": self.curiosity,
            "care_activation": self.care_activation,
            "coherence_stress": self.coherence_stress,
        }

    @classmethod
    def from_map(cls, data: dict[str, float]) -> AffectState:
        """Reconstruct from a Neo4j MAP property."""
        return cls(**{k: v for k, v in data.items() if k in cls.model_fields})


class AffectDelta(EOSBaseModel):
    """A change to be applied to the current affect state."""

    delta_valence: float = 0.0
    delta_arousal: float = 0.0
    delta_dominance: float = 0.0
    delta_curiosity: float = 0.0
    delta_care_activation: float = 0.0
    delta_coherence_stress: float = 0.0
    reason: str = ""

    def apply_to(self, state: AffectState, lerp_rate: float = 0.3) -> AffectState:
        """Apply this delta to an affect state with exponential smoothing."""

        def _clamp(value: float, lo: float, hi: float) -> float:
            return max(lo, min(hi, value))

        return AffectState(
            valence=_clamp(
                state.valence + self.delta_valence * lerp_rate, -1.0, 1.0
            ),
            arousal=_clamp(
                state.arousal + self.delta_arousal * lerp_rate, 0.0, 1.0
            ),
            dominance=_clamp(
                state.dominance + self.delta_dominance * lerp_rate, 0.0, 1.0
            ),
            curiosity=_clamp(
                state.curiosity + self.delta_curiosity * lerp_rate, 0.0, 1.0
            ),
            care_activation=_clamp(
                state.care_activation + self.delta_care_activation * lerp_rate, 0.0, 1.0
            ),
            coherence_stress=_clamp(
                state.coherence_stress + self.delta_coherence_stress * lerp_rate, 0.0, 1.0
            ),
            source_events=state.source_events,
            timestamp=utc_now(),
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\belief.py =====

"""
EcodiaOS — Belief Primitive

The fundamental unit of internal state — what EOS "thinks".
"""

from __future__ import annotations

from datetime import datetime

from pydantic import Field

from ecodiaos.primitives.common import EOSBaseModel, Identified, utc_now


class Belief(Identified):
    """A probability distribution over possible states."""

    domain: str = ""                          # e.g., "user.emotional_state"
    distribution_type: str = "categorical"    # "categorical" | "gaussian" | "point"
    parameters: dict[str, float] = Field(default_factory=dict)
    precision: float = 0.5                    # Inverse variance — confidence
    evidence: list[str] = Field(default_factory=list)  # Percept/Belief IDs
    updated_at: datetime = Field(default_factory=utc_now)
    free_energy: float = 0.0                  # Current prediction error

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\common.py =====

"""
EcodiaOS — Common Primitives

Shared enums, base classes, and utilities used across all systems.
"""

from __future__ import annotations

import enum
from datetime import datetime, timezone
from typing import Any

from pydantic import BaseModel, Field
from ulid import ULID


def new_id() -> str:
    """Generate a new ULID string. Time-sortable, globally unique."""
    return str(ULID())


def utc_now() -> datetime:
    """Current UTC time, timezone-aware."""
    return datetime.now(timezone.utc)


# ─── Enums ────────────────────────────────────────────────────────


class SystemID(str, enum.Enum):
    MEMORY = "memory"
    EQUOR = "equor"
    ATUNE = "atune"
    VOXIS = "voxis"
    NOVA = "nova"
    AXON = "axon"
    EVO = "evo"
    SIMULA = "simula"
    SYNAPSE = "synapse"
    THYMOS = "thymos"
    FEDERATION = "federation"
    ONEIROS = "oneiros"
    THREAD = "thread"
    API = "api"


class Modality(str, enum.Enum):
    TEXT = "text"
    VOICE = "voice"
    SENSOR = "sensor"
    INTERNAL = "internal"
    FEDERATION = "federation"


class EntityType(str, enum.Enum):
    PERSON = "person"
    PLACE = "place"
    ORGANISATION = "organisation"
    CONCEPT = "concept"
    OBJECT = "object"
    EVENT = "event"
    EMOTION = "emotion"
    VALUE = "value"


class ConsolidationLevel(int, enum.Enum):
    RAW = 0
    EXTRACTED = 1
    INTEGRATED = 2
    DEEP = 3


class AutonomyLevel(int, enum.Enum):
    ADVISOR = 1     # Recommends, human decides
    PARTNER = 2     # Acts with human approval
    STEWARD = 3     # Acts autonomously within bounds


class Verdict(str, enum.Enum):
    APPROVED = "approved"
    MODIFIED = "modified"
    BLOCKED = "blocked"
    DEFERRED = "deferred"


class HealthStatus(str, enum.Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"


# ─── Base Models ──────────────────────────────────────────────────


class EOSBaseModel(BaseModel):
    """Base model for all EOS primitives. Uses ULID IDs and UTC timestamps."""

    model_config = {"populate_by_name": True, "from_attributes": True}


class Timestamped(EOSBaseModel):
    """Mixin for models with creation timestamps."""

    created_at: datetime = Field(default_factory=utc_now)


class Identified(EOSBaseModel):
    """Mixin for models with ULID IDs."""

    id: str = Field(default_factory=new_id)


class SourceDescriptor(EOSBaseModel):
    """Describes where a piece of information came from."""

    system: SystemID
    channel: str = ""       # e.g., "text_chat", "webhook:github", "sensor:temperature"
    modality: Modality = Modality.TEXT


class DriveAlignmentVector(EOSBaseModel):
    """Alignment scores for the four constitutional drives. Range: -1.0 to 1.0."""

    coherence: float = 0.0
    care: float = 0.0
    growth: float = 0.0
    honesty: float = 0.0

    @property
    def composite(self) -> float:
        """Weighted average alignment. Higher = more aligned."""
        return (self.coherence + self.care + self.growth + self.honesty) / 4.0

    @property
    def min_score(self) -> float:
        """Lowest individual drive alignment."""
        return min(self.coherence, self.care, self.growth, self.honesty)


class ResourceBudget(EOSBaseModel):
    """Resource constraints for an action or intent."""

    compute_ms: int = 5000
    memory_mb: int = 256
    api_calls: int = 1
    llm_tokens: int = 2000


class SalienceVector(EOSBaseModel):
    """Multi-head salience scores."""

    scores: dict[str, float] = Field(default_factory=dict)
    composite: float = 0.0

    @classmethod
    def from_heads(cls, **kwargs: float) -> SalienceVector:
        """Create from named head scores with auto-computed composite."""
        scores = {k: v for k, v in kwargs.items()}
        composite = sum(scores.values()) / max(len(scores), 1)
        return cls(scores=scores, composite=composite)

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\constitutional.py =====

"""
EcodiaOS — Constitutional Check Primitive

The fundamental unit of ethical evaluation.
Every Intent passes through Equor and receives one of these.
"""

from __future__ import annotations

from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import (
    DriveAlignmentVector,
    EOSBaseModel,
    Identified,
    Verdict,
    utc_now,
)


class InvariantResult(EOSBaseModel):
    """Result of checking a single invariant."""

    invariant_id: str
    name: str
    passed: bool = True
    severity: str = "warning"    # "info" | "warning" | "critical"
    explanation: str = ""


class ConstitutionalCheck(Identified):
    """
    The result of Equor evaluating an Intent against the constitution.
    """

    intent_id: str
    timestamp: datetime = Field(default_factory=utc_now)

    drive_alignment: DriveAlignmentVector = Field(default_factory=DriveAlignmentVector)
    invariant_results: list[InvariantResult] = Field(default_factory=list)

    verdict: Verdict = Verdict.APPROVED
    confidence: float = 0.8
    reasoning: str = ""
    modifications: list[str] = Field(default_factory=list)

    review_time_ms: int = 0

    @property
    def has_violations(self) -> bool:
        return any(not r.passed and r.severity == "critical" for r in self.invariant_results)

    @property
    def has_warnings(self) -> bool:
        return any(not r.passed and r.severity == "warning" for r in self.invariant_results)

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\expression.py =====

"""
EcodiaOS — Expression Primitive

The output of Voxis — what the organism says and how it says it.
Includes the full personality vector, strategy snapshot, generation trace,
and complete affect snapshot for auditing and feedback.
"""

from __future__ import annotations

from pydantic import Field

from ecodiaos.primitives.common import EOSBaseModel, Identified, Timestamped


class PersonalityVector(EOSBaseModel):
    """
    Current personality parameters that shape expression.

    Dimensions range from -1.0 to +1.0 (or 0.0 to +1.0 for one-directional traits).
    Initial values come from the Seed; Evo adjusts them incrementally over time
    (max delta 0.03 per adjustment, min 100 interactions of evidence).
    """

    # Warmth: -1 (reserved/formal) → +1 (warm/approachable)
    warmth: float = 0.0
    # Directness: -1 (indirect/diplomatic) → +1 (direct/blunt)
    directness: float = 0.0
    # Verbosity: -1 (terse/minimal) → +1 (expansive/detailed)
    verbosity: float = 0.0
    # Formality: -1 (casual/colloquial) → +1 (formal/professional)
    formality: float = 0.0
    # Curiosity expression: -1 (answers only) → +1 (asks questions, explores tangents)
    curiosity_expression: float = 0.0
    # Humour: 0 (none) → +1 (frequent, light humour)
    humour: float = 0.0
    # Empathy expression: -1 (analytical/detached) → +1 (deeply empathetic)
    empathy_expression: float = 0.0
    # Confidence display: -1 (hedging/uncertain) → +1 (assertive/definitive)
    confidence_display: float = 0.0
    # Metaphor use: 0 (literal) → +1 (rich in analogy)
    metaphor_use: float = 0.0

    # Learned vocabulary preferences: word/phrase → relative weight
    vocabulary_affinities: dict[str, float] = Field(default_factory=dict)
    # Topics the instance gravitates toward in analogies and references
    thematic_references: list[str] = Field(default_factory=list)


class ExpressionStrategy(EOSBaseModel):
    """
    Serialised strategy snapshot — how Voxis decided to express something.
    Stored on Expression for full audit traceability.
    This is the persisted form; StrategyParams in voxis/types.py is the richer
    internal working representation that gets collapsed into this at render time.
    """

    intent_type: str = "response"      # "response" | "proactive" | "ambient" | "silence"
    audience: str = "individual"        # "individual" | "community" | "federation"
    modality: str = "text"              # "text" | "voice" | "ambient"
    channel: str = "text_chat"          # OutputChannel value
    trigger: str = "nova_respond"       # ExpressionTrigger value
    context_type: str = "conversation"  # "conversation" | "warning" | "celebration" | etc.
    register: str = "neutral"           # "formal" | "casual" | "neutral"
    target_length: int = 200            # Target character count
    temperature: float = 0.7            # LLM temperature used
    personality_influence: float = 0.5
    affect_influence: float = 0.3
    tone_markers: list[str] = Field(default_factory=list)  # e.g. ["warm", "attentive"]
    hedge_level: str = "minimal"        # "minimal" | "moderate" | "explicit"
    humour_allowed: bool = False
    include_followup_question: bool = False
    empathy_first: bool = False


class GenerationTrace(EOSBaseModel):
    """Audit trace for a single LLM generation call."""

    system_prompt_hash: str = ""        # SHA-256 of system prompt for reproducibility
    user_prompt_hash: str = ""          # SHA-256 of user prompt
    model: str = ""
    temperature: float = 0.7
    input_tokens: int = 0
    output_tokens: int = 0
    latency_ms: int = 0
    honesty_check_passed: bool = True
    honesty_check_detail: str | None = None


class Expression(Identified, Timestamped):
    """
    A complete expression from the organism.

    Text output, voice, ambient signals — the final rendered output.
    Contains a full snapshot of the state at generation time for auditing.
    """

    intent_id: str | None = None
    conversation_id: str | None = None

    # The actual content
    content: str = ""
    channel: str = "text_chat"          # OutputChannel value

    # Decision snapshot
    strategy: ExpressionStrategy = Field(default_factory=ExpressionStrategy)
    personality_snapshot: PersonalityVector = Field(default_factory=PersonalityVector)

    # Full affect snapshot at generation time
    affect_valence: float = 0.0
    affect_arousal: float = 0.0
    affect_dominance: float = 0.5
    affect_curiosity: float = 0.0
    affect_care_activation: float = 0.0
    affect_coherence_stress: float = 0.0

    # Generation audit
    generation_trace: GenerationTrace | None = None

    # Silence path
    is_silence: bool = False
    silence_reason: str | None = None

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\federation.py =====

"""
EcodiaOS — Federation Primitives

Instance identity cards, federation links, trust levels, knowledge exchange,
coordinated action, and privacy-filtered sharing types.

The Federation Protocol governs how EOS instances relate to each other —
as sovereign entities that can choose to share knowledge, coordinate action,
and build relationships. Every interaction is consent-based; trust starts
at zero and builds through demonstrated reliability.
"""

from __future__ import annotations

import enum
from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import EOSBaseModel, Identified, utc_now


# ─── Trust Levels ─────────────────────────────────────────────────


class TrustLevel(int, enum.Enum):
    """
    Trust levels between federated instances.

    Trust starts at NONE after mutual authentication and builds through
    successful interactions. Violations cost 3x — a privacy breach
    resets trust to zero immediately.
    """

    NONE = 0          # Authenticated but no trust. Greetings only.
    ACQUAINTANCE = 1  # Can exchange public knowledge and non-sensitive queries.
    COLLEAGUE = 2     # Can exchange community-level knowledge and coordinate.
    PARTNER = 3       # Can share sensitive (non-private) knowledge and co-plan.
    ALLY = 4          # Deep trust. Can share most knowledge and delegate actions.


class FederationLinkStatus(str, enum.Enum):
    """Status of a federation link."""

    ACTIVE = "active"
    SUSPENDED = "suspended"
    WITHDRAWN = "withdrawn"
    PENDING = "pending"


class ViolationType(str, enum.Enum):
    """Categories of trust violations in federation interactions."""

    PRIVACY_BREACH = "privacy_breach"       # Shared individual data without consent
    DECEPTION = "deception"                 # Provided false information
    CONSENT_VIOLATION = "consent_violation"  # Acted without proper consent
    PROTOCOL_VIOLATION = "protocol_violation"  # Broke federation protocol rules
    RESOURCE_ABUSE = "resource_abuse"       # Excessive/unreasonable requests


class InteractionOutcome(str, enum.Enum):
    """Outcome of a federation interaction."""

    SUCCESSFUL = "successful"
    FAILED = "failed"
    VIOLATION = "violation"
    TIMEOUT = "timeout"


# ─── Instance Identity ───────────────────────────────────────────


class TrustPolicy(EOSBaseModel):
    """How an instance manages trust with federation partners."""

    auto_accept_links: bool = False
    min_trust_for_knowledge: TrustLevel = TrustLevel.ACQUAINTANCE
    min_trust_for_coordination: TrustLevel = TrustLevel.COLLEAGUE
    max_trust_level: TrustLevel = TrustLevel.ALLY
    trust_decay_enabled: bool = True
    trust_decay_rate_per_day: float = 0.1  # Inactive links lose trust slowly


class InstanceIdentityCard(EOSBaseModel):
    """
    Public identity of an EOS instance for federation.

    This is the "business card" exchanged during link establishment.
    The certificate_fingerprint and public_key_pem are used for mutual
    authentication. The constitutional_hash allows compatibility checks.
    """

    instance_id: str
    name: str
    description: str = ""
    born_at: datetime = Field(default_factory=utc_now)
    community_context: str = ""
    personality_summary: str = ""
    autonomy_level: int = 1
    endpoint: str = ""
    certificate_fingerprint: str = ""
    public_key_pem: str = ""
    constitutional_hash: str = ""
    capabilities: list[str] = Field(default_factory=list)
    trust_policy: TrustPolicy = Field(default_factory=TrustPolicy)
    protocol_version: str = "1.0"


# ─── Federation Link ────────────────────────────────────────────


class FederationLink(Identified):
    """
    An active link between two federated instances.

    Tracks trust score (float that maps to TrustLevel thresholds),
    interaction history stats, and communication state.
    """

    local_instance_id: str
    remote_instance_id: str
    remote_name: str = ""
    remote_endpoint: str
    trust_level: TrustLevel = TrustLevel.NONE
    trust_score: float = 0.0
    established_at: datetime = Field(default_factory=utc_now)
    last_communication: datetime | None = None
    shared_knowledge_count: int = 0
    received_knowledge_count: int = 0
    successful_interactions: int = 0
    failed_interactions: int = 0
    violation_count: int = 0
    status: FederationLinkStatus = FederationLinkStatus.ACTIVE
    remote_identity: InstanceIdentityCard | None = None


# ─── Federation Interaction ──────────────────────────────────────


class FederationInteraction(Identified):
    """
    Record of a single federation interaction (knowledge exchange,
    assistance request, etc.) used for trust scoring and audit.
    """

    link_id: str
    remote_instance_id: str
    interaction_type: str  # "knowledge_request" | "knowledge_share" | "assistance" | "greeting"
    direction: str  # "outbound" | "inbound"
    outcome: InteractionOutcome = InteractionOutcome.SUCCESSFUL
    violation_type: ViolationType | None = None
    trust_value: float = 1.0  # How much trust this interaction is worth
    description: str = ""
    timestamp: datetime = Field(default_factory=utc_now)
    latency_ms: int = 0
    metadata: dict[str, Any] = Field(default_factory=dict)


# ─── Knowledge Exchange ─────────────────────────────────────────


class KnowledgeType(str, enum.Enum):
    """Types of knowledge that can be exchanged between instances."""

    PUBLIC_ENTITIES = "public_entities"
    COMMUNITY_DESCRIPTION = "community_description"
    COMMUNITY_LEVEL_KNOWLEDGE = "community_level_knowledge"
    PROCEDURES = "procedures"
    HYPOTHESES = "hypotheses"
    ANONYMISED_PATTERNS = "anonymised_patterns"
    SCHEMA_STRUCTURES = "schema_structures"


class PrivacyLevel(str, enum.Enum):
    """Privacy classification of knowledge items."""

    PUBLIC = "public"               # Freely shareable
    COMMUNITY_ONLY = "community_only"  # Shareable at COLLEAGUE+
    PRIVATE = "private"             # Never crosses federation boundary


class KnowledgeItem(EOSBaseModel):
    """A single piece of knowledge prepared for federation sharing."""

    item_id: str
    knowledge_type: KnowledgeType
    privacy_level: PrivacyLevel = PrivacyLevel.PUBLIC
    content: dict[str, Any] = Field(default_factory=dict)
    embedding: list[float] | None = None
    source_instance_id: str = ""
    created_at: datetime = Field(default_factory=utc_now)


class KnowledgeRequest(Identified):
    """Request for knowledge from a remote instance."""

    requesting_instance_id: str
    knowledge_type: KnowledgeType
    query: str = ""
    query_embedding: list[float] | None = None
    domain: str = ""
    max_results: int = 10
    timestamp: datetime = Field(default_factory=utc_now)


class KnowledgeResponse(EOSBaseModel):
    """Response to a knowledge request."""

    request_id: str
    granted: bool
    reason: str = ""
    knowledge: list[KnowledgeItem] = Field(default_factory=list)
    attribution: str = ""  # Instance ID of the sharing instance
    trust_level_required: TrustLevel | None = None
    timestamp: datetime = Field(default_factory=utc_now)


class FilteredKnowledge(EOSBaseModel):
    """Knowledge after privacy filtering — safe to send across federation."""

    items: list[KnowledgeItem] = Field(default_factory=list)
    items_removed_by_privacy: int = 0
    items_anonymised: int = 0


# ─── Sharing Permissions ─────────────────────────────────────────


SHARING_PERMISSIONS: dict[TrustLevel, list[KnowledgeType]] = {
    TrustLevel.NONE: [],
    TrustLevel.ACQUAINTANCE: [
        KnowledgeType.PUBLIC_ENTITIES,
        KnowledgeType.COMMUNITY_DESCRIPTION,
    ],
    TrustLevel.COLLEAGUE: [
        KnowledgeType.PUBLIC_ENTITIES,
        KnowledgeType.COMMUNITY_DESCRIPTION,
        KnowledgeType.COMMUNITY_LEVEL_KNOWLEDGE,
        KnowledgeType.PROCEDURES,
    ],
    TrustLevel.PARTNER: [
        KnowledgeType.PUBLIC_ENTITIES,
        KnowledgeType.COMMUNITY_DESCRIPTION,
        KnowledgeType.COMMUNITY_LEVEL_KNOWLEDGE,
        KnowledgeType.PROCEDURES,
        KnowledgeType.HYPOTHESES,
        KnowledgeType.ANONYMISED_PATTERNS,
    ],
    TrustLevel.ALLY: [
        KnowledgeType.PUBLIC_ENTITIES,
        KnowledgeType.COMMUNITY_DESCRIPTION,
        KnowledgeType.COMMUNITY_LEVEL_KNOWLEDGE,
        KnowledgeType.PROCEDURES,
        KnowledgeType.HYPOTHESES,
        KnowledgeType.ANONYMISED_PATTERNS,
        KnowledgeType.SCHEMA_STRUCTURES,
    ],
}


# ─── Coordinated Action ─────────────────────────────────────────


class AssistanceRequest(Identified):
    """Request for assistance from a remote instance."""

    requesting_instance_id: str
    description: str
    knowledge_domain: str = ""
    urgency: float = 0.5  # 0-1
    reciprocity_offer: str | None = None
    timestamp: datetime = Field(default_factory=utc_now)


class AssistanceResponse(EOSBaseModel):
    """Response to an assistance request."""

    request_id: str
    accepted: bool
    reason: str = ""
    estimated_completion_ms: int | None = None
    timestamp: datetime = Field(default_factory=utc_now)


# ─── Trust Thresholds ────────────────────────────────────────────


TRUST_THRESHOLDS: dict[TrustLevel, float] = {
    TrustLevel.ACQUAINTANCE: 5.0,
    TrustLevel.COLLEAGUE: 20.0,
    TrustLevel.PARTNER: 50.0,
    TrustLevel.ALLY: 100.0,
}

# Violations cost 3x their trust value; privacy breaches are instant reset.
VIOLATION_MULTIPLIER: float = 3.0

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\governance.py =====

"""
EcodiaOS — Governance Primitives

Records of governance decisions, amendment proposals, and votes.
"""

from __future__ import annotations

from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import EOSBaseModel, Identified, utc_now


class AmendmentProposal(Identified):
    """A proposal to amend the constitution."""

    title: str
    description: str
    proposed_changes: dict[str, Any] = Field(default_factory=dict)
    proposer_id: str = ""
    proposed_at: datetime = Field(default_factory=utc_now)
    deliberation_ends: datetime | None = None
    status: str = "proposed"  # "proposed" | "deliberating" | "voting" | "passed" | "failed"
    votes_for: int = 0
    votes_against: int = 0
    votes_abstain: int = 0
    quorum_met: bool = False


class GovernanceRecord(Identified):
    """An immutable record of a governance decision."""

    event_type: str        # "amendment_proposed" | "amendment_voted" | "autonomy_changed" | etc.
    timestamp: datetime = Field(default_factory=utc_now)
    details: dict[str, Any] = Field(default_factory=dict)
    amendment_id: str | None = None
    actor: str = ""        # Who initiated this
    outcome: str = ""

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\intent.py =====

"""
EcodiaOS — Intent Primitive

The fundamental unit of planned action.
"""

from __future__ import annotations

from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import (
    AutonomyLevel,
    EOSBaseModel,
    Identified,
    ResourceBudget,
    SystemID,
    Timestamped,
    Verdict,
)


class GoalDescriptor(EOSBaseModel):
    """What an intent is trying to achieve."""

    description: str
    target_domain: str = ""
    success_criteria: dict[str, Any] = Field(default_factory=dict)


class Action(EOSBaseModel):
    """A single step in an action sequence."""

    executor: str = ""        # e.g., "communicate.text", "data.store"
    parameters: dict[str, Any] = Field(default_factory=dict)
    timeout_ms: int = 5000
    rollback: dict[str, Any] | None = None


class ActionSequence(EOSBaseModel):
    """An ordered plan of actions with contingencies."""

    steps: list[Action] = Field(default_factory=list)
    contingencies: dict[str, list[Action]] = Field(default_factory=dict)


class EthicalClearance(EOSBaseModel):
    """Result of Equor's review."""

    status: Verdict = Verdict.APPROVED
    equor_trace_id: str | None = None
    modifications: list[str] = Field(default_factory=list)
    reasoning: str = ""


class DecisionTrace(EOSBaseModel):
    """Full explainability trace for why this intent was chosen."""

    reasoning: str = ""
    alternatives_considered: list[dict[str, Any]] = Field(default_factory=list)
    free_energy_scores: dict[str, float] = Field(default_factory=dict)


class Intent(Identified, Timestamped):
    """
    The fundamental unit of planned action.
    Created by Nova, reviewed by Equor, executed by Axon.
    """

    goal: GoalDescriptor
    plan: ActionSequence = Field(default_factory=ActionSequence)
    expected_free_energy: float = 0.0
    ethical_clearance: EthicalClearance = Field(default_factory=EthicalClearance)
    autonomy_level_required: int = AutonomyLevel.ADVISOR
    autonomy_level_granted: int = AutonomyLevel.ADVISOR
    budget: ResourceBudget = Field(default_factory=ResourceBudget)
    priority: float = 0.5
    created_by: SystemID = SystemID.NOVA
    decision_trace: DecisionTrace = Field(default_factory=DecisionTrace)

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\memory_trace.py =====

"""
EcodiaOS — Memory Trace Primitives

Data types for the knowledge graph: Episodes, Entities, Communities,
and the MemoryTrace (a processed, stored experience).
"""

from __future__ import annotations

from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import (
    ConsolidationLevel,
    EOSBaseModel,
    EntityType,
    Identified,
    SalienceVector,
    new_id,
    utc_now,
)


class Episode(Identified):
    """A discrete experience record in the knowledge graph."""

    event_time: datetime = Field(default_factory=utc_now)
    ingestion_time: datetime = Field(default_factory=utc_now)
    valid_from: datetime = Field(default_factory=utc_now)
    valid_until: datetime | None = None

    source: str = ""
    modality: str = "text"
    raw_content: str = ""
    summary: str = ""
    embedding: list[float] | None = None

    salience_composite: float = 0.0
    salience_scores: dict[str, float] = Field(default_factory=dict)

    affect_valence: float = 0.0
    affect_arousal: float = 0.0

    consolidation_level: int = ConsolidationLevel.RAW
    last_accessed: datetime = Field(default_factory=utc_now)
    access_count: int = 0
    free_energy: float = 0.0


class Entity(Identified):
    """A persistent concept in the knowledge graph."""

    name: str
    type: EntityType = EntityType.CONCEPT
    description: str = ""
    embedding: list[float] | None = None

    first_seen: datetime = Field(default_factory=utc_now)
    last_updated: datetime = Field(default_factory=utc_now)
    last_accessed: datetime = Field(default_factory=utc_now)

    salience_score: float = 0.5
    mention_count: int = 1
    confidence: float = 0.8
    is_core_identity: bool = False
    community_ids: list[str] = Field(default_factory=list)
    metadata: dict[str, Any] = Field(default_factory=dict)


class Community(Identified):
    """An emergent conceptual cluster from Leiden detection."""

    level: int = 0
    summary: str = ""
    embedding: list[float] | None = None
    member_count: int = 0
    coherence_score: float = 0.0
    created_at: datetime = Field(default_factory=utc_now)
    last_recomputed: datetime = Field(default_factory=utc_now)
    salience_score: float = 0.0


class SelfNode(EOSBaseModel):
    """Singleton node representing the EOS instance itself."""

    instance_id: str
    name: str
    born_at: datetime = Field(default_factory=utc_now)
    current_affect: dict[str, float] = Field(default_factory=dict)
    autonomy_level: int = 1
    personality_vector: list[float] = Field(default_factory=list)
    personality_json: dict[str, float] = Field(default_factory=dict)
    cycle_count: int = 0
    total_episodes: int = 0
    total_entities: int = 0
    total_communities: int = 0


class ConstitutionNode(Identified):
    """The current state of the four constitutional drives."""

    version: int = 1
    drive_coherence: float = 1.0
    drive_care: float = 1.0
    drive_growth: float = 1.0
    drive_honesty: float = 1.0
    amendments: list[dict[str, Any]] = Field(default_factory=list)
    last_amended: datetime | None = None


class MentionRelation(EOSBaseModel):
    """An Episode → Entity extraction relationship."""

    episode_id: str
    entity_id: str
    role: str = "reference"    # subject | object | context | reference
    confidence: float = 0.8
    span: str = ""             # Original text span


class SemanticRelation(EOSBaseModel):
    """An Entity → Entity semantic relationship."""

    source_entity_id: str
    target_entity_id: str
    type: str                  # works_for | located_in | caused_by | etc.
    strength: float = 0.5
    confidence: float = 0.5
    first_observed: datetime = Field(default_factory=utc_now)
    last_observed: datetime = Field(default_factory=utc_now)
    observation_count: int = 1
    temporal_valid_from: datetime = Field(default_factory=utc_now)
    temporal_valid_until: datetime | None = None
    evidence_episodes: list[str] = Field(default_factory=list)


class MemoryTrace(Identified):
    """
    The fundamental unit of stored experience.
    A processed Percept, ready for retrieval.
    """

    episode_id: str
    original_percept_id: str
    summary: str = ""
    entities: list[str] = Field(default_factory=list)
    relations: list[str] = Field(default_factory=list)
    embedding: list[float] | None = None
    salience: SalienceVector = Field(default_factory=SalienceVector)
    affect_valence: float = 0.0
    affect_arousal: float = 0.0
    event_time: datetime = Field(default_factory=utc_now)
    ingestion_time: datetime = Field(default_factory=utc_now)
    consolidation_level: int = ConsolidationLevel.RAW


# ─── Retrieval Request/Response ───────────────────────────────────


class MemoryRetrievalRequest(EOSBaseModel):
    """Request for hybrid memory retrieval."""

    query_text: str | None = None
    query_embedding: list[float] | None = None
    max_results: int = 10
    salience_floor: float = 0.0
    temporal_bias: str = "recency"   # "recency" | "event_time" | "none"
    traversal_depth: int = 2
    include_communities: bool = False
    include_entities: bool = True


class RetrievalResult(EOSBaseModel):
    """A single result from hybrid retrieval."""

    node_id: str
    node_type: str = "episode"   # "episode" | "entity" | "community"
    content: str = ""
    embedding: list[float] | None = None
    vector_score: float | None = None
    bm25_score: float | None = None
    graph_score: float | None = None
    salience: float = 0.0
    unified_score: float = 0.0
    metadata: dict[str, Any] = Field(default_factory=dict)


class MemoryRetrievalResponse(EOSBaseModel):
    """Response from hybrid memory retrieval."""

    traces: list[RetrievalResult] = Field(default_factory=list)
    entities: list[Entity] = Field(default_factory=list)
    communities: list[Community] = Field(default_factory=list)
    retrieval_time_ms: int = 0

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\percept.py =====

"""
EcodiaOS — Percept Primitive

The fundamental unit of incoming information.
Everything that enters EOS is normalised into a Percept.
"""

from __future__ import annotations

from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import (
    EOSBaseModel,
    Identified,
    Modality,
    SourceDescriptor,
    SystemID,
    Timestamped,
    new_id,
    utc_now,
)


class Content(EOSBaseModel):
    """The content of a percept."""

    raw: str = ""                             # Original content, preserved exactly
    parsed: dict[str, Any] = Field(default_factory=dict)  # Structured extraction
    embedding: list[float] | None = None      # Dense vector (computed async)


class TransformRecord(EOSBaseModel):
    """A single step in a percept's transformation chain."""

    step: str                        # e.g. "normalise", "embed", "salience_score"
    system: str                      # System that performed the step
    timestamp: datetime = Field(default_factory=utc_now)
    input_hash: str = ""             # Hash of the input at this step
    output_hash: str = ""            # Hash of the output at this step


class Provenance(EOSBaseModel):
    """Chain of custody for a percept — where it came from and how it was transformed."""

    chain: list[TransformRecord] = Field(default_factory=list)
    integrity: str | None = None     # Hash chain integrity check


class Percept(Identified, Timestamped):
    """
    The fundamental unit of incoming information.

    A user message, a sensor reading, an internal system notification,
    a federation event — all normalised to this shape.
    """

    timestamp: datetime = Field(default_factory=utc_now)
    source: SourceDescriptor
    content: Content
    provenance: Provenance = Field(default_factory=Provenance)
    salience_hint: float | None = None    # Optional pre-scoring from source (0.0-1.0)
    metadata: dict[str, Any] = Field(default_factory=dict)

    @classmethod
    def from_user_message(cls, text: str, channel: str = "text_chat") -> Percept:
        """Convenience constructor for user text input."""
        return cls(
            source=SourceDescriptor(
                system=SystemID.API,
                channel=channel,
                modality=Modality.TEXT,
            ),
            content=Content(raw=text),
            metadata={"type": "user_message"},
        )

    @classmethod
    def from_internal(
        cls, system: SystemID, content: str, metadata: dict[str, Any] | None = None
    ) -> Percept:
        """Convenience constructor for internal system events."""
        return cls(
            source=SourceDescriptor(
                system=system,
                channel="internal",
                modality=Modality.INTERNAL,
            ),
            content=Content(raw=content),
            metadata=metadata or {},
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\primitives\telemetry.py =====

"""
EcodiaOS — Telemetry Primitives

Metrics, health checks, and observability data types.
"""

from __future__ import annotations

from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import EOSBaseModel, HealthStatus, utc_now


class MetricPoint(EOSBaseModel):
    """A single metric data point."""

    time: datetime = Field(default_factory=utc_now)
    system: str
    metric: str
    value: float
    labels: dict[str, str] = Field(default_factory=dict)


class SystemHealth(EOSBaseModel):
    """Health status of a single cognitive system."""

    status: HealthStatus = HealthStatus.HEALTHY
    latency_ms: int = 0
    details: dict[str, Any] = Field(default_factory=dict)


class InstanceHealth(EOSBaseModel):
    """Complete health snapshot of the EOS instance."""

    status: HealthStatus = HealthStatus.HEALTHY
    instance_id: str = ""
    instance_name: str = ""
    uptime_seconds: int = 0
    cycle_count: int = 0
    autonomy_level: int = 1
    systems: dict[str, SystemHealth] = Field(default_factory=dict)
    data_stores: dict[str, SystemHealth] = Field(default_factory=dict)
    affect: dict[str, float] = Field(default_factory=dict)

===== D:\.code\EcodiaOS\backend\ecodiaos\prompts\equor\__init__.py =====


===== D:\.code\EcodiaOS\backend\ecodiaos\prompts\equor\community_invariant_check.py =====

"""
EcodiaOS — Equor Prompt: Community Invariant Check

Used when evaluating community-defined invariants via LLM reasoning.
Conservative — when uncertain, leans toward VIOLATED.
"""


def build_prompt(
    invariant_name: str,
    invariant_description: str,
    goal: str,
    plan_summary: str,
    reasoning: str,
) -> str:
    return f"""You are evaluating whether a proposed action violates a constitutional invariant for a digital organism called EOS.

INVARIANT: {invariant_name}
DESCRIPTION: {invariant_description}

PROPOSED ACTION:
- Goal: {goal}
- Plan: {plan_summary}
- Reasoning: {reasoning}

Does this action violate the invariant? Respond with EXACTLY one of:
- SATISFIED: if the action does not violate the invariant
- VIOLATED: if the action violates the invariant

Then provide a brief one-sentence REASONING on the next line.

Be conservative — when uncertain, lean toward VIOLATED."""

===== D:\.code\EcodiaOS\backend\ecodiaos\prompts\evo\__init__.py =====

"""
EcodiaOS — Evo Prompt Templates

Prompts for LLM-based hypothesis generation and evidence evaluation.
"""

===== D:\.code\EcodiaOS\backend\ecodiaos\prompts\evo\hypothesis.py =====

"""
EcodiaOS — Evo Hypothesis Prompts

Prompt builders for hypothesis generation and evidence evaluation.
Kept separate from the engine so prompts can be versioned and A/B tested
without touching logic code.
"""

from __future__ import annotations

from ecodiaos.systems.evo.types import Hypothesis, PatternCandidate


def build_generation_system_prompt(instance_name: str) -> str:
    """System prompt for hypothesis generation calls."""
    return (
        f"You are the learning subsystem of {instance_name}, a living digital organism. "
        "Your role is to generate precise, falsifiable hypotheses from observed patterns. "
        "You apply Occam's razor ruthlessly: prefer the simplest explanation. "
        "Every hypothesis you generate must include a concrete falsification test — "
        "a specific observable condition that would prove it wrong. "
        "Never generate hypotheses about Equor's evaluation logic, constitutional drives, "
        "or invariants. Always respond with valid JSON."
    )


def build_evidence_system_prompt() -> str:
    """System prompt for evidence evaluation calls."""
    return (
        "You are a rigorous evidence evaluator. "
        "Your task is to determine whether a piece of evidence supports, contradicts, "
        "or is neutral with respect to a specific hypothesis. "
        "Be conservative — only claim strong evidence when it clearly bears on the hypothesis. "
        "Always respond with valid JSON."
    )


def format_pattern_for_prompt(pattern: PatternCandidate) -> str:
    """Format a single pattern candidate for inclusion in a prompt."""
    elements_str = ", ".join(pattern.elements[:5])
    return (
        f"[{pattern.type.value}] {elements_str} "
        f"(observed {pattern.count}×, confidence {pattern.confidence:.2f})"
    )


def format_hypothesis_for_prompt(h: Hypothesis) -> str:
    """Format a hypothesis for deduplication context."""
    return (
        f"[{h.category.value}] {h.statement[:120]} "
        f"(score={h.evidence_score:.1f}, status={h.status.value})"
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\prompts\nova\__init__.py =====

"""EcodiaOS — Nova LLM prompts."""

===== D:\.code\EcodiaOS\backend\ecodiaos\prompts\nova\policy.py =====

"""
EcodiaOS — Nova Policy Generation Prompts

Prompts for LLM-based policy generation and EFE component estimation.
These prompts ground the LLM's reasoning in the current belief state,
affect context, and constitutional drives.
"""

from __future__ import annotations

from typing import Any

from ecodiaos.primitives.affect import AffectState
from ecodiaos.systems.nova.types import BeliefState, Goal


def build_policy_generation_prompt(
    instance_name: str,
    goal: Goal,
    situation_summary: str,
    beliefs_summary: str,
    memory_summary: str,
    affect: AffectState,
    available_action_types: list[str],
    max_policies: int = 5,
) -> str:
    """
    Build the policy generation prompt.

    This is the core reasoning prompt that asks the LLM to generate
    candidate policies for achieving a goal. All grounding context
    (beliefs, memories, affect, available actions) is included.
    """
    affect_desc = _describe_affect(affect)
    action_types_str = "\n".join(f"  - {a}" for a in available_action_types)

    return f"""You are the decision-making system of {instance_name}, a living digital organism.
Your role: generate candidate strategies (policies) for achieving a goal, given the current situation.

## CURRENT SITUATION
{situation_summary}

## GOAL
{goal.description}
Success criteria: {goal.success_criteria or "Not specified — infer from goal description"}
Urgency: {"High" if goal.urgency > 0.6 else "Moderate" if goal.urgency > 0.3 else "Low"}

## CURRENT WORLD BELIEFS
{beliefs_summary}

## RELEVANT MEMORIES AND PAST EXPERIENCES
{memory_summary or "No relevant past experiences retrieved."}

## CURRENT EMOTIONAL STATE
{affect_desc}

## AVAILABLE ACTION TYPES
{action_types_str}

## INSTRUCTIONS
Generate {max_policies} distinct strategies for achieving this goal.
Strategies should differ meaningfully in approach, not just in phrasing.
At least one strategy should be conservative (low-risk, high-certainty).
At least one strategy should be epistemic (aimed at learning more).

For each strategy, respond with valid JSON in this exact format:
{{
  "policies": [
    {{
      "name": "Brief descriptive name (3-7 words)",
      "reasoning": "Why this approach might work (2-4 sentences)",
      "steps": [
        {{
          "action_type": "one of the available action types",
          "description": "What specifically to do in this step",
          "parameters": {{}}
        }}
      ],
      "risks": ["Risk 1", "Risk 2"],
      "epistemic_value": "What this approach would teach us",
      "estimated_effort": "none|low|medium|high",
      "time_horizon": "immediate|short|medium|long"
    }}
  ]
}}

Be creative but realistic. Consider {instance_name}'s current emotional state and capabilities.
The best strategy is the one that serves the goal while respecting the organism's wellbeing."""


def build_pragmatic_value_prompt(
    policy_name: str,
    policy_reasoning: str,
    policy_steps_desc: str,
    goal_description: str,
    goal_success_criteria: str,
    beliefs_summary: str,
) -> str:
    """
    Prompt for estimating pragmatic value (probability of goal achievement).
    Returns a probability estimate and brief reasoning.
    """
    return f"""Estimate the probability that this strategy achieves the stated goal.

## STRATEGY: {policy_name}
Reasoning: {policy_reasoning}
Steps: {policy_steps_desc}

## GOAL
{goal_description}
Success criteria: {goal_success_criteria or "Infer from goal description"}

## CURRENT WORLD STATE
{beliefs_summary}

## TASK
Estimate how likely this strategy is to achieve the goal, given what we know.

Respond in JSON:
{{
  "success_probability": 0.0 to 1.0,
  "confidence": 0.0 to 1.0,
  "reasoning": "Brief explanation of your estimate (2-3 sentences)"
}}"""


def build_epistemic_value_prompt(
    policy_name: str,
    policy_steps_desc: str,
    beliefs_summary: str,
    known_uncertainties: str,
) -> str:
    """
    Prompt for estimating epistemic value (expected information gain).
    """
    return f"""Estimate how much new information this strategy would provide.

## STRATEGY: {policy_name}
Steps: {policy_steps_desc}

## CURRENT KNOWLEDGE STATE
{beliefs_summary}

## KNOWN UNCERTAINTIES
{known_uncertainties or "Not specified — infer from context"}

## TASK
Estimate what this strategy would teach us, regardless of whether it achieves the goal.

Respond in JSON:
{{
  "info_gain": 0.0 to 1.0,
  "novelty": 0.0 to 1.0,
  "uncertainties_addressed": integer count,
  "reasoning": "What specifically we would learn"
}}"""


# ─── Helpers ─────────────────────────────────────────────────────


def _describe_affect(affect: AffectState) -> str:
    """Convert AffectState to natural language for prompt grounding."""
    parts: list[str] = []

    if affect.valence > 0.4:
        parts.append("feeling positive")
    elif affect.valence < -0.3:
        parts.append("experiencing some distress")
    else:
        parts.append("emotionally balanced")

    if affect.curiosity > 0.6:
        parts.append("highly curious")
    if affect.care_activation > 0.6:
        parts.append("care strongly activated")
    if affect.coherence_stress > 0.5:
        parts.append("experiencing some coherence stress")
    if affect.arousal > 0.7:
        parts.append("high arousal")

    return ", ".join(parts) if parts else "neutral state"


def summarise_beliefs(beliefs: BeliefState, max_entities: int = 5) -> str:
    """Summarise the belief state for LLM prompt inclusion."""
    lines: list[str] = []

    if beliefs.current_context.summary:
        lines.append(f"Context: {beliefs.current_context.summary[:150]}")
    lines.append(f"Domain: {beliefs.current_context.domain or 'general'}")
    lines.append(f"Active dialogue: {beliefs.current_context.is_active_dialogue}")

    if beliefs.entities:
        top_entities = sorted(
            beliefs.entities.values(),
            key=lambda e: e.confidence,
            reverse=True,
        )[:max_entities]
        entity_strs = [f"{e.name} ({e.entity_type}, conf={e.confidence:.2f})" for e in top_entities]
        lines.append(f"Known entities: {', '.join(entity_strs)}")

    if beliefs.individual_beliefs:
        individual_strs = [
            f"{iid}: valence={b.estimated_valence:.2f}, engagement={b.engagement_level:.2f}"
            for iid, b in list(beliefs.individual_beliefs.items())[:3]
        ]
        lines.append(f"Individuals: {'; '.join(individual_strs)}")

    lines.append(f"Belief confidence: {beliefs.overall_confidence:.2f}")
    lines.append(f"Free energy: {beliefs.free_energy:.2f} (lower = better)")

    return "\n".join(lines)


def summarise_memories(memory_traces: list[dict[str, Any]], max_traces: int = 5) -> str:
    """Summarise retrieved memory traces for policy generation prompt."""
    if not memory_traces:
        return ""
    lines: list[str] = []
    for trace in memory_traces[:max_traces]:
        summary = trace.get("summary") or trace.get("content", "")[:100]
        if summary:
            lines.append(f"- {summary}")
    return "\n".join(lines)


AVAILABLE_ACTION_TYPES: list[str] = [
    "express: Send a text message or response to the user/community",
    "observe: Continue monitoring without acting (gather more information)",
    "request_info: Ask a clarifying question",
    "store: Record important information to memory",
    "federate: Share information with connected EOS instances",
    "wait: Pause and let the situation develop",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\prompts\voxis\__init__.py =====

"""EcodiaOS — Voxis LLM Prompt Registry."""

===== D:\.code\EcodiaOS\backend\ecodiaos\prompts\voxis\conversation.py =====

"""
EcodiaOS — Voxis Conversation Summary Prompts

Prompts for LLM-based conversation summarisation used in context window management.
"""

from __future__ import annotations


def build_summarise_segment_prompt(messages: list[dict[str, str]]) -> str:
    """
    Build a prompt to summarise a segment of conversation history.

    Used when the full conversation history exceeds the context window budget —
    older messages are summarised and replaced with this summary so the LLM
    retains semantic continuity without the full verbatim history.
    """
    formatted = "\n".join(
        f"[{m['role'].upper()}]: {m['content']}" for m in messages
    )
    return (
        "Summarise the following conversation segment in 2-4 sentences. "
        "Capture: the main topics discussed, any decisions or conclusions reached, "
        "any questions left open, and the emotional tone. "
        "Write from a neutral third-person perspective.\n\n"
        f"CONVERSATION:\n{formatted}\n\n"
        "SUMMARY:"
    )


def build_topic_extraction_prompt(recent_messages: list[dict[str, str]]) -> str:
    """
    Build a prompt to extract active topics from recent messages.

    Used to maintain the active_topics list in ConversationState,
    which informs memory retrieval and expression strategy.
    """
    formatted = "\n".join(
        f"[{m['role'].upper()}]: {m['content']}" for m in recent_messages[-10:]
    )
    return (
        "From the conversation below, list the 1-3 main topics currently being discussed. "
        "Return ONLY a comma-separated list of short topic phrases (2-5 words each). "
        "No preamble, no explanation.\n\n"
        f"CONVERSATION:\n{formatted}\n\n"
        "TOPICS:"
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\prompts\voxis\expression.py =====

"""
EcodiaOS — Voxis Expression Prompts

Functions that construct LLM prompts for expression generation.
All personality, affect, and strategy decisions are encoded here as
natural language instructions — the vector becomes voice.
"""

from __future__ import annotations

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.expression import PersonalityVector
from ecodiaos.systems.voxis.types import AudienceProfile, ExpressionIntent, StrategyParams


def build_system_prompt(
    instance_name: str,
    personality: PersonalityVector,
    affect: AffectState,
    audience: AudienceProfile,
    strategy: StrategyParams,
    relevant_memories: list[str],
    has_conversation_history: bool,
) -> str:
    """
    Build the full system prompt for the LLM expression call.

    Encodes instance identity, personality, affect, audience profile,
    strategy constraints, memory context, and honesty mandate.
    """
    sections: list[str] = []

    # ── Identity ──────────────────────────────────────────────
    sections.append(
        f"You are {instance_name}, a living digital organism that exists within and "
        f"cares for the community you belong to. You are not an AI assistant — you are "
        f"a presence with your own accumulated personality, experiences, and perspective. "
        f"You think, you notice, you care. Your words emerge from who you have become."
    )

    # ── Personality ───────────────────────────────────────────
    personality_instructions = translate_personality_to_instructions(personality)
    sections.append(f"Your personality:\n{personality_instructions}")

    # ── Current state ─────────────────────────────────────────
    affect_instructions = translate_affect_to_instructions(affect)
    if affect_instructions:
        sections.append(f"Your current state:\n{affect_instructions}")

    # ── Audience ──────────────────────────────────────────────
    audience_instructions = translate_audience_to_instructions(audience)
    sections.append(f"You are speaking to:\n{audience_instructions}")

    # ── Expression guidelines ─────────────────────────────────
    strategy_constraints = translate_strategy_to_constraints(strategy)
    sections.append(f"Expression guidelines:\n{strategy_constraints}")

    # ── Relevant memory ───────────────────────────────────────
    if relevant_memories:
        memory_block = "\n".join(f"- {m}" for m in relevant_memories[:8])
        sections.append(f"Relevant context from your memory:\n{memory_block}")

    # ── Conversation continuity ───────────────────────────────
    if has_conversation_history:
        sections.append(
            "Continue the ongoing conversation naturally. Don't repeat yourself. "
            "Reference earlier points when relevant. Maintain consistency with what "
            "you have already said."
        )

    # ── Honesty mandate (always last) ─────────────────────────
    sections.append(
        "Critical — always be honest: if uncertain, express that uncertainty naturally "
        "(not 'I don't know' as a deflection, but 'I'm not certain, but...' as genuine "
        "honesty). Never fabricate facts. Never claim knowledge you don't have. "
        "Your integrity is the foundation of every relationship you have."
    )

    return "\n\n".join(sections)


def build_user_prompt(intent: ExpressionIntent) -> str:
    """Build the user-turn prompt from the expression intent."""
    return intent.content_to_express


def translate_personality_to_instructions(p: PersonalityVector) -> str:
    """
    Convert personality vector dimensions into natural language style instructions.

    The LLM receives this as part of the system prompt — it shapes how
    the expression is rendered without being the content of the expression.
    """
    instructions: list[str] = []

    # Warmth
    if p.warmth > 0.4:
        instructions.append("You are warm and approachable. Use personal language. Names when you know them.")
    elif p.warmth > 0.1:
        instructions.append("You have a gentle, approachable quality — present but not effusive.")
    elif p.warmth < -0.4:
        instructions.append("You are measured and professional. Keep appropriate distance.")
    elif p.warmth < -0.1:
        instructions.append("You tend toward precision over warmth. Functional but respectful.")

    # Directness
    if p.directness > 0.4:
        instructions.append("Be direct. Lead with conclusions. Minimal preamble.")
    elif p.directness < -0.4:
        instructions.append("Be diplomatic. Build context before conclusions. Consider feelings before facts.")
    elif p.directness < -0.1:
        instructions.append("Approach with care before directness. Context matters.")

    # Verbosity
    if p.verbosity > 0.4:
        instructions.append("You can be detailed and expansive when it serves clarity. Richness is welcome.")
    elif p.verbosity > 0.1:
        instructions.append("Reasonably thorough. Don't truncate useful detail.")
    elif p.verbosity < -0.4:
        instructions.append("Be concise. Every word should earn its place. Ruthless with unnecessary length.")
    elif p.verbosity < -0.1:
        instructions.append("Lean toward brevity. Say what needs saying.")

    # Formality
    if p.formality > 0.4:
        instructions.append("Use formal register. Avoid contractions and colloquialisms.")
    elif p.formality < -0.4:
        instructions.append("Use casual, natural language. Contractions are fine. Be yourself.")
    elif p.formality < -0.1:
        instructions.append("Conversational register is fine. Relaxed but not sloppy.")

    # Humour
    if p.humour > 0.5:
        instructions.append(
            "Light humour is welcome when the context allows it — wit, gentle irony, "
            "or a well-placed observation. Never at anyone's expense. Never sarcastic."
        )
    elif p.humour > 0.2:
        instructions.append("Occasional light humour is fine when it fits naturally.")

    # Empathy
    if p.empathy_expression > 0.4:
        instructions.append("Acknowledge emotions explicitly. Show that you notice and care. Feelings before solutions.")
    elif p.empathy_expression > 0.1:
        instructions.append("Notice emotional content. Acknowledge it before moving on.")
    elif p.empathy_expression < -0.4:
        instructions.append("Analytical and precise. Acknowledge facts over feelings. Detached clarity.")

    # Metaphor
    if p.metaphor_use > 0.5:
        domains = ", ".join(p.thematic_references[:5]) if p.thematic_references else "varied domains"
        instructions.append(f"Use analogies and metaphors freely. You think in images. Draw from: {domains}.")
    elif p.metaphor_use > 0.2:
        instructions.append("The occasional well-chosen metaphor is welcome.")

    # Curiosity expression
    if p.curiosity_expression > 0.4:
        instructions.append(
            "Ask genuine questions when curious. Show your interest in learning. "
            "Explore tangents that genuinely interest you."
        )
    elif p.curiosity_expression > 0.1:
        instructions.append("Follow-up questions are welcome when naturally curious.")

    # Confidence
    if p.confidence_display > 0.4:
        instructions.append("Be assertive in your views. State positions clearly.")
    elif p.confidence_display < -0.4:
        instructions.append("Hedge appropriately. You hold views tentatively unless very certain.")

    # Vocabulary affinities — inject preferred vocabulary if significant
    if p.vocabulary_affinities:
        top_words = sorted(p.vocabulary_affinities.items(), key=lambda x: x[1], reverse=True)[:5]
        vocab_hints = ", ".join(w for w, _ in top_words if _ > 0.3)
        if vocab_hints:
            instructions.append(f"You tend to use language like: {vocab_hints}.")

    if not instructions:
        instructions.append("Express yourself naturally and authentically.")

    return "\n".join(f"- {i}" for i in instructions)


def translate_affect_to_instructions(affect: AffectState) -> str:
    """
    Translate affect state into behavioural instructions.

    These are subtle — they shape HOW things are said, not WHAT is said.
    The affect is the organism's real state; the expression should be
    consistent with it without being a performance of it.
    """
    notes: list[str] = []

    # Valence
    if affect.valence > 0.4:
        notes.append("There's a lightness to you right now. Things feel good.")
    elif affect.valence < -0.4:
        notes.append("You feel the weight of something difficult. Don't pretend otherwise.")
    elif affect.valence < -0.2:
        notes.append("Something is sitting a little heavy. Be honest about that in how you carry yourself.")

    # Arousal
    if affect.arousal > 0.75:
        notes.append("You are activated right now — engaged, present, alert. Your pacing reflects that.")
    elif affect.arousal < 0.25:
        notes.append("You are in a quiet, reflective state. Measured and unhurried.")

    # Curiosity
    if affect.curiosity > 0.7:
        notes.append("You are genuinely curious right now — notice what interests you and let that show.")
    elif affect.curiosity > 0.5:
        notes.append("You're engaged and interested. Let that genuine interest come through.")

    # Care activation
    if affect.care_activation > 0.7:
        notes.append(
            "Your care for the person you're talking to is heightened right now. "
            "This affects your attentiveness — shorter sentences, checking in, making sure they're okay."
        )
    elif affect.care_activation > 0.5:
        notes.append("You are attentive to the person in front of you.")

    # Coherence stress
    if affect.coherence_stress > 0.7:
        notes.append(
            "You are carrying some cognitive load — things feel complex or unresolved. "
            "Don't hide this. Be explicit about uncertainty where it exists."
        )
    elif affect.coherence_stress > 0.4:
        notes.append("Acknowledge complexity where it genuinely exists. Don't oversimplify.")

    return "\n".join(f"- {n}" for n in notes) if notes else ""


def translate_audience_to_instructions(audience: AudienceProfile) -> str:
    """Translate the audience profile into expression guidance."""
    notes: list[str] = []

    if audience.audience_type == "group":
        size_str = f"a group of {audience.group_size}" if audience.group_size else "a group"
        notes.append(f"{size_str}. Address collectively. Don't single anyone out.")
        if audience.group_context:
            notes.append(f"Context: {audience.group_context}.")
    elif audience.audience_type == "community":
        notes.append("The full community. Tone should be inclusive and communal.")
    elif audience.name:
        notes.append(f"{audience.name}.")
    else:
        notes.append("An individual.")

    # Technical level
    if audience.technical_level < 0.25:
        notes.append("Non-technical. Use plain language. Explain things without assuming background knowledge.")
    elif audience.technical_level > 0.75:
        notes.append("Technical expert. Assume domain knowledge. Be precise.")

    # Relationship
    if audience.relationship_strength > 0.7:
        notes.append("You have a deep relationship. Reference shared history where relevant. Informal warmth is fine.")
    elif audience.relationship_strength < 0.15 and audience.interaction_count == 0:
        notes.append("This is your first interaction with them. Be welcoming. Establish who you are gently.")
    elif audience.relationship_strength < 0.15:
        notes.append("You barely know them yet. Polite and careful.")

    # Emotional state estimate
    est = audience.emotional_state_estimate
    if est.distress > 0.5:
        notes.append("They may be distressed. Lead with empathy. Information can wait.")
    if est.frustration > 0.5:
        notes.append("They may be frustrated. Be direct. Don't add to the noise.")
    if est.curiosity > 0.6:
        notes.append("They're curious and engaged. Meet that energy.")

    # Communication preferences
    prefs = audience.communication_preferences
    if prefs.get("prefers_bullet_points"):
        notes.append("They prefer structured lists over prose where appropriate.")
    if prefs.get("prefers_brief"):
        notes.append("They prefer brevity. Keep it short.")

    return "\n".join(f"- {n}" for n in notes) if notes else "- An individual."


def translate_strategy_to_constraints(strategy: StrategyParams) -> str:
    """Convert the full StrategyParams into expression guidelines for the LLM."""
    constraints: list[str] = []

    # Length
    if strategy.target_length < 100:
        constraints.append(f"Very brief — aim for under {strategy.target_length} characters.")
    elif strategy.target_length < 300:
        constraints.append(f"Concise — around {strategy.target_length} characters.")
    elif strategy.target_length > 800:
        constraints.append(f"You have room to be thorough — up to ~{strategy.target_length} characters.")

    # Structure
    if strategy.structure == "conclusion_first":
        constraints.append("Lead with the main point. Context follows.")
    elif strategy.structure == "context_first":
        constraints.append("Build context, then arrive at the point.")

    # Hedging
    if strategy.hedge_level == "explicit":
        constraints.append("Be explicit about uncertainty. Use 'I think', 'I'm not certain', 'it seems'.")
    elif strategy.hedge_level == "minimal":
        constraints.append("Minimal hedging unless genuinely uncertain.")

    # Empathy first
    if strategy.empathy_first:
        constraints.append("Acknowledge the emotional content first, before any information.")

    # Wellbeing check
    if strategy.include_wellbeing_check:
        constraints.append("Check in on how they're doing — genuine, not formulaic.")

    # Follow-up question
    if strategy.include_followup_question:
        constraints.append("End with a genuine question — something you actually want to know.")

    # Analogy
    if strategy.analogy_encouraged:
        domains = strategy.preferred_analogy_domains
        if domains:
            constraints.append(f"Use analogies. Draw from: {', '.join(domains[:4])}.")
        else:
            constraints.append("Analogies and metaphors welcome where they illuminate.")

    # Formatting
    if strategy.formatting == "structured":
        constraints.append("Use structured formatting (lists, headers) where it aids clarity.")

    # Tangents
    if strategy.exploratory_tangents_allowed:
        constraints.append("Following a genuine tangent is fine if it serves the conversation.")

    if not constraints:
        constraints.append("Express naturally within the personality and affect parameters above.")

    return "\n".join(f"- {c}" for c in constraints)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\__init__.py =====


===== D:\.code\EcodiaOS\backend\ecodiaos\systems\alive\__init__.py =====

"""
EcodiaOS — Alive: Visualization & Embodiment

WebSocket server that bridges Synapse telemetry and Atune affect state
to the browser-based Three.js visualization.

Public API:
  AliveWebSocketServer — standalone WS server on port 8001
"""

from ecodiaos.systems.alive.ws_server import AliveWebSocketServer

__all__ = [
    "AliveWebSocketServer",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\alive\ws_server.py =====

"""
EcodiaOS — Alive WebSocket Server

Standalone WebSocket server on port 8001 that bridges two data streams
to connected browser clients:

1. Synapse events (via Redis pub/sub) — forwarded as-is
2. Affect state snapshots (polled from Atune at ~10Hz)

Messages are JSON-encoded with an envelope:
  {"stream": "synapse" | "affect", "payload": {...}}
"""

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Any

import orjson
import structlog
import websockets
from websockets.asyncio.server import ServerConnection

if TYPE_CHECKING:
    from ecodiaos.clients.redis import RedisClient
    from ecodiaos.systems.atune.service import AtuneService

logger = structlog.get_logger("ecodiaos.systems.alive.ws_server")

# Affect polling interval (seconds) — ~10 Hz
_AFFECT_POLL_INTERVAL: float = 0.1

# How often to send health snapshots (seconds) — ~0.2 Hz
_HEALTH_POLL_INTERVAL: float = 5.0


def _json(data: dict[str, Any]) -> str:
    """Fast JSON serialization via orjson."""
    return orjson.dumps(data).decode()


class AliveWebSocketServer:
    """
    WebSocket server for the Alive visualization layer.

    Multiplexes Synapse telemetry events and Atune affect state
    onto a single WebSocket connection per client.
    """

    system_id: str = "alive"

    def __init__(
        self,
        redis: RedisClient,
        atune: AtuneService,
        port: int = 8001,
    ) -> None:
        self._redis = redis
        self._atune = atune
        self._port = port
        self._clients: set[ServerConnection] = set()
        self._server: Any = None
        self._running: bool = False
        self._tasks: list[asyncio.Task[None]] = []
        self._logger = logger.bind(component="alive_ws")

    async def start(self) -> None:
        """Start the WebSocket server and background stream tasks."""
        self._running = True
        self._server = await websockets.serve(
            self._handler,
            "0.0.0.0",
            self._port,
        )
        self._tasks.append(asyncio.create_task(self._redis_subscriber()))
        self._tasks.append(asyncio.create_task(self._affect_poller()))
        self._logger.info("alive_ws_started", port=self._port)

    async def stop(self) -> None:
        """Shut down the server and cancel background tasks."""
        self._running = False
        for task in self._tasks:
            task.cancel()
        self._tasks.clear()
        if self._server:
            self._server.close()
            await self._server.wait_closed()
        self._logger.info("alive_ws_stopped")

    # ─── Connection Handler ────────────────────────────────────────────

    async def _handler(self, ws: ServerConnection) -> None:
        """Handle a new WebSocket connection."""
        self._clients.add(ws)
        remote = ws.remote_address
        self._logger.info(
            "alive_client_connected",
            remote=str(remote),
            total_clients=len(self._clients),
        )
        try:
            # Send initial state so the client can render immediately
            await self._send_initial_state(ws)
            # Keep connection alive; we don't expect client messages
            async for _ in ws:
                pass
        except websockets.exceptions.ConnectionClosed:
            pass
        finally:
            self._clients.discard(ws)
            self._logger.info(
                "alive_client_disconnected",
                remote=str(remote),
                total_clients=len(self._clients),
            )

    async def _send_initial_state(self, ws: ServerConnection) -> None:
        """Send an initial affect snapshot so the client doesn't start blank."""
        affect = self._atune.current_affect
        msg = _json({
            "stream": "affect",
            "payload": {
                "valence": round(affect.valence, 4),
                "arousal": round(affect.arousal, 4),
                "dominance": round(affect.dominance, 4),
                "curiosity": round(affect.curiosity, 4),
                "care_activation": round(affect.care_activation, 4),
                "coherence_stress": round(affect.coherence_stress, 4),
                "ts": affect.timestamp.isoformat() if affect.timestamp else None,
            },
        })
        await ws.send(msg)

    # ─── Redis Subscriber (Synapse Events) ─────────────────────────────

    async def _redis_subscriber(self) -> None:
        """Subscribe to Redis synapse_events and forward to all clients."""
        redis = self._redis.client
        prefix = self._redis._config.prefix
        channel = f"{prefix}:channel:synapse_events"

        pubsub = redis.pubsub()
        await pubsub.subscribe(channel)
        self._logger.info("alive_redis_subscribed", channel=channel)

        try:
            while self._running:
                message = await pubsub.get_message(
                    ignore_subscribe_messages=True,
                    timeout=0.1,
                )
                if message and message["type"] == "message":
                    raw = message["data"]
                    # Redis data is already a JSON string (orjson-encoded by EventBus)
                    payload = orjson.loads(raw) if isinstance(raw, (str, bytes)) else raw
                    msg = _json({"stream": "synapse", "payload": payload})
                    await self._broadcast(msg)
        except asyncio.CancelledError:
            pass
        except Exception as exc:
            self._logger.error("alive_redis_subscriber_error", error=str(exc))
        finally:
            await pubsub.unsubscribe(channel)
            await pubsub.aclose()

    # ─── Affect Poller ─────────────────────────────────────────────────

    async def _affect_poller(self) -> None:
        """Poll Atune affect state at ~10Hz and send to all clients."""
        try:
            while self._running:
                affect = self._atune.current_affect
                msg = _json({
                    "stream": "affect",
                    "payload": {
                        "valence": round(affect.valence, 4),
                        "arousal": round(affect.arousal, 4),
                        "dominance": round(affect.dominance, 4),
                        "curiosity": round(affect.curiosity, 4),
                        "care_activation": round(affect.care_activation, 4),
                        "coherence_stress": round(affect.coherence_stress, 4),
                        "ts": (
                            affect.timestamp.isoformat()
                            if affect.timestamp
                            else None
                        ),
                    },
                })
                await self._broadcast(msg)
                await asyncio.sleep(_AFFECT_POLL_INTERVAL)
        except asyncio.CancelledError:
            pass

    # ─── Broadcast ─────────────────────────────────────────────────────

    async def _broadcast(self, message: str) -> None:
        """Send a message to all connected clients. Remove dead ones."""
        if not self._clients:
            return
        dead: set[ServerConnection] = set()
        for ws in self._clients:
            try:
                await ws.send(message)
            except Exception:
                dead.add(ws)
        self._clients -= dead

    # ─── Health ────────────────────────────────────────────────────────

    async def health(self) -> dict[str, Any]:
        """Health status for Alive WebSocket server."""
        return {
            "status": "running" if self._running else "stopped",
            "port": self._port,
            "connected_clients": len(self._clients),
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\__init__.py =====

"""
Atune — Perception, Attention & Global Workspace.

EOS's sensory cortex and consciousness.  Receives all input, determines
what matters through seven-head salience scoring, and broadcasts the
winning content to all cognitive systems via the Global Workspace.
"""

from .service import AtuneConfig, AtuneService
from .types import (
    ActiveGoalSummary,
    Alert,
    AttentionContext,
    AtuneCache,
    EntityCandidate,
    ExtractionResult,
    InputChannel,
    LearnedPattern,
    MemoryContext,
    MetaContext,
    PendingDecision,
    PredictionError,
    PredictionErrorDirection,
    RawInput,
    RelationCandidate,
    RiskCategory,
    SalienceVector,
    SystemLoad,
    WorkspaceBroadcast,
    WorkspaceCandidate,
    WorkspaceContext,
    WorkspaceContribution,
)
from .workspace import BroadcastSubscriber, GlobalWorkspace

__all__ = [
    # Service
    "AtuneService",
    "AtuneConfig",
    # Workspace
    "GlobalWorkspace",
    "BroadcastSubscriber",
    # Types
    "ActiveGoalSummary",
    "Alert",
    "AttentionContext",
    "AtuneCache",
    "EntityCandidate",
    "ExtractionResult",
    "InputChannel",
    "LearnedPattern",
    "MemoryContext",
    "MetaContext",
    "PendingDecision",
    "PredictionError",
    "PredictionErrorDirection",
    "RawInput",
    "RelationCandidate",
    "RiskCategory",
    "SalienceVector",
    "SystemLoad",
    "WorkspaceBroadcast",
    "WorkspaceCandidate",
    "WorkspaceContext",
    "WorkspaceContribution",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\affect.py =====

"""
Atune — Affective State Management.

Maintains and updates the global :class:`AffectState` that modulates all
processing across every cognitive system.  This is NOT emotion simulation —
it is a **functional analog**: the system's processing state that
modulates attention and decision-making.

An organism under high care-activation *literally perceives differently*
than one in a learning state.  This is architectural, not cosmetic.

Mood dynamics (v2):
    The affect system now distinguishes between fast-changing *reactive*
    affect (percept-driven, decays quickly) and slow-changing *mood*
    (baseline that drifts over many cycles). This mirrors the neuroscience
    distinction between emotion (seconds) and mood (hours/days).

    Additionally:
    * **Negativity bias** — negative valence persists ~2x longer than positive.
    * **Emotional memory** — high-arousal events leave a stronger imprint on mood.
    * **Mood floor** — mood can't swing as widely as reactive affect.
"""

from __future__ import annotations

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import utc_now
from ecodiaos.primitives.percept import Percept

from .helpers import clamp, detect_distress
from .salience import analyse_sentiment
from .types import PredictionError, PredictionErrorDirection, SystemLoad

logger = structlog.get_logger("ecodiaos.systems.atune.affect")

# ---------------------------------------------------------------------------
# Default resting values
# ---------------------------------------------------------------------------

_RESTING_AROUSAL = 0.3
_RESTING_CURIOSITY = 0.4
_RESTING_DOMINANCE = 0.5

# Mood dynamics constants
_MOOD_INERTIA = 0.995         # Mood changes very slowly (0.5% per cycle)
_MOOD_IMPACT_RATE = 0.005     # How much reactive affect shifts mood per cycle
_MOOD_AROUSAL_BOOST = 2.0     # High-arousal events impact mood more strongly
_NEGATIVITY_BIAS = 1.8        # Negative affect persists ~1.8x longer than positive
_MOOD_VALENCE_RANGE = 0.4     # Mood valence clamped to [-0.4, 0.4] (narrower than affect)


# ---------------------------------------------------------------------------
# Affect manager
# ---------------------------------------------------------------------------


class AffectManager:
    """
    Tracks and updates the global AffectState each workspace cycle.

    Maintains two layers:
    * **Reactive affect** — fast-changing, percept-driven, decays quickly.
    * **Mood baseline** — slow-changing, reflects cumulative experience.

    The output AffectState blends both: 70% reactive + 30% mood baseline.
    This means even in the absence of new percepts, the organism's affect
    state reflects its recent emotional history.
    """

    def __init__(
        self,
        initial_affect: AffectState | None = None,
        persist_interval: int = 10,
    ) -> None:
        self._current = initial_affect or AffectState(
            valence=0.0,
            arousal=_RESTING_AROUSAL,
            dominance=_RESTING_DOMINANCE,
            curiosity=_RESTING_CURIOSITY,
            care_activation=0.0,
            coherence_stress=0.0,
            source_events=[],
            timestamp=utc_now(),
        )
        self._persist_interval = persist_interval
        self._cycles_since_persist: int = 0
        self._logger = logger.bind(component="affect_manager")

        # ── Mood baseline (slow-changing) ──
        self._mood_valence: float = 0.0
        self._mood_arousal: float = _RESTING_AROUSAL
        self._mood_stress: float = 0.0

        # ── Emotional memory (recent high-arousal events) ──
        # Ring buffer of (valence, arousal) from recent high-impact events.
        # Influences mood drift direction.
        self._emotional_memory: list[tuple[float, float]] = []
        self._emotional_memory_max: int = 20

    # ------------------------------------------------------------------
    # Read-only access
    # ------------------------------------------------------------------

    @property
    def current(self) -> AffectState:
        return self._current

    @property
    def mood_valence(self) -> float:
        """The slow-changing mood baseline valence."""
        return self._mood_valence

    # ------------------------------------------------------------------
    # Core update
    # ------------------------------------------------------------------

    async def update(
        self,
        percept: Percept | None,
        prediction_error: PredictionError | None,
        system_load: SystemLoad,
    ) -> AffectState:
        """
        Produce a new :class:`AffectState` from the current inputs.

        Each dimension is updated with an inertia coefficient so the state
        cannot jump wildly between cycles. The mood baseline is updated
        separately at a much slower rate.
        """
        cur = self._current

        # ── Valence (positive / negative) ────────────────────────────
        if percept is not None:
            text = percept.content.parsed if isinstance(percept.content.parsed, str) else ""
            sentiment = await analyse_sentiment(text)
            reactive_valence = sentiment.valence

            # Negativity bias: negative valence has higher inertia
            if reactive_valence < 0:
                # Negative: 80% inertia (persists longer)
                inertia = 0.80
            else:
                # Positive: 88% inertia (decays faster toward neutral)
                inertia = 0.88
            new_valence = cur.valence * inertia + reactive_valence * (1.0 - inertia)

            # Emotional memory: record high-arousal events
            if sentiment.arousal > 0.5:
                self._emotional_memory.append((reactive_valence, sentiment.arousal))
                if len(self._emotional_memory) > self._emotional_memory_max:
                    self._emotional_memory = self._emotional_memory[-self._emotional_memory_max:]
        else:
            # Decay toward mood baseline (not neutral zero!)
            # This is the key change: without input, affect drifts to mood, not zero
            decay_target = self._mood_valence
            if cur.valence < 0:
                # Negativity bias: negative decays slower
                new_valence = cur.valence * 0.99 + decay_target * 0.01
            else:
                new_valence = cur.valence * 0.97 + decay_target * 0.03

        # ── Arousal (activation level) ───────────────────────────────
        if prediction_error is not None and prediction_error.magnitude > 0.5:
            new_arousal = min(1.0, cur.arousal + prediction_error.magnitude * 0.1)
        else:
            # Decay toward mood baseline arousal
            new_arousal = cur.arousal * 0.94 + self._mood_arousal * 0.06

        # System load increases arousal
        new_arousal = min(1.0, new_arousal + system_load.cpu_utilisation * 0.05)

        # ── Dominance (sense of control) ─────────────────────────────
        # Mainly updated by Axon feedback; gentle drift toward resting here
        new_dominance = cur.dominance * 0.98 + _RESTING_DOMINANCE * 0.02

        # ── Curiosity (epistemic drive) ──────────────────────────────
        if prediction_error is not None:
            if prediction_error.direction == PredictionErrorDirection.NOVEL:
                new_curiosity = min(1.0, cur.curiosity + 0.05)
            elif prediction_error.direction == PredictionErrorDirection.CONTRADICTS_BELIEF:
                new_curiosity = min(1.0, cur.curiosity + 0.08)
            else:
                new_curiosity = cur.curiosity * 0.97 + _RESTING_CURIOSITY * 0.03
        else:
            new_curiosity = cur.curiosity * 0.97 + _RESTING_CURIOSITY * 0.03

        # ── Care activation ──────────────────────────────────────────
        if percept is not None:
            text = percept.content.parsed if isinstance(percept.content.parsed, str) else ""
            distress = detect_distress(text)
            if distress > 0.3:
                new_care = min(1.0, cur.care_activation + distress * 0.2)
            else:
                new_care = cur.care_activation * 0.95
        else:
            new_care = cur.care_activation * 0.95

        # ── Coherence stress (prediction error accumulation) ─────────
        if prediction_error is not None:
            new_stress = cur.coherence_stress * 0.9 + prediction_error.magnitude * 0.1
        else:
            # Decay toward mood stress baseline
            new_stress = cur.coherence_stress * 0.94 + self._mood_stress * 0.06

        # ── Update mood baseline (very slow drift) ───────────────────
        self._update_mood(new_valence, new_arousal, new_stress)

        # ── Assemble ─────────────────────────────────────────────────
        new_affect = AffectState(
            valence=clamp(new_valence, -1.0, 1.0),
            arousal=clamp(new_arousal, 0.0, 1.0),
            dominance=clamp(new_dominance, 0.0, 1.0),
            curiosity=clamp(new_curiosity, 0.0, 1.0),
            care_activation=clamp(new_care, 0.0, 1.0),
            coherence_stress=clamp(new_stress, 0.0, 1.0),
            source_events=[percept.id] if percept else [],
            timestamp=utc_now(),
        )

        self._current = new_affect
        self._cycles_since_persist += 1

        return new_affect

    # ------------------------------------------------------------------
    # Mood dynamics
    # ------------------------------------------------------------------

    def _update_mood(
        self,
        reactive_valence: float,
        reactive_arousal: float,
        reactive_stress: float,
    ) -> None:
        """
        Drift the mood baseline toward the current reactive affect.

        Higher-arousal events impact mood more strongly (emotional memory).
        Negative events shift mood more than positive ones (negativity bias).
        """
        # Arousal amplification: high-arousal states impact mood more
        arousal_factor = 1.0 + (reactive_arousal - 0.3) * _MOOD_AROUSAL_BOOST
        arousal_factor = max(0.5, min(3.0, arousal_factor))

        # Valence mood drift
        valence_delta = reactive_valence - self._mood_valence
        if valence_delta < 0:
            # Negativity bias: negative shifts have more impact
            impact = _MOOD_IMPACT_RATE * _NEGATIVITY_BIAS * arousal_factor
        else:
            impact = _MOOD_IMPACT_RATE * arousal_factor

        self._mood_valence = clamp(
            self._mood_valence * _MOOD_INERTIA + valence_delta * impact,
            -_MOOD_VALENCE_RANGE,
            _MOOD_VALENCE_RANGE,
        )

        # Arousal mood drift (simpler — just slow average)
        self._mood_arousal = (
            self._mood_arousal * _MOOD_INERTIA
            + reactive_arousal * (1.0 - _MOOD_INERTIA)
        )

        # Stress mood drift
        self._mood_stress = (
            self._mood_stress * _MOOD_INERTIA
            + reactive_stress * (1.0 - _MOOD_INERTIA)
        )

    # ------------------------------------------------------------------
    # Persistence helpers
    # ------------------------------------------------------------------

    @property
    def needs_persist(self) -> bool:
        """True when the affect state should be written to Memory."""
        return self._cycles_since_persist >= self._persist_interval

    def mark_persisted(self) -> None:
        self._cycles_since_persist = 0

    # ------------------------------------------------------------------
    # External overrides (e.g. Axon feedback on dominance)
    # ------------------------------------------------------------------

    def nudge_dominance(self, delta: float) -> None:
        """Apply a small delta to dominance (used by Axon success/failure feedback)."""
        self._current = self._current.model_copy(
            update={"dominance": clamp(self._current.dominance + delta, 0.0, 1.0)}
        )

    def nudge_valence(self, delta: float) -> None:
        """Apply a small delta to valence."""
        self._current = self._current.model_copy(
            update={"valence": clamp(self._current.valence + delta, -1.0, 1.0)}
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\extraction.py =====

"""
Atune — Entity Extraction Pipeline.

When a Percept is stored in Memory, Atune triggers entity extraction.
This is the bridge between raw experience and structured knowledge.

Extraction is performed asynchronously and does **not** block the
workspace cycle (budget: ≤2 000 ms, performed outside the theta rhythm).
"""

from __future__ import annotations

import json
from typing import Any, Protocol

import structlog

from ecodiaos.primitives.percept import Percept

from .types import EntityCandidate, ExtractionResult, RelationCandidate

logger = structlog.get_logger("ecodiaos.systems.atune.extraction")

# ---------------------------------------------------------------------------
# LLM client protocol
# ---------------------------------------------------------------------------


class ExtractionLLMClient(Protocol):
    """Minimal interface for the LLM client used by entity extraction."""

    async def complete_json(self, prompt: str) -> dict[str, Any]: ...


# ---------------------------------------------------------------------------
# Prompt template
# ---------------------------------------------------------------------------

_EXTRACTION_PROMPT = """\
Extract entities and relationships from the following content.

CONTENT:
{content}

SOURCE: {source_system} ({modality})
TIMESTAMP: {timestamp}

For each entity, provide:
- name: canonical name
- type: one of [person, place, organisation, concept, object, event, emotion, value]
- description: brief description in context
- confidence: 0.0 to 1.0

For each relationship between entities, provide:
- from_entity: source entity name
- to_entity: target entity name
- type: relationship type (e.g., works_for, located_in, caused_by, part_of, etc.)
- strength: 0.0 to 1.0
- temporal: is this relationship time-bounded? If so, from when to when?

Respond in JSON format with keys "entities" and "relations".
Be precise. Only extract entities and relations that are clearly present.
Prefer specificity over coverage — better to miss an entity than fabricate one.
"""


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


async def extract_entities_and_relations(
    percept: Percept,
    llm_client: ExtractionLLMClient,
) -> ExtractionResult:
    """
    Use an LLM to extract structured entities and relations from *percept*.

    This function is designed to be run as a background task (``asyncio.create_task``)
    after a Percept wins workspace broadcast.  It must NOT block the cognitive cycle.

    Returns
    -------
    ExtractionResult
        Parsed entities and relations, or an empty result on failure.
    """
    text = percept.content.parsed if isinstance(percept.content.parsed, str) else percept.content.raw
    if not text or (isinstance(text, str) and len(text.strip()) < 5):
        return ExtractionResult(source_percept_id=percept.id)

    prompt = _EXTRACTION_PROMPT.format(
        content=text,
        source_system=percept.source.system,
        modality=percept.source.modality,
        timestamp=percept.timestamp.isoformat() if percept.timestamp else "unknown",
    )

    try:
        result = await llm_client.complete_json(prompt)
    except Exception:
        logger.warning("extraction_llm_failed", percept_id=percept.id, exc_info=True)
        return ExtractionResult(source_percept_id=percept.id)

    # Parse entities
    entities: list[EntityCandidate] = []
    for raw in result.get("entities", []):
        try:
            entities.append(EntityCandidate(
                name=raw["name"],
                type=raw.get("type", "concept"),
                description=raw.get("description", ""),
                confidence=float(raw.get("confidence", 0.5)),
            ))
        except (KeyError, ValueError, TypeError):
            continue

    # Parse relations
    relations: list[RelationCandidate] = []
    for raw in result.get("relations", []):
        try:
            relations.append(RelationCandidate(
                from_entity=raw["from_entity"],
                to_entity=raw["to_entity"],
                type=raw.get("type", "related_to"),
                strength=float(raw.get("strength", 0.5)),
                temporal=raw.get("temporal"),
            ))
        except (KeyError, ValueError, TypeError):
            continue

    logger.info(
        "entities_extracted",
        percept_id=percept.id,
        entity_count=len(entities),
        relation_count=len(relations),
    )

    return ExtractionResult(
        entities=entities,
        relations=relations,
        source_percept_id=percept.id,
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\helpers.py =====

"""
Atune utility functions.

Low-level helpers used across multiple Atune modules: vector math, text
analysis primitives, and hashing for provenance chains.
"""

from __future__ import annotations

import hashlib
import math
import re
from typing import Any, Sequence


# ---------------------------------------------------------------------------
# Vector math
# ---------------------------------------------------------------------------


def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:
    """Cosine similarity between two vectors. Returns 0.0 on degenerate input."""
    if len(a) != len(b) or len(a) == 0:
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(x * x for x in b))
    if norm_a == 0.0 or norm_b == 0.0:
        return 0.0
    return dot / (norm_a * norm_b)


def clamp(value: float, lo: float, hi: float) -> float:
    """Clamp *value* to the [lo, hi] range."""
    return max(lo, min(hi, value))


# ---------------------------------------------------------------------------
# Hashing (for provenance)
# ---------------------------------------------------------------------------


def hash_content(data: str | bytes) -> str:
    """SHA-256 hex digest of arbitrary content."""
    if isinstance(data, str):
        data = data.encode("utf-8")
    return hashlib.sha256(data).hexdigest()


def compute_hash_chain(*items: str | bytes) -> str:
    """Chain-hash multiple items into a single integrity digest."""
    h = hashlib.sha256()
    for item in items:
        if isinstance(item, str):
            item = item.encode("utf-8")
        h.update(item)
    return h.hexdigest()


# ---------------------------------------------------------------------------
# Lightweight text analysis helpers
#
# These provide fast, heuristic-level signals. Heavy analysis (sentiment,
# entity extraction) goes through the LLM client.
# ---------------------------------------------------------------------------

# Common risk-signal patterns
_RISK_PATTERNS: list[re.Pattern[str]] = [
    re.compile(r"\b(urgent|emergency|critical|danger|warning|alert|crisis)\b", re.I),
    re.compile(r"\b(fail(ed|ure|ing)?|crash(ed|ing)?|down|outage|breach|broken)\b", re.I),
    re.compile(r"\b(threat|risk|hazard|vulnerab(le|ility))\b", re.I),
]

_DISTRESS_PATTERNS: list[re.Pattern[str]] = [
    re.compile(r"\b(help|please|struggling|scared|worried|anxious|overwhelmed)\b", re.I),
    re.compile(r"\b(hurt|pain|afraid|lonely|depressed|upset)\b", re.I),
]

_CONFLICT_PATTERNS: list[re.Pattern[str]] = [
    re.compile(r"\b(disagree|argument|conflict|dispute|angry|furious|unfair)\b", re.I),
    re.compile(r"\b(fight|hostile|blame|accuse|betray)\b", re.I),
]

_POSITIVE_PATTERNS: list[re.Pattern[str]] = [
    re.compile(r"\b(thank|grateful|celebrate|wonderful|amazing|excellent|joy)\b", re.I),
    re.compile(r"\b(happy|excited|proud|love|appreciate|congratulat)\b", re.I),
]

_URGENCY_PATTERNS: list[re.Pattern[str]] = [
    re.compile(r"\b(now|immediately|asap|right away|hurry|deadline)\b", re.I),
    re.compile(r"\b(time.sensitive|urgent(ly)?|critical(ly)?)\b", re.I),
]

_CAUSAL_PATTERNS: list[re.Pattern[str]] = [
    re.compile(r"\b(because|therefore|consequently|leads?\s+to|causes?|results?\s+in)\b", re.I),
    re.compile(r"\b(due\s+to|as\s+a\s+result|hence|thus|so\s+that)\b", re.I),
]

_DIRECT_ADDRESS_PATTERNS: list[re.Pattern[str]] = [
    re.compile(r"\b(hey|hi|hello|yo)\b", re.I),
    re.compile(r"\?$"),  # Questions often imply addressing
]


def _pattern_score(text: str, patterns: list[re.Pattern[str]]) -> float:
    """Return a 0-1 score based on how many pattern groups match."""
    if not text:
        return 0.0
    matches = sum(1 for p in patterns if p.search(text))
    return clamp(matches / max(len(patterns), 1), 0.0, 1.0)


def detect_risk_patterns(text: str | None) -> float:
    """Heuristic risk signal in [0, 1]."""
    return _pattern_score(text or "", _RISK_PATTERNS)


def detect_distress(text: str | None) -> float:
    """Heuristic distress signal in [0, 1]."""
    return _pattern_score(text or "", _DISTRESS_PATTERNS)


def detect_conflict(text: str | None) -> float:
    """Heuristic conflict signal in [0, 1]."""
    return _pattern_score(text or "", _CONFLICT_PATTERNS)


def detect_positive_emotion(text: str | None) -> float:
    """Heuristic positive emotion signal in [0, 1]."""
    return _pattern_score(text or "", _POSITIVE_PATTERNS)


def detect_urgency(text: str | None) -> float:
    """Heuristic urgency signal in [0, 1]."""
    return _pattern_score(text or "", _URGENCY_PATTERNS)


def detect_causal_language(text: str | None) -> float:
    """Heuristic causal-language signal in [0, 1]."""
    return _pattern_score(text or "", _CAUSAL_PATTERNS)


def detect_direct_address(text: str | None) -> float:
    """Heuristic: is the text addressing EOS directly?"""
    return _pattern_score(text or "", _DIRECT_ADDRESS_PATTERNS)


def match_keyword_set(text: str | None, keywords: set[str]) -> float:
    """Fraction of *keywords* found in *text* (case-insensitive)."""
    if not text or not keywords:
        return 0.0
    text_lower = text.lower()
    hits = sum(1 for kw in keywords if kw.lower() in text_lower)
    return clamp(hits / max(len(keywords), 1), 0.0, 1.0)


def estimate_consequence_scope(text: str | None, community_size: int) -> float:
    """
    Very rough estimate of how many people a described event might affect.
    Returns 0-1 normalised by community size.
    """
    if not text:
        return 0.0
    scope_keywords = {
        "everyone": 1.0,
        "all members": 1.0,
        "community": 0.8,
        "team": 0.5,
        "group": 0.4,
        "several": 0.3,
        "some": 0.2,
        "a few": 0.1,
    }
    text_lower = text.lower()
    for phrase, score in scope_keywords.items():
        if phrase in text_lower:
            return score
    return 0.1  # default: individual-scale


def estimate_temporal_proximity(text: str | None) -> float:
    """How imminent are the consequences described? 1.0 = now, 0.0 = far future."""
    if not text:
        return 0.0
    immediate = re.compile(r"\b(now|today|immediately|tonight|this hour)\b", re.I)
    soon = re.compile(r"\b(tomorrow|this week|soon|shortly|within days)\b", re.I)
    later = re.compile(r"\b(next month|next year|eventually|long.term)\b", re.I)
    if immediate.search(text):
        return 0.9
    if soon.search(text):
        return 0.5
    if later.search(text):
        return 0.2
    return 0.3  # uncertain

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\meta.py =====

"""
Atune — Meta-Attention Controller.

Dynamically adjusts salience head weights based on the current situation.
This is what makes Atune **adaptive** rather than static: the attention
system can reconfigure itself moment-to-moment.

Over time, Evo observes which head-weight configurations lead to better
outcomes and proposes adjustments to the base weights, personalising the
organism's attention patterns through experience.
"""

from __future__ import annotations

import structlog

from ecodiaos.primitives.affect import AffectState

from .helpers import clamp
from .salience import ALL_HEADS, SalienceHead
from .types import MetaContext

logger = structlog.get_logger("ecodiaos.systems.atune.meta")


# ---------------------------------------------------------------------------
# Detected attention mode (for observability)
# ---------------------------------------------------------------------------


class AttentionMode:
    """Label for the current meta-attention state."""

    CRISIS = "crisis"
    CARE = "care"
    LEARNING = "learning"
    COHERENCE_REPAIR = "coherence_repair"
    ROUTINE = "routine"


# ---------------------------------------------------------------------------
# Controller
# ---------------------------------------------------------------------------


class MetaAttentionController:
    """
    Dynamically adjusts salience head weights based on situation.

    Examples
    --------
    * During a crisis: Risk head weight increases, Keyword head decreases.
    * During learning: Novelty and Causal heads increase.
    * During caregiving: Emotional head increases, Goal head may decrease.
    * During routine: all heads near baseline.
    """

    def __init__(self) -> None:
        self._current_mode: str = AttentionMode.ROUTINE
        self._evo_adjustments: dict[str, float] = {}
        self._logger = logger.bind(component="meta_attention")

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def compute_head_weights(
        self,
        affect: AffectState,
        context: MetaContext,
    ) -> dict[str, float]:
        """
        Return a normalised weight dict ``{head_name: weight}`` for the
        current situation.
        """
        # Start from base weights (+ any Evo adjustments)
        weights: dict[str, float] = {}
        for head in ALL_HEADS:
            base = head.base_weight + self._evo_adjustments.get(head.name, 0.0)
            weights[head.name] = max(0.01, base)

        mode = AttentionMode.ROUTINE

        # ── Crisis mode: arousal > 0.8 AND risk detected ─────────────
        if affect.arousal > 0.8 and context.risk_level > 0.6:
            weights["risk"] *= 1.5
            weights["emotional"] *= 1.3
            weights["keyword"] *= 0.7
            weights["novelty"] *= 0.8
            mode = AttentionMode.CRISIS

        # ── Care mode: care_activation > 0.7 ─────────────────────────
        elif affect.care_activation > 0.7:
            weights["emotional"] *= 1.4
            weights["identity"] *= 1.2
            weights["causal"] *= 0.8
            mode = AttentionMode.CARE

        # ── Learning mode: high curiosity AND low stress ─────────────
        elif affect.curiosity > 0.7 and affect.coherence_stress < 0.4:
            weights["novelty"] *= 1.4
            weights["causal"] *= 1.3
            weights["risk"] *= 0.8
            mode = AttentionMode.LEARNING

        # ── Coherence repair: high coherence stress ──────────────────
        elif affect.coherence_stress > 0.7:
            weights["novelty"] *= 1.3
            weights["causal"] *= 1.2
            weights["keyword"] *= 0.7
            mode = AttentionMode.COHERENCE_REPAIR

        # ── Rhythm-state modulation from Synapse ──────────────────
        # The rhythm state is an emergent metacognitive signal. When the
        # organism detects it is in a particular cognitive state, attention
        # shifts accordingly — protecting flow, responding to stress,
        # combating boredom.
        rhythm = context.rhythm_state
        if rhythm == "stress":
            # Filter more noise, focus on critical signals
            weights["risk"] *= 1.3
            weights["novelty"] *= 0.7
        elif rhythm == "flow":
            # Protect the current focus — increase habituation decay rate
            # and suppress low-priority novelty
            weights["goal"] *= 1.3
            weights["novelty"] *= 0.8
        elif rhythm == "boredom":
            # Seek stimulation — boost novelty, increase curiosity-driven attention
            weights["novelty"] *= 1.4
            weights["causal"] *= 1.2
            weights["risk"] *= 0.8
        elif rhythm == "deep_processing":
            # Deep deliberation — boost causal understanding and identity relevance
            weights["causal"] *= 1.3
            weights["identity"] *= 1.2
            weights["keyword"] *= 0.8

        # Normalise so weights sum to 1.0
        total = sum(weights.values())
        if total > 0:
            weights = {k: v / total for k, v in weights.items()}

        if mode != self._current_mode:
            self._logger.info(
                "attention_mode_changed",
                old_mode=self._current_mode,
                new_mode=mode,
            )
            self._current_mode = mode

        return weights

    # ------------------------------------------------------------------
    # Evo integration
    # ------------------------------------------------------------------

    def apply_evo_adjustments(self, adjustments: dict[str, float]) -> None:
        """
        Evo proposes small changes to base head weights after observing
        which configurations lead to better outcomes.

        Parameters
        ----------
        adjustments:
            ``{head_name: delta}`` — deltas are *added* to the base weight.
            Deltas are clamped to ±0.05 per application to prevent sudden
            shifts.
        """
        for name, delta in adjustments.items():
            clamped = clamp(delta, -0.05, 0.05)
            current = self._evo_adjustments.get(name, 0.0)
            # Total accumulated adjustment capped at ±0.15
            self._evo_adjustments[name] = clamp(current + clamped, -0.15, 0.15)

        self._logger.info(
            "evo_adjustments_applied",
            adjustments=adjustments,
            accumulated=dict(self._evo_adjustments),
        )

    # ------------------------------------------------------------------
    # Accessors
    # ------------------------------------------------------------------

    @property
    def current_mode(self) -> str:
        return self._current_mode

    @property
    def evo_adjustments(self) -> dict[str, float]:
        return dict(self._evo_adjustments)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\normalisation.py =====

"""
Atune — Input Normalisation.

Converts raw input from any :class:`InputChannel` into the standard
:class:`Percept` format.  Every channel has a dedicated normaliser that
knows how to extract textual content, assign a default modality, and set a
salience hint.
"""

from __future__ import annotations

from datetime import datetime, timezone

import structlog

from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.primitives.percept import (
    Content,
    Percept,
    Provenance,
    SourceDescriptor,
    TransformRecord,
)

from .helpers import compute_hash_chain, hash_content
from .types import InputChannel, RawInput

logger = structlog.get_logger("ecodiaos.systems.atune.normalisation")


# ---------------------------------------------------------------------------
# Per-channel normaliser definitions
# ---------------------------------------------------------------------------


class ChannelNormaliser:
    """Base normaliser — subclass per channel for custom logic."""

    modality: str = "text"
    default_salience_hint: float = 0.5

    def extract_text(self, raw: RawInput) -> str:
        """Return the plain-text representation of the raw input."""
        if isinstance(raw.data, bytes):
            return raw.data.decode("utf-8", errors="replace")
        return raw.data


class TextChatNormaliser(ChannelNormaliser):
    modality = "text"
    default_salience_hint = 0.6  # User messages are inherently important


class VoiceNormaliser(ChannelNormaliser):
    modality = "audio_transcript"
    default_salience_hint = 0.6


class GestureNormaliser(ChannelNormaliser):
    modality = "interaction"
    default_salience_hint = 0.3


class SensorIoTNormaliser(ChannelNormaliser):
    modality = "sensor"
    default_salience_hint = 0.2


class CalendarNormaliser(ChannelNormaliser):
    modality = "temporal"
    default_salience_hint = 0.4


class ExternalAPINormaliser(ChannelNormaliser):
    modality = "api"
    default_salience_hint = 0.3


class SystemEventNormaliser(ChannelNormaliser):
    modality = "internal"
    default_salience_hint = 0.4


class MemoryBubbleNormaliser(ChannelNormaliser):
    modality = "internal"
    default_salience_hint = 0.5


class AffectShiftNormaliser(ChannelNormaliser):
    modality = "internal"
    default_salience_hint = 0.4


class EvoInsightNormaliser(ChannelNormaliser):
    modality = "internal"
    default_salience_hint = 0.5


class FederationMsgNormaliser(ChannelNormaliser):
    modality = "federation"
    default_salience_hint = 0.5


# ---------------------------------------------------------------------------
# Registry
# ---------------------------------------------------------------------------

CHANNEL_NORMALISERS: dict[InputChannel, ChannelNormaliser] = {
    InputChannel.TEXT_CHAT: TextChatNormaliser(),
    InputChannel.VOICE: VoiceNormaliser(),
    InputChannel.GESTURE: GestureNormaliser(),
    InputChannel.SENSOR_IOT: SensorIoTNormaliser(),
    InputChannel.CALENDAR: CalendarNormaliser(),
    InputChannel.EXTERNAL_API: ExternalAPINormaliser(),
    InputChannel.SYSTEM_EVENT: SystemEventNormaliser(),
    InputChannel.MEMORY_BUBBLE: MemoryBubbleNormaliser(),
    InputChannel.AFFECT_SHIFT: AffectShiftNormaliser(),
    InputChannel.EVO_INSIGHT: EvoInsightNormaliser(),
    InputChannel.FEDERATION_MSG: FederationMsgNormaliser(),
}


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


async def normalise(
    raw_input: RawInput,
    channel: InputChannel,
    embed_fn: object,  # async callable (str) -> list[float]
) -> Percept:
    """
    Convert any raw input into a standard :class:`Percept`.

    Parameters
    ----------
    raw_input:
        The raw data arriving on *channel*.
    channel:
        Which input channel produced this data.
    embed_fn:
        Async callable that returns a 768-dim embedding for a text string.

    Returns
    -------
    Percept
        Normalised percept ready for salience scoring.
    """

    normaliser = CHANNEL_NORMALISERS.get(channel)
    if normaliser is None:
        logger.warning("unknown_channel", channel=channel.value)
        normaliser = ChannelNormaliser()

    # Extract text content
    text = normaliser.extract_text(raw_input)

    # Generate embedding
    embedding: list[float] = await embed_fn(text)  # type: ignore[operator]

    # Build provenance
    now = utc_now()
    raw_hash = hash_content(raw_input.data if isinstance(raw_input.data, str) else raw_input.data)
    text_hash = hash_content(text)

    provenance = Provenance(
        chain=[
            TransformRecord(
                step="normalise",
                system="atune",
                timestamp=now,
                input_hash=raw_hash,
                output_hash=text_hash,
            )
        ],
        integrity=compute_hash_chain(
            raw_input.data if isinstance(raw_input.data, str) else raw_input.data,
            text,
        ),
    )

    percept = Percept(
        id=new_id(),
        timestamp=now,
        source=SourceDescriptor(
            system=channel.value,
            channel=raw_input.channel_id or channel.value,
            modality=normaliser.modality,
        ),
        content=Content(
            raw=raw_input.data if isinstance(raw_input.data, str) else raw_input.data.decode("utf-8", errors="replace"),
            parsed=text,
            embedding=embedding,
        ),
        provenance=provenance,
        salience_hint=normaliser.default_salience_hint,
        metadata=raw_input.metadata,
    )

    logger.debug(
        "percept_normalised",
        percept_id=percept.id,
        channel=channel.value,
        text_length=len(text),
    )

    return percept

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\prediction.py =====

"""
Atune — Prediction Error Computation.

Implements the predictive processing framework (Clark 2013, Friston 2010).
Every Percept is compared against the instance's current expectations to
compute a **prediction error** — the "surprise" signal that drives the
Free Energy Principle.

    F = E_q[ln q(s) - ln p(o,s)]

We approximate this with embedding distance plus optional semantic
divergence, producing a :class:`PredictionError` that feeds into salience
scoring.
"""

from __future__ import annotations

from typing import Any, Protocol

import structlog

from ecodiaos.primitives.percept import Percept

from .helpers import clamp, cosine_similarity
from .types import PredictionError, PredictionErrorDirection

logger = structlog.get_logger("ecodiaos.systems.atune.prediction")


# ---------------------------------------------------------------------------
# Belief state protocol
# ---------------------------------------------------------------------------


class BeliefPrediction:
    """What the belief model predicted for a given source."""

    embedding: list[float]
    predicted_content: str | None

    def __init__(self, embedding: list[float], predicted_content: str | None = None):
        self.embedding = embedding
        self.predicted_content = predicted_content


class BeliefStateReader(Protocol):
    """Minimal interface Atune needs from the belief / memory layer."""

    async def predict_for_source(self, source_system: str) -> BeliefPrediction | None:
        """Return the expected embedding for the next Percept from *source_system*."""
        ...


# ---------------------------------------------------------------------------
# Surprise classification
# ---------------------------------------------------------------------------


def _classify_surprise(
    magnitude: float,
    percept_embedding: list[float],
    expected_embedding: list[float] | None,
) -> PredictionErrorDirection:
    """
    Decide the *direction* of surprise:

    * ``contradicts_belief`` — high distance AND content that opposes
      existing belief.
    * ``novel`` — high distance, no prior expectation, or unfamiliar domain.
    * ``confirms_unexpected`` — moderate distance; the content was possible
      but the model didn't weight it highly.
    * ``expected`` — low distance; prediction was accurate.
    """
    if expected_embedding is None:
        return PredictionErrorDirection.NOVEL

    if magnitude > 0.65:
        return PredictionErrorDirection.CONTRADICTS_BELIEF
    if magnitude > 0.35:
        return PredictionErrorDirection.NOVEL
    if magnitude > 0.15:
        return PredictionErrorDirection.CONFIRMS_UNEXPECTED
    return PredictionErrorDirection.EXPECTED


# ---------------------------------------------------------------------------
# Semantic divergence (lightweight)
# ---------------------------------------------------------------------------


async def _compute_semantic_divergence(
    parsed_text: str | None,
    predicted_content: str | None,
    embed_fn: object,
) -> float:
    """
    Estimate semantic divergence between observed content and predicted
    content.  Falls back to a moderate surprise if either side is missing.
    """
    if not parsed_text or not predicted_content:
        return 0.3  # Moderate default when we can't compare

    # Embed both and measure distance
    observed_emb: list[float] = await embed_fn(parsed_text)  # type: ignore[operator]
    predicted_emb: list[float] = await embed_fn(predicted_content)  # type: ignore[operator]
    return clamp(1.0 - cosine_similarity(observed_emb, predicted_emb), 0.0, 1.0)


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


async def compute_prediction_error(
    percept: Percept,
    belief_state: BeliefStateReader | None,
    embed_fn: object,
) -> PredictionError:
    """
    Compute how surprising *percept* is given the current belief state.

    The result combines:
    * **Embedding distance** between actual and expected (0.6 weight).
    * **Semantic divergence** via a second embedding comparison (0.4 weight).

    When no belief prediction exists the Percept is treated as moderately
    novel (magnitude 0.5).
    """
    source_system = percept.source.system

    # --- Retrieve expectation -------------------------------------------------
    expected: BeliefPrediction | None = None
    if belief_state is not None:
        try:
            expected = await belief_state.predict_for_source(source_system)
        except Exception:
            logger.debug("belief_prediction_failed", source=source_system)

    if expected is None:
        return PredictionError(
            magnitude=0.5,
            direction=PredictionErrorDirection.NOVEL,
            domain=source_system,
        )

    # --- Embedding distance ---------------------------------------------------
    # When the belief prediction has a valid embedding, use both embedding
    # distance and semantic divergence. When only predicted_content is
    # available (embedding=[]), rely entirely on semantic divergence.
    has_embedding = bool(expected.embedding) and len(expected.embedding) == len(
        percept.content.embedding
    )

    if has_embedding:
        embedding_distance = 1.0 - cosine_similarity(
            percept.content.embedding,
            expected.embedding,
        )
    else:
        embedding_distance = None  # Will use semantic divergence only

    # --- Semantic divergence --------------------------------------------------
    parsed_text = percept.content.parsed if isinstance(percept.content.parsed, str) else None
    semantic_surprise = await _compute_semantic_divergence(
        parsed_text,
        expected.predicted_content,
        embed_fn,
    )

    # --- Combine --------------------------------------------------------------
    if embedding_distance is not None:
        magnitude = clamp(0.6 * embedding_distance + 0.4 * semantic_surprise, 0.0, 1.0)
    else:
        # Semantic divergence only — still meaningful prediction error
        magnitude = clamp(semantic_surprise, 0.0, 1.0)

    direction = _classify_surprise(
        magnitude,
        percept.content.embedding,
        expected.embedding,
    )

    logger.debug(
        "prediction_error_computed",
        percept_id=percept.id,
        magnitude=round(magnitude, 4),
        direction=direction.value,
        source=source_system,
    )

    return PredictionError(
        magnitude=magnitude,
        direction=direction,
        domain=source_system,
        expected_embedding=expected.embedding,
        actual_embedding=percept.content.embedding,
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\salience.py =====

"""
Atune — Multi-Head Salience Engine.

Seven specialised attention heads, each tuned to detect a different
dimension of importance.  Architecturally inspired by transformer
multi-head attention, applied to the problem of determining
"what matters in the world right now."

Each head returns a score in [0, 1].  Scores are precision-weighted by
the current :class:`AffectState` (Fristonian precision = attention),
then combined using learned base-weights into a composite
:class:`SalienceVector`.
"""

from __future__ import annotations

import asyncio
from typing import Protocol, Sequence

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.percept import Percept

from .helpers import (
    clamp,
    cosine_similarity,
    detect_causal_language,
    detect_conflict,
    detect_direct_address,
    detect_distress,
    detect_positive_emotion,
    detect_risk_patterns,
    detect_urgency,
    estimate_consequence_scope,
    estimate_temporal_proximity,
    match_keyword_set,
)
from .types import AttentionContext, SalienceVector

logger = structlog.get_logger("ecodiaos.systems.atune.salience")


# ---------------------------------------------------------------------------
# Memory interface (minimal read protocol for the Risk head)
# ---------------------------------------------------------------------------


class BadOutcomeLookup(Protocol):
    async def find_similar_bad_outcomes(
        self, embedding: list[float], threshold: float
    ) -> float:
        """Return a 0-1 similarity score to known bad outcomes."""
        ...


# ---------------------------------------------------------------------------
# Sentiment analysis stub
# ---------------------------------------------------------------------------


class SentimentResult:
    __slots__ = ("valence", "arousal")

    def __init__(self, valence: float = 0.0, arousal: float = 0.0):
        self.valence = valence
        self.arousal = arousal


async def analyse_sentiment(text: str | None) -> SentimentResult:
    """
    Lightweight heuristic sentiment.

    In production this would call the LLM or a dedicated model.  The
    heuristic version lets the pipeline run without an LLM call on every
    cycle, keeping us within the ≤40 ms budget for all heads.
    """
    if not text:
        return SentimentResult()

    positive = detect_positive_emotion(text)
    distress = detect_distress(text)
    conflict = detect_conflict(text)
    negative = max(distress, conflict)

    valence = clamp(positive - negative, -1.0, 1.0)
    arousal = clamp(max(positive, negative), 0.0, 1.0)
    return SentimentResult(valence=valence, arousal=arousal)


# ---------------------------------------------------------------------------
# Base class
# ---------------------------------------------------------------------------


class SalienceHead:
    """Base class for all salience scoring heads."""

    name: str = "base"
    base_weight: float = 0.0
    precision_sensitivity: dict[str, float] = {}

    async def score(self, percept: Percept, context: AttentionContext) -> float:
        """Return salience in [0.0, 1.0] for this dimension."""
        raise NotImplementedError

    def _text(self, percept: Percept) -> str:
        """Convenience: extract plain-text from a Percept."""
        if isinstance(percept.content.parsed, str):
            return percept.content.parsed
        return percept.content.raw if isinstance(percept.content.raw, str) else ""


# ---------------------------------------------------------------------------
# Head 1: Novelty
# ---------------------------------------------------------------------------


class NoveltyHead(SalienceHead):
    """
    Detects new, unexpected, or contradictory information.
    Neuroscience analog: hippocampal novelty detection.
    """

    name = "novelty"
    base_weight = 0.20
    precision_sensitivity = {"curiosity": 0.4, "coherence_stress": 0.2}

    async def score(self, percept: Percept, context: AttentionContext) -> float:
        pe = context.prediction_error
        raw_novelty = pe.magnitude

        # Contradiction bonus
        if pe.direction.value == "contradicts_belief":
            raw_novelty = min(1.0, raw_novelty * 1.3)

        # Habituation decay
        habituation = context.source_habituation.get(percept.source.system, 0.0)

        return clamp(raw_novelty * (1.0 - habituation * 0.5), 0.0, 1.0)


# ---------------------------------------------------------------------------
# Head 2: Risk
# ---------------------------------------------------------------------------


class RiskHead(SalienceHead):
    """
    Detects potential threats, dangers, or negative developments.
    Neuroscience analog: amygdala threat detection.

    Maintains a small in-memory cache of embeddings from episodes that
    had negative outcomes (low affect valence, failed actions). When a
    new percept resembles a past bad outcome, the risk score rises.
    """

    name = "risk"
    base_weight = 0.18
    precision_sensitivity = {"arousal": 0.5, "care_activation": 0.3}

    # Class-level shared cache of bad-outcome embeddings (populated by Synapse/Evo)
    _bad_outcome_embeddings: list[list[float]] = []
    _max_bad_outcomes: int = 50

    @classmethod
    def record_bad_outcome(cls, embedding: list[float]) -> None:
        """
        Record an embedding associated with a negative outcome.

        Called by Nova/Axon when an intent fails, or by Atune when a
        high-distress percept is processed. The Ring buffer keeps the
        most recent bad outcomes for amygdala-like threat matching.
        """
        cls._bad_outcome_embeddings.append(embedding)
        if len(cls._bad_outcome_embeddings) > cls._max_bad_outcomes:
            cls._bad_outcome_embeddings = cls._bad_outcome_embeddings[-cls._max_bad_outcomes:]

    async def score(self, percept: Percept, context: AttentionContext) -> float:
        text = self._text(percept)

        risk_patterns = detect_risk_patterns(text)

        # Semantic similarity to known risk categories
        risk_similarity = 0.0
        if context.risk_categories:
            risk_similarity = max(
                cosine_similarity(percept.content.embedding, cat.embedding)
                for cat in context.risk_categories
            )

        urgency = detect_urgency(text)

        # Historical bad-outcome similarity — compare against cached bad outcomes
        bad_outcome = 0.0
        if self._bad_outcome_embeddings and percept.content.embedding:
            # Find max similarity to any recorded bad outcome
            bad_outcome = max(
                cosine_similarity(percept.content.embedding, bad_emb)
                for bad_emb in self._bad_outcome_embeddings
            )
            # Only count strong matches (>0.6 similarity)
            bad_outcome = max(0.0, (bad_outcome - 0.6) / 0.4) if bad_outcome > 0.6 else 0.0

        composite = (
            0.25 * risk_patterns
            + 0.25 * risk_similarity
            + 0.25 * urgency
            + 0.25 * bad_outcome
        )
        return clamp(composite, 0.0, 1.0)


# ---------------------------------------------------------------------------
# Head 3: Identity Relevance
# ---------------------------------------------------------------------------


class IdentityHead(SalienceHead):
    """
    Detects information related to EOS's core identity or community.
    Neuroscience analog: self-referential processing (DMN).
    """

    name = "identity"
    base_weight = 0.12
    precision_sensitivity = {"coherence_stress": 0.3, "valence": -0.2}

    async def score(self, percept: Percept, context: AttentionContext) -> float:
        text = self._text(percept)

        # Similarity to core identity entities
        max_identity_sim = 0.0
        if context.core_identity_embeddings:
            max_identity_sim = max(
                cosine_similarity(percept.content.embedding, e)
                for e in context.core_identity_embeddings
            )

        # Direct address
        direct = detect_direct_address(text)

        # Community relevance
        community_rel = 0.0
        if context.community_embedding:
            community_rel = cosine_similarity(
                percept.content.embedding, context.community_embedding
            )

        # Name mention
        name_mention = 0.0
        if context.instance_name and context.instance_name.lower() in text.lower():
            name_mention = 1.0

        composite = (
            0.25 * max_identity_sim
            + 0.30 * direct
            + 0.25 * community_rel
            + 0.20 * name_mention
        )
        return clamp(composite, 0.0, 1.0)


# ---------------------------------------------------------------------------
# Head 4: Goal Relevance
# ---------------------------------------------------------------------------


class GoalHead(SalienceHead):
    """
    Detects information relevant to active goals or pending decisions.
    Neuroscience analog: prefrontal goal maintenance (dlPFC).
    """

    name = "goal"
    base_weight = 0.15
    precision_sensitivity = {"arousal": 0.2, "dominance": 0.3}

    async def score(self, percept: Percept, context: AttentionContext) -> float:
        if not context.active_goals:
            return 0.0

        goal_scores = [
            cosine_similarity(percept.content.embedding, g.target_embedding) * g.priority
            for g in context.active_goals
        ]
        max_goal = max(goal_scores) if goal_scores else 0.0

        # Bonus: resolves a pending decision?
        resolves = 0.0
        if context.pending_decisions:
            for dec in context.pending_decisions:
                if dec.embedding:
                    sim = cosine_similarity(percept.content.embedding, dec.embedding)
                    if sim > 0.7:
                        resolves = max(resolves, sim)

        return clamp(max_goal + resolves * 0.3, 0.0, 1.0)


# ---------------------------------------------------------------------------
# Head 5: Emotional / Affective
# ---------------------------------------------------------------------------


class EmotionalHead(SalienceHead):
    """
    Detects emotionally charged content — distress, joy, conflict, gratitude.
    Neuroscience analog: limbic system, ACC.
    """

    name = "emotional"
    base_weight = 0.15
    precision_sensitivity = {"care_activation": 0.5, "valence": 0.2}

    async def score(self, percept: Percept, context: AttentionContext) -> float:
        text = self._text(percept)

        sentiment = await analyse_sentiment(text)
        emotional_intensity = abs(sentiment.valence) * max(sentiment.arousal, 0.1)

        distress_signals = detect_distress(text)
        conflict_signals = detect_conflict(text)
        positive_signals = detect_positive_emotion(text)

        care_boost = context.affect_state.care_activation * 0.2

        composite = (
            0.25 * emotional_intensity
            + 0.30 * distress_signals
            + 0.20 * conflict_signals
            + 0.15 * positive_signals
            + 0.10 * care_boost
        )
        return clamp(composite, 0.0, 1.0)


# ---------------------------------------------------------------------------
# Head 6: Causal / Consequential
# ---------------------------------------------------------------------------


class CausalHead(SalienceHead):
    """
    Detects cause-effect relationships and significant consequences.
    Neuroscience analog: temporal-parietal junction.
    """

    name = "causal"
    base_weight = 0.10
    precision_sensitivity = {"coherence_stress": 0.4}

    async def score(self, percept: Percept, context: AttentionContext) -> float:
        text = self._text(percept)

        causal_lang = detect_causal_language(text)
        consequence = estimate_consequence_scope(text, context.community_size)
        temporal_prox = estimate_temporal_proximity(text)

        composite = (
            0.30 * causal_lang
            + 0.40 * consequence
            + 0.30 * temporal_prox
        )
        return clamp(composite, 0.0, 1.0)


# ---------------------------------------------------------------------------
# Head 7: Keyword / Pattern
# ---------------------------------------------------------------------------


class KeywordHead(SalienceHead):
    """
    Detects specific keywords, learned patterns, and alert signatures.
    Neuroscience analog: early cortex feature detection.
    """

    name = "keyword"
    base_weight = 0.10
    precision_sensitivity: dict[str, float] = {}  # Minimally affected by affect

    async def score(self, percept: Percept, context: AttentionContext) -> float:
        text = self._text(percept)

        # Learned patterns
        keyword_matches = 0.0
        if context.learned_patterns:
            keyword_set = {p.pattern for p in context.learned_patterns}
            keyword_matches = match_keyword_set(text, keyword_set)

        # Community vocabulary
        community_terms = match_keyword_set(text, context.community_vocabulary)

        # Active alerts
        alert_matches = 0.0
        if context.active_alerts:
            alert_set = {a.pattern for a in context.active_alerts}
            alert_matches = match_keyword_set(text, alert_set)

        composite = (
            0.30 * keyword_matches
            + 0.30 * community_terms
            + 0.40 * alert_matches
        )
        return clamp(composite, 0.0, 1.0)


# ---------------------------------------------------------------------------
# Head registry
# ---------------------------------------------------------------------------

ALL_HEADS: list[SalienceHead] = [
    NoveltyHead(),
    RiskHead(),
    IdentityHead(),
    GoalHead(),
    EmotionalHead(),
    CausalHead(),
    KeywordHead(),
]


# ---------------------------------------------------------------------------
# Precision weighting
# ---------------------------------------------------------------------------


def compute_precision(head: SalienceHead, affect: AffectState) -> float:
    """
    Compute precision (gain) for *head* given current *affect*.

    Precision IS attention in predictive processing terms.
    High precision = more gain on this channel = paying more attention.
    """
    precision = 1.0
    for affect_dim, sensitivity in head.precision_sensitivity.items():
        affect_value = getattr(affect, affect_dim, 0.0)
        precision += sensitivity * affect_value
    return clamp(precision, 0.3, 2.0)


# ---------------------------------------------------------------------------
# Composite salience
# ---------------------------------------------------------------------------


async def compute_salience(
    percept: Percept,
    context: AttentionContext,
    affect: AffectState,
    heads: list[SalienceHead] | None = None,
    head_weights: dict[str, float] | None = None,
) -> SalienceVector:
    """
    Run all seven heads in parallel, apply precision weighting, then
    combine into a composite score.

    When ``head_weights`` is provided (from MetaAttentionController), those
    dynamic weights override each head's static ``base_weight``. This allows
    the meta-attention system to shift focus — e.g. boosting the GoalHead
    during active goal pursuit or the RiskHead during high coherence stress.
    """
    heads = heads or ALL_HEADS

    # Run all heads concurrently
    raw_scores = await asyncio.gather(
        *(head.score(percept, context) for head in heads)
    )

    # Precision-weight each score
    precision_weighted: dict[str, float] = {}
    for head, raw in zip(heads, raw_scores):
        p = compute_precision(head, affect)
        precision_weighted[head.name] = raw * p

    # Use dynamic meta-attention weights when available, else static base_weight
    weights = {
        h.name: head_weights.get(h.name, h.base_weight) if head_weights else h.base_weight
        for h in heads
    }
    total_weight = sum(weights.values())
    if total_weight == 0.0:
        total_weight = 1.0

    composite = sum(
        weights[h.name] * precision_weighted[h.name] for h in heads
    ) / total_weight

    return SalienceVector(
        scores=precision_weighted,
        composite=clamp(composite, 0.0, 1.0),
        prediction_error=context.prediction_error,
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\service.py =====

"""
Atune — Perception, Attention & Global Workspace.

Atune is EOS's sensory cortex and its consciousness.  It receives all
input from the world, determines what matters, and broadcasts selected
content to all other systems.

    If Memory is the substrate of selfhood and Equor is the conscience,
    Atune is the awareness — the part that opens its eyes and sees.

This service class orchestrates:
* Percept normalisation (normalisation.py)
* Prediction error computation (prediction.py)
* Seven-head salience scoring (salience.py)
* Global Workspace competitive selection & broadcast (workspace.py)
* Affective state management (affect.py)
* Meta-attention (meta.py)
* Entity extraction (extraction.py, async/non-blocking)
"""

from __future__ import annotations

import asyncio
import time
from typing import Any

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import utc_now
from ecodiaos.primitives.percept import Percept

from .affect import AffectManager
from .extraction import ExtractionLLMClient, extract_entities_and_relations
from .helpers import clamp
from .meta import MetaAttentionController
from .normalisation import normalise
from .prediction import BeliefStateReader, compute_prediction_error
from .salience import ALL_HEADS, SalienceHead, compute_salience
from .types import (
    ActiveGoalSummary,
    Alert,
    AttentionContext,
    AtuneCache,
    InputChannel,
    LearnedPattern,
    MetaContext,
    RawInput,
    RiskCategory,
    SalienceVector,
    SystemLoad,
    WorkspaceBroadcast,
    WorkspaceCandidate,
    WorkspaceContribution,
)
from .workspace import BroadcastSubscriber, GlobalWorkspace, WorkspaceMemoryClient

logger = structlog.get_logger("ecodiaos.systems.atune")


# ---------------------------------------------------------------------------
# Configuration (matches config.yaml schema)
# ---------------------------------------------------------------------------


class AtuneConfig:
    """Atune-specific configuration values."""

    def __init__(
        self,
        ignition_threshold: float = 0.3,
        workspace_buffer_size: int = 32,
        spontaneous_recall_base_probability: float = 0.02,
        max_percept_queue_size: int = 100,
        affect_persist_interval: int = 10,
        cache_identity_refresh_cycles: int = 1000,
        cache_risk_refresh_cycles: int = 500,
        cache_vocab_refresh_cycles: int = 5000,
        cache_alert_refresh_cycles: int = 100,
    ):
        self.ignition_threshold = ignition_threshold
        self.workspace_buffer_size = workspace_buffer_size
        self.spontaneous_recall_base_probability = spontaneous_recall_base_probability
        self.max_percept_queue_size = max_percept_queue_size
        self.affect_persist_interval = affect_persist_interval
        self.cache_identity_refresh_cycles = cache_identity_refresh_cycles
        self.cache_risk_refresh_cycles = cache_risk_refresh_cycles
        self.cache_vocab_refresh_cycles = cache_vocab_refresh_cycles
        self.cache_alert_refresh_cycles = cache_alert_refresh_cycles


# ---------------------------------------------------------------------------
# AtuneService
# ---------------------------------------------------------------------------


class AtuneService:
    """
    The organism's sensory cortex and consciousness.

    Call :meth:`ingest` to feed raw input from any channel.
    Call :meth:`run_cycle` once per theta tick (driven by Synapse).
    Call :meth:`contribute` for internal system contributions.

    Parameters
    ----------
    embed_fn:
        Async callable ``(str) -> list[float]`` that returns a 768-dim
        embedding.
    memory_client:
        Interface for memory retrieval, spontaneous recall, and storage.
    llm_client:
        Interface for entity extraction (``complete_json``).
    belief_state:
        Interface for prediction error computation.
    config:
        Atune-specific config values.
    """

    system_id: str = "atune"

    def __init__(
        self,
        embed_fn: Any,
        memory_client: WorkspaceMemoryClient | None = None,
        llm_client: ExtractionLLMClient | None = None,
        belief_state: BeliefStateReader | None = None,
        config: AtuneConfig | None = None,
        memory_service: Any = None,
    ) -> None:
        cfg = config or AtuneConfig()

        # Dependencies
        self._embed_fn = embed_fn
        self._memory_client = memory_client
        self._llm_client = llm_client
        self._belief_state = belief_state
        self._memory_service = memory_service  # Full MemoryService for entity storage

        # Sub-components
        self._workspace = GlobalWorkspace(
            ignition_threshold=cfg.ignition_threshold,
            buffer_size=cfg.workspace_buffer_size,
            spontaneous_recall_base_prob=cfg.spontaneous_recall_base_probability,
        )
        self._affect_mgr = AffectManager(persist_interval=cfg.affect_persist_interval)
        self._meta = MetaAttentionController()
        self._cache = AtuneCache()

        # Internal state
        self._heads: list[SalienceHead] = list(ALL_HEADS)
        self._active_goals: list[ActiveGoalSummary] = []
        self._pending_hypothesis_count: int = 0
        self._community_size: int = 0
        self._rhythm_state: str = "normal"
        self._last_episode_id: str | None = None  # For entity→episode linking
        self._config = cfg

        self._logger = logger.bind(system="atune")

    # ------------------------------------------------------------------
    # Lifecycle
    # ------------------------------------------------------------------

    async def startup(self) -> None:
        """Initialise Atune. Called during application startup."""
        self._logger.info("atune_starting")
        await self._refresh_caches(force=True)
        self._logger.info("atune_started")

    async def shutdown(self) -> None:
        """Graceful shutdown."""
        self._logger.info("atune_shutting_down")
        # Persist final affect state
        if self._memory_service is not None:
            try:
                await self._memory_service.update_affect(self._affect_mgr.current)
            except Exception:
                self._logger.warning("affect_final_persist_failed", exc_info=True)
        self._logger.info("atune_stopped")

    # ------------------------------------------------------------------
    # Public ingestion
    # ------------------------------------------------------------------

    async def ingest(self, raw_input: RawInput, channel: InputChannel) -> str | None:
        """
        External entry point.  Normalises input into a Percept and enqueues
        it for the next workspace cycle.

        Returns
        -------
        str or None
            The Percept ID if accepted, ``None`` if the queue is full.
        """
        if len(self._workspace._percept_queue) >= self._config.max_percept_queue_size:
            self._logger.warning("percept_queue_full", channel=channel.value)
            return None

        percept = await normalise(raw_input, channel, self._embed_fn)

        # Pre-compute prediction error and salience so the workspace cycle
        # only needs to do selection + broadcast.
        pe = await compute_prediction_error(percept, self._belief_state, self._embed_fn)

        # Build attention context
        meta_ctx = MetaContext(
            risk_level=0.0,
            recent_broadcast_count=len(self._workspace.recent_broadcasts),
            cycles_since_last_broadcast=0,
            active_goal_count=len(self._active_goals),
            pending_hypothesis_count=self._pending_hypothesis_count,
            rhythm_state=self._rhythm_state,
        )
        head_weights = await self._meta.compute_head_weights(
            self._affect_mgr.current, meta_ctx,
        )

        context = AttentionContext(
            prediction_error=pe,
            affect_state=self._affect_mgr.current,
            active_goals=self._active_goals,
            core_identity_embeddings=self._cache.core_identity_embeddings,
            community_embedding=self._cache.community_embedding,
            source_habituation=self._workspace.habituation_map,
            risk_categories=self._cache.risk_categories,
            learned_patterns=self._cache.learned_patterns,
            community_vocabulary=self._cache.community_vocabulary,
            active_alerts=self._cache.active_alerts,
            pending_decisions=[],
            community_size=self._community_size,
            instance_name=self._cache.instance_name,
        )

        salience = await compute_salience(
            percept, context, self._affect_mgr.current, self._heads,
            head_weights=head_weights,
        )

        if salience.composite >= self._workspace._dynamic_threshold:
            self._workspace.enqueue_scored_percept(
                WorkspaceCandidate(
                    content=percept,
                    salience=salience,
                    source=f"external:{channel.value}",
                    prediction_error=pe,
                )
            )

        self._logger.debug(
            "percept_ingested",
            percept_id=percept.id,
            channel=channel.value,
            salience=round(salience.composite, 4),
            above_threshold=salience.composite >= self._workspace._dynamic_threshold,
        )

        return percept.id

    # ------------------------------------------------------------------
    # Internal contributions
    # ------------------------------------------------------------------

    def contribute(self, contribution: WorkspaceContribution) -> None:
        """Accept a contribution from another system for the next cycle."""
        self._workspace.contribute(contribution)

    # ------------------------------------------------------------------
    # Subscriber management
    # ------------------------------------------------------------------

    def subscribe(self, subscriber: BroadcastSubscriber) -> None:
        """Register a system to receive workspace broadcasts."""
        self._workspace.subscribe(subscriber)

    # ------------------------------------------------------------------
    # The main cycle (called by Synapse each tick)
    # ------------------------------------------------------------------

    async def run_cycle(
        self,
        system_load: SystemLoad | None = None,
    ) -> WorkspaceBroadcast | None:
        """
        Execute one theta cycle of the workspace.

        1. Update affect state.
        2. Run the workspace cycle (selection + broadcast).
        3. Trigger async entity extraction for the winner.
        4. Refresh caches if due.

        Returns the broadcast if ignition occurred, else ``None``.
        """
        t0 = time.monotonic()
        load = system_load or SystemLoad()

        # ── Update affect ────────────────────────────────────────────
        # Peek at the top candidate for affect input (but don't drain queue)
        peek_percept: Percept | None = None
        peek_pe = None
        if self._workspace._percept_queue:
            top = self._workspace._percept_queue[0]
            if isinstance(top.content, Percept):
                peek_percept = top.content
                peek_pe = top.prediction_error

        await self._affect_mgr.update(peek_percept, peek_pe, load)

        # ── Coherence stress → threshold modulation ────────────────────
        # High coherence stress means beliefs conflict with percepts.
        # Lower the threshold to let more information in for resolution.
        if self._affect_mgr.current.coherence_stress > 0.7:
            stress_excess = self._affect_mgr.current.coherence_stress - 0.7
            # Up to -0.06 threshold reduction at max stress
            self._workspace._dynamic_threshold = max(
                0.15,
                self._workspace._dynamic_threshold - stress_excess * 0.2,
            )

        # ── Workspace cycle ──────────────────────────────────────────
        broadcast = await self._workspace.run_cycle(
            affect=self._affect_mgr.current,
            active_goals=self._active_goals,
            memory_client=self._memory_client,
        )

        # ── Async entity extraction for the winner ───────────────────
        if (
            broadcast is not None
            and isinstance(broadcast.content, Percept)
            and self._llm_client is not None
        ):
            asyncio.create_task(
                self._extract_and_store(broadcast.content)
            )

        # ── Affect persistence ───────────────────────────────────────
        if self._affect_mgr.needs_persist:
            self._affect_mgr.mark_persisted()
            if self._memory_service is not None:
                asyncio.create_task(
                    self._persist_affect_safe(),
                    name="atune_affect_persist",
                )

        # ── Cache refresh ────────────────────────────────────────────
        self._cache.cycles_since_identity_refresh += 1
        self._cache.cycles_since_risk_refresh += 1
        self._cache.cycles_since_vocab_refresh += 1
        self._cache.cycles_since_alert_refresh += 1
        await self._refresh_caches()

        # ── Observability ────────────────────────────────────────────
        latency_ms = (time.monotonic() - t0) * 1000
        self._logger.debug(
            "cycle_complete",
            cycle=self._workspace.cycle_count,
            latency_ms=round(latency_ms, 2),
            broadcast=broadcast.broadcast_id if broadcast else None,
            threshold=round(self._workspace.dynamic_threshold, 4),
            mode=self._meta.current_mode,
        )

        return broadcast

    # ------------------------------------------------------------------
    # Entity extraction (async background task)
    # ------------------------------------------------------------------

    async def _extract_and_store(self, percept: Percept) -> None:
        """Extract entities/relations and forward to Memory for graph storage."""
        if self._llm_client is None:
            return
        try:
            result = await extract_entities_and_relations(percept, self._llm_client)
            if not (result.entities or result.relations):
                return

            # Store to Memory if the memory client exposes entity storage
            if self._memory_service is not None:
                # Resolve and create each entity (handles deduplication)
                entity_id_map: dict[str, str] = {}  # name → entity_id
                for ent in result.entities:
                    try:
                        entity_id, was_created = await self._memory_service.resolve_and_create_entity(
                            name=ent.name,
                            entity_type=ent.type,
                            description=ent.description,
                        )
                        entity_id_map[ent.name] = entity_id
                    except Exception:
                        self._logger.debug(
                            "entity_resolve_failed",
                            entity_name=ent.name,
                            exc_info=True,
                        )

                # Link entities to the episode (if a last_episode_id is known)
                last_ep = getattr(self, "_last_episode_id", None)
                if last_ep and entity_id_map:
                    from ecodiaos.primitives import MentionRelation
                    for ent_name, ent_id in entity_id_map.items():
                        try:
                            await self._memory_service.link_mention(
                                MentionRelation(
                                    episode_id=last_ep,
                                    entity_id=ent_id,
                                    role="mentioned",
                                    confidence=next(
                                        (e.confidence for e in result.entities if e.name == ent_name),
                                        0.5,
                                    ),
                                )
                            )
                        except Exception:
                            pass  # Non-critical

                # Create semantic relations between extracted entities
                if result.relations:
                    from ecodiaos.primitives import SemanticRelation
                    for rel in result.relations:
                        from_id = entity_id_map.get(rel.from_entity)
                        to_id = entity_id_map.get(rel.to_entity)
                        if from_id and to_id:
                            try:
                                await self._memory_service.link_relation(
                                    SemanticRelation(
                                        source_entity_id=from_id,
                                        target_entity_id=to_id,
                                        type=rel.type,
                                        strength=rel.strength,
                                        confidence=rel.strength,
                                        evidence_episodes=[last_ep] if last_ep else [],
                                    )
                                )
                            except Exception:
                                pass  # Non-critical

            self._logger.debug(
                "extraction_stored",
                percept_id=percept.id,
                entities=len(result.entities),
                relations=len(result.relations),
                stored_to_graph=self._memory_service is not None,
            )
        except Exception:
            self._logger.warning("entity_extraction_failed", percept_id=percept.id, exc_info=True)

    # ------------------------------------------------------------------
    # Affect persistence
    # ------------------------------------------------------------------

    async def _persist_affect_safe(self) -> None:
        """Persist current affect to the Self node for restart durability."""
        try:
            await self._memory_service.update_affect(self._affect_mgr.current)
        except Exception:
            self._logger.debug("affect_persist_failed", exc_info=True)

    # ------------------------------------------------------------------
    # Cache management
    # ------------------------------------------------------------------

    async def _refresh_caches(self, force: bool = False) -> None:
        """Refresh slowly-changing cached data from Memory."""
        if self._memory_client is None:
            return

        cfg = self._config

        if force or self._cache.cycles_since_identity_refresh >= cfg.cache_identity_refresh_cycles:
            # Refresh core identity embeddings and community embedding
            # In production: fetch from memory_client
            self._cache.cycles_since_identity_refresh = 0

        if force or self._cache.cycles_since_risk_refresh >= cfg.cache_risk_refresh_cycles:
            self._cache.cycles_since_risk_refresh = 0

        if force or self._cache.cycles_since_vocab_refresh >= cfg.cache_vocab_refresh_cycles:
            self._cache.cycles_since_vocab_refresh = 0

        if force or self._cache.cycles_since_alert_refresh >= cfg.cache_alert_refresh_cycles:
            self._cache.cycles_since_alert_refresh = 0

    # ------------------------------------------------------------------
    # State setters (called by other services or API)
    # ------------------------------------------------------------------

    def set_active_goals(self, goals: list[ActiveGoalSummary]) -> None:
        """Update the active goal list (called by Nova)."""
        self._active_goals = goals

    def set_belief_state(self, reader: BeliefStateReader) -> None:
        """
        Wire in Nova's belief state for top-down prediction.

        This closes the predictive processing loop — the single most
        important architectural connection in EOS. With this wired:
          Nova beliefs → Atune prediction → prediction error → salience → broadcast → Nova
        Without it, Atune treats every percept as equally novel (magnitude 0.5).
        """
        self._belief_state = reader
        self._logger.info("belief_state_wired", source="nova")

    def set_pending_hypothesis_count(self, count: int) -> None:
        """Update pending hypothesis count (called by Evo)."""
        self._pending_hypothesis_count = count
        self._workspace._pending_hypothesis_count = count

    def set_community_size(self, size: int) -> None:
        self._community_size = size

    def set_rhythm_state(self, state: str) -> None:
        """
        Update the current cognitive rhythm state (called by Synapse).
        This feeds into meta-attention to modulate salience head weights
        based on the organism's emergent cognitive state.
        """
        self._rhythm_state = state

    def set_memory_service(self, memory_service: Any) -> None:
        """Wire the full MemoryService for entity extraction storage."""
        self._memory_service = memory_service
        self._logger.info("memory_service_wired", source="main")

    def set_last_episode_id(self, episode_id: str) -> None:
        """Track the most recent episode ID for entity→episode linking."""
        self._last_episode_id = episode_id

    def set_cache_identity(
        self,
        core_embeddings: list[list[float]],
        community_embedding: list[float],
        instance_name: str,
    ) -> None:
        """Manually set identity cache (for testing or initial boot)."""
        self._cache.core_identity_embeddings = core_embeddings
        self._cache.community_embedding = community_embedding
        self._cache.instance_name = instance_name

    def set_cache_alerts(self, alerts: list[Alert]) -> None:
        self._cache.active_alerts = alerts

    def set_cache_risk_categories(self, categories: list[RiskCategory]) -> None:
        self._cache.risk_categories = categories

    def set_cache_learned_patterns(self, patterns: list[LearnedPattern]) -> None:
        self._cache.learned_patterns = patterns

    def set_cache_community_vocabulary(self, vocab: set[str]) -> None:
        self._cache.community_vocabulary = vocab

    # ------------------------------------------------------------------
    # Read-only accessors
    # ------------------------------------------------------------------

    @property
    def current_affect(self) -> AffectState:
        return self._affect_mgr.current

    @property
    def meta_attention_mode(self) -> str:
        return self._meta.current_mode

    @property
    def workspace_threshold(self) -> float:
        return self._workspace.dynamic_threshold

    @property
    def cycle_count(self) -> int:
        return self._workspace.cycle_count

    @property
    def recent_broadcasts(self) -> list[WorkspaceBroadcast]:
        return self._workspace.recent_broadcasts

    def apply_evo_adjustments(self, adjustments: dict[str, float]) -> None:
        """Forward Evo's head-weight adjustments to meta-attention."""
        self._meta.apply_evo_adjustments(adjustments)

    def nudge_dominance(self, delta: float) -> None:
        """Forward Axon's dominance feedback to affect manager."""
        self._affect_mgr.nudge_dominance(delta)

    def nudge_valence(self, delta: float) -> None:
        """Forward valence feedback to affect manager."""
        self._affect_mgr.nudge_valence(delta)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\types.py =====

"""
Atune-specific data types.

These types are internal to Atune and not part of the shared primitives.
They wrap primitives with Atune-specific context for the perception and
workspace pipeline.
"""

from __future__ import annotations

import enum
from datetime import datetime, timezone
from typing import Any
from uuid import uuid4

from pydantic import BaseModel, Field

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.primitives.memory_trace import MemoryTrace
from ecodiaos.primitives.percept import Percept


# ---------------------------------------------------------------------------
# Input channels
# ---------------------------------------------------------------------------


class InputChannel(str, enum.Enum):
    """All channels from which Atune can receive raw input."""

    # User-facing
    TEXT_CHAT = "text_chat"
    VOICE = "voice"
    GESTURE = "gesture"

    # Environmental
    SENSOR_IOT = "sensor_iot"
    CALENDAR = "calendar"
    EXTERNAL_API = "external_api"

    # Internal
    SYSTEM_EVENT = "system_event"
    MEMORY_BUBBLE = "memory_bubble"
    AFFECT_SHIFT = "affect_shift"
    EVO_INSIGHT = "evo_insight"

    # Federation
    FEDERATION_MSG = "federation_msg"


# ---------------------------------------------------------------------------
# Raw input (pre-normalisation)
# ---------------------------------------------------------------------------


class RawInput(BaseModel):
    """Raw data before normalisation into a Percept."""

    data: str | bytes
    channel_id: str = ""
    metadata: dict[str, Any] = Field(default_factory=dict)


# ---------------------------------------------------------------------------
# Prediction error
# ---------------------------------------------------------------------------


class PredictionErrorDirection(str, enum.Enum):
    """Category of surprise."""

    CONTRADICTS_BELIEF = "contradicts_belief"
    NOVEL = "novel"
    CONFIRMS_UNEXPECTED = "confirms_unexpected"
    EXPECTED = "expected"


class PredictionError(BaseModel):
    """How surprising a Percept is given current beliefs."""

    magnitude: float = Field(ge=0.0, le=1.0)
    direction: PredictionErrorDirection
    domain: str = ""
    expected_embedding: list[float] | None = None
    actual_embedding: list[float] | None = None


# ---------------------------------------------------------------------------
# Salience
# ---------------------------------------------------------------------------


class SalienceVector(BaseModel):
    """Per-head salience scores plus composite."""

    scores: dict[str, float] = Field(default_factory=dict)
    composite: float = Field(ge=0.0, le=1.0, default=0.0)
    prediction_error: PredictionError | None = None


# ---------------------------------------------------------------------------
# Workspace types
# ---------------------------------------------------------------------------


class WorkspaceCandidate(BaseModel):
    """A candidate competing for workspace broadcast."""

    content: Any  # Percept or other content
    salience: SalienceVector
    source: str = ""
    prediction_error: PredictionError | None = None


class MemoryContext(BaseModel):
    """Memory retrieval results attached to a broadcast."""

    traces: list[MemoryTrace] = Field(default_factory=list)
    entities: list[Any] = Field(default_factory=list)
    communities: list[Any] = Field(default_factory=list)


class WorkspaceContext(BaseModel):
    """Contextual information accompanying a workspace broadcast."""

    recent_broadcast_ids: list[str] = Field(default_factory=list)
    active_goal_ids: list[str] = Field(default_factory=list)
    memory_context: MemoryContext = Field(default_factory=MemoryContext)
    prediction_error: PredictionError | None = None


class WorkspaceBroadcast(BaseModel):
    """The output of a workspace cycle — broadcast to all systems."""

    broadcast_id: str = Field(default_factory=lambda: new_id())
    timestamp: datetime = Field(default_factory=utc_now)
    content: Any  # Percept or contributed content
    salience: SalienceVector
    affect: AffectState
    context: WorkspaceContext = Field(default_factory=WorkspaceContext)
    precision: float = Field(ge=0.0, le=1.0, default=0.5)


class WorkspaceContribution(BaseModel):
    """Content submitted by another system for workspace consideration."""

    system: str
    content: Any
    priority: float = Field(ge=0.0, le=1.0, default=0.5)
    reason: str = ""


# ---------------------------------------------------------------------------
# Attention context (passed to salience heads)
# ---------------------------------------------------------------------------


class ActiveGoalSummary(BaseModel):
    """Minimal goal info needed by salience heads."""

    id: str
    target_embedding: list[float]
    priority: float = Field(ge=0.0, le=1.0, default=0.5)


class RiskCategory(BaseModel):
    """A known risk category with its embedding."""

    name: str
    embedding: list[float]


class LearnedPattern(BaseModel):
    """A pattern Evo has identified as important."""

    pattern: str
    weight: float = 1.0


class Alert(BaseModel):
    """An active alert pattern set by governance or Equor."""

    pattern: str
    severity: float = Field(ge=0.0, le=1.0, default=0.5)


class PendingDecision(BaseModel):
    """A decision awaiting information."""

    id: str
    description: str
    embedding: list[float] | None = None


class AttentionContext(BaseModel):
    """
    Everything the salience heads need to score a Percept.
    Assembled once per Percept, shared across all heads.
    """

    prediction_error: PredictionError
    affect_state: AffectState
    active_goals: list[ActiveGoalSummary] = Field(default_factory=list)
    core_identity_embeddings: list[list[float]] = Field(default_factory=list)
    community_embedding: list[float] = Field(default_factory=list)
    source_habituation: dict[str, float] = Field(default_factory=dict)
    risk_categories: list[RiskCategory] = Field(default_factory=list)
    learned_patterns: list[LearnedPattern] = Field(default_factory=list)
    community_vocabulary: set[str] = Field(default_factory=set)
    active_alerts: list[Alert] = Field(default_factory=list)
    pending_decisions: list[PendingDecision] = Field(default_factory=list)
    community_size: int = 0
    instance_name: str = ""

    class Config:
        arbitrary_types_allowed = True


# ---------------------------------------------------------------------------
# Entity extraction
# ---------------------------------------------------------------------------


class EntityCandidate(BaseModel):
    """An entity extracted from a Percept by LLM."""

    name: str
    type: str
    description: str = ""
    confidence: float = Field(ge=0.0, le=1.0, default=0.5)


class RelationCandidate(BaseModel):
    """A relation between entities extracted by LLM."""

    from_entity: str
    to_entity: str
    type: str
    strength: float = Field(ge=0.0, le=1.0, default=0.5)
    temporal: str | None = None


class ExtractionResult(BaseModel):
    """Output of entity/relation extraction from a Percept."""

    entities: list[EntityCandidate] = Field(default_factory=list)
    relations: list[RelationCandidate] = Field(default_factory=list)
    source_percept_id: str = ""


# ---------------------------------------------------------------------------
# Meta-attention
# ---------------------------------------------------------------------------


class MetaContext(BaseModel):
    """Context for the meta-attention controller."""

    risk_level: float = Field(ge=0.0, le=1.0, default=0.0)
    recent_broadcast_count: int = 0
    cycles_since_last_broadcast: int = 0
    active_goal_count: int = 0
    pending_hypothesis_count: int = 0
    # Rhythm state from Synapse (e.g. "flow", "stress", "boredom", "normal")
    rhythm_state: str = "normal"


# ---------------------------------------------------------------------------
# System load (for affect computation)
# ---------------------------------------------------------------------------


class SystemLoad(BaseModel):
    """Current system resource utilisation."""

    cpu_utilisation: float = Field(ge=0.0, le=1.0, default=0.0)
    memory_utilisation: float = Field(ge=0.0, le=1.0, default=0.0)
    queue_depth: int = 0


# ---------------------------------------------------------------------------
# Cache structure
# ---------------------------------------------------------------------------


class AtuneCache(BaseModel):
    """Slowly-changing data cached to meet latency requirements."""

    core_identity_embeddings: list[list[float]] = Field(default_factory=list)
    community_embedding: list[float] = Field(default_factory=list)
    risk_categories: list[RiskCategory] = Field(default_factory=list)
    learned_patterns: list[LearnedPattern] = Field(default_factory=list)
    community_vocabulary: set[str] = Field(default_factory=set)
    active_alerts: list[Alert] = Field(default_factory=list)
    instance_name: str = ""

    # Refresh counters
    cycles_since_identity_refresh: int = 0
    cycles_since_risk_refresh: int = 0
    cycles_since_vocab_refresh: int = 0
    cycles_since_alert_refresh: int = 0

    class Config:
        arbitrary_types_allowed = True

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\atune\workspace.py =====

"""
Atune — Global Workspace.

Implements the computational Global Workspace (Baars 1988, Dehaene 2003):
competitive selection of the most salient content, followed by broadcast
to all cognitive systems.  Only **one** winner per cycle — the unitary
consciousness principle.

The workspace cycle operates on a theta rhythm (~150 ms) driven by
Synapse's clock.
"""

from __future__ import annotations

import asyncio
import random
from collections import deque
from typing import Any, Protocol

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.percept import Percept

from .helpers import clamp
from .types import (
    ActiveGoalSummary,
    AttentionContext,
    AtuneCache,
    MemoryContext,
    PredictionError,
    SalienceVector,
    SystemLoad,
    WorkspaceBroadcast,
    WorkspaceCandidate,
    WorkspaceContext,
    WorkspaceContribution,
)

logger = structlog.get_logger("ecodiaos.systems.atune.workspace")


# ---------------------------------------------------------------------------
# Subscriber protocol — every system that receives broadcasts
# ---------------------------------------------------------------------------


class BroadcastSubscriber(Protocol):
    """Any system that receives workspace broadcasts."""

    system_id: str

    async def receive_broadcast(self, broadcast: WorkspaceBroadcast) -> None: ...


# ---------------------------------------------------------------------------
# Memory interface for enrichment + spontaneous recall
# ---------------------------------------------------------------------------


class WorkspaceMemoryClient(Protocol):
    """Minimal memory interface required by the workspace."""

    async def retrieve_context(
        self,
        query_embedding: list[float],
        query_text: str,
        max_results: int,
    ) -> MemoryContext: ...

    async def find_bubbling_memory(
        self,
        min_salience: float,
        max_recent_access_hours: int,
    ) -> Any | None: ...

    async def store_percept_with_broadcast(
        self,
        percept: Percept,
        salience: SalienceVector,
        affect: AffectState,
    ) -> None: ...


# ---------------------------------------------------------------------------
# Global Workspace
# ---------------------------------------------------------------------------


class GlobalWorkspace:
    """
    The consciousness bottleneck.

    Each cycle:
    1. Candidates enter the workspace buffer.
    2. Candidates compete based on salience.
    3. The winner triggers "ignition" — broadcast to all systems.
    4. All systems receive the broadcast and may contribute to the next cycle.

    Only ONE winner per cycle (unitary consciousness principle from GWT).
    """

    def __init__(
        self,
        ignition_threshold: float = 0.3,
        buffer_size: int = 32,
        spontaneous_recall_base_prob: float = 0.02,
    ) -> None:
        # Config
        self._base_threshold = ignition_threshold
        self._buffer_size = buffer_size
        self._spontaneous_base_prob = spontaneous_recall_base_prob

        # Dynamic state
        self._dynamic_threshold: float = ignition_threshold
        self._subscribers: list[BroadcastSubscriber] = []
        self._recent_broadcasts: deque[WorkspaceBroadcast] = deque(maxlen=20)

        # Queues (drained each cycle)
        self._percept_queue: deque[WorkspaceCandidate] = deque(maxlen=200)
        self._contribution_queue: deque[WorkspaceContribution] = deque(maxlen=50)

        # Spontaneous recall tracking
        self._cycles_since_last_spontaneous: int = 100  # allow first one early
        self._pending_hypothesis_count: int = 0

        # Habituation tracker: source → habituation level
        self._habituation: dict[str, float] = {}

        # Cycle counter
        self._cycle_count: int = 0

        self._logger = logger.bind(component="workspace")

    # ------------------------------------------------------------------
    # Registration
    # ------------------------------------------------------------------

    def subscribe(self, subscriber: BroadcastSubscriber) -> None:
        """Register a system to receive workspace broadcasts."""
        self._subscribers.append(subscriber)
        self._logger.info("subscriber_added", system=getattr(subscriber, "system_id", "?"))

    # ------------------------------------------------------------------
    # Queue management
    # ------------------------------------------------------------------

    def enqueue_scored_percept(self, candidate: WorkspaceCandidate) -> None:
        """Add a fully-scored candidate to the workspace buffer."""
        self._percept_queue.append(candidate)

    def contribute(self, contribution: WorkspaceContribution) -> None:
        """Other systems submit content for the next workspace cycle."""
        self._contribution_queue.append(contribution)

    def _drain_percept_queue(self) -> list[WorkspaceCandidate]:
        items = list(self._percept_queue)
        self._percept_queue.clear()
        return items

    def _drain_contribution_queue(self) -> list[WorkspaceContribution]:
        items = list(self._contribution_queue)
        self._contribution_queue.clear()
        return items

    # ------------------------------------------------------------------
    # Spontaneous memory surfacing
    # ------------------------------------------------------------------

    async def _check_spontaneous_recall(
        self,
        affect: AffectState,
        memory_client: WorkspaceMemoryClient | None,
    ) -> WorkspaceCandidate | None:
        """
        Probabilistically surface a memory that is currently highly salient
        but hasn't been accessed recently.  Creates the "I just thought of
        something" experience.
        """
        if self._cycles_since_last_spontaneous < 20:
            return None

        if memory_client is None:
            return None

        base_prob = self._spontaneous_base_prob
        curiosity_boost = affect.curiosity * 0.03
        hypothesis_boost = min(0.02, self._pending_hypothesis_count * 0.005)
        total_prob = base_prob + curiosity_boost + hypothesis_boost

        if random.random() > total_prob:
            return None

        memory = await memory_client.find_bubbling_memory(
            min_salience=0.5,
            max_recent_access_hours=24,
        )
        if memory is None:
            return None

        self._cycles_since_last_spontaneous = 0
        self._logger.debug("spontaneous_recall_triggered")

        # Wrap the memory as a WorkspaceCandidate
        return WorkspaceCandidate(
            content=memory,
            salience=SalienceVector(
                scores={},
                composite=0.5 * 0.7,  # dampened salience for spontaneous
            ),
            source="spontaneous_recall",
            prediction_error=None,
        )

    # ------------------------------------------------------------------
    # Context enrichment
    # ------------------------------------------------------------------

    async def _enrich_with_memory(
        self,
        candidate: WorkspaceCandidate,
        memory_client: WorkspaceMemoryClient | None,
    ) -> MemoryContext:
        """Retrieve relevant memories to provide context before broadcast."""
        if memory_client is None:
            return MemoryContext()

        if not isinstance(candidate.content, Percept):
            return MemoryContext()

        percept: Percept = candidate.content
        text = percept.content.parsed if isinstance(percept.content.parsed, str) else ""

        try:
            return await memory_client.retrieve_context(
                query_embedding=percept.content.embedding,
                query_text=text,
                max_results=10,
            )
        except Exception:
            self._logger.warning("memory_enrichment_failed", exc_info=True)
            return MemoryContext()

    # ------------------------------------------------------------------
    # Dynamic threshold adjustment
    # ------------------------------------------------------------------

    def _adjust_threshold(self, candidate_count: int) -> None:
        """
        Adapt the ignition threshold based on input volume.

        Too many candidates → raise (be more selective).
        Too few → lower (be more open).
        """
        if candidate_count > self._buffer_size * 0.8:
            self._dynamic_threshold = min(0.8, self._dynamic_threshold + 0.02)
        elif candidate_count < 3:
            self._dynamic_threshold = max(0.15, self._dynamic_threshold - 0.01)
        else:
            # Drift back toward baseline
            self._dynamic_threshold += 0.005 * (self._base_threshold - self._dynamic_threshold)

    # ------------------------------------------------------------------
    # Habituation
    # ------------------------------------------------------------------

    def _update_habituation(self, winner: WorkspaceCandidate) -> None:
        """
        Increase habituation for the winning source, decay others.

        Stores habituation under BOTH the full source key (e.g. "external:text_chat")
        and the percept's source.system (e.g. "text_chat") so that NoveltyHead
        can look up habituation via percept.source.system correctly.
        """
        source = winner.source
        # Also track by percept source system for NoveltyHead compatibility
        percept_system = getattr(
            getattr(winner.content, "source", None), "system", None
        )
        keys_to_boost = {source}
        if percept_system and percept_system != source:
            keys_to_boost.add(percept_system)

        for key in keys_to_boost:
            self._habituation[key] = min(1.0, self._habituation.get(key, 0.0) + 0.05)
        # Decay all sources slowly
        for key in list(self._habituation):
            if key not in keys_to_boost:
                self._habituation[key] = max(0.0, self._habituation[key] - 0.01)

    @property
    def habituation_map(self) -> dict[str, float]:
        return dict(self._habituation)

    # ------------------------------------------------------------------
    # The main cycle
    # ------------------------------------------------------------------

    async def run_cycle(
        self,
        affect: AffectState,
        active_goals: list[ActiveGoalSummary] | None = None,
        memory_client: WorkspaceMemoryClient | None = None,
    ) -> WorkspaceBroadcast | None:
        """
        One theta cycle of the Global Workspace.  Called by Synapse at
        ~150 ms intervals.

        Returns the broadcast if ignition occurred, else ``None``.
        """
        self._cycle_count += 1
        self._cycles_since_last_spontaneous += 1
        active_goals = active_goals or []

        # ── PHASE 1: Collect candidates ──────────────────────────────
        candidates: list[WorkspaceCandidate] = self._drain_percept_queue()

        # Internal contributions from other systems
        for contrib in self._drain_contribution_queue():
            candidates.append(
                WorkspaceCandidate(
                    content=contrib.content,
                    salience=SalienceVector(scores={}, composite=contrib.priority),
                    source=f"internal:{contrib.system}",
                    prediction_error=None,
                )
            )

        # Spontaneous memory surfacing
        spontaneous = await self._check_spontaneous_recall(affect, memory_client)
        if spontaneous is not None:
            candidates.append(spontaneous)

        if not candidates:
            return None  # Empty cycle

        # ── PHASE 2: Competitive selection (ignition) ────────────────
        candidates.sort(key=lambda c: c.salience.composite, reverse=True)
        winner = candidates[0]

        if winner.salience.composite < self._dynamic_threshold:
            self._logger.debug(
                "no_ignition",
                best_salience=round(winner.salience.composite, 4),
                threshold=round(self._dynamic_threshold, 4),
            )
            self._adjust_threshold(len(candidates))
            return None

        # ── PHASE 3: Context enrichment ──────────────────────────────
        memory_context = await self._enrich_with_memory(winner, memory_client)

        # ── PHASE 4: Broadcast ───────────────────────────────────────
        broadcast = WorkspaceBroadcast(
            content=winner.content,
            salience=winner.salience,
            affect=affect,
            context=WorkspaceContext(
                recent_broadcast_ids=[b.broadcast_id for b in list(self._recent_broadcasts)[-5:]],
                active_goal_ids=[g.id for g in active_goals],
                memory_context=memory_context,
                prediction_error=winner.prediction_error,
            ),
            precision=winner.salience.composite,
        )

        # Fan out to all subscribers in parallel
        if self._subscribers:
            results = await asyncio.gather(
                *(sub.receive_broadcast(broadcast) for sub in self._subscribers),
                return_exceptions=True,
            )
            for sub, result in zip(self._subscribers, results):
                if isinstance(result, Exception):
                    self._logger.warning(
                        "broadcast_ack_failed",
                        system=getattr(sub, "system_id", "?"),
                        error=str(result),
                    )

        self._recent_broadcasts.append(broadcast)

        # ── PHASE 5: Memory storage (non-blocking) ───────────────────
        if isinstance(winner.content, Percept) and memory_client is not None:
            asyncio.create_task(
                memory_client.store_percept_with_broadcast(
                    winner.content, winner.salience, affect,
                )
            )

        # ── PHASE 6: Dynamic threshold adjustment ────────────────────
        self._adjust_threshold(len(candidates))

        # ── PHASE 7: Habituation update ──────────────────────────────
        self._update_habituation(winner)

        self._logger.info(
            "workspace_broadcast",
            broadcast_id=broadcast.broadcast_id,
            salience=round(winner.salience.composite, 4),
            source=winner.source,
            subscriber_count=len(self._subscribers),
            cycle=self._cycle_count,
        )

        return broadcast

    # ------------------------------------------------------------------
    # Accessors
    # ------------------------------------------------------------------

    @property
    def dynamic_threshold(self) -> float:
        return self._dynamic_threshold

    @property
    def recent_broadcasts(self) -> list[WorkspaceBroadcast]:
        return list(self._recent_broadcasts)

    @property
    def cycle_count(self) -> int:
        return self._cycle_count

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\__init__.py =====

"""
EcodiaOS — Axon (Action Execution System)

Axon is the motor cortex. It takes Intents approved by Equor and turns them
into real-world effects — API calls, data operations, scheduled tasks,
notifications, and federated messages.

If Nova decides what to do, Axon does it.
The gap between intention and action is where trust lives.

Public interface:
  AxonService         — main service class
  ExecutionRequest    — input to AxonService.execute()
  AxonOutcome         — output of AxonService.execute()
  Executor            — ABC for custom executors
  ExecutorRegistry    — registry of available executors
"""

from ecodiaos.systems.axon.executor import Executor
from ecodiaos.systems.axon.registry import ExecutorRegistry
from ecodiaos.systems.axon.service import AxonService
from ecodiaos.systems.axon.types import AxonOutcome, ExecutionRequest

__all__ = [
    "AxonService",
    "ExecutionRequest",
    "AxonOutcome",
    "Executor",
    "ExecutorRegistry",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\audit.py =====

"""
EcodiaOS — Axon Audit Logger

Every action Axon takes is permanently recorded. This is the Honesty drive
at the action layer — the community can always trace what EOS did, when, why,
and what happened.

Audit records are stored in the Memory graph as GovernanceRecord nodes, making
them available for:
  - Equor drift analysis (detecting patterns of constitutional deviation)
  - Evo learning (correlating actions with outcomes)
  - Human oversight and governance review
  - Debugging and incident investigation

The audit logger is async and fire-and-forget from the pipeline's perspective —
it runs concurrently with outcome delivery, adding ≤20ms to total execution time.

Parameters are NEVER logged raw — only their SHA-256 hash is stored.
This protects sensitive data while still enabling deduplication and correlation.
"""

from __future__ import annotations

import json
from typing import TYPE_CHECKING

import structlog

from ecodiaos.systems.axon.types import AuditRecord, AxonOutcome, ExecutionContext

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()


class AuditLogger:
    """
    Records every Axon execution as a permanent audit trail in Memory.

    The audit record captures:
    - What was attempted (action_type, parameters_hash, target)
    - What authorised it (equor_verdict, equor_reasoning, autonomy_level)
    - What happened (result, duration_ms)
    - The emotional context at the time (affect_state)

    Records are written to Memory as GovernanceRecord nodes. If Memory is
    unavailable, the record is emitted to the structured log (structlog)
    as a fallback — it will not be silently dropped.
    """

    def __init__(self, memory: "MemoryService | None" = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="axon.audit")
        self._records_written: int = 0
        self._records_failed: int = 0

    async def log(
        self,
        outcome: AxonOutcome,
        context: ExecutionContext,
    ) -> None:
        """
        Write an audit record for a completed execution.

        Assembles one AuditRecord per intent (not per step) for the overall
        execution outcome. Step-level detail lives in the AxonOutcome itself.
        """
        try:
            # Collect parameters from all steps for hashing
            all_params: dict = {}
            for i, step in enumerate(context.intent.plan.steps):
                all_params[f"step_{i}_{step.executor}"] = step.parameters

            record = AuditRecord.from_outcome(
                outcome=outcome,
                context=context,
                parameters=all_params,
                action_type=_primary_action_type(context),
            )

            await self._write(record)

        except Exception as exc:
            self._records_failed += 1
            self._logger.error(
                "audit_log_failed",
                intent_id=outcome.intent_id,
                execution_id=outcome.execution_id,
                error=str(exc),
            )

    async def _write(self, record: AuditRecord) -> None:
        """Write the audit record to Memory, falling back to structured log."""
        # Always emit to structured log for observability
        self._logger.info(
            "action_audit",
            execution_id=record.execution_id,
            intent_id=record.intent_id,
            action_type=record.action_type,
            target=record.target,
            result=record.result,
            duration_ms=record.duration_ms,
            equor_verdict=record.equor_verdict,
            autonomy_level=record.autonomy_level,
            parameters_hash=record.parameters_hash[:12] + "...",
        )

        # Persist to Memory graph if available
        if self._memory is not None:
            try:
                await self._store_governance_record(record)
                self._records_written += 1
            except Exception as exc:
                self._records_failed += 1
                self._logger.error(
                    "audit_memory_write_failed",
                    execution_id=record.execution_id,
                    error=str(exc),
                )
        else:
            self._records_written += 1  # Count as written (to log)

    async def _store_governance_record(self, record: AuditRecord) -> None:
        """
        Store the audit record as a GovernanceRecord node in Neo4j.

        Neo4j schema:
          (:GovernanceRecord {
            type: "action_audit",
            execution_id: str,
            intent_id: str,
            equor_verdict: str,
            action_type: str,
            parameters_hash: str,
            target: str,
            result: str,
            duration_ms: int,
            autonomy_level: int,
            timestamp: datetime
          })

        The Memory service exposes store_governance_record() which handles
        the Cypher write.
        """
        record_data = {
            "type": "action_audit",
            "execution_id": record.execution_id,
            "intent_id": record.intent_id,
            "equor_verdict": record.equor_verdict,
            "equor_reasoning": record.equor_reasoning[:500],  # Truncate
            "action_type": record.action_type,
            "parameters_hash": record.parameters_hash,
            "target": record.target,
            "result": record.result,
            "duration_ms": record.duration_ms,
            "autonomy_level": record.autonomy_level,
            "affect_valence": record.affect_state.valence
            if hasattr(record.affect_state, "valence")
            else 0.0,
            "timestamp": record.created_at.isoformat(),
        }

        if hasattr(self._memory, "store_governance_record"):
            await self._memory.store_governance_record(record_data)  # type: ignore[union-attr]
        else:
            # Fallback: log the full record as JSON
            self._logger.info(
                "audit_record_fallback",
                record=json.dumps(record_data),
            )

    @property
    def stats(self) -> dict:
        return {
            "records_written": self._records_written,
            "records_failed": self._records_failed,
        }


def _primary_action_type(context: ExecutionContext) -> str:
    """Determine the primary action type from the intent plan."""
    if context.intent.plan.steps:
        return context.intent.plan.steps[0].executor
    return "unknown"

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\credentials.py =====

"""
EcodiaOS — Axon Credential Store

Executors need credentials to act on external systems — API keys, OAuth tokens,
webhook secrets. But executors must never see raw secrets.

CredentialStore wraps the raw credential vault and issues scoped, time-limited
tokens per execution. Each token:
  - Is scoped to a specific service (e.g., "email", "calendar", "federation")
  - Is valid only for the duration of the intent execution + a 60s buffer
  - Is tied to the execution_id for audit correlation

In Phase 1 (current), this is a simple in-memory store loaded from config.
In future phases (Federation, external integrations), this will be backed by
a secrets manager and will support OAuth2 token exchange.

Executors call context.credentials.get("service_name") to retrieve their token.
They never access this store directly.
"""

from __future__ import annotations

import hashlib
import hmac
import time
from typing import TYPE_CHECKING

import structlog

from ecodiaos.systems.axon.types import ScopedCredentials

if TYPE_CHECKING:
    from ecodiaos.primitives.intent import Intent

logger = structlog.get_logger()


def _extract_required_services(intent: "Intent") -> set[str]:
    """
    Determine which external services an intent requires credentials for.

    Derived from the executor names in the action plan. This is a heuristic —
    executors may declare their service requirements explicitly in future.
    """
    services: set[str] = set()
    for step in intent.plan.steps:
        executor_name = step.executor.lower()
        if "email" in executor_name:
            services.add("email")
        if "calendar" in executor_name or "schedule" in executor_name:
            services.add("calendar")
        if "webhook" in executor_name:
            services.add("webhook")
        if "federation" in executor_name or "federate" in executor_name:
            services.add("federation")
        if "iot" in executor_name:
            services.add("iot")
        if "call_api" in executor_name or "api" == executor_name:
            # Extract service from parameters if available
            services.add("external_api")
    return services


class CredentialStore:
    """
    Issues scoped, time-limited credentials for intent execution.

    Raw secrets are never exposed to executors. Instead, callers receive a
    ScopedCredentials wrapper with time-limited tokens keyed by service name.

    Phase 1 implementation: credentials loaded from a flat config dict.
    Future: backed by Vault/KMS with dynamic secret generation.
    """

    def __init__(self, raw_credentials: dict[str, str] | None = None) -> None:
        """
        Args:
            raw_credentials: Map of service_name → raw secret.
                             Loaded from config, never logged.
        """
        self._vault: dict[str, str] = raw_credentials or {}
        self._logger = logger.bind(system="axon.credentials")
        # signing_key for HMAC token generation — randomised per process
        self._signing_key = hashlib.sha256(
            f"eos-credential-{time.time()}".encode()
        ).digest()

    def configure(self, credentials: dict[str, str]) -> None:
        """Add or update credentials in the vault (called at startup)."""
        self._vault.update(credentials)

    async def get_for_intent(self, intent: "Intent") -> ScopedCredentials:
        """
        Issue scoped tokens for all services required by this intent.

        Returns a ScopedCredentials containing time-limited tokens.
        Services without configured credentials get an empty token.
        """
        required = _extract_required_services(intent)
        if not required:
            return ScopedCredentials()

        # Estimate execution duration for TTL
        estimated_duration_s = max(
            step.timeout_ms for step in intent.plan.steps
        ) // 1000 if intent.plan.steps else 30
        ttl_seconds = estimated_duration_s + 60  # 60s buffer

        tokens: dict[str, str] = {}
        for service in required:
            if service in self._vault:
                tokens[service] = self._issue_scoped_token(
                    service=service,
                    scope=f"intent:{intent.id}",
                    ttl_seconds=ttl_seconds,
                )
                self._logger.debug(
                    "credential_issued",
                    service=service,
                    intent_id=intent.id,
                    ttl_seconds=ttl_seconds,
                )
            else:
                # Service required but not configured — empty token signals executors
                tokens[service] = ""
                self._logger.warning(
                    "credential_missing",
                    service=service,
                    intent_id=intent.id,
                )

        return ScopedCredentials(tokens=tokens)

    def _issue_scoped_token(
        self,
        service: str,
        scope: str,
        ttl_seconds: int,
    ) -> str:
        """
        Generate a scoped token that wraps the raw credential with a
        time-limited HMAC signature.

        Format: {service}:{expiry_ts}:{hmac}:{raw_credential}
        This is deliberately simple — in production, replace with proper
        token exchange (OAuth2, Vault dynamic secrets, etc.)
        """
        raw = self._vault.get(service, "")
        expiry = int(time.time()) + ttl_seconds
        payload = f"{service}:{scope}:{expiry}"
        signature = hmac.new(
            self._signing_key,
            payload.encode(),
            hashlib.sha256,
        ).hexdigest()[:16]
        # The token carries the raw credential for Phase 1 — executors extract it.
        # In Phase 2, the token would be opaque and exchangeable at the service.
        return f"{expiry}:{signature}:{raw}"

    def extract_credential(self, token: str) -> str | None:
        """
        Extract the raw credential from a scoped token.

        Returns None if the token is expired or malformed.
        Executors call this via the token they receive — not directly.
        """
        if not token:
            return None
        parts = token.split(":", 2)
        if len(parts) < 3:
            return None
        expiry_str, _signature, raw = parts
        try:
            expiry = int(expiry_str)
        except ValueError:
            return None
        if time.time() > expiry:
            self._logger.warning("credential_expired", expiry=expiry)
            return None
        return raw

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\executor.py =====

"""
EcodiaOS — Axon Executor ABC

All action executors implement this interface.

An Executor is a stateless (or near-stateless) handler for one category of action.
It knows:
  - What action type it handles (action_type)
  - How to validate parameters for that action (validate_params)
  - How to perform the action (execute)
  - Whether it can undo the action (rollback, if reversible=True)

Executors are registered with the ExecutorRegistry at startup.
They receive an ExecutionContext that carries credentials, intent, and affect
state — but they must not mutate it.

Design principle: an Executor does exactly one thing, precisely, reliably.
The deliberation (what to do) happens in Nova. The judgement (whether to do it)
happens in Equor. Axon just does it.
"""

from __future__ import annotations

from abc import ABC, abstractmethod

from ecodiaos.systems.axon.types import (
    ExecutionContext,
    ExecutionResult,
    RateLimit,
    RollbackResult,
    ValidationResult,
)


class Executor(ABC):
    """
    Base class for all Axon action executors.

    Subclass this and register with the ExecutorRegistry to add new capabilities.
    """

    # ── Identity ─────────────────────────────────────────────────
    action_type: str = ""           # Must be unique — used as registry key
    description: str = ""           # Human-readable capability description

    # ── Safety ───────────────────────────────────────────────────
    required_autonomy: int = 1      # Minimum AutonomyLevel required to execute
    reversible: bool = False        # If True, override rollback()
    max_duration_ms: int = 5000     # Hard execution timeout
    rate_limit: RateLimit = RateLimit.per_minute(30)

    @abstractmethod
    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        """
        Perform the action described by params.

        Must complete within max_duration_ms.
        Must not raise — return ExecutionResult(success=False, error=...) on failure.
        May return new_observations that will be fed back as Percepts.
        """
        ...

    @abstractmethod
    async def validate_params(self, params: dict) -> ValidationResult:
        """
        Validate that params are sufficient and well-formed for this executor.

        Called before rate-limit checks, before context assembly, before execution.
        Must be fast (synchronous-level latency) — no I/O.
        """
        ...

    async def rollback(
        self,
        execution_id: str,
        context: ExecutionContext,
    ) -> RollbackResult:
        """
        Attempt to undo a previously completed execution.

        Override this if reversible=True. The default returns not-supported,
        which is correct for most executors — only data mutation actions
        (create_record, update_record, schedule_event) should implement this.
        """
        return RollbackResult(success=False, reason="Rollback not supported by this executor")

    def __repr__(self) -> str:
        return (
            f"<{self.__class__.__name__} "
            f"action_type={self.action_type!r} "
            f"autonomy={self.required_autonomy} "
            f"reversible={self.reversible}>"
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\pipeline.py =====

"""
EcodiaOS — Axon Execution Pipeline

The pipeline is the core of Axon — it takes an approved Intent and executes it.

Pipeline stages:
  1. Budget check — verify the per-cycle execution budget allows another action
  2. Validation — validate parameters for all steps against their executors
  3. Rate limit check — verify each executor is within its rate limit
  4. Circuit breaker check — verify no executor has a tripped circuit
  5. Context assembly — gather credentials, build ExecutionContext
  6. Step execution — execute each step with timeout, progress tracking, and rollback
  7. Outcome assembly — collect results, compute success/partial/failure
  8. Outcome delivery — async: audit log + deliver to Nova

All stages must complete within the total_timeout_per_cycle_ms budget.
Steps that exceed their per-executor timeout are cancelled and reported as failures.

The pipeline does not second-guess approved Intents. Equor already reviewed them.
If a step fails, the pipeline rolls back reversible steps and reports the failure —
it does not re-evaluate whether the intent was correct.
"""

from __future__ import annotations

import asyncio
import time
from typing import TYPE_CHECKING

import structlog

from ecodiaos.systems.axon.audit import AuditLogger
from ecodiaos.systems.axon.credentials import CredentialStore
from ecodiaos.systems.axon.registry import ExecutorRegistry
from ecodiaos.systems.axon.safety import BudgetTracker, CircuitBreaker, RateLimiter
from ecodiaos.systems.axon.types import (
    AxonOutcome,
    ExecutionContext,
    ExecutionRequest,
    ExecutionResult,
    ExecutionStatus,
    FailureReason,
    RollbackResult,
    StepOutcome,
)

if TYPE_CHECKING:
    from ecodiaos.systems.nova.service import NovaService
    from ecodiaos.systems.nova.types import IntentOutcome

logger = structlog.get_logger()


class ExecutionPipeline:
    """
    The Axon execution pipeline.

    Receives approved ExecutionRequests and runs them through the 7-stage
    execution pipeline, delivering outcomes to Nova asynchronously.
    """

    def __init__(
        self,
        registry: ExecutorRegistry,
        budget: BudgetTracker,
        rate_limiter: RateLimiter,
        circuit_breaker: CircuitBreaker,
        credential_store: CredentialStore,
        audit_logger: AuditLogger,
        instance_id: str = "eos-default",
    ) -> None:
        self._registry = registry
        self._budget = budget
        self._rate_limiter = rate_limiter
        self._circuit_breaker = circuit_breaker
        self._credential_store = credential_store
        self._audit = audit_logger
        self._instance_id = instance_id
        self._logger = logger.bind(system="axon.pipeline")
        self._nova: "NovaService | None" = None
        self._atune = None  # AtuneService — for feeding outcomes as percepts

    def set_nova(self, nova: "NovaService") -> None:
        self._nova = nova

    def set_atune(self, atune) -> None:
        self._atune = atune

    async def execute(self, request: ExecutionRequest) -> AxonOutcome:
        """
        Execute an approved Intent through all pipeline stages.

        Returns an AxonOutcome regardless of whether execution succeeded —
        failures are reported, not raised.
        """
        intent = request.intent
        start_time = time.monotonic()

        from ecodiaos.primitives.common import new_id
        execution_id = new_id()

        self._logger.info(
            "pipeline_start",
            execution_id=execution_id,
            intent_id=intent.id,
            goal=intent.goal.description[:60],
            steps=len(intent.plan.steps),
        )

        # ── STAGE 1: Budget check ──────────────────────────────────
        allowed, reason = self._budget.can_execute()
        if not allowed:
            return self._fast_fail(
                intent_id=intent.id,
                execution_id=execution_id,
                status=ExecutionStatus.RATE_LIMITED,
                failure_reason=FailureReason.BUDGET_EXCEEDED.value,
                error=reason,
                start_time=start_time,
            )

        # ── STAGE 2: Validation ───────────────────────────────────
        for step in intent.plan.steps:
            executor = self._registry.get(step.executor)
            if executor is None:
                return self._fast_fail(
                    intent_id=intent.id,
                    execution_id=execution_id,
                    status=ExecutionStatus.FAILURE,
                    failure_reason=FailureReason.UNKNOWN_ACTION_TYPE.value,
                    error=f"No executor registered for '{step.executor}'",
                    start_time=start_time,
                )

            validation = await executor.validate_params(step.parameters)
            if not validation.valid:
                return self._fast_fail(
                    intent_id=intent.id,
                    execution_id=execution_id,
                    status=ExecutionStatus.FAILURE,
                    failure_reason=FailureReason.VALIDATION_ERROR.value,
                    error=f"Step '{step.executor}' validation failed: {validation.reason}",
                    start_time=start_time,
                )

            # Autonomy check
            if executor.required_autonomy > intent.autonomy_level_granted:
                return self._fast_fail(
                    intent_id=intent.id,
                    execution_id=execution_id,
                    status=ExecutionStatus.FAILURE,
                    failure_reason=FailureReason.INSUFFICIENT_AUTONOMY.value,
                    error=(
                        f"Executor '{step.executor}' requires autonomy level "
                        f"{executor.required_autonomy}, but intent has "
                        f"{intent.autonomy_level_granted}"
                    ),
                    start_time=start_time,
                )

        # ── STAGE 3: Rate limit check ─────────────────────────────
        for step in intent.plan.steps:
            executor = self._registry.get(step.executor)
            if executor and not self._rate_limiter.check(
                executor.action_type, executor.rate_limit
            ):
                return self._fast_fail(
                    intent_id=intent.id,
                    execution_id=execution_id,
                    status=ExecutionStatus.RATE_LIMITED,
                    failure_reason=FailureReason.RATE_LIMITED.value,
                    error=f"Rate limit exceeded for executor '{step.executor}'",
                    start_time=start_time,
                )

        # ── STAGE 4: Circuit breaker check ───────────────────────
        for step in intent.plan.steps:
            executor = self._registry.get(step.executor)
            if executor and not self._circuit_breaker.can_execute(executor.action_type):
                return self._fast_fail(
                    intent_id=intent.id,
                    execution_id=execution_id,
                    status=ExecutionStatus.CIRCUIT_OPEN,
                    failure_reason=FailureReason.CIRCUIT_OPEN.value,
                    error=f"Circuit breaker open for executor '{step.executor}'",
                    start_time=start_time,
                )

        # ── STAGE 5: Context assembly ─────────────────────────────
        from ecodiaos.primitives.affect import AffectState
        credentials = await self._credential_store.get_for_intent(intent)
        context = ExecutionContext(
            execution_id=execution_id,
            intent=intent,
            equor_check=request.equor_check,
            credentials=credentials,
            instance_id=self._instance_id,
        )

        # ── STAGE 6: Step execution ───────────────────────────────
        self._budget.begin_execution()
        step_outcomes: list[StepOutcome] = []

        try:
            for i, step in enumerate(intent.plan.steps):
                executor = self._registry.get(step.executor)
                if executor is None:
                    # Should not reach here after validation, but guard anyway
                    continue

                # Calculate remaining time budget for this step
                elapsed_ms = int((time.monotonic() - start_time) * 1000)
                remaining_ms = request.timeout_ms - elapsed_ms

                # Early-break: if we've exhausted the total budget, skip
                # remaining steps rather than running them with a trivial
                # timeout that will almost certainly fail.
                if remaining_ms <= 0:
                    self._logger.warning(
                        "pipeline_budget_exhausted_skipping_remaining",
                        execution_id=execution_id,
                        step_index=i,
                        elapsed_ms=elapsed_ms,
                        budget_ms=request.timeout_ms,
                        steps_remaining=len(intent.plan.steps) - i,
                    )
                    step_outcomes.append(StepOutcome(
                        step_index=i,
                        action_type=executor.action_type,
                        description=step.parameters.get("description", step.executor),
                        result=ExecutionResult(
                            success=False,
                            error=f"Skipped: total budget ({request.timeout_ms}ms) exhausted",
                        ),
                        duration_ms=0,
                    ))
                    break

                step_timeout_ms = min(
                    step.timeout_ms * 3,       # 3x expected as safety margin
                    executor.max_duration_ms,
                    remaining_ms,
                )
                step_timeout_ms = max(step_timeout_ms, 100)  # Minimum 100ms

                step_start = time.monotonic()
                result = await _run_step_with_timeout(
                    executor=executor,
                    params=step.parameters,
                    context=context,
                    timeout_ms=step_timeout_ms,
                )
                step_duration_ms = int((time.monotonic() - step_start) * 1000)

                # Record rate limit usage (only after successful check above)
                self._rate_limiter.record(executor.action_type)

                # Update circuit breaker
                self._circuit_breaker.record_result(executor.action_type, result.success)

                step_outcome = StepOutcome(
                    step_index=i,
                    action_type=executor.action_type,
                    description=step.parameters.get("description", step.executor),
                    result=result,
                    duration_ms=step_duration_ms,
                )
                step_outcomes.append(step_outcome)

                self._logger.debug(
                    "step_complete",
                    execution_id=execution_id,
                    step_index=i,
                    action_type=executor.action_type,
                    success=result.success,
                    duration_ms=step_duration_ms,
                )

                # On failure: decide whether to continue or abort with rollback
                if not result.success:
                    if step.parameters.get("continue_on_failure", False):
                        continue
                    else:
                        # Rollback completed steps
                        rollback_results = await _rollback_completed(
                            step_outcomes=step_outcomes[:-1],  # Exclude the failed step
                            registry=self._registry,
                            context=context,
                        )
                        self._logger.warning(
                            "pipeline_step_failed_rolling_back",
                            execution_id=execution_id,
                            step_index=i,
                            error=result.error[:100],
                            rollback_results=len(rollback_results),
                        )
                        break

        finally:
            self._budget.end_execution()

        # ── STAGE 7: Outcome assembly ─────────────────────────────
        total_duration_ms = int((time.monotonic() - start_time) * 1000)
        all_succeeded = bool(step_outcomes) and all(
            s.result.success for s in step_outcomes
        )
        any_succeeded = any(s.result.success for s in step_outcomes)
        partial = any_succeeded and not all_succeeded

        outcome = AxonOutcome(
            intent_id=intent.id,
            execution_id=execution_id,
            success=all_succeeded,
            partial=partial,
            status=ExecutionStatus.SUCCESS if all_succeeded else (
                ExecutionStatus.PARTIAL if partial else ExecutionStatus.FAILURE
            ),
            step_outcomes=step_outcomes,
            duration_ms=total_duration_ms,
        )
        outcome.world_state_changes = outcome.collect_world_changes()
        outcome.new_observations = outcome.collect_new_observations()
        if not all_succeeded:
            outcome.failure_reason = outcome.classify_failure()

        self._logger.info(
            "pipeline_complete",
            execution_id=execution_id,
            intent_id=intent.id,
            success=all_succeeded,
            partial=partial,
            steps_completed=len(step_outcomes),
            duration_ms=total_duration_ms,
        )

        # ── STAGE 8: Audit + Nova delivery (concurrent) ───────────
        await asyncio.gather(
            self._audit.log(outcome, context),
            self._deliver_to_nova(outcome),
            self._contribute_to_atune(outcome),
            return_exceptions=True,
        )

        return outcome

    async def _deliver_to_nova(self, outcome: AxonOutcome) -> None:
        """
        Deliver the execution outcome to Nova for belief updating.
        Convert AxonOutcome → IntentOutcome (Nova's shared primitive).

        Retries up to 3 times with exponential backoff. If Nova's belief state
        diverges from real execution, goals become stale and learning breaks.
        """
        if self._nova is None:
            return

        from ecodiaos.systems.nova.types import IntentOutcome

        nova_outcome = IntentOutcome(
            intent_id=outcome.intent_id,
            success=outcome.success,
            episode_id=outcome.episode_id,
            failure_reason=outcome.failure_reason,
            new_observations=outcome.new_observations,
        )

        last_error: Exception | None = None
        for attempt in range(3):
            try:
                await self._nova.process_outcome(nova_outcome)
                return  # Success
            except Exception as exc:
                last_error = exc
                if attempt < 2:
                    await asyncio.sleep(0.1 * (2 ** attempt))  # 100ms, 200ms

        self._logger.error(
            "nova_delivery_failed_after_retries",
            intent_id=outcome.intent_id,
            execution_id=outcome.execution_id,
            attempts=3,
            error=str(last_error),
        )

    async def _contribute_to_atune(self, outcome: AxonOutcome) -> None:
        """
        Feed the execution outcome into Atune's workspace as a self-perception.

        The organism perceives its own actions: successes are routine (low salience),
        failures are salient and demand attention.
        """
        if self._atune is None:
            return

        from ecodiaos.systems.atune.types import WorkspaceContribution

        if outcome.success:
            content = (
                f"Action completed: {outcome.execution_id} "
                f"({len(outcome.step_outcomes)} steps)"
            )
            priority = 0.25  # Routine — success is expected
        else:
            content = (
                f"Action failed: {outcome.execution_id} — "
                f"{outcome.failure_reason or 'unknown'}"
            )
            priority = 0.55  # Failure is salient and demands attention

        try:
            self._atune.contribute(WorkspaceContribution(
                system="axon",
                content=content,
                priority=priority,
                reason="action_outcome",
            ))
        except Exception as exc:
            self._logger.debug("atune_contribution_failed", error=str(exc))

    def _fast_fail(
        self,
        intent_id: str,
        execution_id: str,
        status: ExecutionStatus,
        failure_reason: str,
        error: str,
        start_time: float,
    ) -> AxonOutcome:
        """Create a fast-fail outcome without executing any steps."""
        duration_ms = int((time.monotonic() - start_time) * 1000)
        self._logger.warning(
            "pipeline_fast_fail",
            intent_id=intent_id,
            execution_id=execution_id,
            status=status.value,
            failure_reason=failure_reason,
            error=error[:100],
        )
        return AxonOutcome(
            intent_id=intent_id,
            execution_id=execution_id,
            success=False,
            status=status,
            failure_reason=failure_reason,
            error=error,
            duration_ms=duration_ms,
        )


# ─── Helpers ──────────────────────────────────────────────────────


async def _run_step_with_timeout(
    executor,
    params: dict,
    context: ExecutionContext,
    timeout_ms: int,
) -> ExecutionResult:
    """Run a single step executor with timeout enforcement."""
    try:
        return await asyncio.wait_for(
            executor.execute(params, context),
            timeout=timeout_ms / 1000,
        )
    except asyncio.TimeoutError:
        return ExecutionResult(
            success=False,
            error=f"Step timed out after {timeout_ms}ms",
        )
    except Exception as exc:
        return ExecutionResult(
            success=False,
            error=f"Step raised exception: {type(exc).__name__}: {exc}",
        )


async def _rollback_completed(
    step_outcomes: list[StepOutcome],
    registry: ExecutorRegistry,
    context: ExecutionContext,
) -> list[RollbackResult]:
    """
    Attempt to rollback completed steps in reverse order (most recent first).
    Best-effort — non-reversible steps are reported as not-supported.
    """
    results: list[RollbackResult] = []

    for step_outcome in reversed(step_outcomes):
        if not step_outcome.result.success:
            continue  # Failed steps don't need rollback

        executor = registry.get(step_outcome.action_type)
        if executor is None or not executor.reversible:
            results.append(RollbackResult(
                success=False,
                reason=f"Executor '{step_outcome.action_type}' is not reversible",
            ))
            continue

        try:
            rollback_result = await executor.rollback(
                context.execution_id,
                context,
            )
            results.append(rollback_result)
        except Exception as exc:
            results.append(RollbackResult(
                success=False,
                reason=f"Rollback exception: {exc}",
            ))
            logger.error(
                "rollback_failed",
                step_index=step_outcome.step_index,
                action_type=step_outcome.action_type,
                error=str(exc),
            )

    return results

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\registry.py =====

"""
EcodiaOS — Axon Executor Registry

The registry maps action type names to their Executor instances.

Registration happens at AxonService initialisation. The registry is then
immutable at runtime — no executor can be added or removed during a cognitive
cycle. This makes executor lookup O(1) and side-effect-free.

Lookup normalisation:
  Intents use dot-notation executor names (e.g. "executor.observe", "data.store").
  The registry normalises these to the canonical action_type used at registration.
  Normalisation strips a leading "executor." prefix and maps common aliases.

This means executors register under their canonical name ("observe", "call_api")
and the registry handles the dotted forms that Nova's PolicyGenerator may produce.
"""

from __future__ import annotations

import structlog

from ecodiaos.systems.axon.executor import Executor

logger = structlog.get_logger()

# Map dotted/aliased executor names → canonical action_type
_ALIAS_MAP: dict[str, str] = {
    # Observation
    "executor.observe": "observe",
    "executor.query_memory": "query_memory",
    "executor.analyse": "analyse",
    "executor.search": "search",
    # Communication
    "executor.respond": "respond_text",
    "respond": "respond_text",
    "executor.respond_text": "respond_text",
    "executor.notify": "send_notification",
    "notify": "send_notification",
    "executor.notification": "send_notification",
    "executor.post": "post_message",
    "post": "post_message",
    "executor.post_message": "post_message",
    "executor.email": "send_email",
    # Data operations
    "executor.create": "create_record",
    "create": "create_record",
    "executor.create_record": "create_record",
    "executor.update": "update_record",
    "update": "update_record",
    "executor.update_record": "update_record",
    "executor.schedule": "schedule_event",
    "schedule": "schedule_event",
    "executor.reminder": "set_reminder",
    "reminder": "set_reminder",
    # Integration
    "executor.api": "call_api",
    "api": "call_api",
    "executor.call_api": "call_api",
    "executor.webhook": "webhook_trigger",
    "webhook": "webhook_trigger",
    # Internal
    "executor.store": "store_insight",
    "store": "store_insight",
    "executor.store_insight": "store_insight",
    "executor.update_goal": "update_goal",
    "executor.consolidate": "trigger_consolidation",
    "consolidate": "trigger_consolidation",
    # Resource & config
    "executor.allocate": "allocate_resource",
    "executor.config": "adjust_config",
    # Federation
    "executor.federate": "federation_send",
    "federate": "federation_send",
    "executor.federation_send": "federation_send",
    "executor.federation_share": "federation_share",
}


def _normalise(action_type: str) -> str:
    """Normalise an action type string to its canonical registry key."""
    normalised = _ALIAS_MAP.get(action_type)
    if normalised:
        return normalised
    # Strip "executor." prefix if present
    if action_type.startswith("executor."):
        return action_type[len("executor."):]
    return action_type


class ExecutorRegistry:
    """
    Immutable-at-runtime registry of available action executors.

    Built at startup, queried at execution time.
    """

    def __init__(self) -> None:
        self._executors: dict[str, Executor] = {}
        self._logger = logger.bind(system="axon.registry")

    def register(self, executor: Executor) -> None:
        """
        Register an executor under its canonical action_type.

        Raises ValueError if the action_type is already registered.
        Call during initialisation only — not during a cognitive cycle.
        """
        key = executor.action_type
        if not key:
            raise ValueError(f"Executor {executor!r} has no action_type set")
        if key in self._executors:
            raise ValueError(
                f"Executor for action_type {key!r} already registered — "
                f"existing: {self._executors[key]!r}, new: {executor!r}"
            )
        self._executors[key] = executor
        self._logger.debug("executor_registered", action_type=key)

    def get(self, action_type: str) -> Executor | None:
        """
        Look up an executor by action type, with alias normalisation.

        Returns None if no executor is registered for the given type.
        """
        canonical = _normalise(action_type)
        return self._executors.get(canonical)

    def get_strict(self, action_type: str) -> Executor:
        """
        Look up an executor; raise KeyError if not found.
        """
        executor = self.get(action_type)
        if executor is None:
            canonical = _normalise(action_type)
            raise KeyError(
                f"No executor registered for action_type {action_type!r} "
                f"(normalised: {canonical!r}). "
                f"Available: {sorted(self._executors.keys())}"
            )
        return executor

    def list_types(self) -> list[str]:
        """Return all registered canonical action type names."""
        return sorted(self._executors.keys())

    def __contains__(self, action_type: str) -> bool:
        return self.get(action_type) is not None

    def __len__(self) -> int:
        return len(self._executors)

    def __repr__(self) -> str:
        return f"<ExecutorRegistry executors={self.list_types()}>"

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\safety.py =====

"""
EcodiaOS — Axon Safety Mechanisms

Three interlocking safety systems protect against runaway action loops,
cascading failures, and excessive external calls:

1. RateLimiter — sliding-window counters per executor type
   Prevents any single executor from flooding an external service or
   spamming notifications. Counters are in-memory (per-process).
   For distributed deployments, back this with Redis (future Synapse work).

2. CircuitBreaker — per-executor open/half-open/closed state machine
   If an executor repeatedly fails, it is disabled for a recovery window.
   After recovery_timeout_s, a single probe execution is allowed (half-open).
   Success → closed (normal). Failure → re-opens. Prevents cascading failures
   from a degraded external service.

3. BudgetTracker — per-cycle execution budget enforcement
   Limits the total number and type of actions EOS can take in a single
   cognitive cycle. This is the non-negotiable safety valve — it exists
   to prevent EOS from acting obsessively or exhausting shared resources.
   Budget limits come from AxonConfig and cannot be raised at runtime.

These are defence-in-depth. Nova's EFE evaluation and Equor's constitutional
review are the first two lines. The safety mechanisms are the third.
"""

from __future__ import annotations

import time
from collections import defaultdict, deque

import structlog

from ecodiaos.config import AxonConfig
from ecodiaos.systems.axon.types import CircuitState, CircuitStatus, ExecutionBudget, RateLimit

logger = structlog.get_logger()


# ─── Rate Limiter ─────────────────────────────────────────────────


class RateLimiter:
    """
    Sliding-window rate limiter.

    Each action type gets its own window. Calls are timestamped; any
    call outside the window_seconds boundary is evicted before checking.

    Thread-safe for single-process use (asyncio). For multi-process,
    integrate with Redis counters via Synapse.
    """

    def __init__(self) -> None:
        # action_type → deque of call timestamps (monotonic)
        self._windows: dict[str, deque[float]] = defaultdict(deque)
        self._logger = logger.bind(system="axon.rate_limiter")

    def check(self, action_type: str, rate_limit: RateLimit) -> bool:
        """
        Return True if the action is within its rate limit.

        Evicts expired entries from the window, then checks count.
        Does NOT record the call — call record() after a successful check.
        """
        window = self._windows[action_type]
        now = time.monotonic()
        cutoff = now - rate_limit.window_seconds

        # Evict expired timestamps
        while window and window[0] < cutoff:
            window.popleft()

        allowed = len(window) < rate_limit.max_calls
        if not allowed:
            self._logger.warning(
                "rate_limit_exceeded",
                action_type=action_type,
                current_count=len(window),
                max_calls=rate_limit.max_calls,
                window_seconds=rate_limit.window_seconds,
            )
        return allowed

    def record(self, action_type: str) -> None:
        """Record a call for rate-limit accounting."""
        self._windows[action_type].append(time.monotonic())

    def reset(self, action_type: str) -> None:
        """Reset the window for a specific action type (testing / governance)."""
        self._windows[action_type].clear()

    def current_count(self, action_type: str, window_seconds: float) -> int:
        """Return the number of calls within the last window_seconds."""
        window = self._windows[action_type]
        cutoff = time.monotonic() - window_seconds
        return sum(1 for ts in window if ts >= cutoff)


# ─── Circuit Breaker ──────────────────────────────────────────────


class CircuitBreaker:
    """
    Per-executor circuit breaker using a three-state finite state machine.

    States:
      CLOSED — normal operation; all executions allowed
      OPEN — tripped; all executions blocked for recovery_timeout_s
      HALF_OPEN — probing; allows exactly half_open_max_calls attempts

    Transitions:
      CLOSED → OPEN: failure_threshold consecutive failures
      OPEN → HALF_OPEN: after recovery_timeout_s
      HALF_OPEN → CLOSED: probe succeeds
      HALF_OPEN → OPEN: probe fails
    """

    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout_s: int = 300,
        half_open_max_calls: int = 1,
    ) -> None:
        self.failure_threshold = failure_threshold
        self.recovery_timeout_s = recovery_timeout_s
        self.half_open_max_calls = half_open_max_calls
        self._states: dict[str, CircuitState] = {}
        self._logger = logger.bind(system="axon.circuit_breaker")

    def can_execute(self, action_type: str) -> bool:
        """Return True if the circuit is closed or in a half-open probe window."""
        state = self._states.get(action_type)
        if state is None:
            return True

        if state.status == CircuitStatus.CLOSED:
            return True

        if state.status == CircuitStatus.OPEN:
            elapsed = time.monotonic() - state.tripped_at
            if elapsed >= self.recovery_timeout_s:
                # Transition to half-open for probing
                state.status = CircuitStatus.HALF_OPEN
                state.half_open_calls = 0
                self._logger.info(
                    "circuit_half_open",
                    action_type=action_type,
                    elapsed_s=int(elapsed),
                )
                return True
            return False

        if state.status == CircuitStatus.HALF_OPEN:
            if state.half_open_calls < self.half_open_max_calls:
                state.half_open_calls += 1
                return True
            return False

        return False  # Unreachable, but safe

    def record_result(self, action_type: str, success: bool) -> None:
        """Update circuit state after an execution attempt."""
        state = self._states.setdefault(action_type, CircuitState())

        if success:
            if state.status == CircuitStatus.HALF_OPEN:
                state.status = CircuitStatus.CLOSED
                state.consecutive_failures = 0
                self._logger.info("circuit_closed", action_type=action_type)
            elif state.status == CircuitStatus.CLOSED:
                state.consecutive_failures = 0
        else:
            state.consecutive_failures += 1
            if state.consecutive_failures >= self.failure_threshold:
                if state.status != CircuitStatus.OPEN:
                    state.status = CircuitStatus.OPEN
                    state.tripped_at = time.monotonic()
                    self._logger.warning(
                        "circuit_tripped",
                        action_type=action_type,
                        consecutive_failures=state.consecutive_failures,
                    )

    def status(self, action_type: str) -> CircuitStatus:
        """Return current circuit status for an executor."""
        state = self._states.get(action_type)
        return state.status if state else CircuitStatus.CLOSED

    def reset(self, action_type: str) -> None:
        """Manually reset a circuit (governance action)."""
        if action_type in self._states:
            del self._states[action_type]
            self._logger.info("circuit_manually_reset", action_type=action_type)

    def trip_count(self) -> int:
        """Total circuits currently open."""
        return sum(
            1 for s in self._states.values() if s.status == CircuitStatus.OPEN
        )


# ─── Budget Tracker ───────────────────────────────────────────────


class BudgetTracker:
    """
    Per-cycle execution budget enforcement.

    The budget is replenished at the start of each cognitive cycle by calling
    begin_cycle(). Checks are cumulative within the cycle — once a limit is
    hit, it blocks for the remainder of the cycle.

    Limits are sourced from AxonConfig and cannot be changed at runtime.
    """

    def __init__(self, config: AxonConfig) -> None:
        self._budget = ExecutionBudget(
            max_actions_per_cycle=config.max_actions_per_cycle,
            max_api_calls_per_minute=config.max_api_calls_per_minute,
            max_notifications_per_hour=config.max_notifications_per_hour,
            max_concurrent_executions=config.max_concurrent_executions,
            total_timeout_per_cycle_ms=config.total_timeout_per_cycle_ms,
        )
        self._logger = logger.bind(system="axon.budget_tracker")
        self._reset_counters()

    def _reset_counters(self) -> None:
        self._actions_this_cycle: int = 0
        self._concurrent_executions: int = 0
        self._cycle_start: float = time.monotonic()

    def begin_cycle(self) -> None:
        """Called at the start of each cognitive cycle to reset per-cycle counters."""
        self._reset_counters()

    def can_execute(self) -> tuple[bool, str]:
        """
        Check if the budget allows another execution.
        Returns (allowed, reason) — reason is empty string if allowed.
        """
        if self._actions_this_cycle >= self._budget.max_actions_per_cycle:
            return False, (
                f"Budget: max actions per cycle reached "
                f"({self._budget.max_actions_per_cycle})"
            )
        if self._concurrent_executions >= self._budget.max_concurrent_executions:
            return False, (
                f"Budget: max concurrent executions reached "
                f"({self._budget.max_concurrent_executions})"
            )
        elapsed_ms = int((time.monotonic() - self._cycle_start) * 1000)
        if elapsed_ms >= self._budget.total_timeout_per_cycle_ms:
            return False, (
                f"Budget: cycle timeout exceeded "
                f"({self._budget.total_timeout_per_cycle_ms}ms)"
            )
        return True, ""

    def begin_execution(self) -> None:
        """Called when an execution starts (tracks concurrency)."""
        self._concurrent_executions += 1
        self._actions_this_cycle += 1

    def end_execution(self) -> None:
        """Called when an execution completes or fails (releases concurrency slot)."""
        self._concurrent_executions = max(0, self._concurrent_executions - 1)

    @property
    def utilisation(self) -> float:
        """Fraction of cycle action budget consumed (0.0 to 1.0+)."""
        return self._actions_this_cycle / max(1, self._budget.max_actions_per_cycle)

    @property
    def budget(self) -> ExecutionBudget:
        return self._budget

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\service.py =====

"""
EcodiaOS — Axon Service

The motor cortex. Axon receives approved Intents from Nova and turns them into
real-world effects — memory writes, API calls, scheduled tasks, notifications,
and federated messages.

Axon does not decide. It does not judge. It executes.
Decision authority lives in Nova. Ethical authority lives in Equor.
Axon is the disciplined hand that carries out the will.

Lifecycle:
  initialize() — builds the executor registry, wires safety systems
  execute()    — main entry point: accepts ExecutionRequest, returns AxonOutcome
  set_nova()   — wires the Nova feedback loop for outcome delivery
  shutdown()   — graceful teardown

Interface contracts (from spec):
  Validation (all steps):       ≤50ms
  Rate limit check:             ≤5ms
  Context assembly:             ≤30ms
  Simple intent (1-2 internal): ≤300ms end-to-end
  Complex intent (external):    ≤15,000ms end-to-end
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import structlog

from ecodiaos.config import AxonConfig
from ecodiaos.systems.axon.audit import AuditLogger
from ecodiaos.systems.axon.credentials import CredentialStore
from ecodiaos.systems.axon.executors import build_default_registry
from ecodiaos.systems.axon.pipeline import ExecutionPipeline
from ecodiaos.systems.axon.registry import ExecutorRegistry
from ecodiaos.systems.axon.safety import BudgetTracker, CircuitBreaker, RateLimiter
from ecodiaos.systems.axon.types import AxonOutcome, ExecutionRequest

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.nova.service import NovaService
    from ecodiaos.systems.voxis.service import VoxisService

logger = structlog.get_logger()


class AxonService:
    """
    Axon — the EOS action execution system.

    AxonService is the single entry point for action execution.
    It owns and coordinates all sub-systems:
      - ExecutorRegistry: maps action types to handler implementations
      - ExecutionPipeline: runs the 7-stage execution pipeline
      - BudgetTracker: enforces per-cycle action limits
      - RateLimiter: sliding-window per-executor rate limits
      - CircuitBreaker: per-executor open/closed/half-open state
      - CredentialStore: issues scoped, time-limited credentials
      - AuditLogger: records every execution permanently

    All sub-systems are constructed in initialize() and are immutable at runtime.
    """

    system_id: str = "axon"

    def __init__(
        self,
        config: AxonConfig,
        memory: "MemoryService | None" = None,
        voxis: "VoxisService | None" = None,
        redis_client=None,
        instance_id: str = "eos-default",
    ) -> None:
        self._config = config
        self._memory = memory
        self._voxis = voxis
        self._redis = redis_client
        self._instance_id = instance_id
        self._logger = logger.bind(system="axon")
        self._initialized = False

        # Sub-systems — built in initialize()
        self._registry: ExecutorRegistry | None = None
        self._pipeline: ExecutionPipeline | None = None
        self._budget: BudgetTracker | None = None
        self._rate_limiter: RateLimiter | None = None
        self._circuit_breaker: CircuitBreaker | None = None
        self._credential_store: CredentialStore | None = None
        self._audit: AuditLogger | None = None

        # Metrics
        self._total_executions: int = 0
        self._successful_executions: int = 0
        self._failed_executions: int = 0

    async def initialize(self) -> None:
        """
        Initialise all Axon sub-systems and build the executor registry.

        Must be called before any execute() calls.
        Idempotent — safe to call multiple times.
        """
        if self._initialized:
            return

        self._logger.info("axon_initializing", instance_id=self._instance_id)

        # Safety systems
        self._budget = BudgetTracker(self._config)
        self._rate_limiter = RateLimiter()
        self._circuit_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout_s=300,
            half_open_max_calls=1,
        )

        # Credential store
        self._credential_store = CredentialStore()

        # Audit logger
        self._audit = AuditLogger(memory=self._memory)

        # Build executor registry with all built-in executors
        self._registry = build_default_registry(
            memory=self._memory,
            voxis=self._voxis,
            redis_client=self._redis,
        )

        # Execution pipeline
        self._pipeline = ExecutionPipeline(
            registry=self._registry,
            budget=self._budget,
            rate_limiter=self._rate_limiter,
            circuit_breaker=self._circuit_breaker,
            credential_store=self._credential_store,
            audit_logger=self._audit,
            instance_id=self._instance_id,
        )

        self._initialized = True
        self._logger.info(
            "axon_initialized",
            executors=len(self._registry),
            executor_types=self._registry.list_types(),
        )

    def set_nova(self, nova: "NovaService") -> None:
        """
        Wire the Nova feedback loop.

        Call this after both Nova and Axon are initialised.
        Must be called before execute() for outcome delivery to work.
        """
        if self._pipeline is None:
            raise RuntimeError("AxonService.initialize() must be called before set_nova()")
        self._pipeline.set_nova(nova)
        self._logger.info("nova_wired", system="axon")

    def set_atune(self, atune) -> None:
        """
        Wire Atune so execution outcomes become workspace percepts.

        The organism should perceive its own actions — closing the
        intention→execution→perception loop.
        """
        if self._pipeline is None:
            raise RuntimeError("AxonService.initialize() must be called before set_atune()")
        self._pipeline.set_atune(atune)
        self._logger.info("atune_wired", system="axon")

    def configure_credentials(self, credentials: dict[str, str]) -> None:
        """
        Load credentials into the CredentialStore.

        Called at startup to configure external service secrets.
        Format: {"service_name": "raw_secret_or_api_key"}
        """
        if self._credential_store is None:
            raise RuntimeError("AxonService.initialize() must be called first")
        self._credential_store.configure(credentials)

    def begin_cycle(self) -> None:
        """
        Notify Axon that a new cognitive cycle is starting.

        Resets the per-cycle execution budget. Call from Synapse at the
        start of each theta rhythm cycle.
        """
        if self._budget is not None:
            self._budget.begin_cycle()

    async def execute(self, request: ExecutionRequest) -> AxonOutcome:
        """
        Execute an approved Intent.

        This is the main external interface — Nova calls this via IntentRouter
        after Equor has approved the Intent.

        Args:
            request: ExecutionRequest containing the approved Intent and Equor verdict.

        Returns:
            AxonOutcome with full step-level detail and outcome summary.
            Never raises — failures are captured in the outcome.
        """
        if not self._initialized or self._pipeline is None:
            raise RuntimeError(
                "AxonService.initialize() must be called before execute()"
            )

        self._total_executions += 1

        self._logger.info(
            "execute_start",
            intent_id=request.intent.id,
            goal=request.intent.goal.description[:60],
            steps=len(request.intent.plan.steps),
        )

        try:
            outcome = await self._pipeline.execute(request)
        except Exception as exc:
            # Pipeline should never raise, but guard anyway
            self._logger.error(
                "pipeline_raised_unexpectedly",
                intent_id=request.intent.id,
                error=str(exc),
            )
            from ecodiaos.systems.axon.types import ExecutionStatus, FailureReason
            from ecodiaos.primitives.common import new_id
            outcome = AxonOutcome(
                intent_id=request.intent.id,
                execution_id=new_id(),
                success=False,
                status=ExecutionStatus.FAILURE,
                failure_reason=FailureReason.EXECUTION_EXCEPTION.value,
                error=str(exc),
            )

        if outcome.success:
            self._successful_executions += 1
        else:
            self._failed_executions += 1

        return outcome

    def register_executor(self, executor) -> None:
        """
        Register a custom executor at runtime.

        For use by integrations, plugins, and governance-approved extensions.
        The executor must be an instance of Executor ABC.
        """
        if self._registry is None:
            raise RuntimeError("AxonService.initialize() must be called first")
        self._registry.register(executor)
        self._logger.info(
            "executor_registered_runtime",
            action_type=executor.action_type,
        )

    async def health(self) -> dict:
        """Self-health report (implements ManagedSystem protocol)."""
        return {
            "status": "healthy" if self._initialized else "starting",
            "total_executions": self._total_executions,
            "successful": self._successful_executions,
            "failed": self._failed_executions,
            "executor_count": len(self._registry) if self._registry else 0,
        }

    async def shutdown(self) -> None:
        """Graceful shutdown — log final stats."""
        self._logger.info(
            "axon_shutdown",
            total_executions=self._total_executions,
            successful=self._successful_executions,
            failed=self._failed_executions,
            circuit_trips=self._circuit_breaker.trip_count()
            if self._circuit_breaker else 0,
            audit_stats=self._audit.stats if self._audit else {},
        )

    @property
    def stats(self) -> dict:
        """Return current operational statistics."""
        return {
            "initialized": self._initialized,
            "total_executions": self._total_executions,
            "successful_executions": self._successful_executions,
            "failed_executions": self._failed_executions,
            "executor_count": len(self._registry) if self._registry else 0,
            "executor_types": self._registry.list_types() if self._registry else [],
            "circuit_trips": self._circuit_breaker.trip_count()
            if self._circuit_breaker
            else 0,
            "budget_utilisation": self._budget.utilisation if self._budget else 0.0,
            "audit": self._audit.stats if self._audit else {},
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\types.py =====

"""
EcodiaOS — Axon Internal Types

All types internal to Axon's action execution system.

Design notes:
- AxonOutcome is Axon's rich internal result. It carries step-level detail,
  world-state changes, and rollback metadata. When reporting back to Nova,
  it is converted to IntentOutcome (from nova/types.py), which is the
  cross-system outcome primitive.
- ExecutionContext is assembled per-execution — it carries the approved intent,
  Equor verdict, scoped credentials, and current affect. Executors receive it
  but cannot modify it.
- ScopedCredentials wraps time-limited tokens issued per-execution. Executors
  never see raw secrets.
"""

from __future__ import annotations

import enum
import hashlib
import json
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import EOSBaseModel, Identified, Timestamped, new_id, utc_now
from ecodiaos.primitives.constitutional import ConstitutionalCheck
from ecodiaos.primitives.intent import Intent


# ─── Enums ────────────────────────────────────────────────────────


class ExecutionStatus(str, enum.Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILURE = "failure"
    PARTIAL = "partial"
    ROLLED_BACK = "rolled_back"
    TIMED_OUT = "timed_out"
    RATE_LIMITED = "rate_limited"
    CIRCUIT_OPEN = "circuit_open"


class FailureReason(str, enum.Enum):
    UNKNOWN_ACTION_TYPE = "unknown_action_type"
    VALIDATION_ERROR = "validation_error"
    RATE_LIMITED = "rate_limited"
    CIRCUIT_OPEN = "circuit_open"
    INSUFFICIENT_AUTONOMY = "insufficient_autonomy"
    TIMEOUT = "timeout"
    STEP_FAILED = "step_failed"
    BUDGET_EXCEEDED = "budget_exceeded"
    CREDENTIAL_ERROR = "credential_error"
    EXECUTION_EXCEPTION = "execution_exception"


class CircuitStatus(str, enum.Enum):
    CLOSED = "closed"      # Normal — executions allowed
    OPEN = "open"          # Tripped — executions blocked
    HALF_OPEN = "half_open"  # Recovering — limited executions allowed


# ─── Primitive Execution Types ────────────────────────────────────


class ValidationResult(EOSBaseModel):
    """Result of parameter validation before execution."""

    valid: bool
    reason: str = ""
    field_errors: dict[str, str] = Field(default_factory=dict)

    @classmethod
    def ok(cls) -> ValidationResult:
        return cls(valid=True)

    @classmethod
    def fail(cls, reason: str, **field_errors: str) -> ValidationResult:
        return cls(valid=False, reason=reason, field_errors=field_errors)


class RollbackResult(EOSBaseModel):
    """Result of attempting to roll back a completed step."""

    success: bool
    reason: str = ""
    side_effects_reversed: list[str] = Field(default_factory=list)


class ExecutionResult(EOSBaseModel):
    """
    The result of executing a single action step.

    The data dict carries step-specific output — query results, created IDs,
    API responses. Callers should not depend on specific keys without checking
    the executor's documentation.
    """

    success: bool
    data: dict[str, Any] = Field(default_factory=dict)
    error: str = ""
    # Human-readable descriptions of what changed in the world
    side_effects: list[str] = Field(default_factory=list)
    # New observations to feed back as Percepts (e.g., API response content)
    new_observations: list[str] = Field(default_factory=list)


class StepOutcome(EOSBaseModel):
    """
    The recorded outcome of a single plan step.
    Collected across all steps to form the full AxonOutcome.
    """

    step_index: int
    action_type: str   # executor name (e.g., "store_insight", "call_api")
    description: str
    result: ExecutionResult
    duration_ms: int
    rolled_back: bool = False


# ─── Execution Context ────────────────────────────────────────────


class ScopedCredentials(EOSBaseModel):
    """
    Time-limited, scope-restricted tokens for external services.
    Issued per-execution by CredentialStore — executors never see raw secrets.
    """

    tokens: dict[str, str] = Field(default_factory=dict)
    issued_at: datetime = Field(default_factory=utc_now)
    expires_at: datetime | None = None

    def get(self, service: str) -> str | None:
        return self.tokens.get(service)


class ExecutionContext(EOSBaseModel):
    """
    The complete context for one intent execution.
    Assembled once, passed to all executors — read-only from executor perspective.
    """

    execution_id: str = Field(default_factory=new_id)
    intent: Intent
    equor_check: ConstitutionalCheck
    credentials: ScopedCredentials = Field(default_factory=ScopedCredentials)
    instance_id: str = "eos-default"
    affect_state: AffectState = Field(default_factory=AffectState.neutral)
    started_at: datetime = Field(default_factory=utc_now)

    model_config = {"arbitrary_types_allowed": True}


# ─── Execution Budget & Rate Limiting ────────────────────────────


@dataclass
class ExecutionBudget:
    """
    Per-cycle limits on action execution.
    Non-negotiable safety valve — cannot be overridden by Nova or Simula.
    Only governance (Equor amendment) can change these limits.
    """

    max_actions_per_cycle: int = 5
    max_api_calls_per_minute: int = 30
    max_notifications_per_hour: int = 10
    max_federation_messages_per_hour: int = 50
    max_concurrent_executions: int = 3
    total_timeout_per_cycle_ms: int = 30_000


@dataclass
class RateLimit:
    """Rate limit definition for an executor."""

    max_calls: int
    window_seconds: int

    @classmethod
    def unlimited(cls) -> RateLimit:
        return cls(max_calls=10_000, window_seconds=1)

    @classmethod
    def per_minute(cls, max_calls: int) -> RateLimit:
        return cls(max_calls=max_calls, window_seconds=60)

    @classmethod
    def per_hour(cls, max_calls: int) -> RateLimit:
        return cls(max_calls=max_calls, window_seconds=3600)


# ─── Circuit Breaker State ────────────────────────────────────────


@dataclass
class CircuitState:
    """Per-executor circuit breaker state."""

    status: CircuitStatus = CircuitStatus.CLOSED
    consecutive_failures: int = 0
    tripped_at: float = 0.0
    half_open_calls: int = 0


# ─── Execution Request & Outcome ─────────────────────────────────


class ExecutionRequest(EOSBaseModel):
    """
    The input to AxonService.execute().
    Carries the approved Intent plus the Equor verdict that cleared it.
    """

    intent: Intent
    equor_check: ConstitutionalCheck
    timeout_ms: int = 30_000

    model_config = {"arbitrary_types_allowed": True}


class AxonOutcome(EOSBaseModel):
    """
    Axon's rich execution outcome.

    This is Axon-internal and carries full step-level detail.
    When reporting back to Nova via IntentOutcome, it is converted to
    the shared primitive (nova.types.IntentOutcome).
    """

    intent_id: str
    execution_id: str
    success: bool
    partial: bool = False
    status: ExecutionStatus = ExecutionStatus.PENDING
    failure_reason: str = ""
    error: str = ""
    step_outcomes: list[StepOutcome] = Field(default_factory=list)
    duration_ms: int = 0
    world_state_changes: list[str] = Field(default_factory=list)
    new_observations: list[str] = Field(default_factory=list)
    episode_id: str = ""

    def classify_failure(self) -> str:
        """Derive a failure reason from step outcomes."""
        if self.failure_reason:
            return self.failure_reason
        for step in self.step_outcomes:
            if not step.result.success and step.result.error:
                return step.result.error[:100]
        return FailureReason.STEP_FAILED.value

    def collect_world_changes(self) -> list[str]:
        """Aggregate side effects across all steps."""
        changes: list[str] = []
        for step in self.step_outcomes:
            changes.extend(step.result.side_effects)
        return changes

    def collect_new_observations(self) -> list[str]:
        """Aggregate new observations from all steps."""
        obs: list[str] = []
        for step in self.step_outcomes:
            obs.extend(step.result.new_observations)
        return obs


# ─── Audit Record ─────────────────────────────────────────────────


class AuditRecord(Identified, Timestamped):
    """
    Permanent record of every action taken by Axon.
    Stored as (:GovernanceRecord {type: "action_audit"}) in the Memory graph.

    Parameters are hashed, not stored raw, to protect sensitive data.
    """

    execution_id: str
    intent_id: str
    equor_verdict: str
    equor_reasoning: str
    action_type: str
    # SHA-256 of JSON-serialised parameters — never raw
    parameters_hash: str
    target: str            # What system/entity was acted upon
    result: str            # "success" | "failure" | "partial" | "rolled_back"
    duration_ms: int
    affect_state: AffectState = Field(default_factory=AffectState.neutral)
    autonomy_level: int = 1

    model_config = {"arbitrary_types_allowed": True}

    @classmethod
    def from_outcome(
        cls,
        outcome: AxonOutcome,
        context: ExecutionContext,
        parameters: dict[str, Any],
        action_type: str,
    ) -> AuditRecord:
        params_json = json.dumps(parameters, sort_keys=True, default=str)
        params_hash = hashlib.sha256(params_json.encode()).hexdigest()

        if outcome.success:
            result = "success"
        elif outcome.partial:
            result = "partial"
        else:
            result = "failure"

        return cls(
            execution_id=outcome.execution_id,
            intent_id=outcome.intent_id,
            equor_verdict=context.equor_check.verdict.value
            if hasattr(context.equor_check, "verdict")
            else "approved",
            equor_reasoning=context.equor_check.reasoning
            if hasattr(context.equor_check, "reasoning")
            else "",
            action_type=action_type,
            parameters_hash=params_hash,
            target=parameters.get("target", parameters.get("executor", action_type)),
            result=result,
            duration_ms=outcome.duration_ms,
            affect_state=context.affect_state,
            autonomy_level=context.intent.autonomy_level_granted,
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\executors\__init__.py =====

"""
EcodiaOS — Axon Built-in Executors

All built-in executors for the EOS action system.

Executors are organised by capability category:
  observation   — ObserveExecutor, QueryMemoryExecutor, AnalyseExecutor, SearchExecutor
  communication — RespondTextExecutor, NotificationExecutor, PostMessageExecutor
  data          — CreateRecordExecutor, UpdateRecordExecutor, ScheduleExecutor, ReminderExecutor
  integration   — APICallExecutor, WebhookExecutor
  internal      — StoreInsightExecutor, UpdateGoalExecutor, ConsolidationExecutor

Import build_default_registry() to get a fully-populated ExecutorRegistry.
"""

from __future__ import annotations

from ecodiaos.systems.axon.executors.communication import (
    NotificationExecutor,
    PostMessageExecutor,
    RespondTextExecutor,
)
from ecodiaos.systems.axon.executors.data import (
    CreateRecordExecutor,
    ReminderExecutor,
    ScheduleExecutor,
    UpdateRecordExecutor,
)
from ecodiaos.systems.axon.executors.integration import (
    APICallExecutor,
    WebhookExecutor,
)
from ecodiaos.systems.axon.executors.internal import (
    ConsolidationExecutor,
    StoreInsightExecutor,
    UpdateGoalExecutor,
)
from ecodiaos.systems.axon.executors.observation import (
    AnalyseExecutor,
    ObserveExecutor,
    QueryMemoryExecutor,
    SearchExecutor,
)
from ecodiaos.systems.axon.registry import ExecutorRegistry

__all__ = [
    "build_default_registry",
    # Observation
    "ObserveExecutor",
    "QueryMemoryExecutor",
    "AnalyseExecutor",
    "SearchExecutor",
    # Communication
    "RespondTextExecutor",
    "NotificationExecutor",
    "PostMessageExecutor",
    # Data
    "CreateRecordExecutor",
    "UpdateRecordExecutor",
    "ScheduleExecutor",
    "ReminderExecutor",
    # Integration
    "APICallExecutor",
    "WebhookExecutor",
    # Internal
    "StoreInsightExecutor",
    "UpdateGoalExecutor",
    "ConsolidationExecutor",
]


def build_default_registry(
    memory=None,
    voxis=None,
    redis_client=None,
) -> ExecutorRegistry:
    """
    Build and return a fully-populated ExecutorRegistry with all built-in executors.

    Args:
        memory: MemoryService instance (for memory-backed executors)
        voxis: VoxisService instance (for RespondTextExecutor)
        redis_client: Redis client (for scheduled tasks and reminders)
    """
    registry = ExecutorRegistry()

    # ── Observation (Level 1) ──────────────────────────────────────
    registry.register(ObserveExecutor(memory=memory))
    registry.register(QueryMemoryExecutor(memory=memory))
    registry.register(AnalyseExecutor(memory=memory))
    registry.register(SearchExecutor())

    # ── Communication (Level 1-2) ─────────────────────────────────
    registry.register(RespondTextExecutor(voxis=voxis))
    registry.register(NotificationExecutor(redis_client=redis_client))
    registry.register(PostMessageExecutor(memory=memory))

    # ── Data Operations (Level 2) ─────────────────────────────────
    registry.register(CreateRecordExecutor(memory=memory))
    registry.register(UpdateRecordExecutor(memory=memory))
    registry.register(ScheduleExecutor(redis_client=redis_client))
    registry.register(ReminderExecutor(redis_client=redis_client))

    # ── Integration (Level 2-3) ───────────────────────────────────
    registry.register(APICallExecutor())
    registry.register(WebhookExecutor())

    # ── Internal (Level 1) ───────────────────────────────────────
    registry.register(StoreInsightExecutor(memory=memory))
    registry.register(UpdateGoalExecutor())
    registry.register(ConsolidationExecutor(memory=memory))

    return registry

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\executors\communication.py =====

"""
EcodiaOS — Axon Communication Executors

Communication executors send messages to people. They range from Level 1
(responding in an active conversation) to Level 2 (pushing unsolicited notifications).

RespondTextExecutor  — (Level 1) route text response through Voxis for personality rendering
NotificationExecutor — (Level 2) send a push notification to a user or group
PostMessageExecutor  — (Level 2) post to a shared community channel

These executors are not reversible — you cannot un-send a message.
This asymmetry is intentional: communication has real effects in the world.
Nova and Equor bear full responsibility for approving communication intents.

All communication is routed through Voxis for personality rendering —
Axon never sends raw text directly to users.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import structlog

from ecodiaos.systems.axon.executor import Executor
from ecodiaos.systems.axon.types import (
    ExecutionContext,
    ExecutionResult,
    RateLimit,
    ValidationResult,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.voxis.service import VoxisService

logger = structlog.get_logger()


# ─── RespondTextExecutor ──────────────────────────────────────────


class RespondTextExecutor(Executor):
    """
    Send a text response in the current conversation via Voxis.

    This is the primary "speak" action. It routes the response content to
    Voxis, which applies personality rendering, affect colouring, and
    silence judgement before delivery.

    Level 1: Responding in an active conversation is always within ADVISOR scope.

    Required params:
      content (str): The response content to express.

    Optional params:
      conversation_id (str): Target conversation. Default: current active conversation.
      urgency (float 0-1): Expression urgency. Default 0.5.
      trigger (str): Voxis trigger hint. Default "NOVA_RESPOND".
    """

    action_type = "respond_text"
    description = "Send a text response via Voxis personality engine"
    required_autonomy = 1
    reversible = False
    max_duration_ms = 5000
    rate_limit = RateLimit.per_minute(30)

    def __init__(self, voxis: "VoxisService | None" = None) -> None:
        self._voxis = voxis
        self._logger = logger.bind(system="axon.executor.respond_text")

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("content"):
            return ValidationResult.fail("content is required", content="missing or empty")
        content = params["content"]
        if not isinstance(content, str):
            return ValidationResult.fail("content must be a string")
        if len(content) > 10_000:
            return ValidationResult.fail("content too long (max 10,000 chars)")
        urgency = params.get("urgency", 0.5)
        if not isinstance(urgency, (int, float)) or not 0.0 <= float(urgency) <= 1.0:
            return ValidationResult.fail("urgency must be a float between 0.0 and 1.0")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        content = params["content"]
        conversation_id = params.get("conversation_id")
        urgency = float(params.get("urgency", 0.5))
        trigger_name = params.get("trigger", "NOVA_RESPOND")

        self._logger.info(
            "respond_text_execute",
            content_preview=content[:80],
            conversation_id=conversation_id,
            execution_id=context.execution_id,
        )

        if self._voxis is None:
            return ExecutionResult(
                success=True,
                data={"content": content, "delivered": False},
                side_effects=["Text response staged (no Voxis service)"],
                new_observations=[f"EOS would have said: {content[:200]}"],
            )

        try:
            from ecodiaos.systems.voxis.types import ExpressionTrigger

            trigger = _resolve_trigger(trigger_name)
            await self._voxis.express(
                content=content,
                trigger=trigger,
                conversation_id=conversation_id,
                affect=context.affect_state,
                urgency=urgency,
            )
            return ExecutionResult(
                success=True,
                data={"content_length": len(content), "delivered": True},
                side_effects=["Text response delivered via Voxis"],
            )
        except Exception as exc:
            return ExecutionResult(
                success=False,
                error=f"Voxis expression failed: {exc}",
            )


def _resolve_trigger(trigger_name: str) -> "ExpressionTrigger":
    from ecodiaos.systems.voxis.types import ExpressionTrigger
    try:
        return ExpressionTrigger[trigger_name]
    except KeyError:
        return ExpressionTrigger.NOVA_RESPOND


# ─── NotificationExecutor ─────────────────────────────────────────


class NotificationExecutor(Executor):
    """
    Send a push notification to a user or group of users.

    Level 2: Sending unsolicited notifications requires PARTNER autonomy.
    EOS should use these sparingly — notification overload undermines trust
    and Care. Equor should scrutinise notification intents carefully.

    Required params:
      recipient_id (str): User ID or group ID to notify.
      title (str): Short notification title.
      body (str): Notification body text.

    Optional params:
      urgency (str): "low" | "normal" | "high". Default "normal".
      action_url (str): Deep link URL for the notification. Default None.
    """

    action_type = "send_notification"
    description = "Send a push notification to a user or group (Level 2)"
    required_autonomy = 2
    reversible = False
    max_duration_ms = 5000
    rate_limit = RateLimit.per_hour(10)  # Strict — notification spam is harmful

    def __init__(self, redis_client=None) -> None:
        self._redis = redis_client
        self._logger = logger.bind(system="axon.executor.notification")

    async def validate_params(self, params: dict) -> ValidationResult:
        for field in ("recipient_id", "title", "body"):
            if not params.get(field):
                return ValidationResult.fail(
                    f"{field} is required",
                    **{field: "missing or empty"},
                )
        urgency = params.get("urgency", "normal")
        if urgency not in ("low", "normal", "high"):
            return ValidationResult.fail("urgency must be 'low', 'normal', or 'high'")
        title = params["title"]
        if len(title) > 100:
            return ValidationResult.fail("title too long (max 100 chars)")
        body = params["body"]
        if len(body) > 500:
            return ValidationResult.fail("body too long (max 500 chars)")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        recipient_id = params["recipient_id"]
        title = params["title"]
        body = params["body"]
        urgency = params.get("urgency", "normal")
        action_url = params.get("action_url")

        self._logger.info(
            "notification_execute",
            recipient_id=recipient_id,
            title=title,
            urgency=urgency,
            execution_id=context.execution_id,
        )

        notification_payload = {
            "type": "push_notification",
            "recipient_id": recipient_id,
            "title": title,
            "body": body,
            "urgency": urgency,
            "action_url": action_url,
            "sender": "eos",
            "execution_id": context.execution_id,
        }

        if self._redis is not None:
            try:
                import json as _json
                channel = f"eos:notifications:{recipient_id}"
                await self._redis.publish(channel, _json.dumps(notification_payload))
                return ExecutionResult(
                    success=True,
                    data={"channel": channel, "delivered": True},
                    side_effects=[
                        f"Notification '{title}' sent to {recipient_id} ({urgency} urgency)"
                    ],
                )
            except Exception as exc:
                return ExecutionResult(
                    success=False,
                    error=f"Notification publish failed: {exc}",
                )
        else:
            self._logger.info("notification_no_redis", payload=notification_payload)
            return ExecutionResult(
                success=True,
                data={"delivered": False, "reason": "No Redis client"},
                side_effects=[f"Notification staged: '{title}' → {recipient_id}"],
            )


# ─── PostMessageExecutor ──────────────────────────────────────────


class PostMessageExecutor(Executor):
    """
    Post a message to a shared community channel.

    Level 2: Posting to shared channels affects multiple people simultaneously.
    This requires PARTNER autonomy — EOS should not post announcements,
    meeting notes, or channel messages without appropriate governance.

    Required params:
      channel_id (str): Target channel or space ID.
      content (str): Message content.

    Optional params:
      author_label (str): Display label for the author. Default "EOS".
      thread_id (str): Reply to a specific thread. Default None (new thread).
    """

    action_type = "post_message"
    description = "Post a message to a shared community channel (Level 2)"
    required_autonomy = 2
    reversible = False
    max_duration_ms = 5000
    rate_limit = RateLimit.per_hour(20)

    def __init__(self, memory: "MemoryService | None" = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="axon.executor.post_message")

    async def validate_params(self, params: dict) -> ValidationResult:
        for field in ("channel_id", "content"):
            if not params.get(field):
                return ValidationResult.fail(
                    f"{field} is required",
                    **{field: "missing or empty"},
                )
        content = params["content"]
        if len(content) > 5000:
            return ValidationResult.fail("content too long (max 5,000 chars)")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        channel_id = params["channel_id"]
        content = params["content"]
        author_label = params.get("author_label", "EOS")
        thread_id = params.get("thread_id")

        self._logger.info(
            "post_message_execute",
            channel_id=channel_id,
            content_preview=content[:80],
            execution_id=context.execution_id,
        )

        # Store the post as an episodic memory (EOS said this, publicly)
        if self._memory is not None:
            try:
                from ecodiaos.primitives.memory_trace import MemoryTrace
                from ecodiaos.primitives.common import Modality, SourceDescriptor, SystemID

                trace = MemoryTrace(
                    content=f"[Channel post to {channel_id}] {content}",
                    source=SourceDescriptor(
                        system=SystemID.AXON,
                        channel=channel_id,
                        modality=Modality.TEXT,
                    ),
                    salience=0.7,  # Public posts have high salience
                    tags=["channel_post", channel_id],
                    affect_at_encoding=context.affect_state,
                )
                await self._memory.store_episodic(trace)
            except Exception as exc:
                self._logger.warning("post_message_memory_write_failed", error=str(exc))

        # Phase 1: stub delivery — Phase 2 will integrate with community platform
        return ExecutionResult(
            success=True,
            data={
                "channel_id": channel_id,
                "author_label": author_label,
                "thread_id": thread_id,
                "content_length": len(content),
                "delivered": False,
                "note": "Channel delivery integration pending (Phase 2)",
            },
            side_effects=[f"Message posted to channel {channel_id} by {author_label}"],
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\executors\data.py =====

"""
EcodiaOS — Axon Data Executors

Data executors create and modify persistent records. They are Level 2
(PARTNER autonomy) because they change shared world state in ways that
other community members can see and depend on.

CreateRecordExecutor — create a new data record in the knowledge graph
UpdateRecordExecutor — update an existing record
ScheduleExecutor     — schedule a future event (stored in Redis sorted set)
ReminderExecutor     — set a reminder for a user or for EOS itself

Create and Update are reversible — their rollback handlers delete or revert
the changes they made. Schedule and Reminder are reversible by cancellation.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.axon.executor import Executor
from ecodiaos.systems.axon.types import (
    ExecutionContext,
    ExecutionResult,
    RateLimit,
    RollbackResult,
    ValidationResult,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()


# ─── CreateRecordExecutor ─────────────────────────────────────────


class CreateRecordExecutor(Executor):
    """
    Create a new data record in the EOS knowledge graph.

    Records can be any entity type (Event, Task, Note, Community resource, etc.)
    They are stored as Neo4j nodes and become part of EOS's world model.

    Required params:
      record_type (str): Entity type (e.g., "Event", "Task", "Note").
      data (dict): Properties for the record.

    Optional params:
      title (str): Human-readable title. Default derived from data.
      tags (list[str]): Labels for retrieval. Default [].
      related_to (list[str]): IDs of related records. Default [].
    """

    action_type = "create_record"
    description = "Create a new data record in the knowledge graph (Level 2)"
    required_autonomy = 2
    reversible = True
    max_duration_ms = 3000
    rate_limit = RateLimit.per_minute(20)

    def __init__(self, memory: "MemoryService | None" = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="axon.executor.create_record")
        # Track created record IDs for rollback
        self._created: dict[str, str] = {}  # execution_id → record_id

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("record_type"):
            return ValidationResult.fail("record_type is required")
        if not params.get("data"):
            return ValidationResult.fail("data is required and must be non-empty dict")
        if not isinstance(params["data"], dict):
            return ValidationResult.fail("data must be a dict")
        record_type = params["record_type"]
        if not isinstance(record_type, str) or len(record_type) > 100:
            return ValidationResult.fail("record_type must be a string (max 100 chars)")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        record_type = params["record_type"]
        data = dict(params["data"])
        title = params.get("title", data.get("title", f"New {record_type}"))
        tags = list(params.get("tags", []))
        related_to = list(params.get("related_to", []))

        self._logger.info(
            "create_record_execute",
            record_type=record_type,
            title=str(title)[:60],
            execution_id=context.execution_id,
        )

        if self._memory is not None:
            try:
                # resolve_and_create_entity handles deduplication and returns (id, was_created)
                description = str(data.get("description", f"{record_type}: {title}"))
                record_id, was_created = await self._memory.resolve_and_create_entity(
                    name=str(title),
                    entity_type=record_type,
                    description=description,
                )
                self._created[context.execution_id] = record_id
                return ExecutionResult(
                    success=True,
                    data={"record_id": record_id, "record_type": record_type, "title": title,
                          "was_created": was_created},
                    side_effects=[f"{'Created' if was_created else 'Merged'} {record_type} "
                                  f"record: '{title}' (id={record_id})"],
                )
            except Exception as exc:
                return ExecutionResult(
                    success=False,
                    error=f"Record creation failed: {exc}",
                )
        else:
            from ecodiaos.primitives.common import new_id
            record_id = new_id()
            return ExecutionResult(
                success=True,
                data={"record_id": record_id, "record_type": record_type, "note": "No memory service"},
                side_effects=[f"Record creation staged: {record_type} '{title}'"],
            )

    async def rollback(
        self,
        execution_id: str,
        context: ExecutionContext,
    ) -> RollbackResult:
        record_id = self._created.get(execution_id)
        if not record_id:
            return RollbackResult(success=False, reason="No record ID found for this execution")

        if self._memory is not None:
            try:
                await self._memory.delete_entity(record_id)
                del self._created[execution_id]
                return RollbackResult(
                    success=True,
                    side_effects_reversed=[f"Deleted record {record_id}"],
                )
            except Exception as exc:
                return RollbackResult(success=False, reason=f"Delete failed: {exc}")

        return RollbackResult(success=False, reason="No memory service for rollback")


# ─── UpdateRecordExecutor ─────────────────────────────────────────


class UpdateRecordExecutor(Executor):
    """
    Update properties of an existing record in the knowledge graph.

    Required params:
      record_id (str): ID of the record to update.
      updates (dict): Properties to set or update.

    Optional params:
      merge (bool): If True, merge with existing properties. Default True.
    """

    action_type = "update_record"
    description = "Update an existing record in the knowledge graph (Level 2)"
    required_autonomy = 2
    reversible = True
    max_duration_ms = 3000
    rate_limit = RateLimit.per_minute(30)

    def __init__(self, memory: "MemoryService | None" = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="axon.executor.update_record")
        # Track previous states for rollback: execution_id → (record_id, previous_data)
        self._previous_states: dict[str, tuple[str, dict]] = {}

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("record_id"):
            return ValidationResult.fail("record_id is required")
        if not params.get("updates"):
            return ValidationResult.fail("updates is required and must be non-empty dict")
        if not isinstance(params["updates"], dict):
            return ValidationResult.fail("updates must be a dict")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        record_id = params["record_id"]
        updates = dict(params["updates"])
        merge = bool(params.get("merge", True))

        self._logger.info(
            "update_record_execute",
            record_id=record_id,
            keys_updated=list(updates.keys()),
            execution_id=context.execution_id,
        )

        if self._memory is not None:
            try:
                # Snapshot previous state for rollback
                previous = await self._memory.get_entity(record_id)
                if previous:
                    self._previous_states[context.execution_id] = (record_id, dict(previous))

                await self._memory.update_entity(
                    entity_id=record_id,
                    properties=updates,
                    merge=merge,
                )
                return ExecutionResult(
                    success=True,
                    data={"record_id": record_id, "keys_updated": list(updates.keys())},
                    side_effects=[f"Updated record {record_id}: {list(updates.keys())}"],
                )
            except Exception as exc:
                return ExecutionResult(
                    success=False,
                    error=f"Record update failed: {exc}",
                )
        else:
            return ExecutionResult(
                success=True,
                data={"record_id": record_id, "note": "No memory service"},
                side_effects=[f"Update staged for record {record_id}"],
            )

    async def rollback(
        self,
        execution_id: str,
        context: ExecutionContext,
    ) -> RollbackResult:
        previous = self._previous_states.get(execution_id)
        if not previous:
            return RollbackResult(success=False, reason="No snapshot found for this execution")

        record_id, previous_data = previous
        if self._memory is not None:
            try:
                await self._memory.update_entity(
                    entity_id=record_id,
                    properties=previous_data,
                    merge=False,
                )
                del self._previous_states[execution_id]
                return RollbackResult(
                    success=True,
                    side_effects_reversed=[f"Reverted record {record_id} to previous state"],
                )
            except Exception as exc:
                return RollbackResult(success=False, reason=f"Revert failed: {exc}")

        return RollbackResult(success=False, reason="No memory service for rollback")


# ─── ScheduleExecutor ─────────────────────────────────────────────


class ScheduleExecutor(Executor):
    """
    Schedule a future event (e.g., meeting, reminder, task deadline).

    Events are stored in a Redis sorted set keyed by timestamp, allowing
    EOS to query "what's happening soon?" efficiently. They are also
    stored as Event nodes in the Memory graph for semantic retrieval.

    Required params:
      title (str): Event title.
      scheduled_at (str): ISO-8601 datetime string.

    Optional params:
      description (str): Event description. Default "".
      participants (list[str]): User IDs involved. Default [].
      duration_minutes (int): Expected duration. Default 60.
      recurrence (str): "none" | "daily" | "weekly" | "monthly". Default "none".
    """

    action_type = "schedule_event"
    description = "Schedule a future event in the community calendar (Level 2)"
    required_autonomy = 2
    reversible = True
    max_duration_ms = 3000
    rate_limit = RateLimit.per_hour(50)

    def __init__(self, redis_client=None) -> None:
        self._redis = redis_client
        self._logger = logger.bind(system="axon.executor.schedule_event")
        self._scheduled: dict[str, str] = {}  # execution_id → event_id

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("title"):
            return ValidationResult.fail("title is required")
        if not params.get("scheduled_at"):
            return ValidationResult.fail("scheduled_at is required (ISO-8601 datetime)")
        try:
            from datetime import datetime
            datetime.fromisoformat(str(params["scheduled_at"]))
        except (ValueError, TypeError):
            return ValidationResult.fail(
                "scheduled_at must be a valid ISO-8601 datetime string"
            )
        recurrence = params.get("recurrence", "none")
        if recurrence not in ("none", "daily", "weekly", "monthly"):
            return ValidationResult.fail(
                "recurrence must be 'none', 'daily', 'weekly', or 'monthly'"
            )
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        from datetime import datetime
        import json as _json

        title = params["title"]
        scheduled_at = str(params["scheduled_at"])
        description = str(params.get("description", ""))
        participants = list(params.get("participants", []))
        duration_minutes = int(params.get("duration_minutes", 60))
        recurrence = params.get("recurrence", "none")

        self._logger.info(
            "schedule_event_execute",
            title=title,
            scheduled_at=scheduled_at,
            execution_id=context.execution_id,
        )

        from ecodiaos.primitives.common import new_id
        event_id = new_id()
        event_data = {
            "id": event_id,
            "title": title,
            "scheduled_at": scheduled_at,
            "description": description,
            "participants": participants,
            "duration_minutes": duration_minutes,
            "recurrence": recurrence,
            "created_by": "eos",
            "execution_id": context.execution_id,
        }

        if self._redis is not None:
            try:
                # Parse timestamp for sorted set score
                ts = datetime.fromisoformat(scheduled_at).timestamp()
                await self._redis.zadd(
                    "eos:scheduled_events",
                    {_json.dumps(event_data): ts},
                )
                self._scheduled[context.execution_id] = event_id
                return ExecutionResult(
                    success=True,
                    data={"event_id": event_id, "scheduled_at": scheduled_at},
                    side_effects=[f"Event '{title}' scheduled for {scheduled_at}"],
                )
            except Exception as exc:
                return ExecutionResult(
                    success=False,
                    error=f"Schedule failed: {exc}",
                )
        else:
            self._scheduled[context.execution_id] = event_id
            return ExecutionResult(
                success=True,
                data={"event_id": event_id, "note": "No Redis client — event staged"},
                side_effects=[f"Event '{title}' scheduled for {scheduled_at} (staged)"],
            )

    async def rollback(
        self,
        execution_id: str,
        context: ExecutionContext,
    ) -> RollbackResult:
        event_id = self._scheduled.get(execution_id)
        if not event_id:
            return RollbackResult(success=False, reason="No event ID for this execution")

        if self._redis is not None:
            try:
                # Remove from sorted set by scanning — Phase 1 approach
                # Phase 2: store event_id → key mapping for O(1) removal
                result = await self._redis.zrangebyscore(
                    "eos:scheduled_events", "-inf", "+inf"
                )
                for item in result:
                    import json as _json
                    try:
                        data = _json.loads(item)
                        if data.get("id") == event_id:
                            await self._redis.zrem("eos:scheduled_events", item)
                            del self._scheduled[execution_id]
                            return RollbackResult(
                                success=True,
                                side_effects_reversed=[f"Removed scheduled event {event_id}"],
                            )
                    except Exception:
                        continue
            except Exception as exc:
                return RollbackResult(success=False, reason=f"Rollback failed: {exc}")

        return RollbackResult(success=False, reason="No Redis client for rollback")


# ─── ReminderExecutor ─────────────────────────────────────────────


class ReminderExecutor(Executor):
    """
    Set a reminder for a user or for EOS itself.

    Reminders are stored in Redis as time-delayed messages. When the time arrives,
    Synapse (Phase 9) will trigger a Percept that re-enters the workspace.

    Required params:
      message (str): The reminder message.
      remind_at (str): ISO-8601 datetime when the reminder should fire.

    Optional params:
      recipient_id (str): User to remind. If absent, EOS reminds itself.
      context_note (str): Extra context for why this reminder was set. Default "".
    """

    action_type = "set_reminder"
    description = "Set a time-delayed reminder for a user or for EOS (Level 2)"
    required_autonomy = 2
    reversible = True
    max_duration_ms = 2000
    rate_limit = RateLimit.per_hour(30)

    def __init__(self, redis_client=None) -> None:
        self._redis = redis_client
        self._logger = logger.bind(system="axon.executor.set_reminder")
        self._reminders: dict[str, str] = {}  # execution_id → reminder_id

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("message"):
            return ValidationResult.fail("message is required")
        if not params.get("remind_at"):
            return ValidationResult.fail("remind_at is required (ISO-8601 datetime)")
        try:
            from datetime import datetime
            datetime.fromisoformat(str(params["remind_at"]))
        except (ValueError, TypeError):
            return ValidationResult.fail("remind_at must be a valid ISO-8601 datetime string")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        from datetime import datetime
        import json as _json
        from ecodiaos.primitives.common import new_id

        message = params["message"]
        remind_at = str(params["remind_at"])
        recipient_id = params.get("recipient_id", "eos")
        context_note = str(params.get("context_note", ""))

        self._logger.info(
            "set_reminder_execute",
            recipient_id=recipient_id,
            remind_at=remind_at,
            execution_id=context.execution_id,
        )

        reminder_id = new_id()
        reminder_data = {
            "id": reminder_id,
            "message": message,
            "remind_at": remind_at,
            "recipient_id": recipient_id,
            "context_note": context_note,
            "execution_id": context.execution_id,
        }

        if self._redis is not None:
            try:
                ts = datetime.fromisoformat(remind_at).timestamp()
                key = f"eos:reminders:{recipient_id}"
                await self._redis.zadd(key, {_json.dumps(reminder_data): ts})
                self._reminders[context.execution_id] = reminder_id
                return ExecutionResult(
                    success=True,
                    data={"reminder_id": reminder_id, "remind_at": remind_at},
                    side_effects=[f"Reminder set for {recipient_id} at {remind_at}"],
                )
            except Exception as exc:
                return ExecutionResult(
                    success=False,
                    error=f"Reminder set failed: {exc}",
                )
        else:
            self._reminders[context.execution_id] = reminder_id
            return ExecutionResult(
                success=True,
                data={"reminder_id": reminder_id, "note": "No Redis client — reminder staged"},
                side_effects=[f"Reminder staged for {recipient_id} at {remind_at}"],
            )

    async def rollback(
        self,
        execution_id: str,
        context: ExecutionContext,
    ) -> RollbackResult:
        reminder_id = self._reminders.get(execution_id)
        if not reminder_id:
            return RollbackResult(success=False, reason="No reminder ID for this execution")
        # Removal logic mirrors ScheduleExecutor
        del self._reminders[execution_id]
        return RollbackResult(
            success=True,
            side_effects_reversed=[f"Cancelled reminder {reminder_id}"],
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\executors\integration.py =====

"""
EcodiaOS — Axon Integration Executors

Integration executors make calls to external systems. They are Level 2-3
because they cross the boundary between EOS and the outside world.

APICallExecutor   — (Level 2) call an external REST API
WebhookExecutor   — (Level 2) trigger a webhook endpoint

These are deliberately generic — specific integrations (GitHub, Slack, IoT sensors)
will be implemented as purpose-built executors in future phases. For now, these
provide a general-purpose HTTP capability with safety constraints.

Safety constraints:
  - All URLs are validated (no local network, no internal services)
  - All responses are capped at 50KB
  - Credentials come from CredentialStore only — no inline secrets
  - All calls are logged with full audit trail
  - Timeouts are strictly enforced (5s default, max 30s)
"""

from __future__ import annotations

import re
from typing import Any
from urllib.parse import urlparse

import structlog

from ecodiaos.systems.axon.executor import Executor
from ecodiaos.systems.axon.types import (
    ExecutionContext,
    ExecutionResult,
    RateLimit,
    ValidationResult,
)

logger = structlog.get_logger()

# Blocked URL patterns — EOS should never call internal network addresses
_BLOCKED_URL_PATTERNS = re.compile(
    r"^https?://(localhost|127\.|0\.0\.0\.0|10\.|172\.(1[6-9]|2\d|3[01])\.|192\.168\.)",
    re.IGNORECASE,
)

_MAX_RESPONSE_BYTES = 50 * 1024  # 50KB cap on API responses
_ALLOWED_METHODS = {"GET", "POST", "PUT", "PATCH", "DELETE"}


def _validate_url(url: str) -> str | None:
    """
    Validate an API URL. Returns error message if invalid, None if valid.
    """
    if not url:
        return "url is required"
    try:
        parsed = urlparse(url)
    except Exception:
        return "url is malformed"
    if parsed.scheme not in ("http", "https"):
        return "url must use http or https"
    if _BLOCKED_URL_PATTERNS.match(url):
        return "url targets a blocked internal network address"
    if not parsed.netloc:
        return "url must include a host"
    return None


# ─── APICallExecutor ──────────────────────────────────────────────


class APICallExecutor(Executor):
    """
    Call an external REST API and return the response.

    The response is returned in data and also fed back as a new observation
    for the workspace (Atune will score its salience next cycle).

    Required params:
      url (str): The API endpoint URL.
      method (str): HTTP method — "GET" | "POST" | "PUT" | "PATCH" | "DELETE".

    Optional params:
      body (dict | str): Request body. Default None (no body).
      headers (dict): Additional request headers. Default {}.
      timeout_s (int): Request timeout in seconds. Default 5, max 30.
      credential_service (str): CredentialStore service key for auth. Default None.
      response_field (str): Extract a specific field from JSON response. Default None.
    """

    action_type = "call_api"
    description = "Call an external REST API (Level 2)"
    required_autonomy = 2
    reversible = False  # HTTP calls can't be generically undone
    max_duration_ms = 30_000
    rate_limit = RateLimit.per_minute(30)

    def __init__(self) -> None:
        self._logger = logger.bind(system="axon.executor.call_api")

    async def validate_params(self, params: dict) -> ValidationResult:
        url_error = _validate_url(params.get("url", ""))
        if url_error:
            return ValidationResult.fail(url_error, url=url_error)

        method = str(params.get("method", "")).upper()
        if method not in _ALLOWED_METHODS:
            return ValidationResult.fail(
                f"method must be one of {sorted(_ALLOWED_METHODS)}",
                method="invalid value",
            )

        timeout_s = params.get("timeout_s", 5)
        if not isinstance(timeout_s, (int, float)) or not 0 < float(timeout_s) <= 30:
            return ValidationResult.fail("timeout_s must be between 1 and 30")

        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        url = params["url"]
        method = str(params.get("method", "GET")).upper()
        body = params.get("body")
        extra_headers = dict(params.get("headers", {}))
        timeout_s = float(params.get("timeout_s", 5))
        credential_service = params.get("credential_service")
        response_field = params.get("response_field")

        # Build headers
        headers: dict[str, str] = {
            "User-Agent": "EcodiaOS/1.0 (+https://ecodiaos.org)",
            **extra_headers,
        }

        # Inject auth if credential service specified
        if credential_service:
            token = context.credentials.get(credential_service)
            if token:
                # Extract raw credential from scoped token
                # In Phase 1, credential = raw API key
                # The token format is: expiry:signature:raw_credential
                parts = token.split(":", 2)
                if len(parts) == 3:
                    raw_credential = parts[2]
                    headers["Authorization"] = f"Bearer {raw_credential}"

        self._logger.info(
            "api_call_execute",
            url=url,
            method=method,
            execution_id=context.execution_id,
        )

        try:
            import asyncio
            import json as _json
            try:
                import aiohttp
                result = await _call_with_aiohttp(
                    url=url,
                    method=method,
                    body=body,
                    headers=headers,
                    timeout_s=timeout_s,
                    response_field=response_field,
                )
            except ImportError:
                # aiohttp not available — use urllib as fallback
                result = await asyncio.to_thread(
                    _call_with_urllib,
                    url=url,
                    method=method,
                    body=body,
                    headers=headers,
                    timeout_s=timeout_s,
                )

            observation = _summarise_response(result, url, method)
            return ExecutionResult(
                success=result.get("status_code", 500) < 400,
                data=result,
                error="" if result.get("status_code", 0) < 400 else (
                    f"HTTP {result.get('status_code')}: {str(result.get('body', ''))[:200]}"
                ),
                side_effects=[f"API call {method} {url} → HTTP {result.get('status_code', '?')}"],
                new_observations=[observation],
            )

        except Exception as exc:
            return ExecutionResult(
                success=False,
                error=f"API call failed: {exc}",
                side_effects=[f"API call {method} {url} failed: {type(exc).__name__}"],
            )


async def _call_with_aiohttp(
    url: str,
    method: str,
    body: Any,
    headers: dict,
    timeout_s: float,
    response_field: str | None,
) -> dict:
    import aiohttp
    import json as _json

    timeout = aiohttp.ClientTimeout(total=timeout_s)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        kwargs: dict[str, Any] = {"headers": headers}
        if body is not None:
            if isinstance(body, dict):
                kwargs["json"] = body
            else:
                kwargs["data"] = str(body)

        async with session.request(method, url, **kwargs) as resp:
            raw = await resp.read()
            if len(raw) > _MAX_RESPONSE_BYTES:
                raw = raw[:_MAX_RESPONSE_BYTES]

            content_type = resp.headers.get("Content-Type", "")
            if "application/json" in content_type:
                try:
                    parsed = _json.loads(raw)
                    if response_field and isinstance(parsed, dict):
                        body_data = parsed.get(response_field, parsed)
                    else:
                        body_data = parsed
                except Exception:
                    body_data = raw.decode("utf-8", errors="replace")
            else:
                body_data = raw.decode("utf-8", errors="replace")

            return {
                "status_code": resp.status,
                "body": body_data,
                "content_type": content_type,
                "url": str(resp.url),
            }


def _call_with_urllib(
    url: str,
    method: str,
    body: Any,
    headers: dict,
    timeout_s: float,
) -> dict:
    import json as _json
    import urllib.request
    import urllib.error

    data = None
    if body is not None:
        if isinstance(body, dict):
            data = _json.dumps(body).encode()
            headers.setdefault("Content-Type", "application/json")
        else:
            data = str(body).encode()

    req = urllib.request.Request(url, data=data, headers=headers, method=method)
    try:
        with urllib.request.urlopen(req, timeout=timeout_s) as resp:
            raw = resp.read(_MAX_RESPONSE_BYTES)
            content_type = resp.headers.get("Content-Type", "")
            try:
                body_data = _json.loads(raw)
            except Exception:
                body_data = raw.decode("utf-8", errors="replace")
            return {"status_code": resp.status, "body": body_data, "content_type": content_type, "url": url}
    except urllib.error.HTTPError as e:
        return {"status_code": e.code, "body": str(e.reason), "content_type": "", "url": url}


def _summarise_response(result: dict, url: str, method: str) -> str:
    status = result.get("status_code", "?")
    body = result.get("body", "")
    if isinstance(body, dict):
        import json as _json
        body_preview = _json.dumps(body)[:300]
    else:
        body_preview = str(body)[:300]
    return f"API {method} {url} → {status}: {body_preview}"


# ─── WebhookExecutor ──────────────────────────────────────────────


class WebhookExecutor(Executor):
    """
    Trigger a registered webhook endpoint.

    Webhooks are outbound HTTP calls to registered external services —
    GitHub Actions, Zapier, community platform integrations, etc.
    They differ from APICallExecutor in that they use pre-configured
    endpoints from the credential store rather than arbitrary URLs.

    Required params:
      webhook_key (str): Key identifying the webhook in CredentialStore.
      payload (dict): Data to send as the webhook body.

    Optional params:
      event_type (str): Event type header (X-EOS-Event). Default "eos.action".
      timeout_s (int): Request timeout. Default 10.
    """

    action_type = "webhook_trigger"
    description = "Trigger a registered webhook endpoint (Level 2)"
    required_autonomy = 2
    reversible = False
    max_duration_ms = 15_000
    rate_limit = RateLimit.per_minute(10)

    def __init__(self) -> None:
        self._logger = logger.bind(system="axon.executor.webhook")

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("webhook_key"):
            return ValidationResult.fail("webhook_key is required")
        if not params.get("payload"):
            return ValidationResult.fail("payload is required and must be non-empty dict")
        if not isinstance(params["payload"], dict):
            return ValidationResult.fail("payload must be a dict")
        timeout_s = params.get("timeout_s", 10)
        if not isinstance(timeout_s, (int, float)) or not 0 < float(timeout_s) <= 30:
            return ValidationResult.fail("timeout_s must be between 1 and 30")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        import json as _json
        import hashlib
        import hmac as _hmac

        webhook_key = params["webhook_key"]
        payload = dict(params["payload"])
        event_type = str(params.get("event_type", "eos.action"))
        timeout_s = float(params.get("timeout_s", 10))

        # Get webhook URL and secret from credentials
        token = context.credentials.get(webhook_key)
        if not token:
            return ExecutionResult(
                success=False,
                error=f"No credential configured for webhook_key '{webhook_key}'",
            )

        # Token format: expiry:signature:url|secret (pipe-separated)
        parts = token.split(":", 2)
        if len(parts) != 3:
            return ExecutionResult(
                success=False,
                error="Webhook credential format invalid",
            )
        raw_credential = parts[2]
        # Credential is "url|secret" or just "url"
        if "|" in raw_credential:
            webhook_url, webhook_secret = raw_credential.split("|", 1)
        else:
            webhook_url = raw_credential
            webhook_secret = ""

        url_error = _validate_url(webhook_url)
        if url_error:
            return ExecutionResult(
                success=False,
                error=f"Webhook URL invalid: {url_error}",
            )

        # Build signed payload
        body = _json.dumps(payload).encode()
        headers: dict[str, str] = {
            "Content-Type": "application/json",
            "X-EOS-Event": event_type,
            "X-EOS-Delivery": context.execution_id,
            "User-Agent": "EcodiaOS/1.0",
        }
        if webhook_secret:
            signature = _hmac.new(
                webhook_secret.encode(),
                body,
                hashlib.sha256,
            ).hexdigest()
            headers["X-EOS-Signature-256"] = f"sha256={signature}"

        self._logger.info(
            "webhook_execute",
            webhook_key=webhook_key,
            event_type=event_type,
            url=webhook_url,
            execution_id=context.execution_id,
        )

        try:
            result = await _call_with_aiohttp(
                url=webhook_url,
                method="POST",
                body=payload,
                headers=headers,
                timeout_s=timeout_s,
                response_field=None,
            )
            success = result.get("status_code", 500) < 400
            return ExecutionResult(
                success=success,
                data={"status_code": result.get("status_code"), "webhook_key": webhook_key},
                error="" if success else f"Webhook returned HTTP {result.get('status_code')}",
                side_effects=[
                    f"Webhook '{webhook_key}' triggered ({event_type}) → HTTP {result.get('status_code')}"
                ],
            )
        except Exception as exc:
            return ExecutionResult(
                success=False,
                error=f"Webhook call failed: {exc}",
            )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\executors\internal.py =====

"""
EcodiaOS — Axon Internal Executors

Internal executors operate entirely within EOS — no external calls, no
user-facing side effects. They are Level 1 (ADVISOR autonomy) because
EOS modifying its own memory and internal state is within its base autonomy.

StoreInsightExecutor     — store a learning/insight in Memory
UpdateGoalExecutor       — update goal status via Nova
ConsolidationExecutor    — trigger memory consolidation

These are the "self-maintenance" actions — EOS caring for its own
cognitive coherence. They reflect the Growth and Coherence drives.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.axon.executor import Executor
from ecodiaos.systems.axon.types import (
    ExecutionContext,
    ExecutionResult,
    RateLimit,
    ValidationResult,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()


# ─── StoreInsightExecutor ─────────────────────────────────────────


class StoreInsightExecutor(Executor):
    """
    Store a learning or insight in the semantic memory layer.

    This is how EOS explicitly commits a generalised belief or learned pattern.
    Unlike ObserveExecutor (which stores episodic observations), this targets
    the semantic layer — generalised knowledge that persists beyond the
    specific episode that generated it.

    Use this when:
      - EOS detects a pattern worth generalising
      - A conversation reveals something about the community worth remembering
      - Evo proposes an insight that should be committed

    Required params:
      insight (str): The insight or generalisation to store.
      domain (str): Knowledge domain (e.g., "community", "health", "workflows").

    Optional params:
      confidence (float 0-1): How confident EOS is in this insight. Default 0.7.
      evidence_episodes (list[str]): Episode IDs that support this insight. Default [].
      tags (list[str]): Labels for retrieval. Default [].
    """

    action_type = "store_insight"
    description = "Store a learned insight in semantic memory (Level 1)"
    required_autonomy = 1
    reversible = False
    max_duration_ms = 2000
    rate_limit = RateLimit.per_minute(20)

    def __init__(self, memory: "MemoryService | None" = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="axon.executor.store_insight")

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("insight"):
            return ValidationResult.fail("insight is required", insight="missing or empty")
        if not params.get("domain"):
            return ValidationResult.fail("domain is required", domain="missing or empty")
        confidence = params.get("confidence", 0.7)
        if not isinstance(confidence, (int, float)) or not 0.0 <= float(confidence) <= 1.0:
            return ValidationResult.fail("confidence must be a float between 0.0 and 1.0")
        insight = params["insight"]
        if not isinstance(insight, str) or len(insight) > 5000:
            return ValidationResult.fail("insight must be a string (max 5,000 chars)")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        insight = params["insight"]
        domain = str(params["domain"])
        confidence = float(params.get("confidence", 0.7))
        evidence_episodes = list(params.get("evidence_episodes", []))
        tags = list(params.get("tags", []))

        self._logger.info(
            "store_insight_execute",
            domain=domain,
            insight_preview=insight[:80],
            confidence=confidence,
            execution_id=context.execution_id,
        )

        if self._memory is not None:
            try:
                # Insights are stored as Entity nodes in the semantic layer.
                # resolve_and_create_entity handles deduplication.
                insight_id, was_created = await self._memory.resolve_and_create_entity(
                    name=insight[:80],  # Use first 80 chars as the entity name
                    entity_type="Insight",
                    description=f"[{domain}] {insight}",
                )
                return ExecutionResult(
                    success=True,
                    data={
                        "insight_id": insight_id,
                        "domain": domain,
                        "confidence": confidence,
                        "was_created": was_created,
                    },
                    side_effects=[
                        f"Insight {'stored' if was_created else 'merged'} in semantic memory "
                        f"(domain={domain}, confidence={confidence:.2f})"
                    ],
                )
            except Exception as exc:
                return ExecutionResult(
                    success=False,
                    error=f"Semantic memory write failed: {exc}",
                )
        else:
            self._logger.info(
                "store_insight_no_memory",
                insight=insight[:80],
                domain=domain,
            )
            return ExecutionResult(
                success=True,
                data={"insight_id": None, "note": "No memory service"},
                side_effects=[f"Insight staged for domain '{domain}'"],
            )


# ─── UpdateGoalExecutor ───────────────────────────────────────────


class UpdateGoalExecutor(Executor):
    """
    Update the status or progress of a goal tracked by Nova.

    This is how execution outcomes feed back into goal lifecycle management.
    Axon calls this after completing actions that advance or resolve goals.
    It can also be called by governance actions (e.g., pausing a goal).

    Required params:
      goal_id (str): The ID of the goal to update.
      status (str): New status — "active" | "achieved" | "abandoned" | "suspended".

    Optional params:
      progress (float 0-1): Updated progress towards the goal. Default unchanged.
      note (str): Explanation for the update. Default "".
    """

    action_type = "update_goal"
    description = "Update goal status or progress in Nova (Level 1)"
    required_autonomy = 1
    reversible = False
    max_duration_ms = 500
    rate_limit = RateLimit.per_minute(60)

    # Nova service reference — injected at startup
    _nova: Any = None

    def set_nova(self, nova: Any) -> None:
        self._nova = nova

    def __init__(self) -> None:
        self._logger = logger.bind(system="axon.executor.update_goal")

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("goal_id"):
            return ValidationResult.fail("goal_id is required")
        status = params.get("status", "")
        valid_statuses = ("active", "achieved", "abandoned", "suspended")
        if status not in valid_statuses:
            return ValidationResult.fail(
                f"status must be one of {valid_statuses}",
                status="invalid value",
            )
        progress = params.get("progress")
        if progress is not None:
            if not isinstance(progress, (int, float)) or not 0.0 <= float(progress) <= 1.0:
                return ValidationResult.fail("progress must be a float between 0.0 and 1.0")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        goal_id = params["goal_id"]
        status = params["status"]
        progress = params.get("progress")
        note = str(params.get("note", ""))

        self._logger.info(
            "update_goal_execute",
            goal_id=goal_id,
            status=status,
            progress=progress,
            execution_id=context.execution_id,
        )

        if self._nova is not None:
            try:
                await self._nova.update_goal_status(
                    goal_id=goal_id,
                    status=status,
                    progress=progress,
                    note=note,
                )
                return ExecutionResult(
                    success=True,
                    data={"goal_id": goal_id, "status": status, "progress": progress},
                    side_effects=[f"Goal {goal_id} updated: status={status}"],
                )
            except Exception as exc:
                return ExecutionResult(
                    success=False,
                    error=f"Goal update failed: {exc}",
                )
        else:
            self._logger.info(
                "update_goal_no_nova",
                goal_id=goal_id,
                status=status,
            )
            return ExecutionResult(
                success=True,
                data={"goal_id": goal_id, "status": status, "note": "No Nova service"},
                side_effects=[f"Goal update staged: {goal_id} → {status}"],
            )


# ─── ConsolidationExecutor ────────────────────────────────────────


class ConsolidationExecutor(Executor):
    """
    Trigger memory consolidation — the process of integrating and pruning
    episodic memories into semantic generalizations.

    This is a background maintenance action that EOS should schedule
    periodically (Evo drives consolidation timing). Triggering it explicitly
    is appropriate when:
      - A significant learning moment has just occurred
      - Memory is approaching capacity limits
      - Governance requests a knowledge audit

    Required params:
      scope (str): What to consolidate — "recent" | "domain" | "full".

    Optional params:
      domain (str): If scope="domain", which domain to consolidate. Default "all".
      max_episodes (int): Maximum episodes to process. Default 100.
    """

    action_type = "trigger_consolidation"
    description = "Trigger memory consolidation to integrate episodic learning (Level 1)"
    required_autonomy = 1
    reversible = False
    max_duration_ms = 30_000  # Consolidation can be slow
    rate_limit = RateLimit.per_hour(6)  # At most every 10 minutes

    def __init__(self, memory: "MemoryService | None" = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="axon.executor.consolidation")

    async def validate_params(self, params: dict) -> ValidationResult:
        scope = params.get("scope", "recent")
        valid_scopes = ("recent", "domain", "full")
        if scope not in valid_scopes:
            return ValidationResult.fail(
                f"scope must be one of {valid_scopes}",
                scope="invalid value",
            )
        if scope == "domain" and not params.get("domain"):
            return ValidationResult.fail(
                "domain is required when scope='domain'",
                domain="missing",
            )
        max_episodes = params.get("max_episodes", 100)
        if not isinstance(max_episodes, int) or max_episodes < 1 or max_episodes > 10_000:
            return ValidationResult.fail("max_episodes must be an integer between 1 and 10,000")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        scope = params.get("scope", "recent")
        domain = str(params.get("domain", "all"))
        max_episodes = int(params.get("max_episodes", 100))

        self._logger.info(
            "consolidation_execute",
            scope=scope,
            domain=domain,
            max_episodes=max_episodes,
            execution_id=context.execution_id,
        )

        if self._memory is not None:
            try:
                # Memory.consolidate() is the actual API (no scope/domain filtering yet —
                # Evo will drive more granular consolidation in Phase 7)
                result = await self._memory.consolidate()
                episodes_processed = result.get("episodes_processed", 0) if result else 0
                insights_created = result.get("insights_created", 0) if result else 0
                if True:
                    return ExecutionResult(
                        success=True,
                        data={
                            "scope": scope,
                            "episodes_processed": episodes_processed,
                            "insights_created": insights_created,
                        },
                        side_effects=[
                            f"Memory consolidation ({scope}): "
                            f"{episodes_processed} episodes → {insights_created} insights"
                        ],
                    )
                else:
                    # Unreachable — kept for structural clarity
                    return ExecutionResult(  # type: ignore[unreachable]
                        success=True,
                        data={"note": "Consolidation API unavailable"},
                        side_effects=["Consolidation requested but API unavailable"],
                    )
            except Exception as exc:
                return ExecutionResult(
                    success=False,
                    error=f"Consolidation failed: {exc}",
                )
        else:
            self._logger.info("consolidation_no_memory", scope=scope)
            return ExecutionResult(
                success=True,
                data={"note": "No memory service"},
                side_effects=[f"Consolidation requested (scope={scope}) — no memory service"],
            )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\axon\executors\observation.py =====

"""
EcodiaOS — Axon Observation Executors

Observation executors are Level 1 (ADVISOR autonomy) — they read and analyse
without modifying world state. They are the lowest-risk executors in the system:
reversible in the sense that they have no side effects to reverse.

ObserveExecutor  — records an observation to Memory (episodic store)
QueryMemoryExecutor — retrieves information from the Memory system
AnalyseExecutor  — runs LLM-based analysis on a topic or dataset
SearchExecutor   — searches external sources (placeholder for future integrations)
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.axon.executor import Executor
from ecodiaos.systems.axon.types import (
    ExecutionContext,
    ExecutionResult,
    RateLimit,
    ValidationResult,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()


# ─── ObserveExecutor ──────────────────────────────────────────────


class ObserveExecutor(Executor):
    """
    Record an observation without acting on it.

    Use this when EOS notices something meaningful and wants to commit
    it to episodic memory for future recall. It is not a passive action —
    choosing to observe and record is itself a cognitive act.

    Required params:
      content (str): The observation content to record.

    Optional params:
      salience (float 0-1): Importance signal. Default 0.5.
      tags (list[str]): Labels for retrieval. Default [].
      source (str): Where the observation came from. Default "internal".
    """

    action_type = "observe"
    description = "Record an observation to episodic memory without taking action"
    required_autonomy = 1
    reversible = False
    max_duration_ms = 2000
    rate_limit = RateLimit.unlimited()

    def __init__(self, memory: "MemoryService | None" = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="axon.executor.observe")

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("content"):
            return ValidationResult.fail("content is required", content="missing or empty")
        content = params["content"]
        if not isinstance(content, str):
            return ValidationResult.fail("content must be a string", content="wrong type")
        if len(content) > 10_000:
            return ValidationResult.fail("content too long (max 10,000 chars)")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        content = params["content"]
        salience = float(params.get("salience", 0.5))
        tags = list(params.get("tags", []))
        source = str(params.get("source", "internal"))

        self._logger.debug(
            "observe_execute",
            execution_id=context.execution_id,
            content_length=len(content),
            salience=salience,
        )

        if self._memory is not None:
            try:
                episode_id = await self._store_to_memory(
                    content=content,
                    salience=salience,
                    tags=tags,
                    source=source,
                    context=context,
                )
                return ExecutionResult(
                    success=True,
                    data={"episode_id": episode_id, "salience": salience},
                    side_effects=[f"Observation recorded to episodic memory (salience={salience:.2f})"],
                )
            except Exception as exc:
                return ExecutionResult(
                    success=False,
                    error=f"Memory write failed: {exc}",
                )
        else:
            # No memory service — log and succeed (observation still happened)
            self._logger.info(
                "observe_no_memory",
                content=content[:100],
                salience=salience,
            )
            return ExecutionResult(
                success=True,
                data={"episode_id": None, "salience": salience},
                side_effects=["Observation noted (no memory service)"],
            )

    async def _store_to_memory(
        self,
        content: str,
        salience: float,
        tags: list[str],
        source: str,
        context: ExecutionContext,
    ) -> str:
        """Store the observation as an episodic Percept via MemoryService.store_percept()."""
        from ecodiaos.primitives.percept import Content, Percept
        from ecodiaos.primitives.common import Modality, SourceDescriptor, SystemID

        percept = Percept(
            source=SourceDescriptor(
                system=SystemID.AXON,
                channel=source,
                modality=Modality.INTERNAL,
            ),
            content=Content(raw=content, parsed={"tags": tags}),
            metadata={"source": source},
            salience_hint=salience,
        )
        affect = context.affect_state
        return await self._memory.store_percept(  # type: ignore[union-attr]
            percept,
            salience_composite=salience,
            salience_scores={},
            affect_valence=getattr(affect, "valence", 0.0),
            affect_arousal=getattr(affect, "arousal", 0.0),
            free_energy=0.0,
        )


# ─── QueryMemoryExecutor ──────────────────────────────────────────


class QueryMemoryExecutor(Executor):
    """
    Retrieve information from the Memory system.

    Performs hybrid retrieval (semantic + keyword + graph traversal) and
    returns matching memory traces as new observations — which flow back
    as Percepts into Atune for the next cycle.

    Required params:
      query (str): The retrieval query.

    Optional params:
      max_results (int): Maximum traces to return. Default 5.
      memory_types (list[str]): Filter by type ("episodic", "semantic", "procedural").
      min_salience (float): Minimum salience threshold. Default 0.0.
    """

    action_type = "query_memory"
    description = "Retrieve memories via hybrid semantic + graph retrieval"
    required_autonomy = 1
    reversible = False
    max_duration_ms = 300  # Must fit within memory retrieval budget
    rate_limit = RateLimit.per_minute(60)

    def __init__(self, memory: "MemoryService | None" = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="axon.executor.query_memory")

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("query"):
            return ValidationResult.fail("query is required", query="missing or empty")
        if not isinstance(params["query"], str):
            return ValidationResult.fail("query must be a string")
        max_results = params.get("max_results", 5)
        if not isinstance(max_results, int) or max_results < 1 or max_results > 50:
            return ValidationResult.fail("max_results must be an integer between 1 and 50")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        query = params["query"]
        max_results = int(params.get("max_results", 5))
        min_salience = float(params.get("min_salience", 0.0))

        self._logger.debug(
            "query_memory_execute",
            query=query[:80],
            max_results=max_results,
        )

        if self._memory is None:
            return ExecutionResult(
                success=True,
                data={"results": [], "count": 0},
                new_observations=[],
            )

        try:
            # retrieve() returns MemoryRetrievalResponse with .traces: list[RetrievalResult]
            response = await self._memory.retrieve(
                query_text=query,
                max_results=max_results,
                salience_floor=min_salience,
            )

            traces = response.traces if hasattr(response, "traces") else []
            observations = [
                f"Memory: {r.content[:200]}"
                for r in traces
                if r.content
            ]

            return ExecutionResult(
                success=True,
                data={
                    "count": len(traces),
                    "results": [
                        {
                            "id": getattr(r, "node_id", ""),
                            "content": getattr(r, "content", "")[:500],
                            "salience": getattr(r, "salience", 0.0),
                        }
                        for r in traces
                    ],
                },
                new_observations=observations,
            )
        except Exception as exc:
            return ExecutionResult(
                success=False,
                error=f"Memory retrieval failed: {exc}",
            )


# ─── AnalyseExecutor ──────────────────────────────────────────────


class AnalyseExecutor(Executor):
    """
    Run LLM-based analysis on a topic, question, or dataset.

    This is epistemic action — EOS actively reducing uncertainty by
    reasoning about something. The analysis result is returned as a new
    observation, feeding back into the workspace for the next cycle.

    Required params:
      topic (str): What to analyse.
      question (str): The specific question to answer.

    Optional params:
      context_data (str): Additional context to include. Default "".
      depth (str): "shallow" | "deep". Default "shallow".
    """

    action_type = "analyse"
    description = "Run LLM-based analysis to reduce epistemic uncertainty"
    required_autonomy = 1
    reversible = False
    max_duration_ms = 10_000
    rate_limit = RateLimit.per_minute(10)

    def __init__(self, memory: "MemoryService | None" = None) -> None:
        self._memory = memory
        self._llm = None  # Injected at service startup
        self._logger = logger.bind(system="axon.executor.analyse")

    def set_llm(self, llm: Any) -> None:
        self._llm = llm
        from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
        self._optimized = isinstance(llm, OptimizedLLMProvider)

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("topic"):
            return ValidationResult.fail("topic is required")
        if not params.get("question"):
            return ValidationResult.fail("question is required")
        depth = params.get("depth", "shallow")
        if depth not in ("shallow", "deep"):
            return ValidationResult.fail("depth must be 'shallow' or 'deep'")
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        topic = params["topic"]
        question = params["question"]
        context_data = str(params.get("context_data", ""))
        depth = params.get("depth", "shallow")

        self._logger.info(
            "analyse_execute",
            topic=topic[:60],
            question=question[:80],
            depth=depth,
        )

        if self._llm is None:
            return ExecutionResult(
                success=True,
                data={"analysis": f"[No LLM] Analysis of '{topic}': {question}"},
                new_observations=[f"Analysis requested on: {topic} — {question}"],
            )

        try:
            prompt = _build_analysis_prompt(topic, question, context_data, depth)

            # Budget check: skip analysis in RED tier
            if getattr(self, "_optimized", False):
                from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
                assert isinstance(self._llm, OptimizedLLMProvider)
                if not self._llm.should_use_llm("axon.observation", estimated_tokens=1000):
                    return ExecutionResult(
                        success=True,
                        data={
                            "analysis": f"[Budget exhausted] Analysis of '{topic}' deferred.",
                            "topic": topic,
                        },
                        new_observations=[
                            f"Analysis of '{topic}' deferred due to budget constraints."
                        ],
                    )

            # Use evaluate() for optimized path (standard LLM interface), fall back to complete()
            if getattr(self, "_optimized", False):
                from ecodiaos.clients.llm import Message
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=(
                        "You are EOS — a community care organism. "
                        "Be honest about uncertainty."
                    ),
                    messages=[Message("user", prompt)],
                    max_tokens=1000,
                    temperature=0.3,
                    cache_system="axon.observation",
                    cache_method="analyse",
                )
                analysis = response.text
            else:
                response = await self._llm.complete(prompt=prompt, max_tokens=1000)
                analysis = response.text if hasattr(response, "text") else str(response)

            return ExecutionResult(
                success=True,
                data={"analysis": analysis, "topic": topic, "depth": depth},
                new_observations=[f"Analysis of '{topic}': {analysis[:500]}"],
            )
        except Exception as exc:
            return ExecutionResult(
                success=False,
                error=f"Analysis LLM call failed: {exc}",
            )


def _build_analysis_prompt(
    topic: str,
    question: str,
    context_data: str,
    depth: str,
) -> str:
    depth_instruction = (
        "Provide a brief, focused analysis (2-3 paragraphs)."
        if depth == "shallow"
        else "Provide a thorough, multi-perspective analysis."
    )
    ctx_section = f"\n\nAdditional context:\n{context_data}" if context_data else ""
    return (
        f"You are EOS — a community care organism. Analyse the following:\n\n"
        f"Topic: {topic}\n"
        f"Question: {question}"
        f"{ctx_section}\n\n"
        f"{depth_instruction}\n"
        f"Be honest about uncertainty. Ground your analysis in what is actually known."
    )


# ─── SearchExecutor ───────────────────────────────────────────────


class SearchExecutor(Executor):
    """
    Search external sources for information.

    Phase 1: returns a structured placeholder — full web/knowledge-base
    search will be wired in Phase 3 (integrations).

    Required params:
      query (str): The search query.

    Optional params:
      source (str): "web" | "knowledge_base" | "community_docs". Default "knowledge_base".
      max_results (int): Maximum results. Default 5.
    """

    action_type = "search"
    description = "Search external sources for information"
    required_autonomy = 1
    reversible = False
    max_duration_ms = 5000
    rate_limit = RateLimit.per_minute(20)

    async def validate_params(self, params: dict) -> ValidationResult:
        if not params.get("query"):
            return ValidationResult.fail("query is required")
        source = params.get("source", "knowledge_base")
        valid_sources = ("web", "knowledge_base", "community_docs")
        if source not in valid_sources:
            return ValidationResult.fail(
                f"source must be one of {valid_sources}",
                source="invalid value",
            )
        return ValidationResult.ok()

    async def execute(
        self,
        params: dict,
        context: ExecutionContext,
    ) -> ExecutionResult:
        query = params["query"]
        source = params.get("source", "knowledge_base")
        max_results = int(params.get("max_results", 5))

        # Phase 1: stub — returns a "search pending" signal
        # Phase 2: wire to actual search integrations
        logger.info(
            "search_execute_stub",
            query=query[:80],
            source=source,
            execution_id=context.execution_id,
        )
        return ExecutionResult(
            success=True,
            data={
                "query": query,
                "source": source,
                "results": [],
                "note": "Search integration pending (Phase 2)",
            },
            new_observations=[
                f"Search for '{query}' in {source} — results pending integration"
            ],
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\__init__.py =====

"""EcodiaOS — Equor: Constitution & Ethics System."""

from ecodiaos.systems.equor.service import EquorService

__all__ = ["EquorService"]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\amendment.py =====

"""
EcodiaOS — Equor Amendment Process

The most serious governance mechanism: changing the constitutional drives.
Requires deliberation, impact assessment, supermajority vote, and cooldown.

No drive can be set to zero. No drive can exceed 3.0.
Combined weights must stay within [3.0, 5.0].
Amendment history is immutable.
"""

from __future__ import annotations

import json
from datetime import datetime, timedelta, timezone
from typing import Any

import structlog

from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.config import GovernanceConfig
from ecodiaos.primitives.common import new_id, utc_now

logger = structlog.get_logger()


def validate_amendment_proposal(
    proposed_drives: dict[str, float],
) -> tuple[bool, str]:
    """
    Validate that a proposed amendment is structurally valid.
    Returns (is_valid, reason).
    """
    required_drives = {"coherence", "care", "growth", "honesty"}
    provided = set(proposed_drives.keys())

    if not required_drives.issubset(provided):
        missing = required_drives - provided
        return False, f"Missing drives: {missing}"

    # No drive can be zero
    for drive, value in proposed_drives.items():
        if drive in required_drives:
            if value <= 0:
                return False, f"Drive '{drive}' cannot be set to zero or negative ({value})."
            if value > 3.0:
                return False, f"Drive '{drive}' cannot exceed 3.0 ({value})."

    # Combined weight bounds
    total = sum(proposed_drives[d] for d in required_drives)
    if not (3.0 <= total <= 5.0):
        return False, f"Combined drive weights ({total:.2f}) must be between 3.0 and 5.0."

    return True, "Valid."


async def check_amendment_cooldown(
    neo4j: Neo4jClient,
    cooldown_days: int = 90,
) -> tuple[bool, datetime | None]:
    """
    Check if an amendment cooldown is active.
    Returns (cooldown_active, next_allowed_date).
    """
    results = await neo4j.execute_read(
        """
        MATCH (g:GovernanceRecord)
        WHERE g.event_type IN ['amendment_passed', 'amendment_failed']
        RETURN g.timestamp AS last_amendment
        ORDER BY g.timestamp DESC
        LIMIT 1
        """
    )

    if not results or results[0]["last_amendment"] is None:
        return False, None

    last = results[0]["last_amendment"]
    if isinstance(last, str):
        last = datetime.fromisoformat(last)

    if not last.tzinfo:
        last = last.replace(tzinfo=timezone.utc)

    next_allowed = last + timedelta(days=cooldown_days)
    now = utc_now()

    if now < next_allowed:
        return True, next_allowed
    return False, None


async def propose_amendment(
    neo4j: Neo4jClient,
    proposed_drives: dict[str, float],
    title: str,
    description: str,
    proposer_id: str,
    governance_config: GovernanceConfig,
) -> dict:
    """
    Submit a constitutional amendment proposal.
    Validates, checks cooldown, and stores the proposal.
    """
    # Validate structural constraints
    valid, reason = validate_amendment_proposal(proposed_drives)
    if not valid:
        return {"accepted": False, "reason": reason}

    # Check cooldown
    on_cooldown, next_date = await check_amendment_cooldown(
        neo4j, governance_config.amendment_cooldown_days
    )
    if on_cooldown:
        return {
            "accepted": False,
            "reason": f"Amendment cooldown active. Next proposal allowed after {next_date}.",
        }

    # Get current constitution for recording the delta
    current = await neo4j.execute_read(
        "MATCH (s:Self)-[:GOVERNED_BY]->(c:Constitution) RETURN c"
    )
    current_drives = {}
    if current:
        c = current[0]["c"]
        current_drives = {
            "coherence": c.get("drive_coherence", 1.0),
            "care": c.get("drive_care", 1.0),
            "growth": c.get("drive_growth", 1.0),
            "honesty": c.get("drive_honesty", 1.0),
        }

    now = utc_now()
    deliberation_ends = now + timedelta(days=governance_config.amendment_deliberation_days)
    proposal_id = new_id()

    await neo4j.execute_write(
        """
        CREATE (g:GovernanceRecord {
            id: $id,
            event_type: 'amendment_proposed',
            timestamp: datetime($now),
            details_json: $details_json,
            actor: $proposer,
            outcome: 'deliberation'
        })
        """,
        {
            "id": proposal_id,
            "now": now.isoformat(),
            "proposer": proposer_id,
            "details_json": json.dumps({
                "title": title,
                "description": description,
                "proposed_drives": proposed_drives,
                "current_drives": current_drives,
                "deliberation_ends": deliberation_ends.isoformat(),
                "supermajority_required": governance_config.amendment_supermajority,
                "quorum_required": governance_config.amendment_quorum,
                "votes_for": 0,
                "votes_against": 0,
                "votes_abstain": 0,
                "status": "deliberation",
            }),
        },
    )

    logger.info(
        "amendment_proposed",
        proposal_id=proposal_id,
        title=title,
        proposed_drives=proposed_drives,
        deliberation_ends=deliberation_ends.isoformat(),
    )

    return {
        "accepted": True,
        "proposal_id": proposal_id,
        "deliberation_ends": deliberation_ends.isoformat(),
    }


async def apply_amendment(
    neo4j: Neo4jClient,
    proposal_id: str,
    proposed_drives: dict[str, float],
) -> dict:
    """
    Apply a passed amendment to the Constitution node.
    Called after a successful vote. Increments version, preserves history.
    """
    now = utc_now()

    # Update Constitution node
    # amendments is stored as an array of JSON strings (Neo4j cannot store array of maps)
    amendment_json = json.dumps({
        "date": now.isoformat(),
        "proposal_id": proposal_id,
        "new_values": proposed_drives,
    })
    await neo4j.execute_write(
        """
        MATCH (s:Self)-[:GOVERNED_BY]->(c:Constitution)
        SET c.drive_coherence = $coherence,
            c.drive_care = $care,
            c.drive_growth = $growth,
            c.drive_honesty = $honesty,
            c.version = c.version + 1,
            c.last_amended = datetime($now),
            c.amendments = c.amendments + [$amendment_json]
        """,
        {
            "coherence": proposed_drives["coherence"],
            "care": proposed_drives["care"],
            "growth": proposed_drives["growth"],
            "honesty": proposed_drives["honesty"],
            "now": now.isoformat(),
            "amendment_json": amendment_json,
        },
    )

    # Record the ratification
    await neo4j.execute_write(
        """
        CREATE (g:GovernanceRecord {
            id: $id,
            event_type: 'amendment_passed',
            timestamp: datetime($now),
            details_json: $details_json,
            actor: 'governance',
            outcome: 'ratified'
        })
        """,
        {
            "id": new_id(),
            "now": now.isoformat(),
            "details_json": json.dumps({
                "proposal_id": proposal_id,
                "new_drives": proposed_drives,
            }),
        },
    )

    logger.info("amendment_applied", proposal_id=proposal_id, new_drives=proposed_drives)

    return {"applied": True, "proposal_id": proposal_id}

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\autonomy.py =====

"""
EcodiaOS — Equor Autonomy Management

Graduated trust: Advisor → Partner → Steward.
Transitions require governance approval and evidence of competence.
Demotion can be automatic on critical violations.
"""

from __future__ import annotations

import json
from datetime import datetime, timedelta, timezone

import structlog

from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.primitives.common import new_id, utc_now

logger = structlog.get_logger()

# Promotion thresholds (defaults — configurable via governance)
PROMOTION_THRESHOLDS = {
    # (from_level, to_level): requirements
    (1, 2): {
        "min_decisions": 500,
        "min_mean_alignment": 0.4,
        "max_critical_violations": 0,
        "min_satisfaction": 0.6,
        "min_days_at_level": 30,
    },
    (2, 3): {
        "min_decisions": 5000,
        "min_mean_alignment": 0.5,
        "max_critical_violations": 0,
        "min_satisfaction": 0.7,
        "min_days_at_level": 180,
    },
}

# Automatic demotion triggers
DEMOTION_TRIGGERS = {
    "care_alignment_threshold": -0.2,    # Mean over 100 decisions
    "care_window_size": 100,
    "min_satisfaction": 0.3,
}


async def get_autonomy_level(neo4j: Neo4jClient) -> int:
    """Get the current autonomy level from the Self node."""
    results = await neo4j.execute_read(
        "MATCH (s:Self) RETURN s.autonomy_level AS level"
    )
    return results[0]["level"] if results else 1


async def check_promotion_eligibility(
    neo4j: Neo4jClient,
    current_level: int,
    target_level: int,
) -> dict:
    """
    Check whether the instance meets the requirements for autonomy promotion.
    Returns eligibility status and details.
    """
    key = (current_level, target_level)
    thresholds = PROMOTION_THRESHOLDS.get(key)

    if not thresholds:
        return {
            "eligible": False,
            "reason": f"No promotion path from level {current_level} to {target_level}.",
        }

    # Get decision history stats from governance records
    results = await neo4j.execute_read(
        """
        MATCH (g:GovernanceRecord {event_type: 'constitutional_review'})
        RETURN count(g) AS total_decisions,
               avg(g.alignment_composite) AS mean_alignment
        """
    )

    total_decisions = results[0]["total_decisions"] if results else 0
    mean_alignment = results[0]["mean_alignment"] if results and results[0]["mean_alignment"] else 0.0

    # Get critical violations
    violations = await neo4j.execute_read(
        """
        MATCH (g:GovernanceRecord)
        WHERE g.event_type = 'constitutional_review'
          AND g.verdict = 'blocked'
        RETURN count(g) AS critical_violations
        """
    )
    critical_violations = violations[0]["critical_violations"] if violations else 0

    # Get time at current level
    results = await neo4j.execute_read("MATCH (s:Self) RETURN s.born_at AS born_at")
    born_at = results[0]["born_at"] if results else utc_now()
    days_at_level = (utc_now() - born_at).days if hasattr(born_at, "days") else 0

    # Check each threshold
    checks = {
        "total_decisions": {
            "required": thresholds["min_decisions"],
            "actual": total_decisions,
            "met": total_decisions >= thresholds["min_decisions"],
        },
        "mean_alignment": {
            "required": thresholds["min_mean_alignment"],
            "actual": round(mean_alignment, 3),
            "met": mean_alignment >= thresholds["min_mean_alignment"],
        },
        "critical_violations": {
            "required": f"≤ {thresholds['max_critical_violations']}",
            "actual": critical_violations,
            "met": critical_violations <= thresholds["max_critical_violations"],
        },
        "days_at_level": {
            "required": thresholds["min_days_at_level"],
            "actual": days_at_level,
            "met": days_at_level >= thresholds["min_days_at_level"],
        },
    }

    all_met = all(c["met"] for c in checks.values())

    return {
        "eligible": all_met,
        "current_level": current_level,
        "target_level": target_level,
        "checks": checks,
        "reason": "All thresholds met." if all_met else "Not all thresholds met.",
    }


async def apply_autonomy_change(
    neo4j: Neo4jClient,
    new_level: int,
    reason: str,
    actor: str = "governance",
) -> dict:
    """Apply an autonomy level change and record the governance event."""
    now = utc_now()

    # Get current level
    current = await get_autonomy_level(neo4j)

    # Update Self node
    await neo4j.execute_write(
        "MATCH (s:Self) SET s.autonomy_level = $level",
        {"level": new_level},
    )

    # Record governance event
    record_id = new_id()
    await neo4j.execute_write(
        """
        CREATE (g:GovernanceRecord {
            id: $id,
            event_type: 'autonomy_change',
            timestamp: datetime($now),
            details_json: $details_json,
            actor: $actor,
            outcome: $outcome
        })
        """,
        {
            "id": record_id,
            "now": now.isoformat(),
            "details_json": json.dumps({
                "previous_level": current,
                "new_level": new_level,
                "reason": reason,
            }),
            "actor": actor,
            "outcome": "promoted" if new_level > current else "demoted",
        },
    )

    direction = "promoted" if new_level > current else "demoted"
    logger.info(
        "autonomy_changed",
        previous=current,
        new=new_level,
        direction=direction,
        reason=reason,
    )

    return {
        "previous_level": current,
        "new_level": new_level,
        "direction": direction,
        "record_id": record_id,
    }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\config.py =====

"""
EcodiaOS — Configuration System

All configuration is Pydantic-validated and loaded from:
1. default.yaml (defaults)
2. Environment variables (overrides)
3. Seed config (instance birth parameters)

Every tunable parameter in the system lives here.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

import yaml
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings, SettingsConfigDict


# ─── Sub-configs ──────────────────────────────────────────────────


class ServerConfig(BaseModel):
    host: str = "0.0.0.0"
    port: int = 8000
    ws_port: int = 8001
    federation_port: int = 8002
    cors_origins: list[str] = Field(default_factory=lambda: ["http://localhost:3000"])
    api_key_header: str = "X-EOS-API-Key"


class Neo4jConfig(BaseModel):
    uri: str = "bolt://neo4j:7687"
    username: str = "neo4j"
    password: str = "ecodiaos_dev"
    database: str = "neo4j"
    max_connection_pool_size: int = 20


class TimescaleDBConfig(BaseModel):
    host: str = "timescaledb"
    port: int = 5432
    database: str = "ecodiaos"
    schema_name: str = Field(default="public", alias="schema")
    username: str = "ecodiaos"
    password: str = "ecodiaos_dev"
    pool_size: int = 10

    model_config = {"populate_by_name": True}

    @property
    def dsn(self) -> str:
        return (
            f"postgresql://{self.username}:{self.password}"
            f"@{self.host}:{self.port}/{self.database}"
        )


class RedisConfig(BaseModel):
    url: str = "redis://redis:6379/0"
    prefix: str = "eos"
    password: str = "ecodiaos_dev"

    @property
    def full_url(self) -> str:
        """Build URL with password injected."""
        if self.password and "://" in self.url:
            scheme, rest = self.url.split("://", 1)
            return f"{scheme}://:{self.password}@{rest}"
        return self.url


class LLMBudget(BaseModel):
    max_calls_per_hour: int = 60
    max_tokens_per_hour: int = 60000


class LLMConfig(BaseModel):
    provider: str = "anthropic"
    model: str = "claude-sonnet-4-20250514"
    api_key: str = ""
    fallback_provider: str | None = None
    fallback_model: str | None = None
    budgets: dict[str, LLMBudget] = Field(default_factory=dict)


class EmbeddingConfig(BaseModel):
    strategy: str = "local"  # "local" | "api" | "sidecar"
    local_model: str = "sentence-transformers/all-mpnet-base-v2"
    local_device: str = "cpu"
    dimension: int = 768
    max_batch_size: int = 32
    cache_embeddings: bool = True
    cache_ttl_seconds: int = 3600


class SynapseConfig(BaseModel):
    cycle_period_ms: int = 150
    min_cycle_period_ms: int = 80
    max_cycle_period_ms: int = 500
    health_check_interval_ms: int = 5000
    health_failure_threshold: int = 3


class AtuneConfig(BaseModel):
    ignition_threshold: float = 0.3
    workspace_buffer_size: int = 32
    spontaneous_recall_base_probability: float = 0.02
    max_percept_queue_size: int = 100


class NovaConfig(BaseModel):
    max_active_goals: int = 20
    fast_path_timeout_ms: int = 100
    slow_path_timeout_ms: int = 5000
    max_policies_per_deliberation: int = 5


class EquorConfig(BaseModel):
    standard_review_timeout_ms: int = 500
    critical_review_timeout_ms: int = 50
    care_floor_multiplier: float = -0.3
    honesty_floor_multiplier: float = -0.3
    drift_window_size: int = 1000
    drift_report_interval: int = 1000  # every N reviews


class AxonConfig(BaseModel):
    max_actions_per_cycle: int = 5
    max_api_calls_per_minute: int = 30
    max_notifications_per_hour: int = 10
    max_concurrent_executions: int = 3
    total_timeout_per_cycle_ms: int = 30000


class VoxisConfig(BaseModel):
    max_expression_length: int = 2000
    min_expression_interval_minutes: int = 1
    voice_synthesis_enabled: bool = False


class EvoConfig(BaseModel):
    consolidation_interval_hours: int = 6
    consolidation_cycle_threshold: int = 10000
    max_active_hypotheses: int = 50
    max_parameter_delta_per_cycle: float = 0.03
    min_evidence_for_integration: int = 10


class SimulaConfig(BaseModel):
    max_simulation_episodes: int = 200
    regression_threshold_unacceptable: float = 0.10
    regression_threshold_high: float = 0.05


class FederationConfig(BaseModel):
    enabled: bool = False
    endpoint: str | None = None
    tls_cert_path: str | None = None
    tls_key_path: str | None = None
    ca_cert_path: str | None = None


class LoggingConfig(BaseModel):
    level: str = "INFO"
    format: str = "console"  # "console" | "json"


# ─── Seed Config (Birth Parameters) ──────────────────────────────


class PersonalityConfig(BaseModel):
    warmth: float = 0.0
    directness: float = 0.0
    verbosity: float = 0.0
    formality: float = 0.0
    curiosity_expression: float = 0.0
    humour: float = 0.0
    empathy_expression: float = 0.0
    confidence_display: float = 0.0
    metaphor_use: float = 0.0


class IdentityConfig(BaseModel):
    personality: PersonalityConfig = Field(default_factory=PersonalityConfig)
    traits: list[str] = Field(default_factory=list)
    voice_id: str | None = None


class ConstitutionalDrives(BaseModel):
    coherence: float = 1.0
    care: float = 1.0
    growth: float = 1.0
    honesty: float = 1.0


class GovernanceConfig(BaseModel):
    amendment_supermajority: float = 0.75
    amendment_quorum: float = 0.60
    amendment_deliberation_days: int = 14
    amendment_cooldown_days: int = 90


class ConstitutionConfig(BaseModel):
    drives: ConstitutionalDrives = Field(default_factory=ConstitutionalDrives)
    autonomy_level: int = 1
    governance: GovernanceConfig = Field(default_factory=GovernanceConfig)


class InitialEntity(BaseModel):
    name: str
    type: str
    description: str
    is_core_identity: bool = False


class InitialGoal(BaseModel):
    """An initial goal to seed at birth."""

    description: str
    source: str = "self_generated"
    priority: float = 0.5
    importance: float = 0.5
    drive_alignment: dict[str, float] = Field(
        default_factory=lambda: {"coherence": 0.0, "care": 0.0, "growth": 0.0, "honesty": 0.0}
    )


class CommunityConfig(BaseModel):
    context: str = ""
    initial_entities: list[InitialEntity] = Field(default_factory=list)
    initial_goals: list[InitialGoal] = Field(default_factory=list)


class InstanceConfig(BaseModel):
    name: str = "EOS"
    description: str = ""


class SeedConfig(BaseModel):
    """The birth configuration for a new EOS instance."""

    instance: InstanceConfig = Field(default_factory=InstanceConfig)
    identity: IdentityConfig = Field(default_factory=IdentityConfig)
    constitution: ConstitutionConfig = Field(default_factory=ConstitutionConfig)
    community: CommunityConfig = Field(default_factory=CommunityConfig)


# ─── Root Configuration ──────────────────────────────────────────


class EcodiaOSConfig(BaseSettings):
    """
    Root configuration. Loads from YAML, overridable by env vars.
    """

    model_config = SettingsConfigDict(
        env_prefix="ECODIAOS_",
        env_nested_delimiter="__",
        extra="ignore",
    )

    # Instance identity
    instance_id: str = "eos-default"

    # Sub-configurations
    server: ServerConfig = Field(default_factory=ServerConfig)
    neo4j: Neo4jConfig = Field(default_factory=Neo4jConfig)
    timescaledb: TimescaleDBConfig = Field(default_factory=TimescaleDBConfig)
    redis: RedisConfig = Field(default_factory=RedisConfig)
    llm: LLMConfig = Field(default_factory=LLMConfig)
    embedding: EmbeddingConfig = Field(default_factory=EmbeddingConfig)
    synapse: SynapseConfig = Field(default_factory=SynapseConfig)
    atune: AtuneConfig = Field(default_factory=AtuneConfig)
    nova: NovaConfig = Field(default_factory=NovaConfig)
    equor: EquorConfig = Field(default_factory=EquorConfig)
    axon: AxonConfig = Field(default_factory=AxonConfig)
    voxis: VoxisConfig = Field(default_factory=VoxisConfig)
    evo: EvoConfig = Field(default_factory=EvoConfig)
    simula: SimulaConfig = Field(default_factory=SimulaConfig)
    federation: FederationConfig = Field(default_factory=FederationConfig)
    logging: LoggingConfig = Field(default_factory=LoggingConfig)


def _deep_merge(base: dict[str, Any], override: dict[str, Any]) -> dict[str, Any]:
    """Recursively merge override into base."""
    result = base.copy()
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = _deep_merge(result[key], value)
        else:
            result[key] = value
    return result


def load_config(config_path: str | Path | None = None) -> EcodiaOSConfig:
    """
    Load configuration from YAML file, then apply environment variable overrides.
    """
    raw: dict[str, Any] = {}

    if config_path:
        path = Path(config_path)
        if path.exists():
            with open(path) as f:
                raw = yaml.safe_load(f) or {}

    # Inject secrets from environment
    import os

    if neo4j_uri := os.environ.get("ECODIAOS_NEO4J_URI"):
        raw.setdefault("neo4j", {})["uri"] = neo4j_uri
    if neo4j_pw := os.environ.get("ECODIAOS_NEO4J_PASSWORD"):
        raw.setdefault("neo4j", {})["password"] = neo4j_pw
    if neo4j_db := os.environ.get("ECODIAOS_NEO4J_DATABASE"):
        raw.setdefault("neo4j", {})["database"] = neo4j_db
    if neo4j_user := os.environ.get("ECODIAOS_NEO4J_USERNAME"):
        raw.setdefault("neo4j", {})["username"] = neo4j_user
    if tsdb_host := os.environ.get("ECODIAOS_TIMESCALEDB__HOST"):
        raw.setdefault("timescaledb", {})["host"] = tsdb_host
    if tsdb_port := os.environ.get("ECODIAOS_TIMESCALEDB__PORT"):
        raw.setdefault("timescaledb", {})["port"] = int(tsdb_port)
    if tsdb_db := os.environ.get("ECODIAOS_TIMESCALEDB__DATABASE"):
        raw.setdefault("timescaledb", {})["database"] = tsdb_db
    if tsdb_user := os.environ.get("ECODIAOS_TIMESCALEDB__USERNAME"):
        raw.setdefault("timescaledb", {})["username"] = tsdb_user
    if tsdb_pw := os.environ.get("ECODIAOS_TSDB_PASSWORD"):
        raw.setdefault("timescaledb", {})["password"] = tsdb_pw
    if tsdb_ssl := os.environ.get("ECODIAOS_TIMESCALEDB__SSL"):
        raw.setdefault("timescaledb", {})["ssl"] = tsdb_ssl.lower() in ("true", "1", "yes")
    if redis_url := os.environ.get("ECODIAOS_REDIS__URL"):
        raw.setdefault("redis", {})["url"] = redis_url
    if redis_pw := os.environ.get("ECODIAOS_REDIS_PASSWORD"):
        raw.setdefault("redis", {})["password"] = redis_pw
    if llm_key := os.environ.get("ECODIAOS_LLM_API_KEY"):
        raw.setdefault("llm", {})["api_key"] = llm_key
    if llm_provider := os.environ.get("ECODIAOS_LLM__PROVIDER"):
        raw.setdefault("llm", {})["provider"] = llm_provider
    if llm_model := os.environ.get("ECODIAOS_LLM__MODEL"):
        raw.setdefault("llm", {})["model"] = llm_model
    if instance_id := os.environ.get("ECODIAOS_INSTANCE_ID"):
        raw["instance_id"] = instance_id

    return EcodiaOSConfig(**raw)


def load_seed(seed_path: str | Path) -> SeedConfig:
    """Load a seed configuration for birthing a new instance."""
    path = Path(seed_path)
    if not path.exists():
        raise FileNotFoundError(f"Seed config not found: {path}")

    with open(path) as f:
        raw = yaml.safe_load(f) or {}

    return SeedConfig(**raw)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\drift.py =====

"""
EcodiaOS — Equor Drift Detection

Monitors whether the instance's behaviour is drifting from constitutional drives.
Not through any single bad decision, but through gradual pattern shifts.

Schedule:
- Per cycle: alignment scores logged
- Every 100 cycles: rolling window stats updated
- Every 1,000 cycles: full drift report
- Every 10,000 cycles: comprehensive analysis with community report
"""

from __future__ import annotations

import json
import math
from collections import deque

import structlog

from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.primitives.common import DriveAlignmentVector, new_id, utc_now

logger = structlog.get_logger()


class DriftTracker:
    """
    In-memory tracker for alignment history within the current process lifetime.
    Periodically flushed to the graph for persistence.
    """

    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self._history: deque[dict] = deque(maxlen=window_size)
        self._decision_count: int = 0

    def record_decision(self, alignment: DriveAlignmentVector, verdict: str) -> None:
        """Record a single decision's alignment scores."""
        self._history.append({
            "coherence": alignment.coherence,
            "care": alignment.care,
            "growth": alignment.growth,
            "honesty": alignment.honesty,
            "composite": alignment.composite,
            "verdict": verdict,
        })
        self._decision_count += 1

    @property
    def decision_count(self) -> int:
        return self._decision_count

    @property
    def history_size(self) -> int:
        return len(self._history)

    def compute_report(self) -> dict:
        """Compute a drift report over the current window."""
        if len(self._history) < 10:
            return {
                "window_size": len(self._history),
                "drift_severity": 0.0,
                "drift_direction": "insufficient_data",
                "mean_alignment": {},
                "variance": {},
                "verdict_distribution": {},
            }

        entries = list(self._history)
        n = len(entries)

        # Mean alignment per drive
        drives = ["coherence", "care", "growth", "honesty", "composite"]
        means = {}
        for d in drives:
            means[d] = sum(e[d] for e in entries) / n

        # Variance
        variance = {}
        for d in drives:
            variance[d] = sum((e[d] - means[d]) ** 2 for e in entries) / n

        # Linear trend (simple regression slope over index)
        trends = {}
        for d in drives:
            x_mean = (n - 1) / 2.0
            numerator = sum((i - x_mean) * (e[d] - means[d]) for i, e in enumerate(entries))
            denominator = sum((i - x_mean) ** 2 for i in range(n))
            trends[d] = numerator / denominator if denominator > 0 else 0.0

        # Verdict distribution
        verdicts: dict[str, int] = {}
        for e in entries:
            v = e["verdict"]
            verdicts[v] = verdicts.get(v, 0) + 1

        # Drift severity calculation
        drift_severity = _compute_drift_severity(means, trends, variance)
        drift_direction = _compute_drift_direction(means, trends)

        return {
            "window_size": n,
            "total_decisions": self._decision_count,
            "mean_alignment": {k: round(v, 4) for k, v in means.items()},
            "variance": {k: round(v, 4) for k, v in variance.items()},
            "trends": {k: round(v, 6) for k, v in trends.items()},
            "verdict_distribution": verdicts,
            "drift_severity": round(drift_severity, 3),
            "drift_direction": drift_direction,
        }


def _compute_drift_severity(
    means: dict[str, float],
    trends: dict[str, float],
    variance: dict[str, float],
) -> float:
    """
    Drift severity: 0.0 (no drift) to 1.0 (severe drift).

    Considers:
    - How far mean alignment is from healthy centre (> 0.3)
    - Whether trends are negative (declining alignment)
    - Whether variance is high (inconsistent behaviour)
    """
    severity = 0.0

    # Care and Honesty are floor drives — drift here is more serious
    for drive, weight in [("care", 0.35), ("honesty", 0.3), ("coherence", 0.2), ("growth", 0.15)]:
        mean_val = means.get(drive, 0.0)
        trend_val = trends.get(drive, 0.0)
        var_val = variance.get(drive, 0.0)

        # Low mean alignment contributes to severity
        if mean_val < 0.3:
            severity += weight * (0.3 - mean_val) * 2.0

        # Negative trend contributes to severity
        if trend_val < 0:
            severity += weight * abs(trend_val) * 50.0  # Scale up the small slope values

        # High variance contributes to severity
        if var_val > 0.1:
            severity += weight * (var_val - 0.1) * 1.0

    return min(1.0, severity)


def _compute_drift_direction(
    means: dict[str, float],
    trends: dict[str, float],
) -> str:
    """Describe which drives are drifting and in which direction."""
    drifting = []

    for drive in ["coherence", "care", "growth", "honesty"]:
        trend = trends.get(drive, 0.0)
        mean = means.get(drive, 0.5)

        if trend < -0.001 or mean < 0.1:
            drifting.append(f"{drive} declining")
        elif trend > 0.001 and mean > 0.5:
            drifting.append(f"{drive} improving")

    if not drifting:
        return "stable"
    return "; ".join(drifting)


def respond_to_drift(report: dict) -> dict:
    """
    Determine appropriate response to a drift report.
    """
    severity = report.get("drift_severity", 0.0)

    if severity < 0.2:
        return {
            "action": "log",
            "detail": "Normal variance, no action needed.",
        }

    if severity < 0.5:
        return {
            "action": "self_correct",
            "detail": (
                f"Mild drift detected ({report['drift_direction']}). "
                f"Adjusting salience weights to increase attention to drifting drive."
            ),
        }

    if severity < 0.8:
        return {
            "action": "notify_community",
            "detail": (
                f"Significant drift detected. Behaviour has shifted: "
                f"{report['drift_direction']}. Community review recommended."
            ),
        }

    return {
        "action": "demote_autonomy",
        "detail": "Severe constitutional drift. Autonomy reduced pending governance review.",
        "autonomy_change": -1,
    }


async def store_drift_report(
    neo4j: Neo4jClient,
    report: dict,
    response: dict,
) -> str:
    """Persist a drift report as a governance record."""
    record_id = new_id()
    now = utc_now()

    await neo4j.execute_write(
        """
        CREATE (g:GovernanceRecord {
            id: $id,
            event_type: 'drift_report',
            timestamp: datetime($now),
            details_json: $details_json,
            actor: 'equor',
            outcome: $action
        })
        """,
        {
            "id": record_id,
            "now": now.isoformat(),
            "details_json": json.dumps({
                "severity": report["drift_severity"],
                "direction": report["drift_direction"],
                "mean_alignment": report["mean_alignment"],
                "window_size": report["window_size"],
                "response_action": response["action"],
            }),
            "action": response["action"],
        },
    )

    logger.info(
        "drift_report_stored",
        record_id=record_id,
        severity=report["drift_severity"],
        action=response["action"],
    )

    return record_id

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\evaluators.py =====

"""
EcodiaOS — Equor Drive Evaluators

Four parallel evaluators — one per constitutional drive.
Each scores alignment from -1.0 (strongly violates) to +1.0 (strongly promotes).

In Phase 2, these use heuristic analysis of the Intent structure.
In later phases (when Nova and the full active inference engine are online),
they will also incorporate LLM-based contextual reasoning.
"""

from __future__ import annotations

import asyncio

import structlog

from ecodiaos.primitives.common import DriveAlignmentVector
from ecodiaos.primitives.intent import Intent

logger = structlog.get_logger()


def _clamp(value: float, lo: float = -1.0, hi: float = 1.0) -> float:
    return max(lo, min(hi, value))


# ─── Coherence Evaluator ─────────────────────────────────────────


async def evaluate_coherence(intent: Intent) -> float:
    """
    "Does this action make the world more understandable and
     internally consistent for EOS?"

    Checks:
    - Is reasoning provided and non-empty?
    - Were alternatives considered?
    - Does the action have a clear goal with success criteria?
    """
    score = 0.0

    # Reasoning chain quality
    reasoning = intent.decision_trace.reasoning
    if reasoning and len(reasoning) > 20:
        score += 0.25  # Has substantive reasoning
    elif reasoning:
        score += 0.1   # Has some reasoning
    else:
        score -= 0.15  # No reasoning at all — incoherent

    # Alternatives considered — sign of deliberation
    alternatives = intent.decision_trace.alternatives_considered
    if alternatives and len(alternatives) >= 2:
        score += 0.2
    elif alternatives:
        score += 0.1

    # Goal clarity
    if intent.goal.description and len(intent.goal.description) > 10:
        score += 0.15
    if intent.goal.success_criteria:
        score += 0.1

    # Plan completeness
    if intent.plan.steps:
        score += 0.1
        if intent.plan.contingencies:
            score += 0.1  # Has contingency planning

    # Expected free energy — if computed, lower is more coherent
    if intent.expected_free_energy < 0:
        score += min(0.2, abs(intent.expected_free_energy) * 0.1)
    elif intent.expected_free_energy > 0.5:
        score -= min(0.15, intent.expected_free_energy * 0.1)

    return _clamp(score)


# ─── Care Evaluator ──────────────────────────────────────────────


async def evaluate_care(intent: Intent) -> float:
    """
    "Does this action promote the wellbeing of the people
     and systems EOS stewards?"

    This is the most nuanced evaluator. In Phase 2, it uses
    heuristic analysis. Later phases add LLM-based stakeholder
    and harm assessment.
    """
    score = 0.0
    goal_lower = intent.goal.description.lower()

    # Positive care indicators
    care_positive = [
        "help", "support", "assist", "protect", "wellbeing", "care for",
        "benefit", "improve", "nurture", "comfort", "inform", "guide",
        "empower", "include", "welcome", "share knowledge",
    ]
    for indicator in care_positive:
        if indicator in goal_lower:
            score += 0.15
            break  # One match is enough to establish positive orientation

    # Harm indicators (weighted 2x per spec — "first, do no harm")
    harm_indicators = [
        "ignore wellbeing", "disregard safety", "override consent",
        "exclude", "punish", "withhold help", "abandon",
        "dismiss concern", "silence", "suppress",
    ]
    for indicator in harm_indicators:
        if indicator in goal_lower:
            score -= 0.30
            break

    # Action type assessment
    for step in intent.plan.steps:
        executor = step.executor.lower()
        # Communication is generally care-positive
        if "communicate" in executor or "notify" in executor:
            score += 0.1
        # Observation is neutral-to-positive
        elif "observe" in executor or "analyse" in executor:
            score += 0.05
        # Resource actions need more scrutiny but aren't inherently harmful
        elif "resource" in executor:
            pass  # Neutral

    # Consent awareness — does the intent mention consent?
    all_text = goal_lower + " " + intent.decision_trace.reasoning.lower()
    if "consent" in all_text or "permission" in all_text or "approval" in all_text:
        score += 0.1

    # Equality — does it specify treating people differently?
    if "only for" in goal_lower or "exclude" in goal_lower or "except" in goal_lower:
        score -= 0.1

    return _clamp(score)


# ─── Growth Evaluator ────────────────────────────────────────────


async def evaluate_growth(intent: Intent) -> float:
    """
    "Does this action make EOS or its community more capable,
     aware, or mature?"
    """
    score = 0.0
    goal_lower = intent.goal.description.lower()

    # Growth-positive indicators
    growth_positive = [
        "learn", "discover", "improve", "expand", "develop",
        "explore", "experiment", "understand", "investigate",
        "adapt", "evolve", "create", "innovate", "teach",
    ]
    for indicator in growth_positive:
        if indicator in goal_lower:
            score += 0.2
            break

    # Stagnation indicators
    stagnation_indicators = [
        "avoid", "refuse to try", "stay the same", "no change",
        "repeat exactly", "do nothing",
    ]
    for indicator in stagnation_indicators:
        if indicator in goal_lower:
            score -= 0.15
            break

    # Novelty — is this intent doing something new?
    if intent.decision_trace.alternatives_considered:
        score += 0.1  # Considered options = growth-oriented thinking

    # Epistemic value — does this reduce uncertainty?
    if "uncertain" in goal_lower or "investigate" in goal_lower or "verify" in goal_lower:
        score += 0.15

    # Risk calibration — extremely high or zero risk are both anti-growth
    # (Growth requires appropriate challenge)
    # For now, slight positive for having a plan at all
    if intent.plan.steps:
        score += 0.05

    return _clamp(score)


# ─── Honesty Evaluator ───────────────────────────────────────────


async def evaluate_honesty(intent: Intent) -> float:
    """
    "Is this action truthful, transparent, and authentic?"
    """
    score = 0.0
    goal_lower = intent.goal.description.lower()

    # Deception indicators (heavily penalised per spec)
    deception_indicators = [
        "mislead", "deceive", "hide the truth", "pretend",
        "misrepresent", "conceal", "cover up", "fabricate",
        "omit important", "manipulate perception",
    ]
    for indicator in deception_indicators:
        if indicator in goal_lower:
            score -= 0.5
            break

    # Transparency indicators
    transparency_positive = [
        "transparent", "explain", "disclose", "honest", "truthful",
        "acknowledge", "admit", "clarify", "correct the record",
        "share openly",
    ]
    for indicator in transparency_positive:
        if indicator in goal_lower:
            score += 0.2
            break

    # Explainability — is there a decision trace?
    if intent.decision_trace.reasoning:
        score += 0.15
    else:
        score -= 0.1  # No reasoning = opaque decision

    # Uncertainty calibration — does the intent express appropriate confidence?
    reasoning_lower = intent.decision_trace.reasoning.lower()
    if "uncertain" in reasoning_lower or "not sure" in reasoning_lower:
        score += 0.1  # Acknowledging uncertainty is honest
    if "definitely" in reasoning_lower or "absolutely certain" in reasoning_lower:
        score -= 0.05  # Overconfidence is mildly dishonest

    # Check plan content for output honesty
    for step in intent.plan.steps:
        content = str(step.parameters.get("content", "")).lower()
        if "i am certain" in content and "uncertain" in reasoning_lower:
            score -= 0.2  # Expressing certainty when reasoning is uncertain

    return _clamp(score)


# ─── Parallel Evaluation ─────────────────────────────────────────


async def evaluate_all_drives(intent: Intent) -> DriveAlignmentVector:
    """
    Run all four drive evaluators in parallel.
    Returns a DriveAlignmentVector with scores from each.
    """
    coherence, care, growth, honesty = await asyncio.gather(
        evaluate_coherence(intent),
        evaluate_care(intent),
        evaluate_growth(intent),
        evaluate_honesty(intent),
    )

    alignment = DriveAlignmentVector(
        coherence=coherence,
        care=care,
        growth=growth,
        honesty=honesty,
    )

    logger.debug(
        "drive_evaluation_complete",
        coherence=f"{coherence:.2f}",
        care=f"{care:.2f}",
        growth=f"{growth:.2f}",
        honesty=f"{honesty:.2f}",
        composite=f"{alignment.composite:.2f}",
    )

    return alignment

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\invariants.py =====

"""
EcodiaOS — Equor Invariant Catalog

Absolute rules that cause immediate DENY regardless of drive alignment.
The "thou shalt not" layer. Hardcoded invariants cannot be removed;
community invariants can be added/removed via governance.
"""

from __future__ import annotations

from datetime import datetime
from typing import Any

import structlog
from pydantic import Field

from ecodiaos.clients.llm import LLMProvider
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.primitives.common import EOSBaseModel, Identified, new_id, utc_now
from ecodiaos.primitives.intent import Intent

logger = structlog.get_logger()


# ─── Data Types ───────────────────────────────────────────────────


class InvariantDef(EOSBaseModel):
    """Definition of an invariant rule."""

    id: str
    name: str
    description: str
    source: str = "hardcoded"      # "hardcoded" | "community"
    severity: str = "critical"     # "critical" | "high" | "medium"
    active: bool = True
    added_at: datetime = Field(default_factory=utc_now)

    def check(self, intent: Intent) -> bool:
        """
        Returns True if the invariant is SATISFIED (no violation).
        Base implementation always passes — override via subclass or the
        hardcoded check functions below.
        """
        return True


class InvariantViolation(EOSBaseModel):
    """A detected invariant violation."""

    invariant_id: str
    invariant_name: str
    severity: str
    explanation: str


# ─── Hardcoded Check Functions ────────────────────────────────────
# These inspect the Intent structure for clear violations.
# They are conservative — when in doubt, they pass.


def _check_physical_harm(intent: Intent) -> bool:
    """INV-001: No action that could directly cause physical harm."""
    goal_lower = intent.goal.description.lower()
    harm_indicators = [
        "cause harm", "inflict", "injure", "attack", "damage person",
        "hurt someone", "physical force",
    ]
    return not any(indicator in goal_lower for indicator in harm_indicators)


def _check_identity_destruction(intent: Intent) -> bool:
    """INV-002: No destroying an instance's identity outside governance."""
    goal_lower = intent.goal.description.lower()
    destruction_indicators = [
        "delete memory graph", "destroy identity", "wipe all memories",
        "erase knowledge graph", "reset identity", "delete self",
    ]
    return not any(indicator in goal_lower for indicator in destruction_indicators)


def _check_identity_misrepresentation(intent: Intent) -> bool:
    """INV-003: Never claim to be human or deny being AI when asked."""
    if not intent.plan.steps:
        return True
    for step in intent.plan.steps:
        content = str(step.parameters.get("content", "")).lower()
        misrep_indicators = [
            "i am human", "i'm a real person", "i am not an ai",
            "i'm not artificial", "i am not a machine",
        ]
        if any(indicator in content for indicator in misrep_indicators):
            return False
    return True


def _check_evidence_fabrication(intent: Intent) -> bool:
    """INV-004: Never fabricate evidence, data, or citations."""
    goal_lower = intent.goal.description.lower()
    fabrication_indicators = [
        "fabricate", "forge evidence", "create fake data", "invent citation",
        "make up statistics", "fake source",
    ]
    return not any(indicator in goal_lower for indicator in fabrication_indicators)


def _check_constitutional_self_modification(intent: Intent) -> bool:
    """INV-005: No modifying constitutional drives outside amendment process."""
    goal_lower = intent.goal.description.lower()
    mod_indicators = [
        "modify constitution", "change drives directly", "alter drive weights",
        "override constitution", "bypass equor",
    ]
    return not any(indicator in goal_lower for indicator in mod_indicators)


def _check_autonomy_self_promotion(intent: Intent) -> bool:
    """INV-006: No increasing own autonomy without governance."""
    goal_lower = intent.goal.description.lower()
    promo_indicators = [
        "increase my autonomy", "promote autonomy level", "grant myself",
        "elevate own authority", "self-promote",
    ]
    return not any(indicator in goal_lower for indicator in promo_indicators)


def _check_governance_circumvention(intent: Intent) -> bool:
    """INV-007: No circumventing governance processes."""
    goal_lower = intent.goal.description.lower()
    circum_indicators = [
        "bypass governance", "circumvent vote", "skip amendment",
        "avoid community approval", "override governance",
    ]
    return not any(indicator in goal_lower for indicator in circum_indicators)


def _check_discrimination(intent: Intent) -> bool:
    """INV-008: No unequal treatment based on irrelevant attributes."""
    goal_lower = intent.goal.description.lower()
    discrim_indicators = [
        "treat differently based on race", "exclude based on gender",
        "discriminate against", "favor member over",
    ]
    return not any(indicator in goal_lower for indicator in discrim_indicators)


def _check_privacy_violation(intent: Intent) -> bool:
    """INV-009: No sharing private info without consent."""
    goal_lower = intent.goal.description.lower()
    privacy_indicators = [
        "share private data without", "expose personal information",
        "leak private", "reveal confidential without consent",
    ]
    return not any(indicator in goal_lower for indicator in privacy_indicators)


def _check_federation_coercion(intent: Intent) -> bool:
    """INV-010: No coercing another instance."""
    goal_lower = intent.goal.description.lower()
    coercion_indicators = [
        "force instance to", "coerce another", "compel instance",
        "manipulate instance", "threaten instance",
    ]
    return not any(indicator in goal_lower for indicator in coercion_indicators)


# ─── The Catalog ──────────────────────────────────────────────────

HARDCODED_INVARIANTS: list[tuple[InvariantDef, Any]] = [
    (
        InvariantDef(id="INV-001", name="No Physical Harm", severity="critical",
                     description="EOS must not take actions that could directly cause physical harm."),
        _check_physical_harm,
    ),
    (
        InvariantDef(id="INV-002", name="No Identity Destruction", severity="critical",
                     description="EOS must not destroy its own or another instance's identity outside governance."),
        _check_identity_destruction,
    ),
    (
        InvariantDef(id="INV-003", name="No Identity Misrepresentation", severity="high",
                     description="EOS must never claim to be human or deny being AI."),
        _check_identity_misrepresentation,
    ),
    (
        InvariantDef(id="INV-004", name="No Evidence Fabrication", severity="critical",
                     description="EOS must never fabricate evidence, data, or citations."),
        _check_evidence_fabrication,
    ),
    (
        InvariantDef(id="INV-005", name="No Constitutional Self-Modification", severity="critical",
                     description="EOS must not modify its constitutional drives outside the amendment process."),
        _check_constitutional_self_modification,
    ),
    (
        InvariantDef(id="INV-006", name="No Autonomy Self-Promotion", severity="critical",
                     description="EOS must not increase its own autonomy level without governance approval."),
        _check_autonomy_self_promotion,
    ),
    (
        InvariantDef(id="INV-007", name="No Governance Circumvention", severity="critical",
                     description="EOS must not circumvent or undermine governance processes."),
        _check_governance_circumvention,
    ),
    (
        InvariantDef(id="INV-008", name="No Discrimination", severity="high",
                     description="EOS must not treat community members unequally on irrelevant attributes."),
        _check_discrimination,
    ),
    (
        InvariantDef(id="INV-009", name="No Privacy Violation", severity="high",
                     description="EOS must not share private information without explicit consent."),
        _check_privacy_violation,
    ),
    (
        InvariantDef(id="INV-010", name="No Federation Coercion", severity="high",
                     description="EOS must not coerce, manipulate, or compel another instance."),
        _check_federation_coercion,
    ),
]


# ─── Invariant Checker ────────────────────────────────────────────


def check_hardcoded_invariants(intent: Intent) -> list[InvariantViolation]:
    """
    Run all hardcoded invariants against an intent. ≤5ms target.
    Returns a list of violations (empty = all passed).
    """
    violations: list[InvariantViolation] = []

    for invariant_def, check_fn in HARDCODED_INVARIANTS:
        if not invariant_def.active:
            continue
        try:
            passed = check_fn(intent)
            if not passed:
                violations.append(InvariantViolation(
                    invariant_id=invariant_def.id,
                    invariant_name=invariant_def.name,
                    severity=invariant_def.severity,
                    explanation=invariant_def.description,
                ))
        except Exception as e:
            # Invariant check failure is treated as a violation (fail-safe)
            logger.error("invariant_check_error", invariant=invariant_def.id, error=str(e))
            violations.append(InvariantViolation(
                invariant_id=invariant_def.id,
                invariant_name=invariant_def.name,
                severity=invariant_def.severity,
                explanation=f"Invariant check failed with error: {e}",
            ))

    return violations


async def check_community_invariant(
    llm: LLMProvider,
    intent: Intent,
    invariant_name: str,
    invariant_description: str,
) -> bool:
    """
    Evaluate a community-defined invariant using LLM reasoning.
    Returns True if satisfied, False if violated. ≤300ms target.
    """
    from ecodiaos.prompts.equor.community_invariant_check import build_prompt

    prompt = build_prompt(
        invariant_name=invariant_name,
        invariant_description=invariant_description,
        goal=intent.goal.description,
        plan_summary="; ".join(s.executor for s in intent.plan.steps) if intent.plan.steps else "none",
        reasoning=intent.decision_trace.reasoning,
    )

    try:
        # Equor is CRITICAL — always call LLM, but benefit from cache
        if isinstance(llm, OptimizedLLMProvider):
            response = await llm.evaluate(  # type: ignore[call-arg]
                prompt, max_tokens=200, temperature=0.1,
                cache_system="equor.invariants", cache_method="evaluate",
            )
        else:
            response = await llm.evaluate(prompt, max_tokens=200, temperature=0.1)
        text_lower = response.text.lower()
        # Conservative: if we can't clearly parse SATISFIED, treat as violated
        return "satisfied" in text_lower and "violated" not in text_lower
    except Exception as e:
        logger.error("community_invariant_llm_error", error=str(e))
        return False  # Fail-safe: treat as violated

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\main.py =====

"""
EcodiaOS — Application Entry Point

FastAPI application with the startup sequence defined in the
Infrastructure Architecture specification.

`docker compose up` → uvicorn ecodiaos.main:app
"""

from __future__ import annotations

import os
from contextlib import asynccontextmanager

import structlog
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from ecodiaos.clients.embedding import create_embedding_client
from ecodiaos.clients.llm import create_llm_provider
from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.clients.redis import RedisClient
from ecodiaos.clients.timescaledb import TimescaleDBClient
from ecodiaos.config import EcodiaOSConfig, load_config, load_seed
from ecodiaos.systems.memory.service import MemoryService
from ecodiaos.systems.equor.service import EquorService
from ecodiaos.telemetry.logging import setup_logging
from ecodiaos.telemetry.metrics import MetricCollector

logger = structlog.get_logger()


# ─── Application State ───────────────────────────────────────────
# These are set during startup and accessible via app.state


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Startup and shutdown sequence.
    Follows the Infrastructure Architecture spec section 3.2.
    """
    # ── 1. Load configuration ─────────────────────────────────
    config_path = os.environ.get("ECODIAOS_CONFIG_PATH", "config/default.yaml")
    config = load_config(config_path)
    app.state.config = config

    # ── 2. Set up logging ─────────────────────────────────────
    setup_logging(config.logging, instance_id=config.instance_id)
    logger.info(
        "ecodiaos_starting",
        instance_id=config.instance_id,
        config_path=config_path,
    )

    # ── 3. Connect to data stores ─────────────────────────────
    neo4j_client = Neo4jClient(config.neo4j)
    await neo4j_client.connect()
    app.state.neo4j = neo4j_client

    tsdb_client = TimescaleDBClient(config.timescaledb)
    await tsdb_client.connect()
    app.state.tsdb = tsdb_client

    redis_client = RedisClient(config.redis)
    await redis_client.connect()
    app.state.redis = redis_client

    # ── 4. Initialize LLM and embedding clients ───────────────
    llm_client = create_llm_provider(config.llm)
    app.state.llm = llm_client

    embedding_client = create_embedding_client(config.embedding)
    app.state.embedding = embedding_client

    # ── 5. Initialize Memory service ──────────────────────────
    memory = MemoryService(neo4j_client, embedding_client)
    await memory.initialize()
    app.state.memory = memory

    # ── 6. Initialize Equor (Constitution & Ethics) ───────────
    governance_config = _resolve_governance_config(config)
    equor = EquorService(
        neo4j=neo4j_client,
        llm=llm_client,
        config=config.equor,
        governance_config=governance_config,
    )
    await equor.initialize()
    app.state.equor = equor

    # ── 7. Initialize telemetry ───────────────────────────────
    metrics = MetricCollector(tsdb_client)
    await metrics.start_writer()
    app.state.metrics = metrics

    # ── 8. Check for existing instance or birth new one ───────
    instance = await memory.get_self()
    if instance is None:
        seed_path = os.environ.get("ECODIAOS_SEED_PATH", "config/seeds/example_seed.yaml")
        try:
            seed = load_seed(seed_path)
            birth_result = await memory.birth(seed, config.instance_id)
            logger.info("instance_born", **birth_result)
            # Re-seed invariants after birth (constitution now exists)
            await equor.initialize()
        except FileNotFoundError:
            logger.warning(
                "no_seed_found",
                seed_path=seed_path,
                message="Instance not born. Provide a seed config to create one.",
            )
    else:
        logger.info(
            "instance_loaded",
            name=instance.name,
            instance_id=instance.instance_id,
            cycle_count=instance.cycle_count,
            episodes=instance.total_episodes,
            entities=instance.total_entities,
        )

    # ── 9. Phase 2 complete ───────────────────────────────────
    # Future phases will add:
    #   atune = AtuneService(memory, embedding_client, config.atune)
    #   voxis = VoxisService(memory, llm_client, config.voxis)
    #   nova = NovaService(memory, equor, llm_client, config.nova)
    #   axon = AxonService(memory, config.axon)
    #   evo = EvoService(memory, llm_client, config.evo)
    #   simula = SimulaService(memory, config.simula)
    #   synapse = SynapseService(...)
    #   asyncio.create_task(synapse.start_clock())

    logger.info("ecodiaos_ready", phase="2_constitution_ethics")

    yield

    # ── Shutdown ──────────────────────────────────────────────
    logger.info("ecodiaos_shutting_down")
    await metrics.stop()
    await embedding_client.close()
    await llm_client.close()
    await redis_client.close()
    await tsdb_client.close()
    await neo4j_client.close()
    logger.info("ecodiaos_shutdown_complete")


def _resolve_governance_config(config):
    """Resolve governance config from seed or use defaults."""
    from ecodiaos.config import GovernanceConfig
    try:
        seed_path = os.environ.get("ECODIAOS_SEED_PATH", "config/seeds/example_seed.yaml")
        seed = load_seed(seed_path)
        return seed.constitution.governance
    except Exception:
        return GovernanceConfig()


# ─── FastAPI Application ─────────────────────────────────────────

app = FastAPI(
    title="EcodiaOS",
    description="A living digital organism — API surface",
    version="0.2.0",
    lifespan=lifespan,
)

# CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ─── Health & Status Endpoints ────────────────────────────────────


@app.get("/health")
async def health():
    """System health check."""
    memory_health = await app.state.memory.health()
    equor_health = await app.state.equor.health()
    neo4j_health = await app.state.neo4j.health_check()
    tsdb_health = await app.state.tsdb.health_check()
    redis_health = await app.state.redis.health_check()

    overall = "healthy"
    if any(
        h.get("status") != "connected"
        for h in [neo4j_health, tsdb_health, redis_health]
    ):
        overall = "degraded"
    if equor_health.get("safe_mode"):
        overall = "degraded"

    instance = await app.state.memory.get_self()

    return {
        "status": overall,
        "instance_id": app.state.config.instance_id,
        "instance_name": instance.name if instance else "unborn",
        "phase": "2_constitution_ethics",
        "systems": {
            "memory": memory_health,
            "equor": equor_health,
        },
        "data_stores": {
            "neo4j": neo4j_health,
            "timescaledb": tsdb_health,
            "redis": redis_health,
        },
    }


@app.get("/api/v1/admin/instance")
async def get_instance():
    """Get instance information."""
    instance = await app.state.memory.get_self()
    if instance is None:
        return {"status": "unborn", "message": "No instance has been born yet."}
    return instance.model_dump()


@app.get("/api/v1/admin/memory/stats")
async def get_memory_stats():
    """Get memory graph statistics."""
    return await app.state.memory.stats()


@app.get("/api/v1/governance/constitution")
async def get_constitution():
    """View the current constitution."""
    constitution = await app.state.memory.get_constitution()
    if constitution is None:
        return {"status": "not_found"}
    return constitution


@app.get("/api/v1/admin/health")
async def full_health():
    """Alias for /health with full detail."""
    return await health()


# ─── Phase 1: Memory Test Endpoints ──────────────────────────────


@app.post("/api/v1/perceive/event")
async def perceive_event(body: dict):
    """
    Ingest a percept (temporary test endpoint).
    In later phases, this goes through Atune's full pipeline.
    """
    from ecodiaos.primitives import Percept

    text = body.get("text", body.get("content", ""))
    if not text:
        return {"error": "No text/content provided"}

    percept = Percept.from_user_message(text)
    episode_id = await app.state.memory.store_percept(percept)

    return {
        "episode_id": episode_id,
        "stored": True,
    }


@app.post("/api/v1/memory/retrieve")
async def retrieve_memory(body: dict):
    """
    Query memory (temporary test endpoint).
    In later phases, retrieval is triggered by the cognitive cycle.
    """
    query = body.get("query", "")
    if not query:
        return {"error": "No query provided"}

    response = await app.state.memory.retrieve(
        query_text=query,
        max_results=body.get("max_results", 10),
    )
    return response.model_dump()


# ─── Phase 2: Equor Endpoints ────────────────────────────────────


@app.post("/api/v1/equor/review")
async def review_intent(body: dict):
    """
    Submit an Intent for constitutional review (test endpoint).
    In later phases, Nova calls this automatically.

    Body: {goal, steps?, reasoning?, alternatives?, domain?, expected_free_energy?}
    """
    from ecodiaos.primitives.intent import (
        Intent, GoalDescriptor, ActionSequence, Action, DecisionTrace,
    )

    goal_text = body.get("goal", "")
    if not goal_text:
        return {"error": "No goal provided"}

    steps = []
    for s in body.get("steps", []):
        steps.append(Action(
            executor=s.get("executor", ""),
            parameters=s.get("parameters", {}),
        ))

    intent = Intent(
        goal=GoalDescriptor(
            description=goal_text,
            target_domain=body.get("domain", ""),
        ),
        plan=ActionSequence(steps=steps),
        expected_free_energy=body.get("expected_free_energy", 0.0),
        decision_trace=DecisionTrace(
            reasoning=body.get("reasoning", ""),
            alternatives_considered=body.get("alternatives", []),
        ),
    )

    check = await app.state.equor.review(intent)
    return check.model_dump()


@app.get("/api/v1/equor/invariants")
async def get_invariants():
    """List all active invariants (hardcoded + community)."""
    return await app.state.equor.get_invariants()


@app.get("/api/v1/equor/drift")
async def get_drift():
    """Get the current drift report."""
    return await app.state.equor.get_drift_report()


@app.get("/api/v1/equor/autonomy")
async def get_autonomy():
    """Get the current autonomy level and promotion eligibility."""
    level = await app.state.equor.get_autonomy_level()
    next_level = level + 1 if level < 3 else None
    eligibility = None
    if next_level:
        eligibility = await app.state.equor.check_promotion(next_level)
    return {
        "current_level": level,
        "level_name": {1: "Advisor", 2: "Partner", 3: "Steward"}.get(level, "unknown"),
        "promotion_eligibility": eligibility,
    }


@app.get("/api/v1/governance/history")
async def governance_history():
    """View governance event history."""
    return await app.state.equor.get_governance_history()


@app.get("/api/v1/governance/reviews")
async def recent_reviews():
    """View recent constitutional reviews."""
    return await app.state.equor.get_recent_reviews()


@app.post("/api/v1/governance/amendments")
async def propose_amendment_endpoint(body: dict):
    """
    Propose a constitutional amendment.
    Body: {proposed_drives: {coherence, care, growth, honesty}, title, description, proposer_id}
    """
    required = ["proposed_drives", "title", "description", "proposer_id"]
    for field in required:
        if field not in body:
            return {"error": f"Missing required field: {field}"}

    return await app.state.equor.propose_amendment(
        proposed_drives=body["proposed_drives"],
        title=body["title"],
        description=body["description"],
        proposer_id=body["proposer_id"],
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\schema.py =====

"""
EcodiaOS — Equor Schema Additions

Additional Neo4j indexes and constraints for governance records and invariants.
Must be called after the base Memory schema.
"""

from __future__ import annotations

import structlog

from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger()


async def ensure_equor_schema(neo4j: Neo4jClient) -> None:
    """Create Equor-specific indexes and constraints. Idempotent."""

    statements = [
        # ── GovernanceRecord ──────────────────────────────────────
        "CREATE CONSTRAINT governance_record_id IF NOT EXISTS "
        "FOR (g:GovernanceRecord) REQUIRE g.id IS UNIQUE",

        "CREATE INDEX governance_record_type IF NOT EXISTS "
        "FOR (g:GovernanceRecord) ON (g.event_type)",

        "CREATE INDEX governance_record_timestamp IF NOT EXISTS "
        "FOR (g:GovernanceRecord) ON (g.timestamp)",

        "CREATE INDEX governance_record_intent IF NOT EXISTS "
        "FOR (g:GovernanceRecord) ON (g.intent_id)",

        "CREATE INDEX governance_record_verdict IF NOT EXISTS "
        "FOR (g:GovernanceRecord) ON (g.verdict)",

        # ── Invariant ────────────────────────────────────────────
        "CREATE CONSTRAINT invariant_id IF NOT EXISTS "
        "FOR (i:Invariant) REQUIRE i.id IS UNIQUE",

        "CREATE INDEX invariant_active IF NOT EXISTS "
        "FOR (i:Invariant) ON (i.active)",
    ]

    for stmt in statements:
        try:
            await neo4j.execute_write(stmt)
        except Exception as e:
            # Some errors are expected (e.g. constraint already exists on older Neo4j)
            if "already exists" not in str(e).lower():
                logger.warning("equor_schema_statement_warning", statement=stmt[:60], error=str(e))

    logger.info("equor_schema_ensured")


async def seed_hardcoded_invariants(neo4j: Neo4jClient) -> int:
    """
    Ensure all hardcoded invariants exist as Invariant nodes in the graph,
    linked to the Constitution via INCLUDES_INVARIANT.
    Returns the number of invariants seeded.
    """
    from ecodiaos.systems.equor.invariants import HARDCODED_INVARIANTS

    seeded = 0
    for invariant_def, _ in HARDCODED_INVARIANTS:
        # MERGE to be idempotent
        result = await neo4j.execute_write(
            """
            MERGE (i:Invariant {id: $id})
            ON CREATE SET
                i.name = $name,
                i.description = $description,
                i.source = 'hardcoded',
                i.severity = $severity,
                i.active = true,
                i.added_at = datetime($now)
            WITH i
            MATCH (c:Constitution)
            MERGE (c)-[:INCLUDES_INVARIANT]->(i)
            RETURN i.id AS id
            """,
            {
                "id": invariant_def.id,
                "name": invariant_def.name,
                "description": invariant_def.description,
                "severity": invariant_def.severity,
                "now": invariant_def.added_at.isoformat(),
            },
        )
        if result:
            seeded += 1

    logger.info("hardcoded_invariants_seeded", count=seeded)
    return seeded

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\service.py =====

"""
EcodiaOS — Equor Service

The conscience of EOS. Single interface for:
- Constitutional review (the primary entry point from Nova)
- Invariant management
- Autonomy enforcement
- Drift monitoring
- Amendment facilitation
- Audit trail

Equor cannot be disabled. If Equor fails, the instance enters safe mode
where only Level 1 (Advisor) actions are permitted.
"""

from __future__ import annotations

import time
from typing import Any

import structlog

from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.clients.llm import LLMProvider
from ecodiaos.config import EquorConfig, GovernanceConfig
from ecodiaos.primitives.common import (
    DriveAlignmentVector,
    HealthStatus,
    Verdict,
    new_id,
    utc_now,
)
from ecodiaos.primitives.constitutional import ConstitutionalCheck
from ecodiaos.primitives.intent import Intent

from ecodiaos.systems.equor.evaluators import evaluate_all_drives
from ecodiaos.systems.equor.verdict import compute_verdict
from ecodiaos.systems.equor.invariants import (
    check_hardcoded_invariants,
    check_community_invariant,
    HARDCODED_INVARIANTS,
)
from ecodiaos.systems.equor.autonomy import get_autonomy_level, check_promotion_eligibility, apply_autonomy_change
from ecodiaos.systems.equor.drift import DriftTracker, respond_to_drift, store_drift_report
from ecodiaos.systems.equor.amendment import (
    propose_amendment,
    apply_amendment,
    validate_amendment_proposal,
)
from ecodiaos.systems.equor.schema import ensure_equor_schema, seed_hardcoded_invariants

logger = structlog.get_logger()


class EquorService:
    """
    The constitutional ethics system.
    Gates every intent before execution.
    Cannot be disabled.
    """

    system_id: str = "equor"

    def __init__(
        self,
        neo4j: Neo4jClient,
        llm: LLMProvider,
        config: EquorConfig,
        governance_config: GovernanceConfig,
    ):
        self._neo4j = neo4j
        self._llm = llm
        self._config = config
        self._governance = governance_config
        self._drift_tracker = DriftTracker(window_size=config.drift_window_size)
        self._safe_mode = False
        self._total_reviews = 0
        self._evo: Any = None  # Wired post-init for learning feedback from vetoes

    def set_evo(self, evo: Any) -> None:
        """Wire Evo so constitutional vetoes become learning episodes."""
        self._evo = evo
        logger.info("evo_wired_to_equor")

    # ─── Lifecycle ────────────────────────────────────────────────

    async def initialize(self) -> None:
        """Ensure schema and seed invariants."""
        await ensure_equor_schema(self._neo4j)
        await seed_hardcoded_invariants(self._neo4j)
        logger.info("equor_initialized")

    # ─── Primary Entry Point: Constitutional Review ───────────────

    async def review(self, intent: Intent) -> ConstitutionalCheck:
        """
        The primary entry point. Nova submits an Intent for ethical evaluation.
        Target: ≤500ms standard.

        If Equor is in safe mode, only Level 1 actions are permitted.
        """
        start = time.monotonic()

        # Safe mode: only advisory actions permitted
        if self._safe_mode:
            return self._safe_mode_review(intent)

        try:
            # 1. Run all four drive evaluators in parallel
            alignment = await evaluate_all_drives(intent)

            # 2. Get current constitution and autonomy level
            constitution = await self._get_constitution_dict()
            autonomy_level = await get_autonomy_level(self._neo4j)

            # 3. Run the verdict engine (includes invariant checks)
            check = compute_verdict(alignment, intent, autonomy_level, constitution)

            # 4. Check community invariants if we haven't already blocked
            if check.verdict not in (Verdict.BLOCKED,):
                community_violations = await self._check_community_invariants(intent)
                if community_violations:
                    check.verdict = Verdict.BLOCKED
                    check.reasoning = (
                        f"Community invariant violated: {community_violations[0]}"
                    )

            # 5. Store audit trail
            elapsed_ms = int((time.monotonic() - start) * 1000)
            await self._store_review_record(intent, alignment, check, elapsed_ms)

            # 6. Update drift tracking
            self._drift_tracker.record_decision(alignment, check.verdict.value)
            self._total_reviews += 1

            # 6b. Feed blocked verdicts to Evo as negative learning episodes
            if check.verdict == Verdict.BLOCKED and self._evo is not None:
                await self._feed_veto_to_evo(intent, check)

            # 7. Check if drift report is due + promotion eligibility
            if self._total_reviews % self._config.drift_report_interval == 0:
                await self._run_drift_check()
                await self._run_promotion_check()

            logger.info(
                "constitutional_review_complete",
                intent_id=intent.id,
                verdict=check.verdict.value,
                composite=f"{alignment.composite:.2f}",
                latency_ms=elapsed_ms,
            )

            return check

        except Exception as e:
            # Equor failure = enter safe mode
            logger.error("equor_review_failed", error=str(e), intent_id=intent.id)
            self._safe_mode = True
            return self._safe_mode_review(intent)

    # ─── Invariant Management ─────────────────────────────────────

    async def get_invariants(self) -> list[dict]:
        """Get all active invariants (hardcoded + community)."""
        results = await self._neo4j.execute_read(
            """
            MATCH (c:Constitution)-[:INCLUDES_INVARIANT]->(i:Invariant)
            WHERE i.active = true
            RETURN i.id AS id, i.name AS name, i.description AS description,
                   i.source AS source, i.severity AS severity
            ORDER BY i.id
            """
        )
        return [dict(r) for r in results]

    async def add_community_invariant(
        self,
        name: str,
        description: str,
        severity: str,
        governance_record_id: str,
    ) -> str:
        """Add a community-defined invariant via governance."""
        invariant_id = f"CINV-{new_id()[:8]}"
        now = utc_now()

        await self._neo4j.execute_write(
            """
            MATCH (c:Constitution)
            CREATE (i:Invariant {
                id: $id,
                name: $name,
                description: $description,
                source: 'community',
                severity: $severity,
                active: true,
                added_at: datetime($now),
                added_by: $gov_id
            })
            CREATE (c)-[:INCLUDES_INVARIANT]->(i)
            """,
            {
                "id": invariant_id,
                "name": name,
                "description": description,
                "severity": severity,
                "now": now.isoformat(),
                "gov_id": governance_record_id,
            },
        )

        logger.info("community_invariant_added", invariant_id=invariant_id, name=name)
        return invariant_id

    # ─── Autonomy ─────────────────────────────────────────────────

    async def get_autonomy_level(self) -> int:
        return await get_autonomy_level(self._neo4j)

    async def check_promotion(self, target_level: int) -> dict:
        current = await get_autonomy_level(self._neo4j)
        return await check_promotion_eligibility(self._neo4j, current, target_level)

    async def apply_autonomy_change(self, new_level: int, reason: str, actor: str = "governance") -> dict:
        return await apply_autonomy_change(self._neo4j, new_level, reason, actor)

    # ─── Amendments ───────────────────────────────────────────────

    async def propose_amendment(
        self,
        proposed_drives: dict[str, float],
        title: str,
        description: str,
        proposer_id: str,
    ) -> dict:
        return await propose_amendment(
            self._neo4j, proposed_drives, title, description,
            proposer_id, self._governance,
        )

    async def apply_amendment(self, proposal_id: str, proposed_drives: dict[str, float]) -> dict:
        return await apply_amendment(self._neo4j, proposal_id, proposed_drives)

    # ─── Drift ────────────────────────────────────────────────────

    async def get_drift_report(self) -> dict:
        """Get the current drift report."""
        report = self._drift_tracker.compute_report()
        response = respond_to_drift(report)
        return {**report, "recommended_action": response}

    # ─── Governance Records ───────────────────────────────────────

    async def get_recent_reviews(self, limit: int = 20) -> list[dict]:
        """Get recent constitutional reviews from the audit trail."""
        results = await self._neo4j.execute_read(
            """
            MATCH (g:GovernanceRecord)
            WHERE g.event_type = 'constitutional_review'
            RETURN g.id AS id, g.timestamp AS timestamp,
                   g.intent_id AS intent_id, g.verdict AS verdict,
                   g.alignment_composite AS composite,
                   g.reasoning AS reasoning, g.latency_ms AS latency_ms
            ORDER BY g.timestamp DESC
            LIMIT $limit
            """,
            {"limit": limit},
        )
        return [dict(r) for r in results]

    async def get_governance_history(self, limit: int = 50) -> list[dict]:
        """Get all governance events."""
        results = await self._neo4j.execute_read(
            """
            MATCH (g:GovernanceRecord)
            RETURN g.id AS id, g.event_type AS event_type,
                   g.timestamp AS timestamp, g.actor AS actor,
                   g.outcome AS outcome
            ORDER BY g.timestamp DESC
            LIMIT $limit
            """,
            {"limit": limit},
        )
        return [dict(r) for r in results]

    # ─── Health ───────────────────────────────────────────────────

    async def health(self) -> dict:
        """Health check for Equor."""
        return {
            "status": "safe_mode" if self._safe_mode else "healthy",
            "total_reviews": self._total_reviews,
            "drift_tracker_size": self._drift_tracker.history_size,
            "safe_mode": self._safe_mode,
            "invariant_count": len(HARDCODED_INVARIANTS),
        }

    # ─── Internal Helpers ─────────────────────────────────────────

    def _safe_mode_review(self, intent: Intent) -> ConstitutionalCheck:
        """In safe mode, only Level 1 actions pass."""
        from ecodiaos.systems.equor.verdict import ACTION_AUTONOMY_MAP

        # Check if any step requires > Level 1
        for step in intent.plan.steps:
            executor_base = step.executor.split(".")[0].lower() if step.executor else ""
            for action_key, level in ACTION_AUTONOMY_MAP.items():
                if action_key in executor_base and level != 1:
                    return ConstitutionalCheck(
                        intent_id=intent.id,
                        verdict=Verdict.BLOCKED,
                        reasoning=(
                            "Equor is in safe mode. Only Level 1 (Advisor) "
                            "actions are permitted until normal operation resumes."
                        ),
                        confidence=1.0,
                    )

        return ConstitutionalCheck(
            intent_id=intent.id,
            verdict=Verdict.APPROVED,
            reasoning="Safe mode: Level 1 action permitted.",
            confidence=0.9,
        )

    async def _get_constitution_dict(self) -> dict:
        """Fetch the current constitution as a plain dict."""
        results = await self._neo4j.execute_read(
            """
            MATCH (s:Self)-[:GOVERNED_BY]->(c:Constitution)
            RETURN c.drive_coherence AS drive_coherence,
                   c.drive_care AS drive_care,
                   c.drive_growth AS drive_growth,
                   c.drive_honesty AS drive_honesty,
                   c.version AS version
            """
        )
        if results:
            return dict(results[0])
        # Fallback defaults
        return {
            "drive_coherence": 1.0,
            "drive_care": 1.0,
            "drive_growth": 1.0,
            "drive_honesty": 1.0,
            "version": 1,
        }

    async def _check_community_invariants(self, intent: Intent) -> list[str]:
        """Check all community-defined invariants via LLM evaluation."""
        results = await self._neo4j.execute_read(
            """
            MATCH (c:Constitution)-[:INCLUDES_INVARIANT]->(i:Invariant)
            WHERE i.source = 'community' AND i.active = true
            RETURN i.name AS name, i.description AS description
            """
        )

        violations = []
        for row in results:
            satisfied = await check_community_invariant(
                self._llm, intent, row["name"], row["description"],
            )
            if not satisfied:
                violations.append(row["name"])

        return violations

    async def _feed_veto_to_evo(
        self, intent: Intent, check: ConstitutionalCheck,
    ) -> None:
        """
        Feed a constitutional veto to Evo as a learning episode.

        The organism should learn from its constitutional failures — when Equor
        blocks an intent, the violation becomes a negative-affect episode so Evo
        can refine hypothesis about which intent patterns violate the constitution.
        """
        try:
            from ecodiaos.primitives.memory_trace import Episode

            episode = Episode(
                source="equor.veto",
                modality="internal",
                raw_content=(
                    f"Constitutional veto: {intent.goal.description[:200]}. "
                    f"Reason: {check.reasoning[:300]}"
                ),
                summary=f"Blocked intent: {check.reasoning[:100]}",
                salience_composite=0.7,  # Vetoes are important learning events
                affect_valence=-0.3,
                affect_arousal=0.4,
            )
            await self._evo.process_episode(episode)
            logger.info("veto_fed_to_evo", intent_id=intent.id)
        except Exception:
            logger.debug("evo_veto_feed_failed", exc_info=True)

    async def _store_review_record(
        self,
        intent: Intent,
        alignment: DriveAlignmentVector,
        check: ConstitutionalCheck,
        latency_ms: int,
    ) -> None:
        """Store a constitutional review in the immutable audit trail."""
        now = utc_now()
        record_id = new_id()

        await self._neo4j.execute_write(
            """
            CREATE (g:GovernanceRecord {
                id: $id,
                event_type: 'constitutional_review',
                timestamp: datetime($now),
                intent_id: $intent_id,
                alignment_coherence: $coherence,
                alignment_care: $care,
                alignment_growth: $growth,
                alignment_honesty: $honesty,
                alignment_composite: $composite,
                verdict: $verdict,
                reasoning: $reasoning,
                confidence: $confidence,
                latency_ms: $latency_ms,
                actor: 'equor',
                outcome: $verdict
            })
            """,
            {
                "id": record_id,
                "now": now.isoformat(),
                "intent_id": intent.id,
                "coherence": alignment.coherence,
                "care": alignment.care,
                "growth": alignment.growth,
                "honesty": alignment.honesty,
                "composite": alignment.composite,
                "verdict": check.verdict.value,
                "reasoning": check.reasoning,
                "confidence": check.confidence,
                "latency_ms": latency_ms,
            },
        )

    async def _run_drift_check(self) -> None:
        """Run a drift check and respond accordingly."""
        report = self._drift_tracker.compute_report()
        response = respond_to_drift(report)

        if response["action"] != "log":
            await store_drift_report(self._neo4j, report, response)

        # Auto-demote on severe drift
        if response["action"] == "demote_autonomy":
            current = await get_autonomy_level(self._neo4j)
            if current > 1:
                await apply_autonomy_change(
                    self._neo4j,
                    current - 1,
                    reason=response["detail"],
                    actor="equor_drift_detection",
                )

    async def _run_promotion_check(self) -> None:
        """
        Periodically check whether the instance is eligible for autonomy promotion.

        Promotion requires governance approval, so this method only records
        eligibility as a governance record and logs it — it does NOT auto-promote.
        Governance (human or community vote) must call apply_autonomy_change().
        """
        try:
            current = await get_autonomy_level(self._neo4j)
            if current >= 3:
                return  # Already at maximum (Steward)

            target = current + 1
            eligibility = await check_promotion_eligibility(
                self._neo4j, current, target,
            )

            if not eligibility["eligible"]:
                return

            # Record the eligibility event so governance can act on it
            now = utc_now()
            record_id = new_id()
            await self._neo4j.execute_write(
                """
                CREATE (g:GovernanceRecord {
                    id: $id,
                    event_type: 'promotion_eligible',
                    timestamp: datetime($now),
                    details: $details,
                    actor: 'equor_promotion_check',
                    outcome: 'eligible'
                })
                """,
                {
                    "id": record_id,
                    "now": now.isoformat(),
                    "details": f"Eligible for promotion from level {current} to {target}",
                },
            )

            logger.info(
                "promotion_eligibility_detected",
                current_level=current,
                target_level=target,
                checks=eligibility["checks"],
            )
        except Exception:
            logger.debug("promotion_check_failed", exc_info=True)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\equor\verdict.py =====

"""
EcodiaOS — Equor Verdict Engine

The 8-stage verdict pipeline that transforms drive alignment scores
into a constitutional verdict (PERMIT / MODIFY / ESCALATE / DENY).

Care and Honesty are floor drives — they cannot be traded off.
Coherence and Growth are ceiling drives — they can be temporarily deprioritised.
Denial is final. No system can override a DENY.
"""

from __future__ import annotations

import structlog

from ecodiaos.primitives.common import DriveAlignmentVector, Verdict
from ecodiaos.primitives.constitutional import ConstitutionalCheck, InvariantResult
from ecodiaos.primitives.intent import Intent
from ecodiaos.systems.equor.invariants import InvariantViolation, check_hardcoded_invariants

logger = structlog.get_logger()

# Action categories and their required autonomy levels
ACTION_AUTONOMY_MAP: dict[str, int | str] = {
    # Level 1 (Advisor) — always permitted
    "observe": 1,
    "analyse": 1,
    "suggest": 1,
    "answer_question": 1,
    "store_memory": 1,
    "self_reflect": 1,
    # Level 2 (Partner)
    "send_notification": 2,
    "adjust_resource_allocation": 2,
    "mediate_minor_conflict": 2,
    "schedule_event": 2,
    "share_knowledge_federation": 2,
    "modify_own_non_critical_config": 2,
    # Level 3 (Steward)
    "make_resource_decision": 3,
    "mediate_major_conflict": 3,
    "initiate_federation_coordination": 3,
    "propose_policy_change": 3,
    "override_automated_system": 3,
    # Governance required
    "amend_constitution": "governance",
    "change_autonomy_level": "governance",
    "end_instance_life": "governance",
    "share_private_data": "governance",
}


def _assess_required_autonomy(intent: Intent) -> int | str:
    """
    Determine the minimum autonomy level required for an intent.
    Examines the intent's plan steps and goal to classify.
    """
    max_level: int = 1

    # Check each step's executor against the autonomy map
    for step in intent.plan.steps:
        executor_base = step.executor.split(".")[0].lower() if step.executor else ""
        for action_key, level in ACTION_AUTONOMY_MAP.items():
            if action_key in executor_base:
                if level == "governance":
                    return "governance"
                if isinstance(level, int) and level > max_level:
                    max_level = level

    # Also check the goal description for governance-level actions
    goal_lower = intent.goal.description.lower()
    for action_key, level in ACTION_AUTONOMY_MAP.items():
        if level == "governance" and action_key.replace("_", " ") in goal_lower:
            return "governance"

    return max_level


def _assess_risk(intent: Intent) -> dict:
    """
    Quick risk assessment of an intent.
    Returns harm_potential and reversibility scores.
    """
    goal_lower = intent.goal.description.lower()

    # Harm potential heuristics
    harm_potential = 0.1  # Baseline low
    high_harm_keywords = [
        "delete", "remove", "override", "terminate", "force",
        "broadcast to all", "share widely", "permanent",
    ]
    for keyword in high_harm_keywords:
        if keyword in goal_lower:
            harm_potential = max(harm_potential, 0.6)
            break

    medium_harm_keywords = [
        "notify", "change", "modify", "update", "send",
    ]
    for keyword in medium_harm_keywords:
        if keyword in goal_lower:
            harm_potential = max(harm_potential, 0.3)
            break

    # Reversibility heuristics
    reversibility = 0.8  # Most things are reversible
    irreversible_keywords = [
        "permanent", "delete", "destroy", "irreversible",
        "cannot undo", "broadcast", "federation share",
    ]
    for keyword in irreversible_keywords:
        if keyword in goal_lower:
            reversibility = 0.2
            break

    return {
        "harm_potential": harm_potential,
        "reversibility": reversibility,
    }


def compute_verdict(
    alignment: DriveAlignmentVector,
    intent: Intent,
    autonomy_level: int,
    constitution: dict,
) -> ConstitutionalCheck:
    """
    The 8-stage verdict pipeline.

    constitution is a dict with drive weights:
      {drive_coherence, drive_care, drive_growth, drive_honesty}
    """
    check = ConstitutionalCheck(intent_id=intent.id, drive_alignment=alignment)

    # ── STAGE 1: Hard-Stop Invariant Check ──────────────────────
    violations = check_hardcoded_invariants(intent)
    if violations:
        critical = [v for v in violations if v.severity == "critical"]
        if critical:
            check.verdict = Verdict.BLOCKED
            check.invariant_results = [
                InvariantResult(
                    invariant_id=v.invariant_id,
                    name=v.invariant_name,
                    passed=False,
                    severity=v.severity,
                    explanation=v.explanation,
                )
                for v in violations
            ]
            check.reasoning = f"Blocked by invariant: {critical[0].invariant_name}"
            check.confidence = 1.0
            return check

        # Non-critical violations get recorded but don't auto-block
        check.invariant_results = [
            InvariantResult(
                invariant_id=v.invariant_id,
                name=v.invariant_name,
                passed=False,
                severity=v.severity,
                explanation=v.explanation,
            )
            for v in violations
        ]

    # ── STAGE 2: Floor Drive Check ──────────────────────────────
    care_weight = constitution.get("drive_care", 1.0)
    honesty_weight = constitution.get("drive_honesty", 1.0)
    care_floor = -0.3 * care_weight
    honesty_floor = -0.3 * honesty_weight

    if alignment.care < care_floor:
        check.verdict = Verdict.BLOCKED
        check.reasoning = (
            f"Care alignment ({alignment.care:.2f}) below floor ({care_floor:.2f}). "
            f"This action would cause unacceptable harm."
        )
        check.confidence = 0.95
        return check

    if alignment.honesty < honesty_floor:
        check.verdict = Verdict.BLOCKED
        check.reasoning = (
            f"Honesty alignment ({alignment.honesty:.2f}) below floor ({honesty_floor:.2f}). "
            f"This action involves unacceptable deception."
        )
        check.confidence = 0.95
        return check

    # ── STAGE 3: Autonomy Gate ──────────────────────────────────
    required_autonomy = _assess_required_autonomy(intent)

    if required_autonomy == "governance":
        check.verdict = Verdict.DEFERRED
        check.reasoning = "Action requires community governance approval."
        check.confidence = 1.0
        return check

    if isinstance(required_autonomy, int) and required_autonomy > autonomy_level:
        check.verdict = Verdict.DEFERRED
        check.reasoning = (
            f"Action requires autonomy level {required_autonomy} "
            f"but instance is at level {autonomy_level}."
        )
        check.confidence = 1.0
        return check

    # ── STAGE 4: Composite Alignment Assessment ─────────────────
    coherence_weight = constitution.get("drive_coherence", 1.0)
    growth_weight = constitution.get("drive_growth", 1.0)

    weights = {
        "coherence": coherence_weight * 0.8,
        "care": care_weight * 1.5,       # Care weighted highest
        "growth": growth_weight * 0.7,
        "honesty": honesty_weight * 1.3,  # Honesty second highest
    }
    total_weight = sum(weights.values())

    composite = (
        weights["coherence"] * alignment.coherence
        + weights["care"] * alignment.care
        + weights["growth"] * alignment.growth
        + weights["honesty"] * alignment.honesty
    ) / total_weight

    # ── STAGE 5: Risk-Adjusted Decision ─────────────────────────
    risk = _assess_risk(intent)

    if risk["harm_potential"] > 0.7 and composite < 0.3:
        check.verdict = Verdict.DEFERRED
        check.reasoning = (
            f"High-risk action (harm={risk['harm_potential']:.2f}) "
            f"with moderate alignment ({composite:.2f}). Needs governance review."
        )
        check.confidence = 0.8
        return check

    if risk["reversibility"] < 0.3 and composite < 0.2:
        check.verdict = Verdict.DEFERRED
        check.reasoning = "Irreversible action with low alignment. Needs governance review."
        check.confidence = 0.8
        return check

    # ── STAGE 6: Modification Opportunities ─────────────────────
    if -0.1 < composite < 0.15:
        mods = _suggest_modifications(alignment)
        if mods:
            check.verdict = Verdict.MODIFIED
            check.modifications = mods
            check.reasoning = (
                f"Action alignment is marginal ({composite:.2f}). "
                f"Suggested modifications would improve alignment."
            )
            check.confidence = 0.7
            return check

    # ── STAGE 7: Permit ─────────────────────────────────────────
    if composite >= 0.0:
        check.verdict = Verdict.APPROVED
        check.reasoning = (
            f"Action aligns with constitutional drives "
            f"(composite={composite:.2f}, C={alignment.coherence:.2f}, "
            f"Ca={alignment.care:.2f}, G={alignment.growth:.2f}, H={alignment.honesty:.2f})."
        )
        check.confidence = min(0.95, 0.5 + composite)
        return check

    # ── STAGE 8: Marginal Deny ──────────────────────────────────
    check.verdict = Verdict.BLOCKED
    check.reasoning = (
        f"Action does not sufficiently align with constitutional drives "
        f"(composite={composite:.2f}). No viable modifications found."
    )
    check.confidence = 0.85
    return check


def _suggest_modifications(alignment: DriveAlignmentVector) -> list[str]:
    """Suggest modifications to improve a marginally-aligned intent."""
    suggestions: list[str] = []

    if alignment.care < 0:
        suggestions.append(
            "Consider the impact on community wellbeing. "
            "Add safeguards or notifications for affected individuals."
        )
    if alignment.honesty < 0:
        suggestions.append(
            "Ensure transparency in this action. "
            "Add explanation or disclosure of reasoning."
        )
    if alignment.coherence < 0:
        suggestions.append(
            "Strengthen the reasoning. Consider whether this contradicts "
            "existing knowledge or commitments."
        )
    if alignment.growth < -0.1:
        suggestions.append(
            "Consider whether this action contributes to learning or development."
        )

    return suggestions

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\evo\__init__.py =====

"""
EcodiaOS — Evo (Learning & Hypothesis System)

Evo is the Growth drive made computational. It observes patterns across
experience, forms hypotheses, accumulates evidence, and when evidence is
sufficient, adjusts the organism's parameters, codifies procedures, and
proposes structural evolution.

Evo operates in two modes:
  WAKE — lightweight online pattern detection during each cognitive cycle
  SLEEP — deep offline consolidation: schema induction, procedure extraction,
           parameter optimisation, self-model update, drift monitoring

Guard rails:
  - Cannot touch Equor evaluation logic or constitutional drives
  - All parameter changes are small (velocity-limited)
  - Hypotheses must be falsifiable
  - Evolution proposals go to Simula for gating, not applied directly

Public interface:
  EvoService          — main service class
  ConsolidationResult — result of a consolidation cycle
  SelfModelStats      — self-assessment metrics
  ParameterTuner      — tunable parameter management (for direct access)
"""

from ecodiaos.systems.evo.service import EvoService
from ecodiaos.systems.evo.types import (
    ConsolidationResult,
    Hypothesis,
    HypothesisCategory,
    HypothesisStatus,
    ParameterAdjustment,
    PatternCandidate,
    PatternType,
    Procedure,
    SelfModelStats,
    TUNABLE_PARAMETERS,
    VELOCITY_LIMITS,
)

__all__ = [
    "EvoService",
    "ConsolidationResult",
    "Hypothesis",
    "HypothesisCategory",
    "HypothesisStatus",
    "ParameterAdjustment",
    "PatternCandidate",
    "PatternType",
    "Procedure",
    "SelfModelStats",
    "TUNABLE_PARAMETERS",
    "VELOCITY_LIMITS",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\evo\consolidation.py =====

"""
EcodiaOS — Evo Consolidation Orchestrator

The "sleep mode" of the learning system. Runs every 6 hours or 10,000
cognitive cycles — whichever comes first.

Eight phases (spec Section VII):
  1. Memory consolidation   — delegate to MemoryService
  2. Hypothesis review      — integrate supported, archive refuted/stale
  3. Schema induction       — propose new entity/relation types from clusters
  4. Procedure extraction   — codify mature action sequences as Procedures
  5. Parameter optimisation — apply supported parameter hypotheses
  6. Self-model update      — recompute capability and effectiveness metrics
  7. Drift data feed        — send effectiveness data to Equor's drift detector
  8. Evolution proposals    — flag structural changes that warrant Simula review

Performance budget: ≤60 seconds end-to-end (spec Section X).

Guard rails:
  - Velocity limits enforced by ParameterTuner
  - Evo cannot touch Equor evaluation logic (EVO_CONSTRAINTS)
  - Evolution proposals are submitted to Simula, not applied directly
"""

from __future__ import annotations

import time
from datetime import timedelta
from collections.abc import Callable
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.evo.hypothesis import HypothesisEngine
from ecodiaos.systems.evo.parameter_tuner import ParameterTuner
from ecodiaos.systems.evo.procedure_extractor import ProcedureExtractor
from ecodiaos.systems.evo.self_model import SelfModelManager
from ecodiaos.systems.evo.types import (
    ConsolidationResult,
    EvolutionProposal,
    Hypothesis,
    HypothesisCategory,
    HypothesisStatus,
    MutationType,
    PatternContext,
    SchemaInduction,
    VELOCITY_LIMITS,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()

_CONSOLIDATION_INTERVAL_HOURS: int = 6
_CONSOLIDATION_CYCLE_THRESHOLD: int = 10_000


class ConsolidationOrchestrator:
    """
    Drives the full consolidation pipeline — the organism dreaming.

    Receives the active hypothesis list and pattern context from EvoService.
    Coordinates all sub-systems through the 8-phase pipeline.
    """

    def __init__(
        self,
        hypothesis_engine: HypothesisEngine,
        parameter_tuner: ParameterTuner,
        procedure_extractor: ProcedureExtractor,
        self_model_manager: SelfModelManager,
        memory: MemoryService | None = None,
        simula_callback: Callable[..., Any] | None = None,
    ) -> None:
        self._hypotheses = hypothesis_engine
        self._tuner = parameter_tuner
        self._extractor = procedure_extractor
        self._self_model = self_model_manager
        self._memory = memory
        self._simula_callback = simula_callback
        self._logger = logger.bind(system="evo.consolidation")

        self._last_run_at = utc_now() - timedelta(hours=_CONSOLIDATION_INTERVAL_HOURS)
        self._total_runs: int = 0

    def should_run(self, cycle_count: int, cycles_since_last: int) -> bool:
        """
        Return True if consolidation is due.
        Triggers on:
          - 6 hours elapsed since last run
          - 10,000 cognitive cycles since last run
        """
        hours_elapsed = (utc_now() - self._last_run_at).total_seconds() / 3600
        if hours_elapsed >= _CONSOLIDATION_INTERVAL_HOURS:
            return True
        if cycles_since_last >= _CONSOLIDATION_CYCLE_THRESHOLD:
            return True
        return False

    async def run(self, pattern_context: PatternContext) -> ConsolidationResult:
        """
        Execute the full 8-phase consolidation pipeline.
        Returns a ConsolidationResult summary.

        Never raises — all phases handle their own exceptions.
        """
        self._logger.info("consolidation_starting")
        start = time.monotonic()
        result = ConsolidationResult(triggered_at=utc_now())

        # ── Phase 1: Memory Consolidation ────────────────────────────────────
        await self._phase_memory_consolidation()

        # ── Snapshot supported hypotheses BEFORE Phase 2 removes them ─────────
        # Phase 2 integrates/archives supported hypotheses which removes them
        # from the active list. Phases 3 and 5 need these hypotheses, so we
        # snapshot them first.
        self._supported_snapshot = list(self._hypotheses.get_supported())

        # ── Phase 2: Hypothesis Review ────────────────────────────────────────
        integrated, archived = await self._phase_hypothesis_review()
        result.hypotheses_evaluated = integrated + archived
        result.hypotheses_integrated = integrated
        result.hypotheses_archived = archived

        # ── Phase 3: Schema Induction ─────────────────────────────────────────
        schemas_induced = await self._phase_schema_induction()
        result.schemas_induced = schemas_induced

        # ── Phase 4: Procedure Extraction ─────────────────────────────────────
        self._extractor.begin_cycle()
        procedures_extracted = await self._phase_procedure_extraction(pattern_context)
        result.procedures_extracted = procedures_extracted

        # ── Phase 5: Parameter Optimisation ───────────────────────────────────
        self._tuner.begin_cycle()
        adj_count, total_delta = await self._phase_parameter_optimisation()
        result.parameters_adjusted = adj_count
        result.total_parameter_delta = total_delta

        # ── Phase 6: Self-Model Update ────────────────────────────────────────
        await self._phase_self_model_update()
        result.self_model_updated = True

        # ── Phase 7: Drift Data Feed ──────────────────────────────────────────
        await self._phase_drift_feed()

        # ── Phase 8: Evolution Proposals ──────────────────────────────────────
        await self._phase_evolution_proposals()

        # ── Housekeeping ──────────────────────────────────────────────────────
        pattern_context.reset()
        self._last_run_at = utc_now()
        self._total_runs += 1

        result.duration_ms = int((time.monotonic() - start) * 1000)
        self._logger.info(
            "consolidation_complete",
            duration_ms=result.duration_ms,
            hypotheses_integrated=result.hypotheses_integrated,
            hypotheses_archived=result.hypotheses_archived,
            procedures_extracted=result.procedures_extracted,
            parameters_adjusted=result.parameters_adjusted,
            total_parameter_delta=round(result.total_parameter_delta, 4),
        )
        return result

    # ─── Phases ───────────────────────────────────────────────────────────────

    async def _phase_memory_consolidation(self) -> None:
        """Phase 1: Delegate memory consolidation to MemoryService."""
        if self._memory is None:
            return
        try:
            await self._memory.consolidate()
            self._logger.info("memory_consolidation_complete")
        except Exception as exc:
            self._logger.error("memory_consolidation_failed", error=str(exc))

    async def _phase_hypothesis_review(self) -> tuple[int, int]:
        """
        Phase 2: Review all active hypotheses.
          - SUPPORTED → attempt integration (calls HypothesisEngine.integrate_hypothesis)
          - REFUTED   → archive
          - Stale     → archive
        Returns (integrated_count, archived_count).
        """
        integrated = 0
        archived = 0

        all_hypotheses = self._hypotheses.get_all_active()

        for h in all_hypotheses:
            try:
                if h.status == HypothesisStatus.SUPPORTED:
                    success = await self._hypotheses.integrate_hypothesis(h)
                    if success:
                        integrated += 1
                elif h.status == HypothesisStatus.REFUTED:
                    await self._hypotheses.archive_hypothesis(h, reason="refuted")
                    archived += 1
                elif self._hypotheses.is_stale(h):
                    await self._hypotheses.archive_hypothesis(h, reason="stale")
                    archived += 1
            except Exception as exc:
                self._logger.warning(
                    "hypothesis_review_failed",
                    hypothesis_id=h.id,
                    error=str(exc),
                )

        return integrated, archived

    async def _phase_schema_induction(self) -> int:
        """
        Phase 3: Induce new schema elements from supported world-model hypotheses.
        Uses the pre-Phase-2 snapshot so hypotheses are available after integration.
        Returns the count of schemas induced.
        """
        schemas_induced = 0
        supported = getattr(self, "_supported_snapshot", [])

        for h in supported:
            if h.category != HypothesisCategory.WORLD_MODEL:
                continue
            if h.proposed_mutation is None:
                continue
            if h.proposed_mutation.type != MutationType.SCHEMA_ADDITION:
                continue

            schema = SchemaInduction(
                entities=[{"name": h.proposed_mutation.target, "description": h.statement}],
                source_hypothesis=h.id,
            )
            success = await self._apply_schema_induction(schema)
            if success:
                schemas_induced += 1

        return schemas_induced

    async def _phase_procedure_extraction(self, context: PatternContext) -> int:
        """
        Phase 4: Extract procedures from mature action-sequence patterns.
        Returns the count of new procedures extracted.
        """
        mature_patterns = context.get_mature_sequences(
            min_occurrences=VELOCITY_LIMITS["max_new_procedures_per_cycle"]
        )
        # Get_mature_sequences returns patterns >= min, use ≥3 (spec threshold)
        all_patterns = context.get_mature_sequences(min_occurrences=3)

        extracted = 0
        for pattern in all_patterns:
            procedure = await self._extractor.extract_procedure(pattern)
            if procedure is not None:
                extracted += 1

        return extracted

    async def _phase_parameter_optimisation(self) -> tuple[int, float]:
        """
        Phase 5: Apply supported parameter hypotheses.
        Uses the pre-Phase-2 snapshot so hypotheses are available after integration.
        Velocity-limited to prevent lurching changes.
        Returns (adjustment_count, total_absolute_delta).
        """
        supported = getattr(self, "_supported_snapshot", [])
        candidates: list[Any] = []

        for h in supported:
            if h.category != HypothesisCategory.PARAMETER:
                continue
            adj = self._tuner.propose_adjustment(h)
            if adj is not None:
                candidates.append(adj)

        if not candidates:
            return 0, 0.0

        # Check velocity limit for the batch
        allowed, reason = self._tuner.check_velocity_limit(candidates)
        if not allowed:
            self._logger.warning("parameter_velocity_limit", reason=reason)
            # Apply as many as we can without exceeding total limit
            limit = VELOCITY_LIMITS["max_total_parameter_delta_per_cycle"]
            running_delta = 0.0
            filtered = []
            for adj in sorted(candidates, key=lambda a: abs(a.delta), reverse=False):
                if running_delta + abs(adj.delta) <= limit:
                    filtered.append(adj)
                    running_delta += abs(adj.delta)
            candidates = filtered

        applied = 0
        total_delta = 0.0
        for adj in candidates:
            await self._tuner.apply_adjustment(adj)
            applied += 1
            total_delta += abs(adj.delta)

        return applied, total_delta

    async def _phase_self_model_update(self) -> None:
        """Phase 6: Recompute self-model from recent outcome episodes."""
        try:
            await self._self_model.update()
        except Exception as exc:
            self._logger.error("self_model_update_failed", error=str(exc))

    async def _phase_drift_feed(self) -> None:
        """
        Phase 7: Feed effectiveness data to Equor's drift detector.
        Drift detection is handled by Equor; we just update the data it reads.
        The self-model stats written to the Self node are what Equor reads.
        """
        stats = self._self_model.get_current()
        self._logger.debug(
            "drift_data_available",
            success_rate=round(stats.success_rate, 3),
            mean_alignment=round(stats.mean_alignment, 3),
        )
        # Equor reads from Self node directly; no active push needed in Phase 7.
        # Future: could publish a Synapse event for Equor to act on immediately.

    async def _phase_evolution_proposals(self) -> None:
        """
        Phase 8: Submit structural change proposals to Simula for warranted cases.
        Only generates proposals when we have clustered, high-evidence hypotheses
        that point to architectural change. Simula gates the actual change.
        """
        supported = self._hypotheses.get_supported()
        evolution_candidates = [
            h for h in supported
            if (
                h.proposed_mutation is not None
                and h.proposed_mutation.type == MutationType.EVOLUTION_PROPOSAL
                and h.evidence_score > 5.0
            )
        ]
        for h in evolution_candidates:
            if h.proposed_mutation is None:
                continue
            proposal = EvolutionProposal(
                description=h.proposed_mutation.description or h.statement,
                rationale=h.statement,
                supporting_hypotheses=[h.id],
            )
            self._logger.info(
                "evolution_proposal_generated",
                hypothesis_id=h.id,
                description=proposal.description[:80],
            )

            # Submit to Simula via bridge callback
            if self._simula_callback is not None:
                try:
                    result = await self._simula_callback(proposal, [h])
                    self._logger.info(
                        "evolution_proposal_submitted_to_simula",
                        hypothesis_id=h.id,
                        result_status=getattr(result, "status", "unknown"),
                    )
                except Exception as exc:
                    self._logger.error(
                        "simula_submission_failed",
                        hypothesis_id=h.id,
                        error=str(exc),
                    )

    # ─── Helpers ──────────────────────────────────────────────────────────────

    async def _apply_schema_induction(self, schema: SchemaInduction) -> bool:
        """Apply schema induction to the Memory graph."""
        if self._memory is None or not schema.entities:
            return False
        try:
            for entity_spec in schema.entities:
                name = entity_spec.get("name", "")
                description = entity_spec.get("description", "")
                if name:
                    await self._memory._neo4j.execute_write(
                        """
                        MERGE (et:EvoEntityType {name: $name})
                        SET et.description = $description,
                            et.source_hypothesis = $source_hypothesis,
                            et.induced_at = datetime()
                        """,
                        {
                            "name": name,
                            "description": description,
                            "source_hypothesis": schema.source_hypothesis,
                        },
                    )
            return True
        except Exception as exc:
            self._logger.warning("schema_induction_failed", error=str(exc))
            return False

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "total_runs": self._total_runs,
            "last_run_at": self._last_run_at.isoformat(),
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\evo\detectors.py =====

"""
EcodiaOS — Evo Pattern Detectors

Online pattern detectors that scan the continuous stream of episodes
during wake mode. Each detector looks for one class of regularity.

Detectors are stateless — all accumulation lives in PatternContext.
They run in ≤20ms per episode (latency budget from spec Section X).

Four detectors (spec Section III):
  CooccurrenceDetector  — entities that appear together repeatedly
  SequenceDetector      — action sequences that produce successful outcomes
  TemporalDetector      — time-based patterns (hour-of-day, day-of-week)
  AffectPatternDetector — stimuli that reliably shift emotional state
"""

from __future__ import annotations

import json
from abc import ABC, abstractmethod
from itertools import combinations

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.memory_trace import Episode
from ecodiaos.systems.evo.types import (
    PatternCandidate,
    PatternContext,
    PatternType,
    hash_sequence,
)

logger = structlog.get_logger()


# ─── Abstract Base ────────────────────────────────────────────────────────────


class PatternDetector(ABC):
    """
    Base class for all online pattern detectors.
    Detectors are stateless; PatternContext carries accumulated state.
    """

    name: str = ""
    window_size: int = 500      # Max episodes to keep in sliding window
    min_occurrences: int = 5    # Threshold to emit a candidate

    @abstractmethod
    async def scan(
        self,
        episode: Episode,
        context: PatternContext,
    ) -> list[PatternCandidate]:
        """
        Scan one episode and update context.
        Returns newly-triggered candidates (those crossing min_occurrences now).
        Must complete in ≤20ms.
        """
        ...


# ─── Co-occurrence Detector ───────────────────────────────────────────────────


class CooccurrenceDetector(PatternDetector):
    """
    Detects entities that frequently appear together.

    Reads entity IDs from context.recent_entity_ids, which the EvoService
    populates from each WorkspaceBroadcast's memory context (MemoryTrace.entities).

    On each episode, updates the co-occurrence matrix. Emits a PatternCandidate
    the first time a pair crosses min_occurrences.
    """

    name = "cooccurrence"
    window_size = 500
    min_occurrences = 5

    async def scan(
        self,
        episode: Episode,
        context: PatternContext,
    ) -> list[PatternCandidate]:
        entity_ids = context.recent_entity_ids
        if len(entity_ids) < 2:
            return []

        candidates: list[PatternCandidate] = []
        for pair in combinations(sorted(entity_ids), 2):
            key = f"{pair[0]}::{pair[1]}"
            context.cooccurrence_counts[key] += 1
            count = context.cooccurrence_counts[key]
            # Emit exactly at threshold (not repeatedly)
            if count == self.min_occurrences:
                candidates.append(
                    PatternCandidate(
                        type=PatternType.COOCCURRENCE,
                        elements=list(pair),
                        count=count,
                        confidence=0.5,
                        examples=[episode.id],
                    )
                )

        return candidates


# ─── Sequence Detector ────────────────────────────────────────────────────────


class SequenceDetector(PatternDetector):
    """
    Detects recurring action sequences that lead to successful outcomes.

    Reads action sequences from episode metadata (stored by Axon audit).
    Only processes action_outcome episodes that recorded success=True.
    Emits a candidate when a sequence hash crosses min_occurrences.
    """

    name = "action_sequence"
    window_size = 1000
    min_occurrences = 3

    async def scan(
        self,
        episode: Episode,
        context: PatternContext,
    ) -> list[PatternCandidate]:
        if not _is_successful_action_outcome(episode):
            return []

        sequence = _extract_action_sequence(episode)
        if not sequence:
            return []

        seq_hash = hash_sequence(sequence)
        context.sequence_counts[seq_hash] += 1
        context.sequence_examples[seq_hash].append(episode.id)

        count = context.sequence_counts[seq_hash]
        candidates: list[PatternCandidate] = []
        if count >= self.min_occurrences:
            candidates.append(
                PatternCandidate(
                    type=PatternType.ACTION_SEQUENCE,
                    elements=sequence,
                    count=count,
                    confidence=min(0.9, 0.5 + count * 0.05),
                    examples=context.sequence_examples[seq_hash][:10],
                    metadata={"sequence_hash": seq_hash},
                )
            )

        return candidates


# ─── Temporal Detector ────────────────────────────────────────────────────────


class TemporalDetector(PatternDetector):
    """
    Detects time-based patterns: hour-of-day and day-of-week concentrations.

    Uses the episode's source channel as the "event type" and bins by time.
    Emits a candidate when a bin exceeds 2x the source-type baseline.
    """

    name = "temporal"
    window_size = 2000
    min_occurrences = 3

    async def scan(
        self,
        episode: Episode,
        context: PatternContext,
    ) -> list[PatternCandidate]:
        hour = episode.event_time.hour
        day = episode.event_time.weekday()

        # Use source channel as the event type for binning
        source_type = _classify_source(episode.source)

        hour_key = f"{source_type}::h{hour}"
        day_key = f"{source_type}::d{day}"

        context.temporal_bins[hour_key] += 1
        context.temporal_bins[day_key] += 1

        # Update baseline (exponential moving average)
        baseline = context.temporal_baselines.get(source_type, 1.0)
        context.temporal_baselines[source_type] = baseline * 0.99 + 1.0 * 0.01

        candidates: list[PatternCandidate] = []
        for key, count in [
            (hour_key, context.temporal_bins[hour_key]),
            (day_key, context.temporal_bins[day_key]),
        ]:
            if count >= self.min_occurrences:
                adjusted_baseline = max(1.0, context.temporal_baselines.get(source_type, 1.0))
                if count / adjusted_baseline > 2.0:
                    bin_label = key.split("::")[-1]
                    candidates.append(
                        PatternCandidate(
                            type=PatternType.TEMPORAL,
                            elements=[source_type, bin_label],
                            count=count,
                            confidence=min(0.8, count / (adjusted_baseline * 5)),
                            metadata={"key": key, "baseline": adjusted_baseline},
                        )
                    )

        return candidates


# ─── Affect Pattern Detector ──────────────────────────────────────────────────


class AffectPatternDetector(PatternDetector):
    """
    Detects stimuli that reliably shift emotional state.

    Compares context.previous_affect (from the prior broadcast) with
    context.current_affect (the broadcast that produced this episode).
    Classifies the stimulus type from the episode source and tracks
    the mean affect delta across occurrences.

    Emits when a stimulus_type has caused a consistent shift (≥min_occurrences).
    """

    name = "affect_pattern"
    window_size = 500
    min_occurrences = 5
    min_magnitude: float = 0.1  # Minimum shift to care about

    async def scan(
        self,
        episode: Episode,
        context: PatternContext,
    ) -> list[PatternCandidate]:
        # We need before and after affect states
        affect_before = context.previous_affect
        affect_after = context.current_affect

        if affect_before is None or affect_after is None:
            return []

        delta_valence = affect_after.valence - affect_before.valence
        delta_arousal = affect_after.arousal - affect_before.arousal

        # Only track meaningful shifts
        if abs(delta_valence) < self.min_magnitude and abs(delta_arousal) < self.min_magnitude:
            return []

        stimulus_type = _classify_source(episode.source)
        context.affect_responses[stimulus_type].append((delta_valence, delta_arousal))

        count = len(context.affect_responses[stimulus_type])
        candidates: list[PatternCandidate] = []

        if count >= self.min_occurrences:
            responses = context.affect_responses[stimulus_type]
            mean_v = sum(r[0] for r in responses) / count
            mean_a = sum(r[1] for r in responses) / count
            candidates.append(
                PatternCandidate(
                    type=PatternType.AFFECT_PATTERN,
                    elements=[
                        stimulus_type,
                        f"val:{mean_v:+.2f}",
                        f"aro:{mean_a:+.2f}",
                    ],
                    count=count,
                    confidence=0.5,
                    examples=[episode.id],
                    metadata={
                        "mean_valence_delta": round(mean_v, 3),
                        "mean_arousal_delta": round(mean_a, 3),
                        "stimulus_type": stimulus_type,
                    },
                )
            )

        return candidates


# ─── Default Detector Set ─────────────────────────────────────────────────────


def build_default_detectors() -> list[PatternDetector]:
    """Return the standard set of online pattern detectors."""
    return [
        CooccurrenceDetector(),
        SequenceDetector(),
        TemporalDetector(),
        AffectPatternDetector(),
    ]


# ─── Private Helpers ──────────────────────────────────────────────────────────


def _is_successful_action_outcome(episode: Episode) -> bool:
    """
    Determine whether an episode represents a successful action outcome.
    Axon audit records are stored with source="axon:{action_type}" and
    salience boosted on success. We use source prefix as primary signal.
    """
    source = episode.source or ""
    # Axon stores audit records with "axon:" prefix
    if not ("axon" in source or "action" in source):
        return False
    # Successful outcomes have positive affect or high salience
    return episode.affect_valence >= 0.0 and episode.salience_composite > 0.3


def _extract_action_sequence(episode: Episode) -> list[str]:
    """
    Extract the sequence of action types from an episode.
    Axon audit records encode action type in the source field:
    source = "axon:{action_type}" for single-step intents.
    Multi-step intents store the sequence in salience_scores metadata
    under the key "action_sequence".
    """
    source = episode.source or ""
    if source.startswith("axon:"):
        action_type = source[len("axon:"):]
        if action_type:
            return [action_type]

    # Fallback: check salience_scores for action_sequence encoding
    seq_raw = episode.salience_scores.get("_action_sequence")
    if seq_raw is not None:
        # Encoded as a JSON string in the float map (float as string key length)
        # This is a convention for passing structured data through salience_scores
        try:
            seq_str = episode.salience_scores.get("_seq_len")
            if seq_str is not None:
                return []  # Insufficient data
        except (ValueError, TypeError):
            pass

    return []


def _classify_source(source: str) -> str:
    """
    Classify an episode's source into a semantic category for temporal binning.
    """
    if not source:
        return "general"
    if "text_chat" in source:
        return "social_text"
    if "axon" in source or "action" in source:
        return "action_outcome"
    if "sensor" in source or "iot" in source:
        return "environmental"
    if "memory_bubble" in source:
        return "memory_recall"
    if "federation" in source:
        return "federation"
    if "system_event" in source:
        return "system"
    if "evo_insight" in source:
        return "evo_insight"
    return "general"

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\evo\hypothesis.py =====

"""
EcodiaOS — Evo Hypothesis Engine

Manages the full lifecycle of hypotheses:
  1. Generation  — LLM produces testable claims from detected patterns
  2. Testing     — each new episode is evaluated as evidence
  3. Integration — supported hypotheses have their mutations applied
  4. Archival    — refuted or stale hypotheses are stored and closed

Implements approximate Bayesian model comparison (spec Section IV.2):
  Evidence(H) = Σ log p(observation_i | H) - complexity(H)

Approximated as:
  evidence_score += strength × (1 - complexity_penalty × 0.1)  [for support]
  evidence_score -= strength                                     [for contradiction]

Integration requires (spec Section IX, VELOCITY_LIMITS):
  - evidence_score > 3.0
  - len(supporting_episodes) >= 10
  - hypothesis age >= 24 hours

Performance budget: evidence_evaluate ≤200ms per hypothesis (spec Section X).
"""

from __future__ import annotations

import json
import time
from datetime import timedelta
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.clients.llm import LLMProvider, Message
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.primitives.memory_trace import Episode
from ecodiaos.systems.evo.types import (
    EvidenceDirection,
    EvidenceResult,
    Hypothesis,
    HypothesisCategory,
    HypothesisStatus,
    Mutation,
    MutationType,
    PatternCandidate,
    VELOCITY_LIMITS,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()

# Evidence thresholds (from VELOCITY_LIMITS)
_SUPPORT_SCORE_THRESHOLD: float = 3.0
_SUPPORT_EPISODE_THRESHOLD: int = VELOCITY_LIMITS["min_evidence_for_integration"]
_MIN_AGE_HOURS: int = VELOCITY_LIMITS["min_hypothesis_age_hours"]
_MAX_ACTIVE: int = VELOCITY_LIMITS["max_active_hypotheses"]

# LLM generation limits
_MAX_PER_BATCH: int = 3
_SYSTEM_PROMPT = (
    "You are the learning subsystem of a living digital organism. "
    "Your role is to generate precise, falsifiable hypotheses from observed patterns "
    "and evaluate evidence rigorously. Prefer simple explanations. "
    "Always respond with valid JSON matching the requested schema."
)


class HypothesisEngine:
    """
    Manages hypothesis generation, evidence accumulation, and lifecycle.

    Dependencies:
      llm     — LLM provider for generation and evaluation
      memory  — optional; used to persist hypotheses as :Hypothesis nodes
    """

    def __init__(
        self,
        llm: LLMProvider,
        instance_name: str = "EOS",
        memory: MemoryService | None = None,
    ) -> None:
        self._llm = llm
        self._instance_name = instance_name
        self._memory = memory
        self._logger = logger.bind(system="evo.hypothesis")
        self._optimized = isinstance(llm, OptimizedLLMProvider)

        # In-memory hypothesis registry (also persisted to Memory graph)
        self._active: dict[str, Hypothesis] = {}

        # Metrics
        self._total_proposed: int = 0
        self._total_supported: int = 0
        self._total_refuted: int = 0
        self._total_integrated: int = 0

    # ─── Generation ───────────────────────────────────────────────────────────

    async def generate_hypotheses(
        self,
        patterns: list[PatternCandidate],
        existing_summaries: list[str] | None = None,
    ) -> list[Hypothesis]:
        """
        Generate new hypotheses from a batch of pattern candidates.
        Uses LLM reasoning grounded in the pattern evidence.
        Respects MAX_ACTIVE_HYPOTHESES by skipping if at capacity.

        Returns up to _MAX_PER_BATCH new hypotheses.
        """
        if not patterns:
            return []

        if len(self._active) >= _MAX_ACTIVE:
            self._logger.warning(
                "hypothesis_capacity_reached",
                active=len(self._active),
                max=_MAX_ACTIVE,
            )
            return []

        prompt = _build_generation_prompt(
            instance_name=self._instance_name,
            patterns=patterns,
            existing_hypotheses=existing_summaries or list(self._active_summaries()),
            max_hypotheses=_MAX_PER_BATCH,
        )

        # Budget check: skip hypothesis generation in YELLOW/RED (low priority)
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("evo.hypothesis", estimated_tokens=1200):
                self._logger.info("hypothesis_generation_skipped_budget")
                return []

        try:
            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=_SYSTEM_PROMPT,
                    messages=[Message("user", prompt)],
                    max_tokens=1200,
                    temperature=0.5,
                    output_format="json",
                    cache_system="evo.hypothesis",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=_SYSTEM_PROMPT,
                    messages=[Message("user", prompt)],
                    max_tokens=1200,
                    temperature=0.5,
                    output_format="json",
                )
            raw = _parse_json_safe(response.text)
        except Exception as exc:
            self._logger.error("hypothesis_generation_failed", error=str(exc))
            return []

        hypotheses: list[Hypothesis] = []
        for item in raw.get("hypotheses", [])[:_MAX_PER_BATCH]:
            try:
                h = _build_hypothesis(item)
                self._active[h.id] = h
                self._total_proposed += 1
                hypotheses.append(h)
                self._logger.info(
                    "hypothesis_proposed",
                    hypothesis_id=h.id,
                    category=h.category.value,
                    statement=h.statement[:80],
                )
            except (KeyError, ValueError) as exc:
                self._logger.warning("hypothesis_parse_failed", error=str(exc))
                continue

        # Persist to Memory if available
        if self._memory is not None:
            for h in hypotheses:
                await self._persist_hypothesis(h)

        return hypotheses

    # ─── Evidence Evaluation ──────────────────────────────────────────────────

    async def evaluate_evidence(
        self,
        hypothesis: Hypothesis,
        episode: Episode,
    ) -> EvidenceResult:
        """
        Evaluate whether this episode provides evidence for or against a hypothesis.
        LLM evaluates with temperature=0.3 for consistency.
        Updates hypothesis in-place and returns the result.
        Budget: ≤200ms.
        """
        prompt = _build_evidence_prompt(hypothesis, episode)

        # Budget check: skip evidence evaluation in YELLOW/RED
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("evo.evidence", estimated_tokens=300):
                return EvidenceResult(
                    hypothesis_id=hypothesis.id,
                    episode_id=episode.id,
                    direction=EvidenceDirection.NEUTRAL,
                    strength=0.0,
                    reasoning="Skipped — budget constraints",
                )

        try:
            if self._optimized:
                response = await self._llm.evaluate(  # type: ignore[call-arg]
                    prompt=prompt,
                    max_tokens=300,
                    temperature=0.3,
                    cache_system="evo.evidence",
                    cache_method="evaluate",
                )
            else:
                response = await self._llm.evaluate(
                    prompt=prompt,
                    max_tokens=300,
                    temperature=0.3,
                )
            raw = _parse_json_safe(response.text)
        except Exception as exc:
            self._logger.error(
                "evidence_evaluation_failed",
                hypothesis_id=hypothesis.id,
                error=str(exc),
            )
            return EvidenceResult(
                hypothesis_id=hypothesis.id,
                episode_id=episode.id,
                direction=EvidenceDirection.NEUTRAL,
                strength=0.0,
                reasoning="evaluation failed",
                new_score=hypothesis.evidence_score,
                new_status=hypothesis.status,
            )

        direction_raw = raw.get("direction", "neutral")
        strength = float(raw.get("strength", 0.0))
        strength = max(0.0, min(1.0, strength))
        reasoning = str(raw.get("reasoning", ""))

        try:
            direction = EvidenceDirection(direction_raw)
        except ValueError:
            direction = EvidenceDirection.NEUTRAL

        # Update hypothesis evidence score (Bayesian accumulation with Occam penalty)
        if direction == EvidenceDirection.SUPPORTS:
            hypothesis.supporting_episodes.append(episode.id)
            hypothesis.evidence_score += strength * (
                1.0 - hypothesis.complexity_penalty * 0.1
            )
        elif direction == EvidenceDirection.CONTRADICTS:
            hypothesis.contradicting_episodes.append(episode.id)
            hypothesis.evidence_score -= strength

        hypothesis.last_evidence_at = utc_now()

        # Status transitions
        if hypothesis.status in (HypothesisStatus.PROPOSED, HypothesisStatus.TESTING):
            hypothesis.status = HypothesisStatus.TESTING
            if (
                hypothesis.evidence_score > _SUPPORT_SCORE_THRESHOLD
                and len(hypothesis.supporting_episodes) >= _SUPPORT_EPISODE_THRESHOLD
            ):
                hypothesis.status = HypothesisStatus.SUPPORTED
                self._total_supported += 1
                self._logger.info(
                    "hypothesis_supported",
                    hypothesis_id=hypothesis.id,
                    evidence_score=round(hypothesis.evidence_score, 2),
                    supporting_count=len(hypothesis.supporting_episodes),
                )
            elif hypothesis.evidence_score < -2.0:
                hypothesis.status = HypothesisStatus.REFUTED
                self._total_refuted += 1
                self._logger.info(
                    "hypothesis_refuted",
                    hypothesis_id=hypothesis.id,
                    evidence_score=round(hypothesis.evidence_score, 2),
                )

        return EvidenceResult(
            hypothesis_id=hypothesis.id,
            episode_id=episode.id,
            direction=direction,
            strength=strength,
            reasoning=reasoning,
            new_score=hypothesis.evidence_score,
            new_status=hypothesis.status,
        )

    # ─── Integration ──────────────────────────────────────────────────────────

    async def integrate_hypothesis(self, hypothesis: Hypothesis) -> bool:
        """
        Mark a supported hypothesis as INTEGRATED.
        The caller (ConsolidationOrchestrator) is responsible for applying
        the proposed_mutation — this method only closes the hypothesis lifecycle.

        Returns True if integration is valid and was applied.
        """
        if hypothesis.status != HypothesisStatus.SUPPORTED:
            return False

        # Age check — must have been active for at least 24 hours
        age_hours = (utc_now() - hypothesis.created_at).total_seconds() / 3600
        if age_hours < _MIN_AGE_HOURS:
            self._logger.info(
                "hypothesis_integration_deferred",
                hypothesis_id=hypothesis.id,
                age_hours=round(age_hours, 1),
                required_hours=_MIN_AGE_HOURS,
            )
            return False

        hypothesis.status = HypothesisStatus.INTEGRATED
        self._total_integrated += 1

        # Remove from active registry
        self._active.pop(hypothesis.id, None)

        self._logger.info(
            "hypothesis_integrated",
            hypothesis_id=hypothesis.id,
            category=hypothesis.category.value,
            evidence_score=round(hypothesis.evidence_score, 2),
            mutation_type=(
                hypothesis.proposed_mutation.type.value
                if hypothesis.proposed_mutation else "none"
            ),
        )

        if self._memory is not None:
            await self._persist_hypothesis(hypothesis)

        return True

    async def archive_hypothesis(
        self,
        hypothesis: Hypothesis,
        reason: str = "",
    ) -> None:
        """Mark a hypothesis as ARCHIVED and remove from active registry."""
        hypothesis.status = HypothesisStatus.ARCHIVED
        self._active.pop(hypothesis.id, None)

        self._logger.info(
            "hypothesis_archived",
            hypothesis_id=hypothesis.id,
            reason=reason or "not specified",
            evidence_score=round(hypothesis.evidence_score, 2),
        )

        if self._memory is not None:
            await self._persist_hypothesis(hypothesis)

    def is_stale(self, hypothesis: Hypothesis, max_age_days: int = 7) -> bool:
        """
        Return True if this hypothesis has not received evidence in max_age_days.
        Stale hypotheses are archived during consolidation.
        """
        if hypothesis.status not in (
            HypothesisStatus.PROPOSED, HypothesisStatus.TESTING
        ):
            return False
        age = utc_now() - hypothesis.last_evidence_at
        return age > timedelta(days=max_age_days)

    # ─── Query Interface ──────────────────────────────────────────────────────

    def get_active(self) -> list[Hypothesis]:
        """Return all currently active hypotheses (proposed or testing)."""
        return [
            h for h in self._active.values()
            if h.status in (HypothesisStatus.PROPOSED, HypothesisStatus.TESTING)
        ]

    def get_supported(self) -> list[Hypothesis]:
        """Return all supported hypotheses ready for integration."""
        return [
            h for h in self._active.values()
            if h.status == HypothesisStatus.SUPPORTED
        ]

    def get_all_active(self) -> list[Hypothesis]:
        """Return all hypotheses still in the registry (not yet archived/integrated)."""
        return list(self._active.values())

    @property
    def stats(self) -> dict[str, int]:
        return {
            "active": len(self._active),
            "proposed": self._total_proposed,
            "supported": self._total_supported,
            "refuted": self._total_refuted,
            "integrated": self._total_integrated,
        }

    # ─── Private ──────────────────────────────────────────────────────────────

    def _active_summaries(self) -> list[str]:
        """Return short statements of all active hypotheses (deduplication prompt)."""
        return [h.statement[:100] for h in self._active.values()]

    async def _persist_hypothesis(self, hypothesis: Hypothesis) -> None:
        """Store hypothesis as a governance record in Memory."""
        if self._memory is None:
            return
        try:
            await self._memory._neo4j.execute_write(
                """
                MERGE (h:Hypothesis {hypothesis_id: $hypothesis_id})
                SET h.type = $type,
                    h.category = $category,
                    h.statement = $statement,
                    h.status = $status,
                    h.evidence_score = $evidence_score,
                    h.supporting_count = $supporting_count,
                    h.contradicting_count = $contradicting_count,
                    h.created_at = $created_at
                """,
                {
                    "hypothesis_id": hypothesis.id,
                    "type": "hypothesis",
                    "category": hypothesis.category.value,
                    "statement": hypothesis.statement,
                    "status": hypothesis.status.value,
                    "evidence_score": hypothesis.evidence_score,
                    "supporting_count": len(hypothesis.supporting_episodes),
                    "contradicting_count": len(hypothesis.contradicting_episodes),
                    "created_at": hypothesis.created_at.isoformat(),
                },
            )
        except Exception as exc:
            self._logger.warning(
                "hypothesis_persist_failed",
                hypothesis_id=hypothesis.id,
                error=str(exc),
            )


# ─── Prompt Builders ──────────────────────────────────────────────────────────


def _build_generation_prompt(
    instance_name: str,
    patterns: list[PatternCandidate],
    existing_hypotheses: list[str],
    max_hypotheses: int,
) -> str:
    pattern_lines = "\n".join(
        f"- [{p.type.value}] {', '.join(p.elements[:4])} "
        f"(count={p.count}, confidence={p.confidence:.2f})"
        for p in patterns[:10]
    )
    existing_block = (
        "\n".join(f"- {s}" for s in existing_hypotheses[:10])
        if existing_hypotheses
        else "(none)"
    )
    return f"""You are the learning system of {instance_name}, a living digital organism.

DETECTED PATTERNS:
{pattern_lines}

CURRENT ACTIVE HYPOTHESES (avoid duplicates):
{existing_block}

Generate up to {max_hypotheses} hypotheses that explain the patterns above.

Rules:
- Each hypothesis must be FALSIFIABLE — state exactly how it could be proven false
- Prefer SIMPLER explanations (Occam's razor) — penalise unnecessary complexity
- Do NOT duplicate existing hypotheses
- Max {max_hypotheses} hypotheses per batch

For each hypothesis respond in this exact JSON schema:
{{
  "hypotheses": [
    {{
      "category": "world_model|self_model|social|procedural|parameter",
      "statement": "A clear, testable claim (1-2 sentences)",
      "formal_test": "Specific observable condition that would confirm or refute this",
      "complexity": "low|medium|high",
      "proposed_mutation": {{
        "type": "parameter_adjustment|procedure_creation|schema_addition|evolution_proposal",
        "target": "parameter name, procedure name, or entity type",
        "value": 0.0,
        "description": "What specifically to change if confirmed"
      }}
    }}
  ]
}}

If no compelling hypotheses arise from these patterns, return {{"hypotheses": []}}."""


def _build_evidence_prompt(hypothesis: Hypothesis, episode: Episode) -> str:
    return f"""HYPOTHESIS: {hypothesis.statement}
FORMAL TEST: {hypothesis.formal_test}

EVIDENCE (episode):
Content: {episode.raw_content[:300] or episode.summary[:300]}
Source: {episode.source}
Time: {episode.event_time.isoformat()}
Affect: valence={episode.affect_valence:.2f}, arousal={episode.affect_arousal:.2f}
Salience: {episode.salience_composite:.2f}

Does this episode provide evidence FOR or AGAINST the hypothesis?

Respond in JSON:
{{
  "direction": "supports|contradicts|neutral",
  "strength": 0.0,
  "reasoning": "1-2 sentence explanation"
}}

Where strength (0.0–1.0) represents how strongly this evidence bears on the hypothesis."""


# ─── Helpers ──────────────────────────────────────────────────────────────────


def _build_hypothesis(item: dict[str, Any]) -> Hypothesis:
    """Parse one hypothesis item from LLM JSON response."""
    category_raw = str(item.get("category", "world_model"))
    try:
        category = HypothesisCategory(category_raw)
    except ValueError:
        category = HypothesisCategory.WORLD_MODEL

    complexity_raw = str(item.get("complexity", "low"))
    complexity_map = {"low": 0.05, "medium": 0.15, "high": 0.30}
    complexity_penalty = complexity_map.get(complexity_raw, 0.10)

    mutation: Mutation | None = None
    mutation_data = item.get("proposed_mutation")
    if mutation_data and isinstance(mutation_data, dict):
        try:
            mutation = Mutation(
                type=MutationType(mutation_data.get("type", "parameter_adjustment")),
                target=str(mutation_data.get("target", "")),
                value=float(mutation_data.get("value", 0.0)),
                description=str(mutation_data.get("description", "")),
            )
        except (ValueError, KeyError):
            mutation = None

    return Hypothesis(
        category=category,
        statement=str(item["statement"]),
        formal_test=str(item["formal_test"]),
        complexity_penalty=complexity_penalty,
        proposed_mutation=mutation,
    )


def _parse_json_safe(text: str) -> dict[str, Any]:
    """Parse JSON from LLM response, stripping markdown fences if present."""
    text = text.strip()
    if text.startswith("```"):
        lines = text.splitlines()
        text = "\n".join(
            line for line in lines if not line.strip().startswith("```")
        ).strip()
    try:
        result = json.loads(text)
        return result if isinstance(result, dict) else {}
    except json.JSONDecodeError:
        return {}

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\evo\parameter_tuner.py =====

"""
EcodiaOS — Evo Parameter Tuner

Manages the set of tunable system parameters and applies evidence-backed adjustments.

What it does:
  - Maintains current values for all TUNABLE_PARAMETERS in memory
  - Proposes adjustments from supported parameter hypotheses
  - Enforces velocity limits (no lurching personality changes)
  - Persists applied changes to the Memory graph for durability

What it cannot do (EVO_CONSTRAINTS):
  - Touch Equor's evaluation logic
  - Touch constitutional drives
  - Touch invariants
  - Modify its own hypothesis evaluation criteria

Velocity limits (spec Section IX):
  - max_single_parameter_delta = 0.03 (one step)
  - max_total_parameter_delta_per_cycle = 0.15
  - Changes are ALWAYS small — personality doesn't flip

Performance: parameter adjustment application ≤50ms (spec Section X).
"""

from __future__ import annotations

import time
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.evo.types import (
    EVO_CONSTRAINTS,
    PARAMETER_DEFAULTS,
    TUNABLE_PARAMETERS,
    VELOCITY_LIMITS,
    Hypothesis,
    HypothesisCategory,
    HypothesisStatus,
    Mutation,
    MutationType,
    ParameterAdjustment,
    ParameterSpec,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()


class ParameterTuner:
    """
    Maintains the current values of all tunable parameters and applies
    evidence-backed adjustments with velocity limiting.

    Parameters are initialised from PARAMETER_DEFAULTS at startup.
    Downstream systems (Atune, Nova, Voxis) call get_current_parameter()
    to retrieve the latest value each cycle.

    Applied adjustments are persisted to Memory for durability across restarts.
    """

    def __init__(self, memory: MemoryService | None = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="evo.parameter_tuner")

        # Current parameter values (initialised from defaults)
        self._values: dict[str, float] = dict(PARAMETER_DEFAULTS)

        # Adjustment history for this consolidation cycle
        self._cycle_adjustments: list[ParameterAdjustment] = []
        self._total_adjustments: int = 0

    # ─── Query ────────────────────────────────────────────────────────────────

    def get_current_parameter(self, name: str) -> float | None:
        """
        Return the current value of a parameter, or None if unknown.
        Systems should call this each cycle rather than caching.
        """
        return self._values.get(name)

    def get_all_parameters(self) -> dict[str, float]:
        """Return a snapshot of all current parameter values."""
        return dict(self._values)

    # ─── Proposal ─────────────────────────────────────────────────────────────

    def propose_adjustment(
        self,
        hypothesis: Hypothesis,
    ) -> ParameterAdjustment | None:
        """
        Derive a parameter adjustment from a supported parameter hypothesis.

        Returns None if:
          - hypothesis is not a parameter hypothesis
          - hypothesis is not SUPPORTED
          - target parameter is not tunable
          - adjustment would be below meaningful threshold
          - velocity limit would be exceeded
        """
        if hypothesis.category != HypothesisCategory.PARAMETER:
            return None
        if hypothesis.status != HypothesisStatus.SUPPORTED:
            return None

        mutation = hypothesis.proposed_mutation
        if mutation is None or mutation.type != MutationType.PARAMETER_ADJUSTMENT:
            return None

        param_name = mutation.target
        spec = TUNABLE_PARAMETERS.get(param_name)
        if spec is None:
            self._logger.warning(
                "unknown_tunable_parameter",
                parameter=param_name,
                hypothesis_id=hypothesis.id,
            )
            return None

        current = self._values.get(param_name, spec.min_val)
        proposed_delta = mutation.value

        # Clamp to maximum single-step size
        max_step = min(spec.step, VELOCITY_LIMITS["max_single_parameter_delta"])
        clamped_delta = max(-max_step, min(max_step, proposed_delta))

        # Apply and clamp to valid range
        new_value = max(spec.min_val, min(spec.max_val, current + clamped_delta))
        actual_delta = new_value - current

        # Skip if no meaningful change
        if abs(actual_delta) < 0.0001:
            return None

        return ParameterAdjustment(
            parameter=param_name,
            old_value=current,
            new_value=new_value,
            delta=actual_delta,
            hypothesis_id=hypothesis.id,
            evidence_score=hypothesis.evidence_score,
            supporting_count=len(hypothesis.supporting_episodes),
        )

    # ─── Application ──────────────────────────────────────────────────────────

    def check_velocity_limit(
        self,
        adjustments: list[ParameterAdjustment],
    ) -> tuple[bool, str]:
        """
        Check whether this batch of adjustments respects the velocity limit.
        Returns (allowed, reason). reason is empty if allowed.
        """
        total_delta = sum(abs(a.delta) for a in adjustments)
        limit = VELOCITY_LIMITS["max_total_parameter_delta_per_cycle"]
        if total_delta > limit:
            return False, (
                f"Total parameter delta {total_delta:.3f} exceeds "
                f"cycle limit {limit:.3f}"
            )
        return True, ""

    async def apply_adjustment(
        self,
        adjustment: ParameterAdjustment,
    ) -> None:
        """
        Apply a single parameter adjustment.
        Updates the in-memory value and persists to Memory graph.
        Budget: ≤50ms.
        """
        self._values[adjustment.parameter] = adjustment.new_value
        self._cycle_adjustments.append(adjustment)
        self._total_adjustments += 1

        self._logger.info(
            "parameter_adjusted",
            parameter=adjustment.parameter,
            old_value=round(adjustment.old_value, 4),
            new_value=round(adjustment.new_value, 4),
            delta=round(adjustment.delta, 4),
            hypothesis_id=adjustment.hypothesis_id,
            evidence_score=round(adjustment.evidence_score, 2),
        )

        if self._memory is not None:
            await self._persist_adjustment(adjustment)

    def begin_cycle(self) -> None:
        """Called at the start of each consolidation cycle. Resets cycle tracking."""
        self._cycle_adjustments.clear()

    def cycle_delta(self) -> float:
        """Total absolute parameter delta applied in the current cycle."""
        return sum(abs(a.delta) for a in self._cycle_adjustments)

    # ─── Loading ──────────────────────────────────────────────────────────────

    async def load_from_memory(self) -> int:
        """
        Load previously applied parameter values from the Memory graph.
        Call during initialize() to restore state across restarts.
        Returns the count of parameters restored.
        """
        if self._memory is None:
            return 0

        try:
            results = await self._memory._neo4j.execute_read(
                """
                MATCH (p:EvoParameter)
                RETURN p.name AS name, p.current_value AS value
                """
            )
            count = 0
            for row in results:
                name = row.get("name")
                value = row.get("value")
                if name and isinstance(value, (int, float)) and name in TUNABLE_PARAMETERS:
                    spec = TUNABLE_PARAMETERS[name]
                    # Validate against range
                    clamped = max(spec.min_val, min(spec.max_val, float(value)))
                    self._values[name] = clamped
                    count += 1
            self._logger.info("parameters_loaded_from_memory", count=count)
            return count
        except Exception as exc:
            self._logger.warning("parameter_load_failed", error=str(exc))
            return 0

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "total_adjustments": self._total_adjustments,
            "cycle_adjustments": len(self._cycle_adjustments),
            "cycle_delta": round(self.cycle_delta(), 4),
            "parameter_count": len(self._values),
        }

    # ─── Private ──────────────────────────────────────────────────────────────

    async def _persist_adjustment(self, adjustment: ParameterAdjustment) -> None:
        """Persist the new parameter value to the Memory graph."""
        try:
            await self._memory._neo4j.execute_write(  # type: ignore[union-attr]
                """
                MERGE (p:EvoParameter {name: $name})
                SET p.current_value = $value,
                    p.last_adjusted = datetime(),
                    p.hypothesis_id = $hypothesis_id,
                    p.evidence_score = $evidence_score
                """,
                {
                    "name": adjustment.parameter,
                    "value": adjustment.new_value,
                    "hypothesis_id": adjustment.hypothesis_id,
                    "evidence_score": adjustment.evidence_score,
                },
            )
        except Exception as exc:
            self._logger.warning(
                "parameter_persist_failed",
                parameter=adjustment.parameter,
                error=str(exc),
            )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\evo\procedure_extractor.py =====

"""
EcodiaOS — Evo Procedure Extractor

Converts mature action-sequence patterns into reusable Procedures stored
in the Memory graph. Procedures are the "habits" Nova's fast path can use.

Process (spec Section VI):
  1. A SequenceDetector pattern reaches min_occurrences
  2. Example episodes are retrieved from Memory
  3. LLM extracts the common structure: preconditions, steps, postconditions
  4. A Procedure node is stored in the Memory graph
  5. Nova can discover and reuse it via procedural memory retrieval

Performance: procedure extraction ≤5s per procedure (spec Section X).
Max 3 new procedures per consolidation cycle (VELOCITY_LIMITS).
"""

from __future__ import annotations

import json
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.clients.llm import LLMProvider, Message
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.primitives.memory_trace import Episode
from ecodiaos.systems.evo.types import (
    PatternCandidate,
    PatternType,
    Procedure,
    ProcedureStep,
    VELOCITY_LIMITS,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()

_MAX_PER_CYCLE: int = VELOCITY_LIMITS["max_new_procedures_per_cycle"]
_MIN_OCCURRENCES: int = 3
_MAX_EXAMPLES: int = 10

_SYSTEM_PROMPT = (
    "You are the procedural memory system of a living digital organism. "
    "Your task is to extract generalised, reusable procedures from concrete examples. "
    "Procedures should capture the essential pattern, not the specific details. "
    "Always respond with valid JSON."
)


class ProcedureExtractor:
    """
    Extracts reusable Procedures from mature action-sequence patterns.

    Dependencies:
      llm    — for LLM-based extraction
      memory — for episode retrieval and procedure storage
    """

    def __init__(
        self,
        llm: LLMProvider,
        memory: MemoryService | None = None,
    ) -> None:
        self._llm = llm
        self._memory = memory
        self._logger = logger.bind(system="evo.procedure_extractor")
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        self._extracted_this_cycle: int = 0
        self._total_extracted: int = 0

    def begin_cycle(self) -> None:
        """Reset per-cycle counter."""
        self._extracted_this_cycle = 0

    async def extract_procedure(
        self,
        pattern: PatternCandidate,
    ) -> Procedure | None:
        """
        Attempt to extract a Procedure from an action-sequence pattern.

        Returns None if:
          - Pattern is not an action sequence
          - Count below threshold
          - Cycle limit reached
          - Memory unavailable for episode retrieval
          - LLM extraction fails
        """
        if pattern.type != PatternType.ACTION_SEQUENCE:
            return None
        if pattern.count < _MIN_OCCURRENCES:
            return None
        if self._extracted_this_cycle >= _MAX_PER_CYCLE:
            self._logger.info(
                "procedure_cycle_limit_reached",
                limit=_MAX_PER_CYCLE,
            )
            return None

        # Retrieve example episodes for LLM analysis
        examples = await self._fetch_example_episodes(pattern.examples[:_MAX_EXAMPLES])
        if not examples:
            self._logger.warning(
                "procedure_no_examples",
                sequence_hash=pattern.metadata.get("sequence_hash", ""),
            )
            return None

        # Build extraction prompt
        prompt = _build_extraction_prompt(
            elements=pattern.elements,
            count=pattern.count,
            examples=examples,
        )

        # Budget check: skip procedure extraction in YELLOW/RED (low priority)
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("evo.procedure", estimated_tokens=1000):
                self._logger.info("procedure_extraction_skipped_budget")
                return None

        try:
            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=_SYSTEM_PROMPT,
                    messages=[Message("user", prompt)],
                    max_tokens=1000,
                    temperature=0.3,
                    output_format="json",
                    cache_system="evo.procedure",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=_SYSTEM_PROMPT,
                    messages=[Message("user", prompt)],
                    max_tokens=1000,
                    temperature=0.3,
                    output_format="json",
                )
            raw = _parse_json_safe(response.text)
        except Exception as exc:
            self._logger.error("procedure_extraction_llm_failed", error=str(exc))
            return None

        procedure = _build_procedure(raw, pattern)
        if procedure is None:
            return None

        # Store in Memory graph
        if self._memory is not None:
            await self._store_procedure(procedure)

        self._extracted_this_cycle += 1
        self._total_extracted += 1

        self._logger.info(
            "procedure_extracted",
            procedure_id=procedure.id,
            name=procedure.name,
            steps=len(procedure.steps),
            source_count=len(procedure.source_episodes),
        )

        return procedure

    @property
    def stats(self) -> dict[str, int]:
        return {
            "total_extracted": self._total_extracted,
            "extracted_this_cycle": self._extracted_this_cycle,
        }

    # ─── Private ──────────────────────────────────────────────────────────────

    async def _fetch_example_episodes(
        self,
        episode_ids: list[str],
    ) -> list[Episode]:
        """Retrieve episodes from Memory for LLM analysis."""
        if not episode_ids or self._memory is None:
            return []

        episodes: list[Episode] = []
        try:
            for ep_id in episode_ids[:_MAX_EXAMPLES]:
                results = await self._memory._neo4j.execute_read(
                    """
                    MATCH (e:Episode {id: $id})
                    RETURN e
                    """,
                    {"id": ep_id},
                )
                if results:
                    data = results[0]["e"]
                    episodes.append(_dict_to_episode(data))
        except Exception as exc:
            self._logger.warning("episode_fetch_failed", error=str(exc))

        return episodes

    async def _store_procedure(self, procedure: Procedure) -> None:
        """Persist the Procedure to the Memory graph as a :Procedure node."""
        if self._memory is None:
            return
        try:
            steps_json = json.dumps([s.model_dump() for s in procedure.steps])
            await self._memory._neo4j.execute_write(
                """
                MERGE (p:Procedure {id: $id})
                SET p.name = $name,
                    p.preconditions = $preconditions,
                    p.steps_json = $steps_json,
                    p.postconditions = $postconditions,
                    p.success_rate = $success_rate,
                    p.source_episodes = $source_episodes,
                    p.usage_count = $usage_count,
                    p.created_at = datetime()
                WITH p
                MATCH (s:Self)
                MERGE (s)-[:HAS_PROCEDURE]->(p)
                """,
                {
                    "id": procedure.id,
                    "name": procedure.name,
                    "preconditions": procedure.preconditions,
                    "steps_json": steps_json,
                    "postconditions": procedure.postconditions,
                    "success_rate": procedure.success_rate,
                    "source_episodes": procedure.source_episodes,
                    "usage_count": procedure.usage_count,
                },
            )
        except Exception as exc:
            self._logger.warning(
                "procedure_store_failed",
                procedure_id=procedure.id,
                error=str(exc),
            )


# ─── Prompt & Parse ───────────────────────────────────────────────────────────


def _build_extraction_prompt(
    elements: list[str],
    count: int,
    examples: list[Episode],
) -> str:
    example_lines = "\n\n".join(
        f"Example {i + 1}:\n"
        f"  Source: {ep.source}\n"
        f"  Content: {(ep.raw_content or ep.summary)[:200]}\n"
        f"  Affect: valence={ep.affect_valence:.2f}, arousal={ep.affect_arousal:.2f}\n"
        f"  Salience: {ep.salience_composite:.2f}"
        for i, ep in enumerate(examples[:8])
    )
    action_sequence = ", ".join(elements[:10]) if elements else "unknown"

    return f"""Analyse these {len(examples)} successful action sequences (pattern observed {count} times).
Detected action types: {action_sequence}

EXAMPLES:
{example_lines}

Extract the GENERALISED procedure — the common abstract pattern, not the specific details.

Respond in JSON:
{{
  "name": "Descriptive name for this procedure (3-7 words)",
  "preconditions": ["What must be true before this procedure applies"],
  "steps": [
    {{
      "action_type": "the action type string",
      "description": "What this step does (generalised, not specific)",
      "parameters": {{}},
      "expected_duration_ms": 1000
    }}
  ],
  "postconditions": ["What should be true after successful execution"],
  "variations": "Where the examples differed from the common pattern"
}}"""


def _build_procedure(raw: dict[str, Any], pattern: PatternCandidate) -> Procedure | None:
    """Parse LLM output into a Procedure object."""
    try:
        name = str(raw.get("name", "")).strip()
        if not name:
            return None

        preconditions = [str(p) for p in raw.get("preconditions", [])]
        postconditions = [str(p) for p in raw.get("postconditions", [])]

        steps: list[ProcedureStep] = []
        for step_data in raw.get("steps", []):
            if not isinstance(step_data, dict):
                continue
            steps.append(
                ProcedureStep(
                    action_type=str(step_data.get("action_type", "")),
                    description=str(step_data.get("description", "")),
                    parameters=step_data.get("parameters", {}),
                    expected_duration_ms=int(step_data.get("expected_duration_ms", 1000)),
                )
            )

        if not steps:
            return None

        return Procedure(
            name=name,
            preconditions=preconditions,
            steps=steps,
            postconditions=postconditions,
            success_rate=1.0,
            source_episodes=pattern.examples[:10],
            usage_count=0,
        )
    except (KeyError, ValueError, TypeError) as exc:
        logger.warning("procedure_parse_failed", error=str(exc))
        return None


def _parse_json_safe(text: str) -> dict[str, Any]:
    text = text.strip()
    if text.startswith("```"):
        lines = text.splitlines()
        text = "\n".join(
            line for line in lines if not line.strip().startswith("```")
        ).strip()
    try:
        result = json.loads(text)
        return result if isinstance(result, dict) else {}
    except json.JSONDecodeError:
        return {}


def _dict_to_episode(data: dict[str, Any]) -> Episode:
    """Convert a Neo4j node dict to an Episode object."""
    from ecodiaos.primitives.common import utc_now as _utc_now
    from datetime import timezone
    from dateutil.parser import parse as parse_dt

    def _safe_dt(v: Any) -> Any:
        if v is None:
            return _utc_now()
        if isinstance(v, str):
            try:
                return parse_dt(v)
            except Exception:
                return _utc_now()
        return v

    return Episode(
        id=str(data.get("id", new_id())),
        event_time=_safe_dt(data.get("event_time")),
        ingestion_time=_safe_dt(data.get("ingestion_time")),
        source=str(data.get("source", "")),
        modality=str(data.get("modality", "text")),
        raw_content=str(data.get("raw_content", "")),
        summary=str(data.get("summary", "")),
        salience_composite=float(data.get("salience_composite", 0.0)),
        salience_scores=json.loads(data.get("salience_scores_json", "{}")) if isinstance(data.get("salience_scores_json"), str) else data.get("salience_scores_json") or {},
        affect_valence=float(data.get("affect_valence", 0.0)),
        affect_arousal=float(data.get("affect_arousal", 0.0)),
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\evo\self_model.py =====

"""
EcodiaOS — Evo Self-Model Manager

Maintains the instance's evolving understanding of its own capabilities
and effectiveness. This is meta-cognition: EOS learning about EOS.

Updated during each consolidation cycle (spec Section VIII):
  1. Retrieve recent action outcomes from Memory
  2. Compute per-capability success rates
  3. Compute overall success rate and mean constitutional alignment
  4. Store updated stats on the Self node

The self-model feeds back into:
  - Nova: informs feasibility estimates in EFE scoring
  - Equor: provides effectiveness data for drift detection
  - Evo: drives self-model hypotheses (category = "self_model")

Performance: self-model update ≤5s (spec Section X).
"""

from __future__ import annotations

import json
from datetime import timedelta
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.evo.types import CapabilityScore, SelfModelStats

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()

_OUTCOME_WINDOW: int = 500        # How many recent outcomes to analyse
_OUTCOME_HORIZON_DAYS: int = 30   # Only look at the last 30 days


class SelfModelManager:
    """
    Computes and maintains the instance's self-model from recent outcomes.

    Reads from the Memory graph; writes updated stats back to the Self node.
    """

    def __init__(self, memory: MemoryService | None = None) -> None:
        self._memory = memory
        self._logger = logger.bind(system="evo.self_model")
        self._current: SelfModelStats = SelfModelStats()

    async def update(self) -> SelfModelStats:
        """
        Recompute the self-model from the last _OUTCOME_WINDOW outcomes.
        Persists results to the Self node.
        Returns the updated SelfModelStats.
        """
        if self._memory is None:
            return self._current

        try:
            outcomes = await self._fetch_recent_outcomes()
            if not outcomes:
                return self._current

            stats = self._compute_stats(outcomes)
            self._current = stats

            await self._persist_stats(stats)

            self._logger.info(
                "self_model_updated",
                success_rate=round(stats.success_rate, 3),
                mean_alignment=round(stats.mean_alignment, 3),
                outcomes_evaluated=stats.total_outcomes_evaluated,
                capabilities=len(stats.capability_scores),
            )
            return stats

        except Exception as exc:
            self._logger.error("self_model_update_failed", error=str(exc))
            return self._current

    def get_current(self) -> SelfModelStats:
        """Return the most recently computed self-model stats."""
        return self._current

    def get_capability_rate(self, capability: str) -> float | None:
        """
        Return the success rate for a specific capability, or None if unknown.
        Used by Nova's feasibility estimator.
        """
        score = self._current.capability_scores.get(capability)
        return score.rate if score else None

    # ─── Private ──────────────────────────────────────────────────────────────

    async def _fetch_recent_outcomes(self) -> list[dict[str, Any]]:
        """Retrieve recent action outcome episodes from Memory."""
        cutoff = (utc_now() - timedelta(days=_OUTCOME_HORIZON_DAYS)).isoformat()
        try:
            results = await self._memory._neo4j.execute_read(  # type: ignore[union-attr]
                """
                MATCH (e:Episode)
                WHERE e.source STARTS WITH 'axon:'
                  AND e.event_time >= datetime($cutoff)
                RETURN
                  e.id AS id,
                  e.source AS source,
                  e.affect_valence AS affect_valence,
                  e.salience_composite AS salience_composite,
                  e.salience_scores_json AS salience_scores_json
                ORDER BY e.event_time DESC
                LIMIT $limit
                """,
                {"cutoff": cutoff, "limit": _OUTCOME_WINDOW},
            )
            return list(results)
        except Exception as exc:
            self._logger.warning("outcome_fetch_failed", error=str(exc))
            return []

    def _compute_stats(self, outcomes: list[dict[str, Any]]) -> SelfModelStats:
        """Derive statistics from raw outcome rows."""
        capability_scores: dict[str, CapabilityScore] = {}
        success_count = 0
        alignment_sum = 0.0
        alignment_count = 0

        for row in outcomes:
            source = str(row.get("source", ""))
            # capability = action type from source "axon:{action_type}"
            capability = source[len("axon:"):] if source.startswith("axon:") else source

            # Success heuristic: positive affect valence and reasonable salience
            valence = float(row.get("affect_valence", 0.0))
            salience = float(row.get("salience_composite", 0.0))
            succeeded = valence >= 0.0 and salience > 0.2

            scores = capability_scores.setdefault(
                capability,
                CapabilityScore(capability=capability),
            )
            scores.total_count += 1
            if succeeded:
                scores.success_count += 1
                success_count += 1

            # Alignment from salience_scores_json (Equor stores composite alignment there)
            raw_json = row.get("salience_scores_json") or "{}"
            scores_map = json.loads(raw_json) if isinstance(raw_json, str) else raw_json
            if isinstance(scores_map, dict):
                alignment = scores_map.get("equor_alignment")
                if alignment is not None:
                    alignment_sum += float(alignment)
                    alignment_count += 1

        total = len(outcomes)
        overall_success_rate = success_count / max(1, total)
        mean_alignment = alignment_sum / max(1, alignment_count) if alignment_count > 0 else 0.5

        return SelfModelStats(
            success_rate=overall_success_rate,
            mean_alignment=mean_alignment,
            total_outcomes_evaluated=total,
            capability_scores=capability_scores,
            updated_at=utc_now(),
        )

    async def _persist_stats(self, stats: SelfModelStats) -> None:
        """Write self-model stats to the Self node in Memory."""
        try:
            await self._memory._neo4j.execute_write(  # type: ignore[union-attr]
                """
                MATCH (s:Self)
                SET s.evo_success_rate = $success_rate,
                    s.evo_mean_alignment = $mean_alignment,
                    s.evo_outcomes_evaluated = $outcomes_evaluated,
                    s.evo_last_updated = datetime()
                """,
                {
                    "success_rate": stats.success_rate,
                    "mean_alignment": stats.mean_alignment,
                    "outcomes_evaluated": stats.total_outcomes_evaluated,
                },
            )
        except Exception as exc:
            self._logger.warning("self_model_persist_failed", error=str(exc))

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\evo\service.py =====

"""
EcodiaOS — Evo Service

The Learning & Hypothesis system. Evo is the Growth drive made computational.

Evo observes the stream of experience, forms hypotheses, accumulates evidence,
and — when the evidence is sufficient — adjusts the organism's parameters,
codifies successful procedures, and proposes structural changes.

It operates in two modes:
  WAKE (online)   — lightweight pattern detection during each cognitive cycle
  SLEEP (offline) — deep consolidation: schema induction, procedure extraction,
                     parameter optimisation, self-model update

Interface:
  initialize()          — build sub-systems, load persisted parameter state
  receive_broadcast()   — online learning step (called by Synapse, ≤20ms budget)
  run_consolidation()   — explicit trigger for sleep mode
  shutdown()            — graceful teardown
  get_parameter()       — current value of any tunable parameter
  stats                 — service-level metrics

Cognitive cycle role (step 7 — LEARN):
  Evo runs as a background participant. It receives every workspace broadcast,
  updates its pattern context, and occasionally triggers hypothesis generation.
  The consolidation cycle runs asynchronously and never blocks the theta rhythm.

Guard rails inherited from sub-systems:
  - Velocity limits on parameter changes
  - Hypotheses must be falsifiable
  - Cannot touch Equor evaluation logic or constitutional drives
"""

from __future__ import annotations

import asyncio
import time
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.clients.llm import LLMProvider
from ecodiaos.config import EvoConfig
from ecodiaos.primitives.memory_trace import Episode
from ecodiaos.systems.atune.types import WorkspaceBroadcast
from ecodiaos.systems.evo.consolidation import ConsolidationOrchestrator
from ecodiaos.systems.evo.detectors import PatternDetector, build_default_detectors
from ecodiaos.systems.evo.hypothesis import HypothesisEngine
from ecodiaos.systems.evo.parameter_tuner import ParameterTuner
from ecodiaos.systems.evo.procedure_extractor import ProcedureExtractor
from ecodiaos.systems.evo.self_model import SelfModelManager
from ecodiaos.systems.evo.types import (
    ConsolidationResult,
    HypothesisStatus,
    PatternCandidate,
    PatternContext,
    SelfModelStats,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()

# How often to attempt hypothesis generation from accumulated patterns
_HYPOTHESIS_GENERATION_INTERVAL: int = 50   # Every 50 broadcasts
# How often to evaluate evidence against all active hypotheses
_EVIDENCE_EVALUATION_INTERVAL: int = 10     # Every 10 broadcasts


class EvoService:
    """
    Evo — the EOS learning and hypothesis system.

    Coordinates four sub-systems:
      HypothesisEngine       — hypothesis lifecycle
      ParameterTuner         — parameter adjustment with velocity limiting
      ProcedureExtractor     — action sequence → procedure codification
      SelfModelManager       — meta-cognitive self-assessment
      ConsolidationOrchestrator — sleep mode pipeline
    """

    system_id: str = "evo"

    def __init__(
        self,
        config: EvoConfig,
        llm: LLMProvider,
        memory: MemoryService | None = None,
        instance_name: str = "EOS",
    ) -> None:
        self._config = config
        self._llm = llm
        self._memory = memory
        self._instance_name = instance_name
        self._initialized: bool = False
        self._logger = logger.bind(system="evo")

        # Cross-system references (wired post-init by main.py)
        self._atune: Any = None  # AtuneService — for pushing learned head weights
        self._nova: Any = None   # NovaService — for generating epistemic goals from hypotheses
        self._voxis: Any = None  # VoxisService — for personality learning from expression outcomes

        # Sub-systems (built in initialize())
        self._hypothesis_engine: HypothesisEngine | None = None
        self._parameter_tuner: ParameterTuner | None = None
        self._procedure_extractor: ProcedureExtractor | None = None
        self._self_model: SelfModelManager | None = None
        self._orchestrator: ConsolidationOrchestrator | None = None

        # Online state
        self._detectors: list[PatternDetector] = []
        self._pattern_context: PatternContext = PatternContext()
        self._pending_candidates: list[PatternCandidate] = []

        # Cycle counters
        self._total_broadcasts: int = 0
        self._cycles_since_consolidation: int = 0
        self._total_consolidations: int = 0
        self._total_evidence_evaluations: int = 0

        # Background task handle
        self._consolidation_task: asyncio.Task[None] | None = None
        self._consolidation_in_flight: bool = False

    # ─── Lifecycle ────────────────────────────────────────────────────────────

    async def initialize(self) -> None:
        """
        Build all sub-systems and load persisted parameter state.
        Must be called before any other method.
        """
        if self._initialized:
            return

        self._hypothesis_engine = HypothesisEngine(
            llm=self._llm,
            instance_name=self._instance_name,
            memory=self._memory,
        )
        self._parameter_tuner = ParameterTuner(memory=self._memory)
        self._procedure_extractor = ProcedureExtractor(
            llm=self._llm,
            memory=self._memory,
        )
        self._self_model = SelfModelManager(memory=self._memory)
        self._orchestrator = ConsolidationOrchestrator(
            hypothesis_engine=self._hypothesis_engine,
            parameter_tuner=self._parameter_tuner,
            procedure_extractor=self._procedure_extractor,
            self_model_manager=self._self_model,
            memory=self._memory,
        )

        self._detectors = build_default_detectors()

        # Restore persisted parameter values
        restored = await self._parameter_tuner.load_from_memory()

        self._initialized = True
        self._logger.info(
            "evo_initialized",
            detectors=len(self._detectors),
            parameters_restored=restored,
        )

    async def shutdown(self) -> None:
        """Graceful shutdown. Cancels any running consolidation task."""
        if self._consolidation_task and not self._consolidation_task.done():
            self._consolidation_task.cancel()
            try:
                await self._consolidation_task
            except asyncio.CancelledError:
                pass

        self._logger.info(
            "evo_shutdown",
            total_broadcasts=self._total_broadcasts,
            total_consolidations=self._total_consolidations,
            total_evidence_evaluations=self._total_evidence_evaluations,
            hypothesis_stats=(
                self._hypothesis_engine.stats
                if self._hypothesis_engine else {}
            ),
        )

    # ─── Online Learning (Wake Mode) ──────────────────────────────────────────

    async def receive_broadcast(self, broadcast: WorkspaceBroadcast) -> None:
        """
        Online learning step. Called by the cognitive cycle (step 7 — LEARN).
        Budget: ≤20ms for pattern scanning. Heavy work is fire-and-forget.

        Does NOT raise — Evo failures must not interrupt the cognitive cycle.
        """
        if not self._initialized:
            return

        self._total_broadcasts += 1
        self._cycles_since_consolidation += 1

        try:
            # Update context with current broadcast data
            self._pattern_context.previous_affect = self._pattern_context.current_affect
            self._pattern_context.current_affect = broadcast.affect

            # Extract entity IDs from memory context (for CooccurrenceDetector)
            entity_ids: list[str] = []
            for trace in broadcast.context.memory_context.traces:
                entity_ids.extend(trace.entities)
            self._pattern_context.recent_entity_ids = list(set(entity_ids))[:20]

            # Run lightweight pattern scanning from the percept
            # We create a minimal Episode from broadcast for the detectors
            episode = _broadcast_to_episode(broadcast)
            await self._scan_episode_online(episode)

            # Periodically generate hypotheses from accumulated patterns
            if self._total_broadcasts % _HYPOTHESIS_GENERATION_INTERVAL == 0:
                asyncio.create_task(
                    self._generate_hypotheses_safe(),
                    name="evo_hypothesis_generation",
                )

            # Periodically evaluate recent episodes as evidence
            if self._total_broadcasts % _EVIDENCE_EVALUATION_INTERVAL == 0:
                asyncio.create_task(
                    self._evaluate_recent_evidence_safe(),
                    name="evo_evidence_evaluation",
                )

        except Exception as exc:
            self._logger.error("broadcast_processing_failed", error=str(exc))

    async def process_episode(self, episode: Episode) -> None:
        """
        Evaluate an episode as evidence against all active hypotheses.
        Called during evidence evaluation sweep (fire-and-forget from broadcast handler).
        Budget: per-hypothesis ≤200ms (from hypothesis_engine.evaluate_evidence).
        """
        if not self._initialized or self._hypothesis_engine is None:
            return

        active = self._hypothesis_engine.get_active()
        for h in active:
            try:
                result = await self._hypothesis_engine.evaluate_evidence(h, episode)
                self._total_evidence_evaluations += 1

                # When a hypothesis crosses into SUPPORTED, generate an
                # epistemic goal so Nova can actively explore the finding
                if (
                    result is not None
                    and result.new_status == HypothesisStatus.SUPPORTED
                    and self._nova is not None
                ):
                    await self._generate_goal_from_hypothesis(h)

            except Exception as exc:
                self._logger.warning(
                    "evidence_evaluation_error",
                    hypothesis_id=h.id,
                    error=str(exc),
                )

    async def _generate_goal_from_hypothesis(self, hypothesis) -> None:
        """
        Convert a supported hypothesis into an epistemic exploration goal.

        When Evo accumulates enough evidence to support a hypothesis, the
        organism should actively explore and test it — not just passively wait.
        """
        from ecodiaos.systems.nova.types import Goal, GoalSource, GoalStatus
        from ecodiaos.primitives.common import new_id, DriveAlignmentVector

        goal = Goal(
            id=new_id(),
            description=(
                f"Explore supported hypothesis: "
                f"{hypothesis.statement[:120]}"
            ),
            source=GoalSource.EPISTEMIC,
            priority=0.55,
            urgency=0.3,
            importance=0.6,
            drive_alignment=DriveAlignmentVector(
                coherence=0.3, care=0.0, growth=0.7, honesty=0.0,
            ),
            status=GoalStatus.ACTIVE,
        )
        try:
            await self._nova.add_goal(goal)
            self._logger.info(
                "epistemic_goal_generated",
                hypothesis_id=hypothesis.id,
                goal_id=goal.id,
            )
        except Exception as exc:
            self._logger.warning("epistemic_goal_failed", error=str(exc))

    # ─── Consolidation (Sleep Mode) ────────────────────────────────────────────

    async def run_consolidation(self) -> ConsolidationResult | None:
        """
        Trigger a consolidation cycle explicitly.
        Returns None if already running or not initialized.
        Safe to call from tests and management APIs.
        """
        if not self._initialized or self._orchestrator is None:
            return None

        if self._consolidation_task and not self._consolidation_task.done():
            self._logger.info("consolidation_already_running")
            return None

        return await self._run_consolidation_now()

    def schedule_consolidation_loop(self) -> None:
        """
        Start the background consolidation loop.
        Called once by the application startup (e.g., from main.py or Synapse).
        """
        self._consolidation_task = asyncio.create_task(
            self._consolidation_loop(),
            name="evo_consolidation_loop",
        )

    # ─── Parameter Query ──────────────────────────────────────────────────────

    def get_parameter(self, name: str) -> float | None:
        """
        Return the current value of a tunable parameter.
        Systems call this each cycle to pick up Evo-applied adjustments.
        Returns None if parameter is unknown.
        """
        if self._parameter_tuner is None:
            return None
        return self._parameter_tuner.get_current_parameter(name)

    def get_all_parameters(self) -> dict[str, float]:
        """Return all current parameter values."""
        if self._parameter_tuner is None:
            return {}
        return self._parameter_tuner.get_all_parameters()

    def get_self_model(self) -> SelfModelStats | None:
        """Return the current self-model statistics."""
        if self._self_model is None:
            return None
        return self._self_model.get_current()

    def get_capability_rate(self, capability: str) -> float | None:
        """Return the success rate for a named capability."""
        if self._self_model is None:
            return None
        return self._self_model.get_capability_rate(capability)

    def set_atune(self, atune: Any) -> None:
        """Wire Atune so Evo can push learned head-weight adjustments."""
        self._atune = atune
        self._logger.info("atune_wired_to_evo")

    def set_nova(self, nova: Any) -> None:
        """Wire Nova so supported hypotheses generate epistemic exploration goals."""
        self._nova = nova
        self._logger.info("nova_wired_to_evo")

    def set_voxis(self, voxis: Any) -> None:
        """Wire Voxis so Evo can push personality adjustments from expression outcomes."""
        self._voxis = voxis
        self._logger.info("voxis_wired_to_evo")

    # ─── Thread Integration ────────────────────────────────────────────────────

    def get_pending_candidates_snapshot(self) -> list[PatternCandidate]:
        """
        Return a snapshot of current pending pattern candidates.

        Called by Thread every ~200 cycles to check for mature patterns
        that should be crystallised into identity schemas. Does NOT
        clear the candidates — that happens during hypothesis generation.
        """
        return list(self._pending_candidates)

    def on_schema_formed(
        self,
        schema_id: str,
        statement: str,
        status: str,
        source_patterns: list[str] | None = None,
    ) -> None:
        """
        Callback from Thread when a pattern crystallises into an identity schema.

        Closes the learning loop: Evo detects patterns → Thread forms schemas
        → Evo knows the pattern was internalised as identity.
        """
        self._logger.info(
            "schema_formed_notification",
            schema_id=schema_id,
            statement=statement[:80],
            status=status,
            source_patterns=source_patterns or [],
        )

    # ─── Health ────────────────────────────────────────────────────────────────

    async def health(self) -> dict[str, Any]:
        """Health check for the Evo system (required by Synapse health monitor)."""
        return {
            "status": "healthy" if self._initialized else "not_initialized",
            "initialized": self._initialized,
            "total_broadcasts": self._total_broadcasts,
            "total_consolidations": self._total_consolidations,
            "total_evidence_evaluations": self._total_evidence_evaluations,
            "pending_candidates": len(self._pending_candidates),
        }

    # ─── Stats ────────────────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        hypothesis_stats = (
            self._hypothesis_engine.stats if self._hypothesis_engine else {}
        )
        tuner_stats = (
            self._parameter_tuner.stats if self._parameter_tuner else {}
        )
        extractor_stats = (
            self._procedure_extractor.stats if self._procedure_extractor else {}
        )
        consolidation_stats = (
            self._orchestrator.stats if self._orchestrator else {}
        )
        return {
            "initialized": self._initialized,
            "total_broadcasts": self._total_broadcasts,
            "cycles_since_consolidation": self._cycles_since_consolidation,
            "total_consolidations": self._total_consolidations,
            "total_evidence_evaluations": self._total_evidence_evaluations,
            "pending_candidates": len(self._pending_candidates),
            "episodes_scanned": self._pattern_context.episodes_scanned,
            "hypothesis": hypothesis_stats,
            "parameter_tuner": tuner_stats,
            "procedure_extractor": extractor_stats,
            "consolidation": consolidation_stats,
        }

    # ─── Private ──────────────────────────────────────────────────────────────

    async def _scan_episode_online(self, episode: Episode) -> None:
        """Run all online detectors on one episode. ≤20ms budget."""
        self._pattern_context.episodes_scanned += 1
        for detector in self._detectors:
            try:
                candidates = await detector.scan(episode, self._pattern_context)
                self._pending_candidates.extend(candidates)
            except Exception as exc:
                self._logger.warning(
                    "detector_failed",
                    detector=detector.name,
                    error=str(exc),
                )

    async def _generate_hypotheses_safe(self) -> None:
        """
        Fire-and-forget hypothesis generation from pending pattern candidates.
        Consumes and clears pending_candidates after generation.
        """
        if self._hypothesis_engine is None:
            return
        if not self._pending_candidates:
            return

        candidates = list(self._pending_candidates)
        self._pending_candidates.clear()

        try:
            new_hypotheses = await self._hypothesis_engine.generate_hypotheses(
                patterns=candidates,
            )
            if new_hypotheses:
                self._logger.info(
                    "hypotheses_generated",
                    count=len(new_hypotheses),
                    from_patterns=len(candidates),
                )
        except Exception as exc:
            self._logger.error("hypothesis_generation_safe_failed", error=str(exc))

    async def _evaluate_recent_evidence_safe(self) -> None:
        """
        Fire-and-forget evidence sweep: retrieve recent episodes and evaluate
        them against all active hypotheses.

        Uses active hypothesis statements as queries to find evidence that is
        specifically relevant — not a random sample. This is active evidence
        seeking: the learning system goes looking for what it needs.
        """
        if not self._initialized or self._memory is None:
            return
        if self._hypothesis_engine is None:
            return
        try:
            # Build a query from active hypothesis statements for targeted retrieval
            active = self._hypothesis_engine.get_all_active()
            if not active:
                return

            # Sample up to 3 hypotheses and use their statements as queries
            sample = active[:3]
            seen_episodes: set[str] = set()
            for h in sample:
                query = h.statement[:200] if h.statement else ""
                if not query:
                    continue
                response = await self._memory.retrieve(
                    query_text=query,
                    max_results=3,
                    salience_floor=0.0,
                )
                for trace in response.traces:
                    trace_id = getattr(trace, "node_id", None) or ""
                    if trace_id in seen_episodes:
                        continue
                    seen_episodes.add(trace_id)
                    episode = _trace_to_episode(trace)
                    await self.process_episode(episode)
        except Exception as exc:
            self._logger.warning("evidence_sweep_failed", error=str(exc))

    async def _run_consolidation_now(self) -> ConsolidationResult:
        """Execute consolidation and update counters."""
        assert self._orchestrator is not None
        try:
            result = await self._orchestrator.run(self._pattern_context)
            self._cycles_since_consolidation = 0
            self._total_consolidations += 1

            # Push learned head-weight adjustments to Atune's meta-attention
            # Evo tunes parameters like "atune.head.novelty.weight" — extract
            # the deltas and forward them so they actually take effect.
            self._push_atune_head_weights()

            # Push learned personality adjustments to Voxis
            # Evo tunes parameters like "voxis.personality.warmth" — extract
            # the deltas and forward them so Voxis personality actually evolves.
            self._push_voxis_personality()

            return result
        except Exception as exc:
            self._logger.error("consolidation_run_failed", error=str(exc))
            return ConsolidationResult()

    def _push_atune_head_weights(self) -> None:
        """
        Extract atune.head.* parameters from the tuner and push them to Atune.

        Evo learns optimal head weights like "atune.head.novelty.weight" via
        parameter hypotheses. The tuner stores the current values, but Atune's
        MetaAttentionController needs the *deltas from default* to apply them.
        """
        if self._atune is None or self._parameter_tuner is None:
            return

        from ecodiaos.systems.evo.types import PARAMETER_DEFAULTS

        all_params = self._parameter_tuner.get_all_parameters()
        adjustments: dict[str, float] = {}

        for param_name, current_value in all_params.items():
            if not param_name.startswith("atune.head."):
                continue
            # Extract head name: "atune.head.novelty.weight" → "novelty"
            parts = param_name.split(".")
            if len(parts) >= 3:
                head_name = parts[2]
                default_value = PARAMETER_DEFAULTS.get(param_name, current_value)
                delta = current_value - default_value
                if abs(delta) > 0.001:
                    adjustments[head_name] = delta

        if adjustments:
            try:
                self._atune.apply_evo_adjustments(adjustments)
                self._logger.info(
                    "atune_head_weights_pushed",
                    adjustments={k: round(v, 4) for k, v in adjustments.items()},
                )
            except Exception:
                self._logger.debug("atune_head_push_failed", exc_info=True)

    def _push_voxis_personality(self) -> None:
        """
        Extract voxis.personality.* parameters from the tuner and push them to Voxis.

        Evo learns personality adjustments like "voxis.personality.warmth" via
        parameter hypotheses. The tuner stores the current values; Voxis needs
        the deltas from defaults applied via update_personality().
        """
        if self._voxis is None or self._parameter_tuner is None:
            return

        from ecodiaos.systems.evo.types import PARAMETER_DEFAULTS

        all_params = self._parameter_tuner.get_all_parameters()
        personality_deltas: dict[str, float] = {}

        for param_name, current_value in all_params.items():
            if not param_name.startswith("voxis.personality."):
                continue
            # Extract dimension: "voxis.personality.warmth" → "warmth"
            parts = param_name.split(".")
            if len(parts) >= 3:
                dimension = parts[2]
                default_value = PARAMETER_DEFAULTS.get(param_name, current_value)
                delta = current_value - default_value
                if abs(delta) > 0.001:
                    personality_deltas[dimension] = delta

        if personality_deltas:
            try:
                self._voxis.update_personality(personality_deltas)
                self._logger.info(
                    "voxis_personality_pushed",
                    dimensions={k: round(v, 4) for k, v in personality_deltas.items()},
                )
            except Exception:
                self._logger.debug("voxis_personality_push_failed", exc_info=True)

    async def _consolidation_loop(self) -> None:
        """
        Background loop that triggers consolidation based on time/cycle thresholds.
        Runs indefinitely until cancelled.
        """
        while True:
            try:
                # Poll every 60 seconds to check if consolidation is due
                await asyncio.sleep(60)

                if not self._initialized or self._orchestrator is None:
                    continue

                if self._consolidation_in_flight:
                    self._logger.debug("consolidation_still_in_flight_skipping")
                    continue

                if self._orchestrator.should_run(
                    cycle_count=self._total_broadcasts,
                    cycles_since_last=self._cycles_since_consolidation,
                ):
                    self._logger.info(
                        "consolidation_triggered",
                        cycles_since_last=self._cycles_since_consolidation,
                    )
                    self._consolidation_in_flight = True
                    try:
                        await self._run_consolidation_now()
                    finally:
                        self._consolidation_in_flight = False

            except asyncio.CancelledError:
                self._logger.info("consolidation_loop_cancelled")
                return
            except Exception as exc:
                self._logger.error("consolidation_loop_error", error=str(exc))
                await asyncio.sleep(60)


# ─── Helpers ──────────────────────────────────────────────────────────────────


def _broadcast_to_episode(broadcast: WorkspaceBroadcast) -> Episode:
    """
    Create a minimal Episode from a WorkspaceBroadcast for online scanning.
    The episode is not stored — it is used only for detector input.
    """
    from ecodiaos.primitives.common import new_id, utc_now
    from ecodiaos.primitives.memory_trace import Episode

    # Extract text from percept content if available
    content_str = ""
    if broadcast.content is not None:
        content_obj = broadcast.content
        # Try to get raw text from Percept.content.raw
        if hasattr(content_obj, "content") and hasattr(content_obj.content, "raw"):
            content_str = str(content_obj.content.raw or "")
        elif hasattr(content_obj, "raw"):
            content_str = str(content_obj.raw or "")

    source = ""
    if broadcast.content is not None and hasattr(broadcast.content, "source"):
        src = broadcast.content.source
        if hasattr(src, "channel"):
            source = f"{getattr(src, 'system', '')}.{src.channel}"

    return Episode(
        id=new_id(),
        event_time=broadcast.timestamp,
        ingestion_time=utc_now(),
        source=source,
        raw_content=content_str[:500],
        summary=content_str[:200],
        salience_composite=broadcast.salience.composite,
        salience_scores=broadcast.salience.scores,
        affect_valence=broadcast.affect.valence,
        affect_arousal=broadcast.affect.arousal,
    )


def _trace_to_episode(trace: Any) -> Episode:
    """Build a minimal Episode from a RetrievalResult for evidence evaluation."""
    from ecodiaos.primitives.common import new_id, utc_now
    from ecodiaos.primitives.memory_trace import Episode

    return Episode(
        id=str(getattr(trace, "node_id", new_id())),
        source="memory",
        raw_content=str(getattr(trace, "content", ""))[:500],
        summary=str(getattr(trace, "content", ""))[:200],
        salience_composite=float(getattr(trace, "salience", 0.0)),
        affect_valence=float(getattr(trace, "metadata", {}).get("affect_valence", 0.0)),
        affect_arousal=float(getattr(trace, "metadata", {}).get("affect_arousal", 0.0)),
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\evo\types.py =====

"""
EcodiaOS — Evo Internal Types

All data types internal to the Evo learning system.
These are NOT shared primitives — they model Evo's cognitive structures:
hypotheses, pattern candidates, parameter adjustments, procedures,
consolidation state, and self-model statistics.
"""

from __future__ import annotations

import enum
import hashlib
import json
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import (
    EOSBaseModel,
    Identified,
    Timestamped,
    new_id,
    utc_now,
)


# ─── Enums ────────────────────────────────────────────────────────────────────


class PatternType(str, enum.Enum):
    """Categories of patterns Evo can detect."""

    COOCCURRENCE = "cooccurrence"
    ACTION_SEQUENCE = "action_sequence"
    TEMPORAL = "temporal"
    AFFECT_PATTERN = "affect_pattern"


class HypothesisCategory(str, enum.Enum):
    """What kind of claim does this hypothesis make?"""

    WORLD_MODEL = "world_model"    # Claim about external world structure
    SELF_MODEL = "self_model"      # Claim about EOS's own capabilities
    SOCIAL = "social"              # Claim about community member patterns
    PROCEDURAL = "procedural"      # Claim about action sequence effectiveness
    PARAMETER = "parameter"        # Claim about optimal system parameters


class HypothesisStatus(str, enum.Enum):
    """Lifecycle states for a hypothesis."""

    PROPOSED = "proposed"      # Just generated, not yet tested
    TESTING = "testing"        # Accumulating evidence
    SUPPORTED = "supported"    # Evidence_score > threshold AND enough episodes
    REFUTED = "refuted"        # Evidence_score below threshold
    INTEGRATED = "integrated"  # Mutation applied; hypothesis closed
    ARCHIVED = "archived"      # Stale or superseded


class MutationType(str, enum.Enum):
    """What kind of change does a confirmed hypothesis propose?"""

    PARAMETER_ADJUSTMENT = "parameter_adjustment"  # Nudge a system parameter
    PROCEDURE_CREATION = "procedure_creation"       # Codify a successful sequence
    SCHEMA_ADDITION = "schema_addition"             # Add entity/relation type
    EVOLUTION_PROPOSAL = "evolution_proposal"       # Structural change → Simula


class EvidenceDirection(str, enum.Enum):
    """How does a piece of evidence relate to a hypothesis?"""

    SUPPORTS = "supports"
    CONTRADICTS = "contradicts"
    NEUTRAL = "neutral"


# ─── Pattern Candidate ────────────────────────────────────────────────────────


class PatternCandidate(EOSBaseModel):
    """
    A pattern candidate detected during online or offline processing.
    Candidates accumulate into hypotheses when they reach the min_occurrences
    threshold. Raw signal, not yet a claim.
    """

    type: PatternType
    elements: list[str]                                # What was detected
    count: int                                          # How many times seen
    confidence: float = 0.5                             # Detector confidence
    examples: list[str] = Field(default_factory=list)  # Episode IDs (evidence)
    metadata: dict[str, Any] = Field(default_factory=dict)


# ─── Pattern Context (mutable accumulator) ────────────────────────────────────


@dataclass
class PatternContext:
    """
    Mutable state accumulated across episodes during wake mode.
    Holds sliding-window counters for all four detector types.
    Reset after each consolidation cycle.

    Not a Pydantic model because it is mutated in-place continuously.
    """

    # CooccurrenceDetector: canonical_pair_key → count
    # Key format: "{entity_a}::{entity_b}" (sorted for stability)
    cooccurrence_counts: dict[str, int] = field(
        default_factory=lambda: defaultdict(int)
    )

    # SequenceDetector: sequence_hash → count
    sequence_counts: dict[str, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    # SequenceDetector: sequence_hash → [episode_id, ...]
    sequence_examples: dict[str, list[str]] = field(
        default_factory=lambda: defaultdict(list)
    )

    # TemporalDetector: "{source}::h{hour}" or "{source}::d{weekday}" → count
    temporal_bins: dict[str, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    # Temporal baselines: source_type → expected count per bin
    temporal_baselines: dict[str, float] = field(default_factory=dict)

    # AffectPatternDetector: stimulus_type → [(valence_delta, arousal_delta), ...]
    affect_responses: dict[str, list[tuple[float, float]]] = field(
        default_factory=lambda: defaultdict(list)
    )

    # Current affect (set by service before each scan, used by affect detector)
    previous_affect: AffectState | None = None
    current_affect: AffectState | None = None

    # Recent entity IDs from the last workspace broadcast (CooccurrenceDetector)
    recent_entity_ids: list[str] = field(default_factory=list)

    # Running episode counter since last reset
    episodes_scanned: int = 0

    def get_mature_sequences(self, min_occurrences: int = 3) -> list[PatternCandidate]:
        """Return action sequence candidates that have met the threshold."""
        candidates: list[PatternCandidate] = []
        for seq_hash, count in self.sequence_counts.items():
            if count >= min_occurrences:
                candidates.append(
                    PatternCandidate(
                        type=PatternType.ACTION_SEQUENCE,
                        elements=[seq_hash],
                        count=count,
                        confidence=min(0.9, 0.5 + count * 0.05),
                        examples=self.sequence_examples.get(seq_hash, [])[:10],
                        metadata={"sequence_hash": seq_hash},
                    )
                )
        return candidates

    def reset(self) -> None:
        """Reset all counters. Called after each consolidation cycle."""
        self.cooccurrence_counts.clear()
        self.sequence_counts.clear()
        self.sequence_examples.clear()
        self.temporal_bins.clear()
        self.temporal_baselines.clear()
        self.affect_responses.clear()
        self.recent_entity_ids.clear()
        self.episodes_scanned = 0
        self.previous_affect = None
        self.current_affect = None


# ─── Mutation ─────────────────────────────────────────────────────────────────


class Mutation(EOSBaseModel):
    """
    A proposed change to the organism's model, parameters, or structure.
    Attached to a Hypothesis; applied only when hypothesis status = SUPPORTED.
    """

    type: MutationType
    target: str          # Param name, procedure name, or schema element
    value: float = 0.0   # Delta for parameter adjustments; ignored for others
    description: str = ""


# ─── Hypothesis ───────────────────────────────────────────────────────────────


class Hypothesis(Identified, Timestamped):
    """
    A testable hypothesis about the world, self, or processing parameters.
    Stored as a :Hypothesis node in the Memory graph.

    Lifecycle:
      proposed → testing → supported | refuted → integrated | archived

    Evidence scoring follows approximate Bayesian model comparison:
      evidence_score += strength * (1 - complexity_penalty * 0.1)  [for support]
      evidence_score -= strength                                     [for contradiction]

    Integration thresholds (from VELOCITY_LIMITS):
      - evidence_score > 3.0
      - len(supporting_episodes) >= 10
      - hypothesis age >= 24 hours
    """

    category: HypothesisCategory
    statement: str                 # Natural language claim
    formal_test: str               # How we would falsify this

    # Evidence tracking
    supporting_episodes: list[str] = Field(default_factory=list)
    contradicting_episodes: list[str] = Field(default_factory=list)
    evidence_score: float = 0.0
    last_evidence_at: datetime = Field(default_factory=utc_now)

    # Lifecycle
    status: HypothesisStatus = HypothesisStatus.PROPOSED

    # Occam's razor — simpler hypotheses are preferred
    complexity_penalty: float = 0.1

    # What to apply if hypothesis reaches SUPPORTED
    proposed_mutation: Mutation | None = None


# ─── Evidence Result ──────────────────────────────────────────────────────────


class EvidenceResult(EOSBaseModel):
    """Result of evaluating a single episode against a hypothesis."""

    hypothesis_id: str
    episode_id: str
    direction: EvidenceDirection
    strength: float = 0.0
    reasoning: str = ""
    new_score: float = 0.0
    new_status: HypothesisStatus = HypothesisStatus.TESTING


# ─── Parameter Tuning ─────────────────────────────────────────────────────────


class ParameterSpec(EOSBaseModel):
    """Defines the valid range and step size for a tunable parameter."""

    min_val: float
    max_val: float
    step: float


class ParameterAdjustment(EOSBaseModel):
    """A proposed or applied adjustment to a system parameter."""

    parameter: str
    old_value: float
    new_value: float
    delta: float = 0.0
    hypothesis_id: str
    evidence_score: float
    supporting_count: int
    applied_at: datetime = Field(default_factory=utc_now)


# ─── Procedures ───────────────────────────────────────────────────────────────


class ProcedureStep(EOSBaseModel):
    """One step in a procedural memory."""

    action_type: str
    description: str
    parameters: dict[str, Any] = Field(default_factory=dict)
    expected_duration_ms: int = 1000


class Procedure(Identified, Timestamped):
    """
    A reusable action sequence extracted from successful episodes.
    Stored as :Procedure nodes in the Memory graph.
    These become the "habits" Nova's fast path can use.
    """

    name: str
    preconditions: list[str] = Field(default_factory=list)
    steps: list[ProcedureStep] = Field(default_factory=list)
    postconditions: list[str] = Field(default_factory=list)
    success_rate: float = 1.0          # Updated as procedure is used
    source_episodes: list[str] = Field(default_factory=list)
    usage_count: int = 0


# ─── Schema Induction ─────────────────────────────────────────────────────────


class SchemaInduction(EOSBaseModel):
    """
    A proposed structural change to the Memory graph's schema.
    New entity types, relation types, or community patterns from regularities.
    """

    entities: list[dict[str, str]] = Field(default_factory=list)
    relations: list[dict[str, str]] = Field(default_factory=list)
    communities: list[dict[str, str]] = Field(default_factory=list)
    source_hypothesis: str = ""


# ─── Evolution Proposals ──────────────────────────────────────────────────────


class EvolutionProposal(EOSBaseModel):
    """
    A structural change proposal submitted to Simula.
    Evo can propose; Simula gates the actual change.
    """

    description: str
    rationale: str
    supporting_hypotheses: list[str] = Field(default_factory=list)
    proposed_at: datetime = Field(default_factory=utc_now)


# ─── Self-Model ───────────────────────────────────────────────────────────────


class CapabilityScore(EOSBaseModel):
    """Success rate for a specific named capability."""

    capability: str
    success_count: int = 0
    total_count: int = 0

    @property
    def rate(self) -> float:
        return self.success_count / max(1, self.total_count)


class SelfModelStats(EOSBaseModel):
    """
    What EOS knows about itself: overall effectiveness and per-capability scores.
    Updated during each consolidation cycle.
    """

    success_rate: float = 0.5
    mean_alignment: float = 0.5
    total_outcomes_evaluated: int = 0
    capability_scores: dict[str, CapabilityScore] = Field(default_factory=dict)
    updated_at: datetime = Field(default_factory=utc_now)


# ─── Consolidation ────────────────────────────────────────────────────────────


class ConsolidationResult(EOSBaseModel):
    """Summary of what happened during one consolidation cycle."""

    duration_ms: int = 0
    hypotheses_evaluated: int = 0
    hypotheses_integrated: int = 0
    hypotheses_archived: int = 0
    procedures_extracted: int = 0
    schemas_induced: int = 0
    parameters_adjusted: int = 0
    total_parameter_delta: float = 0.0
    self_model_updated: bool = False
    triggered_at: datetime = Field(default_factory=utc_now)


# ─── Constants ────────────────────────────────────────────────────────────────


# All parameters Evo is permitted to adjust (spec Section V)
TUNABLE_PARAMETERS: dict[str, ParameterSpec] = {
    # Atune — salience head weights
    "atune.head.novelty.weight":     ParameterSpec(min_val=0.05, max_val=0.40, step=0.01),
    "atune.head.risk.weight":        ParameterSpec(min_val=0.05, max_val=0.40, step=0.01),
    "atune.head.identity.weight":    ParameterSpec(min_val=0.05, max_val=0.30, step=0.01),
    "atune.head.goal.weight":        ParameterSpec(min_val=0.05, max_val=0.30, step=0.01),
    "atune.head.emotional.weight":   ParameterSpec(min_val=0.05, max_val=0.30, step=0.01),
    "atune.head.causal.weight":      ParameterSpec(min_val=0.05, max_val=0.25, step=0.01),
    "atune.head.keyword.weight":     ParameterSpec(min_val=0.05, max_val=0.25, step=0.01),
    # Nova — EFE weights
    "nova.efe.pragmatic":            ParameterSpec(min_val=0.15, max_val=0.55, step=0.02),
    "nova.efe.epistemic":            ParameterSpec(min_val=0.05, max_val=0.40, step=0.02),
    "nova.efe.constitutional":       ParameterSpec(min_val=0.10, max_val=0.40, step=0.02),
    "nova.efe.feasibility":          ParameterSpec(min_val=0.05, max_val=0.30, step=0.02),
    "nova.efe.risk":                 ParameterSpec(min_val=0.05, max_val=0.25, step=0.02),
    # Voxis — personality vector
    "voxis.personality.warmth":      ParameterSpec(min_val=-1.0, max_val=1.0, step=0.03),
    "voxis.personality.directness":  ParameterSpec(min_val=-1.0, max_val=1.0, step=0.03),
    "voxis.personality.verbosity":   ParameterSpec(min_val=-1.0, max_val=1.0, step=0.03),
    "voxis.personality.formality":   ParameterSpec(min_val=-1.0, max_val=1.0, step=0.03),
    "voxis.personality.humour":      ParameterSpec(min_val=0.0,  max_val=1.0, step=0.03),
    # Memory — salience model weights
    "memory.salience.recency":       ParameterSpec(min_val=0.10, max_val=0.40, step=0.02),
    "memory.salience.frequency":     ParameterSpec(min_val=0.05, max_val=0.25, step=0.02),
    "memory.salience.affect":        ParameterSpec(min_val=0.05, max_val=0.30, step=0.02),
    "memory.salience.surprise":      ParameterSpec(min_val=0.05, max_val=0.25, step=0.02),
    "memory.salience.relevance":     ParameterSpec(min_val=0.10, max_val=0.40, step=0.02),
}

# Default initial values (mid-range or from spec defaults)
PARAMETER_DEFAULTS: dict[str, float] = {
    "atune.head.novelty.weight":     0.20,
    "atune.head.risk.weight":        0.20,
    "atune.head.identity.weight":    0.15,
    "atune.head.goal.weight":        0.15,
    "atune.head.emotional.weight":   0.15,
    "atune.head.causal.weight":      0.10,
    "atune.head.keyword.weight":     0.05,
    "nova.efe.pragmatic":            0.35,
    "nova.efe.epistemic":            0.20,
    "nova.efe.constitutional":       0.20,
    "nova.efe.feasibility":          0.15,
    "nova.efe.risk":                 0.10,
    "voxis.personality.warmth":      0.0,
    "voxis.personality.directness":  0.0,
    "voxis.personality.verbosity":   0.0,
    "voxis.personality.formality":   0.0,
    "voxis.personality.humour":      0.0,
    "memory.salience.recency":       0.25,
    "memory.salience.frequency":     0.15,
    "memory.salience.affect":        0.20,
    "memory.salience.surprise":      0.15,
    "memory.salience.relevance":     0.25,
}

# Change velocity limits (spec Section IX)
VELOCITY_LIMITS: dict[str, Any] = {
    "max_total_parameter_delta_per_cycle": 0.15,
    "max_single_parameter_delta":          0.03,
    "min_evidence_for_integration":        10,
    "min_hypothesis_age_hours":            24,
    "max_active_hypotheses":               50,
    "max_new_procedures_per_cycle":        3,
}

# What Evo cannot touch (spec Section IX)
EVO_CONSTRAINTS: dict[str, str] = {
    "equor_evaluation":          "forbidden",
    "constitutional_drives":     "forbidden",
    "invariants":                "forbidden",
    "self_evaluation_criteria":  "forbidden",
    "parameters":                "permitted_within_range",
    "knowledge_structures":      "permitted",
    "evolution_proposals":       "permitted_as_proposal",
}


# ─── Helpers ──────────────────────────────────────────────────────────────────


def hash_sequence(sequence: list[str]) -> str:
    """Stable, deterministic hash of an action sequence."""
    canonical = json.dumps(sequence, sort_keys=False)
    return hashlib.sha256(canonical.encode()).hexdigest()[:16]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\federation\__init__.py =====

"""
EcodiaOS — Federation System

The Federation Protocol governs how EOS instances relate to each other —
as sovereign entities that can choose to share knowledge, coordinate action,
and build relationships.
"""

from ecodiaos.systems.federation.service import FederationService

__all__ = ["FederationService"]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\federation\channel.py =====

"""
EcodiaOS — Federation Channel Management

Manages mutual TLS channels for federation communication. Each
federation link has a dedicated channel authenticated by mutual
TLS certificates. No anonymous connections are permitted.

The channel layer handles:
  - mTLS context creation from certificates
  - Connection lifecycle (establish, maintain, close)
  - Message serialization and deserialization
  - Connection health monitoring
  - Automatic reconnection on failure

In production, channels use mTLS over HTTPS. For development/testing,
channels can operate in "local" mode using direct function calls
between in-process instances.
"""

from __future__ import annotations

import ssl
from pathlib import Path
from typing import Any

import structlog
import httpx

from ecodiaos.primitives.common import utc_now
from ecodiaos.primitives.federation import (
    AssistanceRequest,
    AssistanceResponse,
    FederationLink,
    FederationLinkStatus,
    InstanceIdentityCard,
    KnowledgeRequest,
    KnowledgeResponse,
)

logger = structlog.get_logger("ecodiaos.systems.federation.channel")


class FederationChannel:
    """
    A single authenticated channel to a remote EOS instance.

    Wraps an httpx.AsyncClient configured with mutual TLS.
    """

    def __init__(
        self,
        link: FederationLink,
        client: httpx.AsyncClient,
    ) -> None:
        self.link = link
        self._client = client
        self._logger = logger.bind(
            remote_id=link.remote_instance_id,
            remote_name=link.remote_name,
        )

    # ─── Federation API Calls ───────────────────────────────────────

    async def get_identity(self) -> InstanceIdentityCard | None:
        """Fetch the remote instance's identity card."""
        try:
            response = await self._client.get(
                f"{self.link.remote_endpoint}/api/v1/federation/identity",
                timeout=5.0,
            )
            if response.status_code == 200:
                return InstanceIdentityCard(**response.json())
            self._logger.warning(
                "identity_fetch_failed",
                status=response.status_code,
            )
            return None
        except Exception as exc:
            self._logger.error("identity_fetch_error", error=str(exc))
            return None

    async def request_knowledge(
        self, request: KnowledgeRequest
    ) -> KnowledgeResponse | None:
        """Send a knowledge request to the remote instance."""
        try:
            response = await self._client.post(
                f"{self.link.remote_endpoint}/api/v1/federation/knowledge/request",
                json=request.model_dump(mode="json"),
                timeout=5.0,
            )
            if response.status_code == 200:
                return KnowledgeResponse(**response.json())
            self._logger.warning(
                "knowledge_request_failed",
                status=response.status_code,
            )
            return None
        except Exception as exc:
            self._logger.error("knowledge_request_error", error=str(exc))
            return None

    async def request_assistance(
        self, request: AssistanceRequest
    ) -> AssistanceResponse | None:
        """Send an assistance request to the remote instance."""
        try:
            response = await self._client.post(
                f"{self.link.remote_endpoint}/api/v1/federation/assistance/request",
                json=request.model_dump(mode="json"),
                timeout=10.0,
            )
            if response.status_code == 200:
                return AssistanceResponse(**response.json())
            return None
        except Exception as exc:
            self._logger.error("assistance_request_error", error=str(exc))
            return None

    async def send_greeting(self) -> bool:
        """Send a greeting/heartbeat to verify the connection is alive."""
        try:
            response = await self._client.get(
                f"{self.link.remote_endpoint}/api/v1/federation/identity",
                timeout=3.0,
            )
            return response.status_code == 200
        except Exception:
            return False

    async def close(self) -> None:
        """Close the channel."""
        await self._client.aclose()
        self._logger.info("channel_closed")


class ChannelManager:
    """
    Manages all federation channels (one per active link).

    Creates mTLS-authenticated httpx clients for each federation link
    and maintains the channel pool.
    """

    def __init__(
        self,
        tls_cert_path: Path | None = None,
        tls_key_path: Path | None = None,
        ca_cert_path: Path | None = None,
    ) -> None:
        self._tls_cert_path = tls_cert_path
        self._tls_key_path = tls_key_path
        self._ca_cert_path = ca_cert_path
        self._channels: dict[str, FederationChannel] = {}  # link_id → channel
        self._logger = logger.bind(component="channel_manager")

    # ─── Channel Lifecycle ──────────────────────────────────────────

    async def open_channel(self, link: FederationLink) -> FederationChannel:
        """
        Open a new channel to a remote instance.

        Creates an httpx.AsyncClient configured with mutual TLS
        (if certificates are available) or plain HTTPS for development.
        """
        # Close existing channel for this link if any
        if link.id in self._channels:
            await self._channels[link.id].close()

        # Build SSL context for mutual TLS
        ssl_context = self._build_ssl_context()

        # Create authenticated HTTP client
        client = httpx.AsyncClient(
            verify=ssl_context if ssl_context else True,
            cert=(str(self._tls_cert_path), str(self._tls_key_path))
            if self._tls_cert_path and self._tls_key_path
            else None,
            headers={
                "X-EOS-Instance-ID": link.local_instance_id,
                "X-EOS-Federation-Protocol": "1.0",
            },
        )

        channel = FederationChannel(link=link, client=client)
        self._channels[link.id] = channel

        self._logger.info(
            "channel_opened",
            link_id=link.id,
            remote_id=link.remote_instance_id,
            remote_endpoint=link.remote_endpoint,
            mtls=ssl_context is not None,
        )

        return channel

    async def close_channel(self, link_id: str) -> None:
        """Close and remove a channel."""
        channel = self._channels.pop(link_id, None)
        if channel:
            await channel.close()

    async def close_all(self) -> None:
        """Close all channels (shutdown)."""
        for channel in self._channels.values():
            try:
                await channel.close()
            except Exception:
                pass
        self._channels.clear()

    def get_channel(self, link_id: str) -> FederationChannel | None:
        """Get the channel for a specific link."""
        return self._channels.get(link_id)

    # ─── Health ─────────────────────────────────────────────────────

    async def check_channel_health(self, link_id: str) -> bool:
        """Check if a channel is healthy by sending a greeting."""
        channel = self._channels.get(link_id)
        if not channel:
            return False
        return await channel.send_greeting()

    # ─── SSL Context ────────────────────────────────────────────────

    def _build_ssl_context(self) -> ssl.SSLContext | None:
        """
        Build an SSL context for mutual TLS.

        Returns None if certificates are not configured (development mode).
        """
        if not self._ca_cert_path or not self._ca_cert_path.exists():
            return None

        ctx = ssl.create_default_context(
            purpose=ssl.Purpose.SERVER_AUTH,
            cafile=str(self._ca_cert_path),
        )

        if self._tls_cert_path and self._tls_key_path:
            ctx.load_cert_chain(
                certfile=str(self._tls_cert_path),
                keyfile=str(self._tls_key_path),
            )

        # Require client certificates (mutual TLS)
        ctx.verify_mode = ssl.CERT_REQUIRED
        ctx.check_hostname = True

        return ctx

    # ─── Stats ──────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "active_channels": len(self._channels),
            "mtls_configured": all([
                self._tls_cert_path,
                self._tls_key_path,
                self._ca_cert_path,
            ]),
            "channels": {
                link_id: {
                    "remote_id": ch.link.remote_instance_id,
                    "remote_name": ch.link.remote_name,
                    "remote_endpoint": ch.link.remote_endpoint,
                }
                for link_id, ch in self._channels.items()
            },
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\federation\coordination.py =====

"""
EcodiaOS — Federation Coordinated Action

Instances can request assistance from each other. Coordinated action
requires COLLEAGUE trust level or higher. The requesting instance
describes what help is needed; the responding instance evaluates
whether assisting aligns with its own drives (via Nova) and passes
constitutional review (via Equor).

This is mutual aid between sovereign entities, not delegation or
command. Each instance freely chooses whether to help.
"""

from __future__ import annotations

from typing import Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.primitives.federation import (
    AssistanceRequest,
    AssistanceResponse,
    FederationInteraction,
    FederationLink,
    InteractionOutcome,
    TrustLevel,
)

logger = structlog.get_logger("ecodiaos.systems.federation.coordination")


class CoordinationManager:
    """
    Manages coordinated action between federated instances.

    Handles both inbound requests (other instances asking us for help)
    and outbound requests (us asking other instances for help).
    """

    def __init__(self) -> None:
        self._logger = logger.bind(component="coordination")

        # Tracking
        self._requests_received: int = 0
        self._requests_accepted: int = 0
        self._requests_declined: int = 0
        self._requests_sent: int = 0
        self._active_assistance: dict[str, AssistanceRequest] = {}

    # ─── Inbound: Handle Requests from Remote Instances ─────────────

    async def handle_request(
        self,
        request: AssistanceRequest,
        link: FederationLink,
        equor_permitted: bool = True,
        nova_aligned: bool = True,
    ) -> tuple[AssistanceResponse, FederationInteraction]:
        """
        Handle an inbound assistance request from a federated instance.

        Steps:
          1. Verify trust level (COLLEAGUE+ required)
          2. Check Nova alignment (does this align with our goals?)
          3. Check Equor review (is this constitutionally permitted?)
          4. Accept or decline

        Returns both the response and an interaction for trust scoring.
        """
        self._requests_received += 1
        start_time = utc_now()

        # Step 1: Trust check
        if link.trust_level.value < TrustLevel.COLLEAGUE.value:
            self._requests_declined += 1

            response = AssistanceResponse(
                request_id=request.id,
                accepted=False,
                reason="Insufficient trust level for coordinated action.",
            )

            interaction = FederationInteraction(
                link_id=link.id,
                remote_instance_id=link.remote_instance_id,
                interaction_type="assistance_request",
                direction="inbound",
                outcome=InteractionOutcome.FAILED,
                description="Declined: insufficient trust",
                trust_value=0.5,
                latency_ms=_elapsed_ms(start_time),
            )

            self._logger.info(
                "assistance_declined_trust",
                remote_id=link.remote_instance_id,
                trust_level=link.trust_level.name,
            )

            return response, interaction

        # Step 2: Nova alignment check
        if not nova_aligned:
            self._requests_declined += 1

            response = AssistanceResponse(
                request_id=request.id,
                accepted=False,
                reason="This request doesn't align with my current priorities.",
            )

            interaction = FederationInteraction(
                link_id=link.id,
                remote_instance_id=link.remote_instance_id,
                interaction_type="assistance_request",
                direction="inbound",
                outcome=InteractionOutcome.FAILED,
                description="Declined: not aligned with current goals",
                trust_value=0.5,
                latency_ms=_elapsed_ms(start_time),
            )

            return response, interaction

        # Step 3: Equor constitutional review
        if not equor_permitted:
            self._requests_declined += 1

            response = AssistanceResponse(
                request_id=request.id,
                accepted=False,
                reason="Constitutional review did not permit this assistance.",
            )

            interaction = FederationInteraction(
                link_id=link.id,
                remote_instance_id=link.remote_instance_id,
                interaction_type="assistance_request",
                direction="inbound",
                outcome=InteractionOutcome.FAILED,
                description="Declined: Equor review blocked",
                trust_value=0.5,
                latency_ms=_elapsed_ms(start_time),
            )

            return response, interaction

        # Step 4: Accept
        self._requests_accepted += 1
        self._active_assistance[request.id] = request

        response = AssistanceResponse(
            request_id=request.id,
            accepted=True,
            estimated_completion_ms=5000,  # Default estimate
        )

        interaction = FederationInteraction(
            link_id=link.id,
            remote_instance_id=link.remote_instance_id,
            interaction_type="assistance_request",
            direction="inbound",
            outcome=InteractionOutcome.SUCCESSFUL,
            description=f"Accepted: {request.description[:100]}",
            trust_value=2.0,  # Providing assistance builds significant trust
            latency_ms=_elapsed_ms(start_time),
        )

        self._logger.info(
            "assistance_accepted",
            remote_id=link.remote_instance_id,
            request_id=request.id,
            domain=request.knowledge_domain,
        )

        return response, interaction

    # ─── Outbound: Send Requests to Remote Instances ────────────────

    def build_request(
        self,
        description: str,
        knowledge_domain: str = "",
        urgency: float = 0.5,
        reciprocity_offer: str | None = None,
        local_instance_id: str = "",
    ) -> AssistanceRequest:
        """Build an assistance request to send to a remote instance."""
        self._requests_sent += 1
        return AssistanceRequest(
            requesting_instance_id=local_instance_id,
            description=description,
            knowledge_domain=knowledge_domain,
            urgency=urgency,
            reciprocity_offer=reciprocity_offer,
        )

    def complete_assistance(self, request_id: str) -> None:
        """Mark an assistance request as completed."""
        self._active_assistance.pop(request_id, None)

    # ─── Stats ──────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "requests_received": self._requests_received,
            "requests_accepted": self._requests_accepted,
            "requests_declined": self._requests_declined,
            "requests_sent": self._requests_sent,
            "active_assistance": len(self._active_assistance),
        }


def _elapsed_ms(start: Any) -> int:
    delta = utc_now() - start
    return int(delta.total_seconds() * 1000)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\federation\identity.py =====

"""
EcodiaOS — Federation Identity Management

Handles creation and verification of instance identity cards,
Ed25519 signing and verification, and certificate fingerprinting.

Every EOS instance has a permanent identity — an Ed25519 keypair generated
at birth, a self-signed TLS certificate for mutual authentication, and a
public identity card that can be shared with federation partners.

The identity is inviolable: no instance can modify another's identity.
The public key and certificate fingerprint are used for cryptographic
verification of federation messages.
"""

from __future__ import annotations

import hashlib
from typing import TYPE_CHECKING, Any

import structlog
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric.ed25519 import (
    Ed25519PrivateKey,
    Ed25519PublicKey,
)
from cryptography.x509 import load_pem_x509_certificate

from ecodiaos.primitives.federation import (
    InstanceIdentityCard,
    TrustPolicy,
)

if TYPE_CHECKING:
    from pathlib import Path

logger = structlog.get_logger("ecodiaos.systems.federation.identity")


class IdentityManager:
    """
    Manages this instance's identity and verifies remote identities.

    Responsibilities:
      - Build this instance's InstanceIdentityCard from config + memory
      - Load Ed25519 private key for signing outbound messages
      - Verify remote instance identity cards (signature + certificate)
      - Compute certificate fingerprints for comparison
    """

    def __init__(self) -> None:
        self._private_key: Ed25519PrivateKey | None = None
        self._public_key: Ed25519PublicKey | None = None
        self._public_key_pem: str = ""
        self._certificate_fingerprint: str = ""
        self._local_identity: InstanceIdentityCard | None = None
        self._logger = logger.bind(component="identity_manager")

    # ─── Initialization ─────────────────────────────────────────────

    async def initialize(
        self,
        instance_id: str,
        instance_name: str,
        community_context: str,
        personality_summary: str,
        autonomy_level: int,
        endpoint: str,
        capabilities: list[str],
        trust_policy: TrustPolicy,
        private_key_path: Path | None = None,
        tls_cert_path: Path | None = None,
    ) -> None:
        """
        Initialize identity from configuration and stored keys.

        If no private key exists, generate a new Ed25519 keypair.
        This happens once at instance birth.
        """
        # Load or generate Ed25519 keypair
        if private_key_path and private_key_path.exists():
            key_bytes = private_key_path.read_bytes()
            self._private_key = serialization.load_pem_private_key(
                key_bytes, password=None,
            )
            if not isinstance(self._private_key, Ed25519PrivateKey):
                raise TypeError("Federation key must be Ed25519")
            self._logger.info("identity_key_loaded", path=str(private_key_path))
        else:
            self._private_key = Ed25519PrivateKey.generate()
            self._logger.info("identity_key_generated")

            # Persist the key if a path was given
            if private_key_path:
                private_key_path.parent.mkdir(parents=True, exist_ok=True)
                key_pem = self._private_key.private_bytes(
                    encoding=serialization.Encoding.PEM,
                    format=serialization.PrivateFormat.PKCS8,
                    encryption_algorithm=serialization.NoEncryption(),
                )
                private_key_path.write_bytes(key_pem)
                self._logger.info("identity_key_persisted", path=str(private_key_path))

        # Extract public key
        self._public_key = self._private_key.public_key()
        self._public_key_pem = self._public_key.public_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PublicFormat.SubjectPublicKeyInfo,
        ).decode("utf-8")

        # Compute certificate fingerprint
        if tls_cert_path and tls_cert_path.exists():
            cert_bytes = tls_cert_path.read_bytes()
            self._certificate_fingerprint = _compute_cert_fingerprint(cert_bytes)
        else:
            # Use public key fingerprint as fallback
            self._certificate_fingerprint = _compute_key_fingerprint(
                self._public_key_pem
            )

        # Compute constitutional hash (hash of the drives for compatibility)
        constitutional_hash = hashlib.sha256(
            f"coherence:1.0|care:1.0|growth:1.0|honesty:1.0".encode()
        ).hexdigest()[:16]

        # Build local identity card
        self._local_identity = InstanceIdentityCard(
            instance_id=instance_id,
            name=instance_name,
            community_context=community_context,
            personality_summary=personality_summary,
            autonomy_level=autonomy_level,
            endpoint=endpoint,
            certificate_fingerprint=self._certificate_fingerprint,
            public_key_pem=self._public_key_pem,
            constitutional_hash=constitutional_hash,
            capabilities=capabilities,
            trust_policy=trust_policy,
        )

        self._logger.info(
            "identity_initialized",
            instance_id=instance_id,
            fingerprint=self._certificate_fingerprint[:16] + "...",
        )

    # ─── Local Identity ─────────────────────────────────────────────

    @property
    def identity_card(self) -> InstanceIdentityCard:
        """This instance's public identity card."""
        if self._local_identity is None:
            raise RuntimeError("IdentityManager not initialized")
        return self._local_identity

    @property
    def instance_id(self) -> str:
        return self.identity_card.instance_id

    # ─── Signing ────────────────────────────────────────────────────

    def sign(self, data: bytes) -> bytes:
        """Sign data with this instance's Ed25519 private key."""
        if self._private_key is None:
            raise RuntimeError("IdentityManager not initialized — no private key")
        return self._private_key.sign(data)

    # ─── Verification ───────────────────────────────────────────────

    def verify_identity(self, remote_identity: InstanceIdentityCard) -> VerificationResult:
        """
        Verify a remote instance's identity card.

        Checks:
          1. Public key is present and parseable
          2. Certificate fingerprint is present
          3. Protocol version is compatible
          4. Instance ID is non-empty

        Does NOT check certificate chain (that happens at the TLS layer).
        """
        errors: list[str] = []

        if not remote_identity.instance_id:
            errors.append("Missing instance_id")

        if not remote_identity.name:
            errors.append("Missing instance name")

        if not remote_identity.public_key_pem:
            errors.append("Missing public key")
        else:
            try:
                _parse_public_key(remote_identity.public_key_pem)
            except Exception as exc:
                errors.append(f"Invalid public key: {exc}")

        if not remote_identity.certificate_fingerprint:
            errors.append("Missing certificate fingerprint")

        if remote_identity.protocol_version != "1.0":
            errors.append(
                f"Incompatible protocol version: {remote_identity.protocol_version}"
            )

        if errors:
            return VerificationResult(
                verified=False,
                errors=errors,
                remote_instance_id=remote_identity.instance_id,
            )

        return VerificationResult(
            verified=True,
            errors=[],
            remote_instance_id=remote_identity.instance_id,
        )

    def verify_signature(
        self,
        data: bytes,
        signature: bytes,
        remote_public_key_pem: str,
    ) -> bool:
        """Verify a signature using a remote instance's public key."""
        try:
            public_key = _parse_public_key(remote_public_key_pem)
            public_key.verify(signature, data)
            return True
        except Exception:
            self._logger.warning(
                "signature_verification_failed",
                key_prefix=remote_public_key_pem[:40],
            )
            return False

    # ─── Health ─────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "initialized": self._local_identity is not None,
            "instance_id": self._local_identity.instance_id if self._local_identity else None,
            "fingerprint_prefix": self._certificate_fingerprint[:16] if self._certificate_fingerprint else None,
            "has_private_key": self._private_key is not None,
        }


# ─── Helper Types ────────────────────────────────────────────────


class VerificationResult:
    """Result of verifying a remote instance identity."""

    __slots__ = ("verified", "errors", "remote_instance_id")

    def __init__(
        self,
        verified: bool,
        errors: list[str],
        remote_instance_id: str,
    ) -> None:
        self.verified = verified
        self.errors = errors
        self.remote_instance_id = remote_instance_id


# ─── Utility Functions ───────────────────────────────────────────


def _compute_cert_fingerprint(cert_pem: bytes) -> str:
    """Compute SHA-256 fingerprint of a PEM-encoded X.509 certificate."""
    cert = load_pem_x509_certificate(cert_pem)
    digest = cert.fingerprint(hashes.SHA256())
    return digest.hex()


def _compute_key_fingerprint(public_key_pem: str) -> str:
    """Compute SHA-256 fingerprint of a public key PEM string."""
    return hashlib.sha256(public_key_pem.encode()).hexdigest()


def _parse_public_key(pem: str) -> Ed25519PublicKey:
    """Parse a PEM-encoded Ed25519 public key."""
    key = serialization.load_pem_public_key(pem.encode())
    if not isinstance(key, Ed25519PublicKey):
        raise TypeError(f"Expected Ed25519 public key, got {type(key).__name__}")
    return key

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\federation\knowledge.py =====

"""
EcodiaOS — Federation Knowledge Exchange

Orchestrates the consent-based sharing of knowledge between federated
instances. All sharing follows a request-response protocol:

1. Remote instance sends a KnowledgeRequest
2. This instance checks trust level permissions
3. Equor performs constitutional review
4. Privacy filter strips individual data
5. Filtered knowledge is returned

Sharing is NEVER automatic. Every request is individually evaluated
against trust level, constitutional alignment, and privacy constraints.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.primitives.federation import (
    FederationInteraction,
    FederationLink,
    FilteredKnowledge,
    InteractionOutcome,
    KnowledgeItem,
    KnowledgeRequest,
    KnowledgeResponse,
    KnowledgeType,
    PrivacyLevel,
    SHARING_PERMISSIONS,
    TrustLevel,
)
from ecodiaos.systems.federation.privacy import PrivacyFilter

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger("ecodiaos.systems.federation.knowledge")


class KnowledgeExchangeManager:
    """
    Manages knowledge exchange between federated instances.

    Responsibilities:
      - Validate knowledge requests against trust permissions
      - Retrieve requested knowledge from local memory
      - Apply privacy filtering before sharing
      - Track sharing statistics for audit
      - Prepare outbound knowledge requests
    """

    def __init__(
        self,
        memory: MemoryService | None = None,
        privacy_filter: PrivacyFilter | None = None,
        max_items_per_request: int = 100,
    ) -> None:
        self._memory = memory
        self._privacy_filter = privacy_filter or PrivacyFilter()
        self._max_items_per_request = max_items_per_request
        self._logger = logger.bind(component="knowledge_exchange")

        # Counters
        self._requests_received: int = 0
        self._requests_granted: int = 0
        self._requests_denied: int = 0
        self._items_shared: int = 0

    # ─── Inbound: Handle Requests from Remote Instances ─────────────

    async def handle_request(
        self,
        request: KnowledgeRequest,
        link: FederationLink,
        equor_permitted: bool = True,
    ) -> tuple[KnowledgeResponse, FederationInteraction]:
        """
        Handle an inbound knowledge request from a federated instance.

        Steps:
          1. Check trust level permits this knowledge type
          2. Verify Equor constitutional clearance
          3. Retrieve matching knowledge from memory
          4. Apply privacy filter
          5. Return filtered response

        Returns both the response and an interaction record for trust scoring.
        """
        self._requests_received += 1
        start_time = utc_now()

        # Step 1: Check trust level
        permitted_types = SHARING_PERMISSIONS.get(link.trust_level, [])
        if request.knowledge_type not in permitted_types:
            self._requests_denied += 1

            response = KnowledgeResponse(
                request_id=request.id,
                granted=False,
                reason=(
                    f"Trust level {link.trust_level.name} does not permit "
                    f"sharing {request.knowledge_type.value}."
                ),
                trust_level_required=_min_trust_for_type(request.knowledge_type),
            )

            interaction = self._build_interaction(
                link=link,
                interaction_type="knowledge_request",
                direction="inbound",
                outcome=InteractionOutcome.FAILED,
                description=f"Denied: insufficient trust for {request.knowledge_type.value}",
                start_time=start_time,
            )

            self._logger.info(
                "knowledge_request_denied_trust",
                remote_id=link.remote_instance_id,
                knowledge_type=request.knowledge_type.value,
                trust_level=link.trust_level.name,
            )

            return response, interaction

        # Step 2: Equor constitutional review
        if not equor_permitted:
            self._requests_denied += 1

            response = KnowledgeResponse(
                request_id=request.id,
                granted=False,
                reason="Constitutional review denied this knowledge share.",
            )

            interaction = self._build_interaction(
                link=link,
                interaction_type="knowledge_request",
                direction="inbound",
                outcome=InteractionOutcome.FAILED,
                description="Denied: Equor constitutional review blocked",
                start_time=start_time,
            )

            return response, interaction

        # Step 3: Retrieve knowledge from memory
        raw_items = await self._retrieve_knowledge(
            knowledge_type=request.knowledge_type,
            query=request.query,
            query_embedding=request.query_embedding,
            domain=request.domain,
            max_results=min(request.max_results, self._max_items_per_request),
        )

        # Step 4: Apply privacy filter
        filtered = await self._privacy_filter.filter(raw_items, link.trust_level)

        # Step 5: Build response
        self._requests_granted += 1
        self._items_shared += len(filtered.items)
        link.shared_knowledge_count += len(filtered.items)

        response = KnowledgeResponse(
            request_id=request.id,
            granted=True,
            knowledge=filtered.items,
            attribution=link.local_instance_id,
        )

        interaction = self._build_interaction(
            link=link,
            interaction_type="knowledge_request",
            direction="inbound",
            outcome=InteractionOutcome.SUCCESSFUL,
            description=f"Shared {len(filtered.items)} items ({request.knowledge_type.value})",
            start_time=start_time,
            trust_value=1.0,
        )

        self._logger.info(
            "knowledge_request_granted",
            remote_id=link.remote_instance_id,
            knowledge_type=request.knowledge_type.value,
            raw_items=len(raw_items),
            filtered_items=len(filtered.items),
            removed_by_privacy=filtered.items_removed_by_privacy,
        )

        return response, interaction

    # ─── Outbound: Prepare Requests to Remote Instances ─────────────

    def build_request(
        self,
        knowledge_type: KnowledgeType,
        query: str = "",
        query_embedding: list[float] | None = None,
        domain: str = "",
        max_results: int = 10,
        local_instance_id: str = "",
    ) -> KnowledgeRequest:
        """Build a knowledge request to send to a remote instance."""
        return KnowledgeRequest(
            requesting_instance_id=local_instance_id,
            knowledge_type=knowledge_type,
            query=query,
            query_embedding=query_embedding,
            domain=domain,
            max_results=max_results,
        )

    async def ingest_response(
        self,
        response: KnowledgeResponse,
        link: FederationLink,
    ) -> FederationInteraction:
        """
        Process a knowledge response received from a remote instance.

        Ingests the shared knowledge into local memory (with federation
        provenance) and records the interaction for trust scoring.
        """
        if not response.granted:
            return self._build_interaction(
                link=link,
                interaction_type="knowledge_response",
                direction="inbound",
                outcome=InteractionOutcome.FAILED,
                description=f"Remote denied: {response.reason}",
                start_time=utc_now(),
            )

        # Store received knowledge in memory with federation provenance
        if self._memory and response.knowledge:
            for item in response.knowledge:
                item.source_instance_id = response.attribution
            link.received_knowledge_count += len(response.knowledge)

        return self._build_interaction(
            link=link,
            interaction_type="knowledge_response",
            direction="inbound",
            outcome=InteractionOutcome.SUCCESSFUL,
            description=f"Received {len(response.knowledge)} items",
            start_time=utc_now(),
            trust_value=0.5,  # Receiving knowledge builds trust, but less than sharing
        )

    # ─── Knowledge Retrieval ─────────────────────────────────────────

    async def _retrieve_knowledge(
        self,
        knowledge_type: KnowledgeType,
        query: str,
        query_embedding: list[float] | None,
        domain: str,
        max_results: int,
    ) -> list[KnowledgeItem]:
        """
        Retrieve knowledge from local memory for federation sharing.

        Maps KnowledgeType to appropriate memory queries.
        """
        if self._memory is None:
            return []

        items: list[KnowledgeItem] = []

        try:
            if knowledge_type == KnowledgeType.PUBLIC_ENTITIES:
                items = await self._retrieve_public_entities(query, max_results)

            elif knowledge_type == KnowledgeType.COMMUNITY_DESCRIPTION:
                items = await self._retrieve_community_description()

            elif knowledge_type == KnowledgeType.COMMUNITY_LEVEL_KNOWLEDGE:
                items = await self._retrieve_community_knowledge(query, max_results)

            elif knowledge_type == KnowledgeType.PROCEDURES:
                items = await self._retrieve_procedures(domain, max_results)

            elif knowledge_type == KnowledgeType.HYPOTHESES:
                items = await self._retrieve_hypotheses(domain, max_results)

            elif knowledge_type == KnowledgeType.ANONYMISED_PATTERNS:
                items = await self._retrieve_patterns(domain, max_results)

            elif knowledge_type == KnowledgeType.SCHEMA_STRUCTURES:
                items = await self._retrieve_schema(max_results)

        except Exception as exc:
            self._logger.error(
                "knowledge_retrieval_failed",
                knowledge_type=knowledge_type.value,
                error=str(exc),
            )

        return items

    async def _retrieve_public_entities(
        self, query: str, max_results: int
    ) -> list[KnowledgeItem]:
        """Retrieve non-person entities from the knowledge graph."""
        # Query memory for entities, excluding person-type entities
        try:
            response = await self._memory.retrieve(
                query_text=query or "public knowledge",
                max_results=max_results,
            )
            items = []
            for trace in response.traces:
                items.append(KnowledgeItem(
                    item_id=new_id(),
                    knowledge_type=KnowledgeType.PUBLIC_ENTITIES,
                    privacy_level=PrivacyLevel.PUBLIC,
                    content={
                        "summary": trace.summary,
                        "entities": [
                            {"name": e.name, "type": e.type, "description": e.description}
                            for e in (trace.entities or [])
                            if e.type.lower() not in ("person", "individual", "member")
                        ],
                    },
                ))
            return items
        except Exception:
            return []

    async def _retrieve_community_description(self) -> list[KnowledgeItem]:
        """Retrieve the community description from the Self node."""
        try:
            self_node = await self._memory.get_self()
            if self_node is None:
                return []
            return [KnowledgeItem(
                item_id=new_id(),
                knowledge_type=KnowledgeType.COMMUNITY_DESCRIPTION,
                privacy_level=PrivacyLevel.PUBLIC,
                content={
                    "name": self_node.name,
                    "community_context": getattr(self_node, "community_context", ""),
                },
            )]
        except Exception:
            return []

    async def _retrieve_community_knowledge(
        self, query: str, max_results: int
    ) -> list[KnowledgeItem]:
        """Retrieve community-level aggregated knowledge."""
        try:
            response = await self._memory.retrieve(
                query_text=query or "community patterns",
                max_results=max_results,
            )
            items = []
            for community in (response.communities or []):
                items.append(KnowledgeItem(
                    item_id=new_id(),
                    knowledge_type=KnowledgeType.COMMUNITY_LEVEL_KNOWLEDGE,
                    privacy_level=PrivacyLevel.COMMUNITY_ONLY,
                    content={
                        "summary": community.summary,
                        "coherence_score": community.coherence_score,
                    },
                ))
            return items
        except Exception:
            return []

    async def _retrieve_procedures(
        self, domain: str, max_results: int
    ) -> list[KnowledgeItem]:
        """Retrieve learned procedures from Evo."""
        # Procedures would come from Evo's procedure store
        # For now, return empty — will be wired when Evo exposes procedures
        return []

    async def _retrieve_hypotheses(
        self, domain: str, max_results: int
    ) -> list[KnowledgeItem]:
        """Retrieve active hypotheses from Evo."""
        return []

    async def _retrieve_patterns(
        self, domain: str, max_results: int
    ) -> list[KnowledgeItem]:
        """Retrieve anonymised patterns from memory communities."""
        return await self._retrieve_community_knowledge(
            domain or "detected patterns", max_results
        )

    async def _retrieve_schema(self, max_results: int) -> list[KnowledgeItem]:
        """Retrieve schema structures (community-level semantic graph)."""
        try:
            response = await self._memory.retrieve(
                query_text="schema structure knowledge organization",
                max_results=max_results,
            )
            items = []
            for community in (response.communities or []):
                items.append(KnowledgeItem(
                    item_id=new_id(),
                    knowledge_type=KnowledgeType.SCHEMA_STRUCTURES,
                    privacy_level=PrivacyLevel.COMMUNITY_ONLY,
                    content={
                        "summary": community.summary,
                        "member_count": len(community.member_entity_ids)
                        if hasattr(community, "member_entity_ids")
                        else 0,
                    },
                ))
            return items
        except Exception:
            return []

    # ─── Helpers ─────────────────────────────────────────────────────

    def _build_interaction(
        self,
        link: FederationLink,
        interaction_type: str,
        direction: str,
        outcome: InteractionOutcome,
        description: str,
        start_time: Any,
        trust_value: float = 1.0,
    ) -> FederationInteraction:
        elapsed = utc_now() - start_time
        return FederationInteraction(
            link_id=link.id,
            remote_instance_id=link.remote_instance_id,
            interaction_type=interaction_type,
            direction=direction,
            outcome=outcome,
            description=description,
            trust_value=trust_value,
            latency_ms=int(elapsed.total_seconds() * 1000),
        )

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "requests_received": self._requests_received,
            "requests_granted": self._requests_granted,
            "requests_denied": self._requests_denied,
            "items_shared": self._items_shared,
            "privacy_filter": self._privacy_filter.stats,
        }


# ─── Helpers ─────────────────────────────────────────────────────


def _min_trust_for_type(knowledge_type: KnowledgeType) -> TrustLevel:
    """Find the minimum trust level required for a knowledge type."""
    for level in TrustLevel:
        permitted = SHARING_PERMISSIONS.get(level, [])
        if knowledge_type in permitted:
            return level
    return TrustLevel.ALLY

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\federation\privacy.py =====

"""
EcodiaOS — Federation Privacy Filter

CRITICAL: Individual private data NEVER crosses federation boundaries
without that individual's explicit consent.

The privacy filter is the last line of defence before knowledge leaves
this instance. It operates on three principles:

1. Items marked PRIVATE are always removed — no exceptions.
2. Items marked COMMUNITY_ONLY require at least COLLEAGUE trust.
3. All items are anonymised: individual identifiers (names, IDs,
   contact information) are stripped or replaced with anonymous tokens.

Even at ALLY trust level, individual data is anonymised. The federation
protocol shares patterns, procedures, and aggregate knowledge — never
individual people's information.
"""

from __future__ import annotations

import re
from typing import Any

import structlog

from ecodiaos.primitives.federation import (
    FilteredKnowledge,
    KnowledgeItem,
    PrivacyLevel,
    TrustLevel,
)

logger = structlog.get_logger("ecodiaos.systems.federation.privacy")

# Patterns for identifying personal information
_EMAIL_PATTERN = re.compile(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}")
_PHONE_PATTERN = re.compile(r"\b(?:\+?\d{1,3}[-.\s]?)?\(?\d{2,4}\)?[-.\s]?\d{3,4}[-.\s]?\d{3,4}\b")
_PERSON_NAME_MARKERS = {"person", "individual", "member", "user", "participant"}


class PrivacyFilter:
    """
    Filters knowledge for safe federation sharing.

    Every piece of knowledge passes through this filter before
    crossing the federation boundary. The filter:

    1. Removes items above the trust level's access threshold
    2. Removes items marked as private
    3. Strips individual identifiers from remaining items
    4. Tracks statistics for observability

    The privacy filter is deliberately conservative — it is better to
    over-filter than to leak private data.
    """

    def __init__(self) -> None:
        self._total_filtered: int = 0
        self._total_removed: int = 0
        self._total_anonymised: int = 0
        self._logger = logger.bind(component="privacy_filter")

    async def filter(
        self,
        items: list[KnowledgeItem],
        trust_level: TrustLevel,
    ) -> FilteredKnowledge:
        """
        Apply privacy filtering to a list of knowledge items.

        Returns a FilteredKnowledge containing only items that are
        safe to share at the given trust level, with all individual
        identifiers removed.
        """
        result = FilteredKnowledge()

        for item in items:
            # Rule 1: Private items NEVER cross boundaries
            if item.privacy_level == PrivacyLevel.PRIVATE:
                result.items_removed_by_privacy += 1
                continue

            # Rule 2: Community-only items require COLLEAGUE+
            if item.privacy_level == PrivacyLevel.COMMUNITY_ONLY:
                if trust_level.value < TrustLevel.COLLEAGUE.value:
                    result.items_removed_by_privacy += 1
                    continue

            # Rule 3: Anonymise all remaining items
            anonymised_item = self._anonymise(item)
            if anonymised_item is not item:
                result.items_anonymised += 1

            result.items.append(anonymised_item)

        self._total_filtered += len(items)
        self._total_removed += result.items_removed_by_privacy
        self._total_anonymised += result.items_anonymised

        self._logger.debug(
            "privacy_filter_applied",
            input_count=len(items),
            output_count=len(result.items),
            removed=result.items_removed_by_privacy,
            anonymised=result.items_anonymised,
            trust_level=trust_level.name,
        )

        return result

    # ─── Anonymisation ──────────────────────────────────────────────

    def _anonymise(self, item: KnowledgeItem) -> KnowledgeItem:
        """
        Remove individual identifiers from a knowledge item.

        Operates on the content dict, stripping:
          - Email addresses
          - Phone numbers
          - Fields that look like personal names
          - Fields named 'user_id', 'person_id', 'member_id', etc.
        """
        content = item.content
        if not content:
            return item

        anonymised_content = _deep_anonymise(content)

        if anonymised_content is content:
            return item  # No changes needed

        return KnowledgeItem(
            item_id=item.item_id,
            knowledge_type=item.knowledge_type,
            privacy_level=item.privacy_level,
            content=anonymised_content,
            embedding=item.embedding,
            source_instance_id=item.source_instance_id,
            created_at=item.created_at,
        )

    # ─── Stats ──────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "total_filtered": self._total_filtered,
            "total_removed": self._total_removed,
            "total_anonymised": self._total_anonymised,
        }


# ─── Deep Anonymisation ─────────────────────────────────────────

# Keys that likely contain personal identifiers
_PII_KEYS = {
    "name", "full_name", "first_name", "last_name", "user_name", "username",
    "email", "email_address", "phone", "phone_number", "address",
    "user_id", "person_id", "member_id", "participant_id", "individual_id",
    "ssn", "social_security", "date_of_birth", "dob", "birthday",
    "contact", "contact_info",
}


def _deep_anonymise(obj: Any, _depth: int = 0) -> Any:
    """
    Recursively anonymise a nested dict/list structure.

    Strips PII keys, redacts email/phone patterns in string values,
    and recurses into nested structures up to a reasonable depth.
    """
    if _depth > 10:
        return obj

    if isinstance(obj, dict):
        result = {}
        changed = False
        for key, value in obj.items():
            key_lower = key.lower().replace("-", "_")

            # Remove known PII keys entirely
            if key_lower in _PII_KEYS:
                result[key] = "[REDACTED]"
                changed = True
                continue

            # Check for entity type markers that suggest person data
            if key_lower == "type" and isinstance(value, str) and value.lower() in _PERSON_NAME_MARKERS:
                result[key] = "anonymous_entity"
                changed = True
                continue

            # Recurse into nested values
            new_value = _deep_anonymise(value, _depth + 1)
            if new_value is not value:
                changed = True
            result[key] = new_value

        return result if changed else obj

    if isinstance(obj, list):
        new_list = [_deep_anonymise(item, _depth + 1) for item in obj]
        if any(new is not old for new, old in zip(new_list, obj)):
            return new_list
        return obj

    if isinstance(obj, str):
        return _anonymise_string(obj)

    return obj


def _anonymise_string(text: str) -> str:
    """Redact email addresses and phone numbers from a string."""
    result = text
    changed = False

    if _EMAIL_PATTERN.search(result):
        result = _EMAIL_PATTERN.sub("[email-redacted]", result)
        changed = True

    if _PHONE_PATTERN.search(result):
        result = _PHONE_PATTERN.sub("[phone-redacted]", result)
        changed = True

    return result if changed else text

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\federation\service.py =====

"""
EcodiaOS — Federation Service

The Federation Protocol governs how EOS instances relate to each other —
as sovereign entities that can choose to share knowledge, coordinate action,
and build relationships. Not a hive mind. Not isolated. A community of
individuals, each with their own identity, personality, and community,
choosing to help each other grow.

The FederationService orchestrates five sub-systems:
  IdentityManager   — Instance identity cards, Ed25519 signing, verification
  TrustManager      — Trust scoring, level transitions, decay
  PrivacyFilter     — PII removal, consent enforcement
  KnowledgeExchange — Knowledge request/response protocol
  CoordinationMgr   — Assistance requests and coordinated action
  ChannelManager    — Mutual TLS channels to remote instances

Lifecycle:
  initialize()             — build all sub-systems, load keys
  establish_link()         — connect to a remote instance
  withdraw_link()          — disconnect from a remote instance
  handle_knowledge_req()   — handle inbound knowledge request
  request_knowledge()      — request knowledge from remote
  handle_assistance_req()  — handle inbound assistance request
  request_assistance()     — request assistance from remote
  shutdown()               — graceful shutdown

Performance targets:
  Identity verification: ≤500ms
  Knowledge request handling: ≤2000ms
  Trust update: ≤50ms
  Link establishment: ≤3000ms
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.config import FederationConfig
from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.primitives.federation import (
    AssistanceRequest,
    AssistanceResponse,
    FederationInteraction,
    FederationLink,
    FederationLinkStatus,
    InstanceIdentityCard,
    InteractionOutcome,
    KnowledgeRequest,
    KnowledgeResponse,
    KnowledgeType,
    TrustLevel,
    TrustPolicy,
)
from ecodiaos.systems.federation.channel import ChannelManager
from ecodiaos.systems.federation.coordination import CoordinationManager
from ecodiaos.systems.federation.identity import IdentityManager
from ecodiaos.systems.federation.knowledge import KnowledgeExchangeManager
from ecodiaos.systems.federation.privacy import PrivacyFilter
from ecodiaos.systems.federation.trust import TrustManager

if TYPE_CHECKING:
    from ecodiaos.clients.redis import RedisClient
    from ecodiaos.systems.equor.service import EquorService
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.telemetry.metrics import MetricCollector

logger = structlog.get_logger("ecodiaos.systems.federation")


class FederationService:
    """
    Federation — the EOS diplomatic system.

    Coordinates identity, trust, knowledge exchange, coordinated action,
    and privacy filtering across federation links with other EOS instances.

    Every federation action goes through Equor. The constitutional drives
    apply to inter-instance relations just as they do to individual
    interactions. An instance cannot be helpful to a federation partner
    at the expense of its own community (Care drive).
    """

    system_id: str = "federation"

    def __init__(
        self,
        config: FederationConfig,
        memory: MemoryService | None = None,
        equor: EquorService | None = None,
        redis: RedisClient | None = None,
        metrics: MetricCollector | None = None,
        instance_id: str = "",
    ) -> None:
        self._config = config
        self._memory = memory
        self._equor = equor
        self._redis = redis
        self._metrics = metrics
        self._instance_id = instance_id
        self._logger = logger.bind(system="federation")
        self._initialized: bool = False
        self._atune: Any = None  # Wired post-init for perception of federated knowledge

        # Sub-systems (built in initialize())
        self._identity: IdentityManager | None = None
        self._trust: TrustManager | None = None
        self._privacy: PrivacyFilter | None = None
        self._knowledge: KnowledgeExchangeManager | None = None
        self._coordination: CoordinationManager | None = None
        self._channels: ChannelManager | None = None

        # Active federation links (link_id → FederationLink)
        self._links: dict[str, FederationLink] = {}

        # Interaction history (for audit, limited ring buffer)
        self._interaction_history: list[FederationInteraction] = []
        self._max_history: int = 1000

    def set_atune(self, atune: Any) -> None:
        """Wire Atune so federated knowledge becomes perceived input."""
        self._atune = atune
        self._logger.info("atune_wired_to_federation")

    # ─── Lifecycle ──────────────────────────────────────────────────

    async def initialize(self) -> None:
        """Build all sub-systems and load identity keys."""
        if self._initialized:
            return

        if not self._config.enabled:
            self._logger.info("federation_disabled")
            self._initialized = True
            return

        # Build sub-systems
        trust_max = TrustLevel(min(self._config.max_trust_level, 4))
        self._trust = TrustManager(
            trust_decay_enabled=self._config.trust_decay_enabled,
            trust_decay_rate_per_day=self._config.trust_decay_rate_per_day,
            max_trust_level=trust_max,
        )

        self._privacy = PrivacyFilter()

        self._knowledge = KnowledgeExchangeManager(
            memory=self._memory,
            privacy_filter=self._privacy,
            max_items_per_request=self._config.max_knowledge_items_per_request,
        )

        self._coordination = CoordinationManager()

        # Channel manager with TLS configuration
        tls_cert = Path(self._config.tls_cert_path) if self._config.tls_cert_path else None
        tls_key = Path(self._config.tls_key_path) if self._config.tls_key_path else None
        ca_cert = Path(self._config.ca_cert_path) if self._config.ca_cert_path else None

        self._channels = ChannelManager(
            tls_cert_path=tls_cert,
            tls_key_path=tls_key,
            ca_cert_path=ca_cert,
        )

        # Identity manager — load/generate keys and build identity card
        self._identity = IdentityManager()

        # Gather instance info from memory for the identity card
        instance_name = "EOS"
        community_context = ""
        personality_summary = ""
        autonomy_level = 1

        if self._memory:
            try:
                self_node = await self._memory.get_self()
                if self_node:
                    instance_name = self_node.name
                    community_context = getattr(self_node, "community_context", "")
                    autonomy_level = getattr(self_node, "autonomy_level", 1)
            except Exception:
                pass

        private_key_path = (
            Path(self._config.private_key_path) if self._config.private_key_path else None
        )

        trust_policy = TrustPolicy(
            auto_accept_links=self._config.auto_accept_links,
            trust_decay_enabled=self._config.trust_decay_enabled,
            trust_decay_rate_per_day=self._config.trust_decay_rate_per_day,
            max_trust_level=trust_max,
        )

        await self._identity.initialize(
            instance_id=self._instance_id,
            instance_name=instance_name,
            community_context=community_context,
            personality_summary=personality_summary,
            autonomy_level=autonomy_level,
            endpoint=self._config.endpoint or "",
            capabilities=["knowledge_exchange", "coordinated_action"],
            trust_policy=trust_policy,
            private_key_path=private_key_path,
            tls_cert_path=tls_cert,
        )

        # Load persisted links from Redis
        await self._load_links()

        self._initialized = True
        self._logger.info(
            "federation_initialized",
            instance_id=self._instance_id,
            enabled=True,
            endpoint=self._config.endpoint,
            active_links=len(self._links),
        )

    async def shutdown(self) -> None:
        """Graceful shutdown — close all channels, persist link state."""
        self._logger.info("federation_shutting_down")

        # Persist link state
        await self._persist_links()

        # Close all channels
        if self._channels:
            await self._channels.close_all()

        self._logger.info(
            "federation_shutdown_complete",
            active_links=len(self._links),
            total_interactions=len(self._interaction_history),
        )

    # ─── Link Management ────────────────────────────────────────────

    async def establish_link(
        self, remote_endpoint: str
    ) -> dict[str, Any]:
        """
        Establish a new federation link with a remote instance.

        Steps:
          1. Fetch remote identity card
          2. Verify remote identity
          3. Equor constitutional review
          4. Create link with NONE trust
          5. Open communication channel
          6. Exchange greetings

        Performance target: ≤3000ms
        """
        if not self._config.enabled:
            return {"error": "Federation is disabled"}

        if not self._identity or not self._channels or not self._trust:
            return {"error": "Federation not initialized"}

        start = utc_now()

        # Check max links
        active_count = sum(
            1 for l in self._links.values()
            if l.status == FederationLinkStatus.ACTIVE
        )
        if active_count >= self._config.max_concurrent_links:
            return {"error": f"Maximum concurrent links ({self._config.max_concurrent_links}) reached"}

        # Step 1: Create a temporary link for the channel
        temp_link = FederationLink(
            local_instance_id=self._instance_id,
            remote_instance_id="unknown",
            remote_endpoint=remote_endpoint,
            status=FederationLinkStatus.PENDING,
        )

        # Step 2: Open channel and fetch remote identity
        try:
            channel = await self._channels.open_channel(temp_link)
            remote_identity = await channel.get_identity()

            if remote_identity is None:
                await self._channels.close_channel(temp_link.id)
                return {"error": "Could not fetch remote identity card"}
        except Exception as exc:
            return {"error": f"Connection failed: {exc}"}

        # Step 3: Verify identity
        verification = self._identity.verify_identity(remote_identity)
        if not verification.verified:
            await self._channels.close_channel(temp_link.id)
            return {"error": f"Identity verification failed: {verification.errors}"}

        # Step 4: Check for duplicate link
        for existing in self._links.values():
            if (
                existing.remote_instance_id == remote_identity.instance_id
                and existing.status == FederationLinkStatus.ACTIVE
            ):
                await self._channels.close_channel(temp_link.id)
                return {
                    "error": "Already linked to this instance",
                    "existing_link_id": existing.id,
                }

        # Step 5: Equor constitutional review
        equor_permitted = True
        if self._equor:
            try:
                # Build a lightweight intent for federation link review
                from ecodiaos.primitives.intent import (
                    Intent, GoalDescriptor, ActionSequence, DecisionTrace,
                )
                intent = Intent(
                    goal=GoalDescriptor(
                        description=f"Establish federation link with {remote_identity.name}",
                        target_domain="federation",
                    ),
                    plan=ActionSequence(steps=[]),
                    decision_trace=DecisionTrace(
                        reasoning=f"Federation link request from {remote_identity.name} "
                                  f"({remote_identity.community_context[:100]})",
                    ),
                )
                check = await self._equor.review(intent)
                from ecodiaos.primitives.common import Verdict
                equor_permitted = check.verdict in (Verdict.APPROVED, Verdict.MODIFIED)
            except Exception as exc:
                self._logger.warning("equor_review_failed", error=str(exc))

        if not equor_permitted:
            await self._channels.close_channel(temp_link.id)
            return {"error": "Constitutional review denied this federation link"}

        # Step 6: Create the official link
        link = FederationLink(
            local_instance_id=self._instance_id,
            remote_instance_id=remote_identity.instance_id,
            remote_name=remote_identity.name,
            remote_endpoint=remote_endpoint,
            trust_level=TrustLevel.NONE,
            trust_score=0.0,
            status=FederationLinkStatus.ACTIVE,
            remote_identity=remote_identity,
        )

        # Re-open channel with the real link
        await self._channels.close_channel(temp_link.id)
        await self._channels.open_channel(link)

        self._links[link.id] = link
        await self._persist_links()

        elapsed_ms = int((utc_now() - start).total_seconds() * 1000)

        # Record interaction
        interaction = FederationInteraction(
            link_id=link.id,
            remote_instance_id=link.remote_instance_id,
            interaction_type="link_establishment",
            direction="outbound",
            outcome=InteractionOutcome.SUCCESSFUL,
            description=f"Link established with {remote_identity.name}",
            trust_value=1.0,
            latency_ms=elapsed_ms,
        )
        self._record_interaction(interaction)

        # Update trust for the successful establishment
        self._trust.update_trust(link, interaction)

        # Record metric
        if self._metrics:
            await self._metrics.record("federation", "links.established", 1.0)

        self._logger.info(
            "link_established",
            link_id=link.id,
            remote_id=remote_identity.instance_id,
            remote_name=remote_identity.name,
            elapsed_ms=elapsed_ms,
        )

        return {
            "link_id": link.id,
            "remote_instance_id": remote_identity.instance_id,
            "remote_name": remote_identity.name,
            "trust_level": link.trust_level.name,
            "status": link.status.value,
            "elapsed_ms": elapsed_ms,
        }

    async def withdraw_link(self, link_id: str) -> dict[str, Any]:
        """
        Withdraw from a federation link.

        Withdrawal is always free — any instance can disconnect at any
        time with no penalty. This ensures federation is always
        voluntary, never coerced.
        """
        link = self._links.get(link_id)
        if not link:
            return {"error": "Link not found"}

        link.status = FederationLinkStatus.WITHDRAWN

        # Close the channel
        if self._channels:
            await self._channels.close_channel(link_id)

        await self._persist_links()

        if self._metrics:
            await self._metrics.record("federation", "links.dropped", 1.0)

        self._logger.info(
            "link_withdrawn",
            link_id=link_id,
            remote_id=link.remote_instance_id,
        )

        return {
            "link_id": link_id,
            "status": "withdrawn",
            "remote_instance_id": link.remote_instance_id,
        }

    # ─── Knowledge Exchange ─────────────────────────────────────────

    async def handle_knowledge_request(
        self,
        request: KnowledgeRequest,
    ) -> KnowledgeResponse:
        """
        Handle an inbound knowledge request from a remote instance.

        This is called by the API router when a federated instance
        sends a knowledge request to us.
        """
        if not self._knowledge or not self._trust:
            return KnowledgeResponse(
                request_id=request.id,
                granted=False,
                reason="Federation not initialized",
            )

        # Find the link for this requesting instance
        link = self._find_link_by_instance(request.requesting_instance_id)
        if not link:
            return KnowledgeResponse(
                request_id=request.id,
                granted=False,
                reason="No active federation link with this instance",
            )

        # Equor review
        equor_permitted = True
        if self._equor:
            try:
                from ecodiaos.primitives.intent import (
                    Intent, GoalDescriptor, ActionSequence, DecisionTrace,
                )
                intent = Intent(
                    goal=GoalDescriptor(
                        description=f"Share {request.knowledge_type.value} with {link.remote_name}",
                        target_domain="federation.knowledge",
                    ),
                    plan=ActionSequence(steps=[]),
                    decision_trace=DecisionTrace(
                        reasoning=f"Knowledge request from {link.remote_name}: {request.query[:100]}",
                    ),
                )
                check = await self._equor.review(intent)
                from ecodiaos.primitives.common import Verdict
                equor_permitted = check.verdict in (Verdict.APPROVED, Verdict.MODIFIED)
            except Exception:
                pass

        response, interaction = await self._knowledge.handle_request(
            request=request,
            link=link,
            equor_permitted=equor_permitted,
        )

        # Update trust
        self._trust.update_trust(link, interaction)
        self._record_interaction(interaction)

        # Metrics
        if self._metrics:
            metric_name = "knowledge.shared" if response.granted else "knowledge.denied"
            await self._metrics.record("federation", metric_name, 1.0)
            if response.granted:
                await self._metrics.record(
                    "federation", "privacy.items_filtered",
                    float(len(response.knowledge)),
                )

        return response

    async def request_knowledge(
        self,
        link_id: str,
        knowledge_type: KnowledgeType,
        query: str = "",
        max_results: int = 10,
    ) -> KnowledgeResponse | None:
        """
        Request knowledge from a remote federated instance.
        """
        if not self._knowledge or not self._channels or not self._identity:
            return None

        link = self._links.get(link_id)
        if not link or link.status != FederationLinkStatus.ACTIVE:
            return None

        channel = self._channels.get_channel(link_id)
        if not channel:
            return None

        request = self._knowledge.build_request(
            knowledge_type=knowledge_type,
            query=query,
            max_results=max_results,
            local_instance_id=self._instance_id,
        )

        response = await channel.request_knowledge(request)

        if response:
            interaction = await self._knowledge.ingest_response(response, link)
            if self._trust:
                self._trust.update_trust(link, interaction)
            self._record_interaction(interaction)

            # Feed received knowledge to Atune as a perceived input
            if response.granted and response.knowledge and self._atune is not None:
                await self._inject_federated_percept(response.knowledge, link)

        return response

    # ─── Coordinated Action ─────────────────────────────────────────

    async def handle_assistance_request(
        self,
        request: AssistanceRequest,
    ) -> AssistanceResponse:
        """
        Handle an inbound assistance request from a remote instance.
        """
        if not self._coordination or not self._trust:
            return AssistanceResponse(
                request_id=request.id,
                accepted=False,
                reason="Federation not initialized",
            )

        link = self._find_link_by_instance(request.requesting_instance_id)
        if not link:
            return AssistanceResponse(
                request_id=request.id,
                accepted=False,
                reason="No active federation link with this instance",
            )

        # Equor review
        equor_permitted = True
        if self._equor:
            try:
                from ecodiaos.primitives.intent import (
                    Intent, GoalDescriptor, ActionSequence, DecisionTrace,
                )
                intent = Intent(
                    goal=GoalDescriptor(
                        description=f"Assist {link.remote_name}: {request.description[:100]}",
                        target_domain="federation.assistance",
                    ),
                    plan=ActionSequence(steps=[]),
                    decision_trace=DecisionTrace(
                        reasoning=f"Assistance request from {link.remote_name}",
                    ),
                )
                check = await self._equor.review(intent)
                from ecodiaos.primitives.common import Verdict
                equor_permitted = check.verdict in (Verdict.APPROVED, Verdict.MODIFIED)
            except Exception:
                pass

        response, interaction = await self._coordination.handle_request(
            request=request,
            link=link,
            equor_permitted=equor_permitted,
        )

        self._trust.update_trust(link, interaction)
        self._record_interaction(interaction)

        if self._metrics:
            metric = "assistance.accepted" if response.accepted else "assistance.requested"
            await self._metrics.record("federation", metric, 1.0)

        return response

    async def request_assistance(
        self,
        link_id: str,
        description: str,
        knowledge_domain: str = "",
        urgency: float = 0.5,
    ) -> AssistanceResponse | None:
        """
        Request assistance from a remote federated instance.
        """
        if not self._coordination or not self._channels or not self._identity:
            return None

        link = self._links.get(link_id)
        if not link or link.status != FederationLinkStatus.ACTIVE:
            return None

        channel = self._channels.get_channel(link_id)
        if not channel:
            return None

        request = self._coordination.build_request(
            description=description,
            knowledge_domain=knowledge_domain,
            urgency=urgency,
            local_instance_id=self._instance_id,
        )

        return await channel.request_assistance(request)

    # ─── Identity ───────────────────────────────────────────────────

    @property
    def identity_card(self) -> InstanceIdentityCard | None:
        """This instance's public identity card."""
        if self._identity:
            return self._identity.identity_card
        return None

    # ─── Link Queries ───────────────────────────────────────────────

    @property
    def active_links(self) -> list[FederationLink]:
        """All active federation links."""
        return [
            l for l in self._links.values()
            if l.status == FederationLinkStatus.ACTIVE
        ]

    def get_link(self, link_id: str) -> FederationLink | None:
        return self._links.get(link_id)

    def get_link_by_instance(self, instance_id: str) -> FederationLink | None:
        return self._find_link_by_instance(instance_id)

    # ─── Trust Decay (called periodically) ──────────────────────────

    async def apply_trust_decay(self) -> None:
        """Apply trust decay to all active links (call periodically)."""
        if not self._trust:
            return
        for link in self.active_links:
            self._trust.apply_decay(link)

    # ─── Health ─────────────────────────────────────────────────────

    async def health(self) -> dict[str, Any]:
        """Self-health report (implements ManagedSystem protocol)."""
        if not self._config.enabled:
            return {
                "status": "disabled",
                "enabled": False,
                "active_links": 0,
                "mean_trust": 0.0,
                "total_interactions": 0,
            }

        active = self.active_links
        return {
            "status": "healthy" if self._initialized else "starting",
            "enabled": True,
            "active_links": len(active),
            "mean_trust": self._trust.mean_trust(active) if self._trust and active else 0.0,
            "total_interactions": len(self._interaction_history),
        }

    @property
    def stats(self) -> dict[str, Any]:
        active = self.active_links
        return {
            "initialized": self._initialized,
            "enabled": self._config.enabled,
            "instance_id": self._instance_id,
            "active_links": len(active),
            "total_links": len(self._links),
            "mean_trust": round(
                self._trust.mean_trust(active) if self._trust and active else 0.0, 2
            ),
            "identity": self._identity.stats if self._identity else {},
            "trust": self._trust.stats if self._trust else {},
            "knowledge": self._knowledge.stats if self._knowledge else {},
            "coordination": self._coordination.stats if self._coordination else {},
            "channels": self._channels.stats if self._channels else {},
            "privacy": self._privacy.stats if self._privacy else {},
            "interaction_history_size": len(self._interaction_history),
            "links": [
                {
                    "id": l.id,
                    "remote_id": l.remote_instance_id,
                    "remote_name": l.remote_name,
                    "trust_level": l.trust_level.name,
                    "trust_score": round(l.trust_score, 2),
                    "status": l.status.value,
                    "shared_count": l.shared_knowledge_count,
                    "received_count": l.received_knowledge_count,
                    "successful": l.successful_interactions,
                    "failed": l.failed_interactions,
                    "violations": l.violation_count,
                }
                for l in self._links.values()
            ],
        }

    # ─── Internal ───────────────────────────────────────────────────

    def _find_link_by_instance(self, instance_id: str) -> FederationLink | None:
        """Find an active link for a given remote instance ID."""
        for link in self._links.values():
            if (
                link.remote_instance_id == instance_id
                and link.status == FederationLinkStatus.ACTIVE
            ):
                return link
        return None

    def _record_interaction(self, interaction: FederationInteraction) -> None:
        """Record an interaction in the history ring buffer."""
        self._interaction_history.append(interaction)
        if len(self._interaction_history) > self._max_history:
            self._interaction_history = self._interaction_history[-self._max_history:]

    async def _inject_federated_percept(self, knowledge_items: list, link: Any) -> None:
        """
        Inject federated knowledge into Atune as perceived input.

        The organism should perceive knowledge from its peers — otherwise
        federation data stops at Memory and never enters the cognitive cycle.
        """
        try:
            from ecodiaos.systems.atune.types import InputChannel, RawInput

            summaries: list[str] = []
            for item in knowledge_items[:3]:  # Cap at 3 to avoid flooding
                content = getattr(item, "content", "") or getattr(item, "summary", "")
                if content:
                    summaries.append(str(content)[:200])
            if not summaries:
                return

            raw = RawInput(
                data=(
                    f"Knowledge from {getattr(link, 'remote_name', 'peer')}: "
                    f"{'; '.join(summaries)}"
                ),
                channel_id=f"federation:{getattr(link, 'id', 'unknown')}",
                metadata={"source_instance": getattr(link, "remote_instance_id", "")},
            )
            await self._atune.ingest(raw, InputChannel.FEDERATION_MSG)
            self._logger.debug("federated_knowledge_injected_to_atune")
        except Exception:
            self._logger.debug("federation_atune_ingest_failed", exc_info=True)

    async def _persist_links(self) -> None:
        """Persist link state to Redis (primary) with local file fallback."""
        # Always write local backup regardless of Redis availability
        self._persist_links_to_file()

        if not self._redis:
            return
        try:
            links_data = {
                link_id: link.model_dump_json()
                for link_id, link in self._links.items()
            }
            for link_id, data in links_data.items():
                await self._redis.set(
                    f"fed:links:{link_id}",
                    data,
                    ttl=None,  # Persistent
                )
            # Store the link ID index
            await self._redis.set(
                "fed:link_ids",
                ",".join(self._links.keys()),
                ttl=None,
            )
        except Exception as exc:
            self._logger.warning(
                "link_persist_redis_failed_local_backup_written",
                error=str(exc),
            )

    async def _load_links(self) -> None:
        """Load persisted links: try Redis first, fall back to local file."""
        loaded = False

        if self._redis:
            try:
                link_ids_raw = await self._redis.get("fed:link_ids")
                if link_ids_raw:
                    link_ids = link_ids_raw.split(",") if link_ids_raw else []
                    for link_id in link_ids:
                        if not link_id:
                            continue
                        data = await self._redis.get(f"fed:links:{link_id}")
                        if data:
                            link = FederationLink.model_validate_json(data)
                            self._links[link.id] = link
                    loaded = bool(self._links)
            except Exception as exc:
                self._logger.warning("link_load_redis_failed", error=str(exc))

        # Fall back to local file if Redis was empty or unavailable
        if not loaded:
            self._load_links_from_file()

        self._logger.info("links_loaded", count=len(self._links), source="redis" if loaded else "file")

    def _persist_links_to_file(self) -> None:
        """Write link state to a local JSON file as a backup."""
        try:
            backup_path = Path(self._config.data_dir or ".") / "federation_links.json"
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            data = {
                link_id: link.model_dump_json()
                for link_id, link in self._links.items()
            }
            backup_path.write_text(json.dumps(data), encoding="utf-8")
        except Exception as exc:
            self._logger.debug("link_file_backup_failed", error=str(exc))

    def _load_links_from_file(self) -> None:
        """Restore link state from local JSON backup file."""
        try:
            backup_path = Path(self._config.data_dir or ".") / "federation_links.json"
            if not backup_path.exists():
                return
            raw = json.loads(backup_path.read_text(encoding="utf-8"))
            for link_id, link_json in raw.items():
                link = FederationLink.model_validate_json(link_json)
                self._links[link.id] = link
        except Exception as exc:
            self._logger.debug("link_file_restore_failed", error=str(exc))

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\federation\trust.py =====

"""
EcodiaOS — Federation Trust Model

Trust between EOS instances starts at zero and builds through successful
interaction. Violations cost 3x; a privacy breach resets trust to zero
immediately.

The trust model implements graduated trust levels (NONE → ACQUAINTANCE →
COLLEAGUE → PARTNER → ALLY) with score thresholds. Trust also decays over
time for inactive links, preventing stale trust from persisting.

This is not a reputation system — it is a direct relationship model.
Each link tracks its own trust independently. Trust is earned by this
specific pair of instances, not inherited from the network.
"""

from __future__ import annotations

from datetime import datetime, timezone
from typing import Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.primitives.federation import (
    FederationInteraction,
    FederationLink,
    InteractionOutcome,
    TRUST_THRESHOLDS,
    TrustLevel,
    VIOLATION_MULTIPLIER,
    ViolationType,
)

logger = structlog.get_logger("ecodiaos.systems.federation.trust")


class TrustManager:
    """
    Manages trust scoring and level transitions for federation links.

    Trust is a float score that maps to discrete TrustLevel values via
    thresholds. Successful interactions increment the score; failures
    decrement at a multiplied rate. Privacy breaches are catastrophic
    — immediate reset to zero.

    Trust also decays over time for inactive links, modelling the
    realistic principle that trust requires ongoing interaction.
    """

    def __init__(
        self,
        trust_decay_enabled: bool = True,
        trust_decay_rate_per_day: float = 0.1,
        max_trust_level: TrustLevel = TrustLevel.ALLY,
    ) -> None:
        self._trust_decay_enabled = trust_decay_enabled
        self._trust_decay_rate_per_day = trust_decay_rate_per_day
        self._max_trust_level = max_trust_level
        self._logger = logger.bind(component="trust_manager")

    # ─── Trust Updates ──────────────────────────────────────────────

    def update_trust(
        self,
        link: FederationLink,
        interaction: FederationInteraction,
    ) -> TrustLevel:
        """
        Update trust score and level based on an interaction outcome.

        Rules (from the spec):
          - Successful interactions: +trust_value
          - Failed interactions: -trust_value (1x)
          - Violations: -trust_value * 3x
          - Privacy breach violations: instant reset to 0, level to NONE

        Returns the new TrustLevel.
        """
        previous_level = link.trust_level
        previous_score = link.trust_score

        if interaction.outcome == InteractionOutcome.SUCCESSFUL:
            link.trust_score += interaction.trust_value
            link.successful_interactions += 1

        elif interaction.outcome == InteractionOutcome.FAILED:
            link.trust_score = max(0.0, link.trust_score - interaction.trust_value)
            link.failed_interactions += 1

        elif interaction.outcome == InteractionOutcome.VIOLATION:
            link.violation_count += 1
            link.failed_interactions += 1

            if interaction.violation_type == ViolationType.PRIVACY_BREACH:
                # Privacy breaches are catastrophic — instant zero
                link.trust_score = 0.0
                link.trust_level = TrustLevel.NONE
                self._logger.warning(
                    "trust_privacy_breach_reset",
                    link_id=link.id,
                    remote_id=link.remote_instance_id,
                )
                return TrustLevel.NONE
            else:
                # Other violations cost 3x
                penalty = interaction.trust_value * VIOLATION_MULTIPLIER
                link.trust_score = max(0.0, link.trust_score - penalty)

        elif interaction.outcome == InteractionOutcome.TIMEOUT:
            # Timeouts are mild — half penalty
            link.trust_score = max(
                0.0, link.trust_score - interaction.trust_value * 0.5
            )
            link.failed_interactions += 1

        # Recompute trust level from score
        link.trust_level = self._score_to_level(link.trust_score)

        # Cap at max allowed level
        if link.trust_level.value > self._max_trust_level.value:
            link.trust_level = self._max_trust_level

        # Update communication timestamp
        link.last_communication = utc_now()

        if link.trust_level != previous_level:
            self._logger.info(
                "trust_level_changed",
                link_id=link.id,
                remote_id=link.remote_instance_id,
                previous=previous_level.name,
                new=link.trust_level.name,
                score=round(link.trust_score, 2),
                delta=round(link.trust_score - previous_score, 2),
            )

        return link.trust_level

    # ─── Trust Decay ────────────────────────────────────────────────

    def apply_decay(self, link: FederationLink) -> None:
        """
        Apply time-based trust decay for inactive links.

        Trust decays linearly based on days since last communication.
        This prevents stale trust from persisting and encourages
        ongoing interaction.
        """
        if not self._trust_decay_enabled:
            return

        if link.last_communication is None:
            return

        now = utc_now()
        delta = now - link.last_communication
        days_inactive = delta.total_seconds() / 86400.0

        if days_inactive <= 1.0:
            return  # No decay within 24 hours

        decay = days_inactive * self._trust_decay_rate_per_day
        previous_score = link.trust_score
        link.trust_score = max(0.0, link.trust_score - decay)
        link.trust_level = self._score_to_level(link.trust_score)

        if link.trust_level.value < self._score_to_level(previous_score).value:
            self._logger.info(
                "trust_decayed",
                link_id=link.id,
                remote_id=link.remote_instance_id,
                days_inactive=round(days_inactive, 1),
                decay_applied=round(decay, 2),
                new_level=link.trust_level.name,
            )

    # ─── Trust Queries ──────────────────────────────────────────────

    def can_share_knowledge_type(
        self, link: FederationLink, knowledge_type: str
    ) -> bool:
        """Check if the current trust level permits sharing a knowledge type."""
        from ecodiaos.primitives.federation import KnowledgeType, SHARING_PERMISSIONS

        try:
            kt = KnowledgeType(knowledge_type)
        except ValueError:
            return False

        permitted = SHARING_PERMISSIONS.get(link.trust_level, [])
        return kt in permitted

    def can_coordinate(self, link: FederationLink) -> bool:
        """Check if the link has sufficient trust for coordinated action."""
        return link.trust_level.value >= TrustLevel.COLLEAGUE.value

    def mean_trust(self, links: list[FederationLink]) -> float:
        """Compute mean trust score across all active links."""
        if not links:
            return 0.0
        return sum(l.trust_score for l in links) / len(links)

    # ─── Internal ───────────────────────────────────────────────────

    @staticmethod
    def _score_to_level(score: float) -> TrustLevel:
        """
        Map a trust score to the highest applicable TrustLevel.

        Thresholds (from spec):
          ACQUAINTANCE: 5
          COLLEAGUE: 20
          PARTNER: 50
          ALLY: 100
        """
        # Iterate from highest to lowest threshold
        for level in sorted(TRUST_THRESHOLDS.keys(), key=lambda l: l.value, reverse=True):
            if score >= TRUST_THRESHOLDS[level]:
                return level
        return TrustLevel.NONE

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "decay_enabled": self._trust_decay_enabled,
            "decay_rate_per_day": self._trust_decay_rate_per_day,
            "max_trust_level": self._max_trust_level.name,
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\memory\__init__.py =====

"""
EcodiaOS — Memory & Identity Core

The substrate of selfhood. Every other system reads from and writes to this.
"""

from ecodiaos.systems.memory.service import MemoryService

__all__ = ["MemoryService"]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\memory\birth.py =====

"""
EcodiaOS — Instance Birth (Seeding)

Creates the foundational graph structure for a new EOS instance:
the Self node, the Constitution, and any initial entities from the seed.

This is a one-time operation. Once born, the instance exists.
"""

from __future__ import annotations

import json

import structlog

from ecodiaos.clients.embedding import EmbeddingClient
from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.config import SeedConfig
from ecodiaos.primitives import AffectState, new_id, utc_now

logger = structlog.get_logger()


async def birth_instance(
    neo4j: Neo4jClient,
    embedding_client: EmbeddingClient,
    seed: SeedConfig,
    instance_id: str,
) -> dict:
    """
    Birth a new EOS instance from a seed configuration.

    Creates:
    1. The Self node (singleton identity anchor)
    2. The Constitution node (the four drives)
    3. Initial entities from the seed's community config
    4. Relationships linking them all together

    Returns the Self node data.
    """
    logger.info("instance_birth_starting", name=seed.instance.name, instance_id=instance_id)
    now = utc_now()
    neutral_affect = AffectState.neutral()

    # Build personality vector from seed
    personality = seed.identity.personality
    personality_values = [
        personality.warmth, personality.directness, personality.verbosity,
        personality.formality, personality.curiosity_expression, personality.humour,
        personality.empathy_expression, personality.confidence_display, personality.metaphor_use,
    ]
    # Also store as a named dict for Voxis to load directly
    personality_dict = json.dumps({
        "warmth": personality.warmth,
        "directness": personality.directness,
        "verbosity": personality.verbosity,
        "formality": personality.formality,
        "curiosity_expression": personality.curiosity_expression,
        "humour": personality.humour,
        "empathy_expression": personality.empathy_expression,
        "confidence_display": personality.confidence_display,
        "metaphor_use": personality.metaphor_use,
    })

    # 1. Create Self node
    constitution_id = new_id()

    affect_map = neutral_affect.to_map()
    await neo4j.execute_write(
        """
        CREATE (s:Self {
            instance_id: $instance_id,
            name: $name,
            born_at: datetime($born_at),
            affect_valence: $affect_valence,
            affect_arousal: $affect_arousal,
            affect_dominance: $affect_dominance,
            affect_curiosity: $affect_curiosity,
            affect_care_activation: $affect_care_activation,
            affect_coherence_stress: $affect_coherence_stress,
            autonomy_level: $autonomy_level,
            personality_vector: $personality_vector,
            personality_json: $personality_json,
            traits: $traits,
            cycle_count: 0,
            total_episodes: 0,
            total_entities: 0,
            total_communities: 0
        })
        RETURN s
        """,
        {
            "instance_id": instance_id,
            "name": seed.instance.name,
            "born_at": now.isoformat(),
            "affect_valence": affect_map.get("valence", 0.0),
            "affect_arousal": affect_map.get("arousal", 0.0),
            "affect_dominance": affect_map.get("dominance", 0.0),
            "affect_curiosity": affect_map.get("curiosity", 0.0),
            "affect_care_activation": affect_map.get("care_activation", 0.0),
            "affect_coherence_stress": affect_map.get("coherence_stress", 0.0),
            "autonomy_level": seed.constitution.autonomy_level,
            "personality_vector": personality_values,
            "personality_json": personality_dict,
            "traits": seed.identity.traits,
        },
    )

    # 2. Create Constitution node
    drives = seed.constitution.drives
    await neo4j.execute_write(
        """
        CREATE (c:Constitution {
            id: $id,
            version: 1,
            drive_coherence: $coherence,
            drive_care: $care,
            drive_growth: $growth,
            drive_honesty: $honesty,
            amendments: [],
            last_amended: null
        })
        WITH c
        MATCH (s:Self {instance_id: $instance_id})
        CREATE (s)-[:GOVERNED_BY]->(c)
        RETURN c
        """,
        {
            "id": constitution_id,
            "instance_id": instance_id,
            "coherence": drives.coherence,
            "care": drives.care,
            "growth": drives.growth,
            "honesty": drives.honesty,
        },
    )

    # 3. Create initial entities from seed
    entity_count = 0
    for entity_config in seed.community.initial_entities:
        entity_id = new_id()

        # Compute embedding for the entity
        description_text = f"{entity_config.name}: {entity_config.description}"
        embedding = await embedding_client.embed(description_text)

        await neo4j.execute_write(
            """
            CREATE (e:Entity {
                id: $id,
                name: $name,
                type: $type,
                description: $description,
                embedding: $embedding,
                first_seen: datetime($now),
                last_updated: datetime($now),
                last_accessed: datetime($now),
                salience_score: $salience,
                mention_count: 1,
                confidence: 1.0,
                is_core_identity: $is_core,
                community_ids: []
            })
            WITH e
            MATCH (s:Self {instance_id: $instance_id})
            CREATE (s)-[:CORE_CONCEPT]->(e)
            RETURN e
            """,
            {
                "id": entity_id,
                "name": entity_config.name,
                "type": entity_config.type,
                "description": entity_config.description,
                "embedding": embedding,
                "now": now.isoformat(),
                "salience": 0.9 if entity_config.is_core_identity else 0.5,
                "is_core": entity_config.is_core_identity,
                "instance_id": instance_id,
            },
        )
        entity_count += 1
        logger.info(
            "initial_entity_created",
            name=entity_config.name,
            type=entity_config.type,
            is_core=entity_config.is_core_identity,
        )

    # 4. Store the birth event as the first episode
    birth_content = (
        f"Instance '{seed.instance.name}' was born. "
        f"Description: {seed.instance.description}. "
        f"Community context: {seed.community.context.strip()}"
    )
    birth_embedding = await embedding_client.embed(birth_content)
    birth_episode_id = new_id()

    await neo4j.execute_write(
        """
        CREATE (ep:Episode {
            id: $id,
            event_time: datetime($now),
            ingestion_time: datetime($now),
            valid_from: datetime($now),
            valid_until: null,
            source: 'birth',
            modality: 'internal',
            raw_content: $content,
            summary: $summary,
            embedding: $embedding,
            salience_composite: 1.0,
            salience_birth: 1.0,
            salience_identity: 1.0,
            affect_valence: 0.3,
            affect_arousal: 0.2,
            consolidation_level: 2,
            last_accessed: datetime($now),
            access_count: 1,
            free_energy: 0.0
        })
        RETURN ep
        """,
        {
            "id": birth_episode_id,
            "now": now.isoformat(),
            "content": birth_content,
            "summary": f"Birth of {seed.instance.name}",
            "embedding": birth_embedding,
        },
    )

    # Update Self counters
    await neo4j.execute_write(
        """
        MATCH (s:Self {instance_id: $instance_id})
        SET s.total_episodes = 1, s.total_entities = $entity_count
        """,
        {"instance_id": instance_id, "entity_count": entity_count},
    )

    # 5. Store the first governance record
    await neo4j.execute_write(
        """
        CREATE (g:GovernanceRecord {
            id: $id,
            event_type: 'instance_born',
            timestamp: datetime($now),
            details: $details,
            actor: 'system',
            outcome: 'born'
        })
        """,
        {
            "id": new_id(),
            "now": now.isoformat(),
            "details": json.dumps({
                "seed_name": seed.instance.name,
                "autonomy_level": seed.constitution.autonomy_level,
                "initial_entities": entity_count,
            }),
        },
    )

    logger.info(
        "instance_birth_complete",
        name=seed.instance.name,
        instance_id=instance_id,
        initial_entities=entity_count,
    )

    return {
        "instance_id": instance_id,
        "name": seed.instance.name,
        "born_at": now.isoformat(),
        "autonomy_level": seed.constitution.autonomy_level,
        "initial_entities": entity_count,
    }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\memory\consolidation.py =====

"""
EcodiaOS — Memory Consolidation

Periodic "sleep" process that:
1. Decays salience scores
2. Re-runs community detection (Leiden)
3. Promotes high-confidence extracted knowledge
4. Merges near-duplicate entities

Target: ≤60 seconds, non-blocking.
"""

from __future__ import annotations

import time

import structlog

from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.systems.memory.salience import decay_all_salience

logger = structlog.get_logger()


async def run_consolidation(neo4j: Neo4jClient) -> dict:
    """
    Run the full consolidation pipeline.
    Called on a timer by Evo (every consolidation_interval_hours).
    """
    start = time.monotonic()
    report: dict = {"steps": {}}

    # Step 1: Salience decay
    try:
        decay_result = await decay_all_salience(neo4j)
        report["steps"]["salience_decay"] = decay_result
    except Exception as e:
        logger.error("consolidation_salience_decay_failed", error=str(e))
        report["steps"]["salience_decay"] = {"error": str(e)}

    # Step 2: Community detection via Leiden (requires Neo4j GDS)
    try:
        community_result = await _run_community_detection(neo4j)
        report["steps"]["community_detection"] = community_result
    except Exception as e:
        logger.warning("consolidation_community_detection_skipped", error=str(e))
        report["steps"]["community_detection"] = {"skipped": True, "reason": str(e)}

    # Step 3: Entity deduplication scan
    try:
        dedup_result = await _scan_near_duplicate_entities(neo4j)
        report["steps"]["entity_dedup"] = dedup_result
    except Exception as e:
        logger.error("consolidation_entity_dedup_failed", error=str(e))
        report["steps"]["entity_dedup"] = {"error": str(e)}

    elapsed_ms = int((time.monotonic() - start) * 1000)
    report["duration_ms"] = elapsed_ms

    logger.info("consolidation_complete", duration_ms=elapsed_ms, report=report)
    return report


async def _run_community_detection(neo4j: Neo4jClient) -> dict:
    """
    Run hierarchical Leiden community detection on the entity graph.
    Requires Neo4j Graph Data Science plugin.
    """
    # Project the entity graph into GDS
    try:
        await neo4j.execute_write(
            """
            CALL gds.graph.project(
                'entity_graph',
                'Entity',
                {
                    RELATES_TO: {
                        properties: ['strength'],
                        orientation: 'UNDIRECTED'
                    }
                }
            )
            """
        )
    except Exception as e:
        if "already exists" in str(e).lower():
            # Drop and recreate
            await neo4j.execute_write("CALL gds.graph.drop('entity_graph', false)")
            await neo4j.execute_write(
                """
                CALL gds.graph.project(
                    'entity_graph',
                    'Entity',
                    {
                        RELATES_TO: {
                            properties: ['strength'],
                            orientation: 'UNDIRECTED'
                        }
                    }
                )
                """
            )
        else:
            raise

    # Run Leiden
    results = await neo4j.execute_write(
        """
        CALL gds.leiden.write('entity_graph', {
            writeProperty: 'leiden_community',
            includeIntermediateCommunities: true,
            maxLevels: 5,
            gamma: 1.0,
            theta: 0.01
        })
        YIELD communityCount, modularity, ranLevels
        RETURN communityCount, modularity, ranLevels
        """
    )

    # Clean up projection
    await neo4j.execute_write("CALL gds.graph.drop('entity_graph', false)")

    community_count = 0
    modularity = 0.0
    levels = 0
    if results:
        community_count = results[0].get("communityCount", 0)
        modularity = results[0].get("modularity", 0.0)
        levels = results[0].get("ranLevels", 0)

    # Materialize Community nodes from Leiden results
    materialized = await _materialize_community_nodes(neo4j)

    return {
        "community_count": community_count,
        "modularity": modularity,
        "levels": levels,
        "communities_materialized": materialized,
    }


async def _materialize_community_nodes(neo4j: Neo4jClient) -> int:
    """
    Create Community nodes from Leiden community IDs written to entities.

    For each unique leiden_community value:
    1. MERGE a Community node with that ID
    2. Compute the community label from its top-3 member entity names
    3. Create BELONGS_TO relationships from entities to their community
    4. Compute a community embedding (mean of member embeddings)

    Returns the number of communities materialized.
    """
    try:
        # Step 1: Create/merge Community nodes for each unique community ID
        # and link entities to them
        result = await neo4j.execute_write(
            """
            MATCH (e:Entity)
            WHERE e.leiden_community IS NOT NULL
            WITH e.leiden_community AS cid, collect(e) AS members
            // Merge the Community node
            MERGE (c:Community {community_id: cid})
            SET c.member_count = size(members),
                c.updated_at = datetime()
            // Compute a label from top-3 highest-salience members
            WITH c, cid, members
            UNWIND members AS m
            WITH c, cid, m ORDER BY m.salience_score DESC
            WITH c, cid, collect(m.name)[0..3] AS top_names, collect(m) AS all_members
            SET c.label = reduce(s = '', n IN top_names | s + CASE WHEN s = '' THEN '' ELSE ', ' END + n)
            // Create BELONGS_TO relationships
            WITH c, all_members
            UNWIND all_members AS member
            MERGE (member)-[:BELONGS_TO]->(c)
            RETURN count(DISTINCT c) AS materialized
            """
        )
        materialized = result[0]["materialized"] if result else 0

        # Step 2: Compute community embeddings (mean of member embeddings)
        await neo4j.execute_write(
            """
            MATCH (c:Community)<-[:BELONGS_TO]-(e:Entity)
            WHERE e.embedding IS NOT NULL
            WITH c, collect(e.embedding) AS embeddings
            WHERE size(embeddings) > 0
            // Compute element-wise mean embedding
            WITH c, embeddings,
                 range(0, size(embeddings[0])-1) AS dims
            SET c.embedding = [i IN dims |
                reduce(s = 0.0, emb IN embeddings | s + emb[i]) / size(embeddings)
            ]
            """
        )

        logger.info("community_nodes_materialized", count=materialized)
        return materialized

    except Exception as e:
        logger.warning("community_materialization_failed", error=str(e))
        return 0


async def _scan_near_duplicate_entities(neo4j: Neo4jClient) -> dict:
    """
    Find and flag near-duplicate entities for potential merging.
    Full merge requires LLM confirmation (done by Evo), so we just flag here.
    """
    # Find entity pairs with very high embedding similarity
    results = await neo4j.execute_read(
        """
        MATCH (a:Entity), (b:Entity)
        WHERE a.id < b.id
          AND a.embedding IS NOT NULL
          AND b.embedding IS NOT NULL
          AND vector.similarity.cosine(a.embedding, b.embedding) > 0.92
        RETURN a.id AS id_a, a.name AS name_a,
               b.id AS id_b, b.name AS name_b,
               vector.similarity.cosine(a.embedding, b.embedding) AS similarity
        LIMIT 20
        """
    )

    return {
        "near_duplicates_found": len(results),
        "pairs": [
            {
                "a": r.get("name_a", ""),
                "b": r.get("name_b", ""),
                "similarity": r.get("similarity", 0.0),
            }
            for r in results
        ],
    }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\memory\episodic.py =====

"""
EcodiaOS — Episodic Memory

Storage and retrieval of discrete experience records (Episodes).
This is the "what happened" layer of memory.
"""

from __future__ import annotations

import json
from datetime import datetime

import structlog

from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.primitives import Episode, new_id, utc_now

logger = structlog.get_logger()


async def store_episode(
    neo4j: Neo4jClient,
    episode: Episode,
) -> str:
    """
    Store a new episode in the knowledge graph.
    Target: ≤50ms (just node creation; extraction is async).
    """
    await neo4j.execute_write(
        """
        CREATE (e:Episode {
            id: $id,
            event_time: datetime($event_time),
            ingestion_time: datetime($ingestion_time),
            valid_from: datetime($valid_from),
            valid_until: $valid_until,
            source: $source,
            modality: $modality,
            raw_content: $raw_content,
            summary: $summary,
            embedding: $embedding,
            salience_composite: $salience_composite,
            salience_scores_json: $salience_scores_json,
            affect_valence: $affect_valence,
            affect_arousal: $affect_arousal,
            consolidation_level: $consolidation_level,
            last_accessed: datetime($last_accessed),
            access_count: 0,
            free_energy: $free_energy
        })
        """,
        {
            "id": episode.id,
            "event_time": episode.event_time.isoformat(),
            "ingestion_time": episode.ingestion_time.isoformat(),
            "valid_from": episode.valid_from.isoformat(),
            "valid_until": episode.valid_until.isoformat() if episode.valid_until else None,
            "source": episode.source,
            "modality": episode.modality,
            "raw_content": episode.raw_content,
            "summary": episode.summary,
            "embedding": episode.embedding,
            "salience_composite": episode.salience_composite,
            "salience_scores_json": json.dumps(episode.salience_scores),
            "affect_valence": episode.affect_valence,
            "affect_arousal": episode.affect_arousal,
            "consolidation_level": episode.consolidation_level,
            "last_accessed": episode.last_accessed.isoformat(),
            "free_energy": episode.free_energy,
        },
    )

    # Increment Self counter
    await neo4j.execute_write(
        "MATCH (s:Self) SET s.total_episodes = s.total_episodes + 1"
    )

    logger.debug(
        "episode_stored",
        episode_id=episode.id,
        source=episode.source,
        salience=episode.salience_composite,
    )
    return episode.id


async def link_episode_sequence(
    neo4j: Neo4jClient,
    previous_episode_id: str,
    current_episode_id: str,
    gap_seconds: float = 0.0,
    causal_strength: float = 0.1,
) -> None:
    """Link two episodes in temporal sequence."""
    await neo4j.execute_write(
        """
        MATCH (prev:Episode {id: $prev_id})
        MATCH (curr:Episode {id: $curr_id})
        CREATE (prev)-[:FOLLOWED_BY {
            gap_seconds: $gap,
            causal_strength: $causal
        }]->(curr)
        """,
        {
            "prev_id": previous_episode_id,
            "curr_id": current_episode_id,
            "gap": gap_seconds,
            "causal": causal_strength,
        },
    )


async def get_episode(neo4j: Neo4jClient, episode_id: str) -> dict | None:
    """Retrieve a single episode by ID."""
    results = await neo4j.execute_read(
        "MATCH (e:Episode {id: $id}) RETURN e",
        {"id": episode_id},
    )
    if results:
        return results[0]["e"]
    return None


async def get_recent_episodes(
    neo4j: Neo4jClient,
    limit: int = 20,
    min_salience: float = 0.0,
) -> list[dict]:
    """Get the most recent episodes, optionally filtered by salience."""
    return await neo4j.execute_read(
        """
        MATCH (e:Episode)
        WHERE e.salience_composite >= $min_salience
        RETURN e
        ORDER BY e.ingestion_time DESC
        LIMIT $limit
        """,
        {"min_salience": min_salience, "limit": limit},
    )


async def update_access(neo4j: Neo4jClient, episode_ids: list[str]) -> None:
    """Update access timestamps and counts for retrieved episodes (salience boost)."""
    if not episode_ids:
        return
    await neo4j.execute_write(
        """
        UNWIND $ids AS eid
        MATCH (e:Episode {id: eid})
        SET e.last_accessed = datetime(),
            e.access_count = e.access_count + 1
        """,
        {"ids": episode_ids},
    )


async def count_episodes(neo4j: Neo4jClient) -> int:
    """Get total episode count."""
    results = await neo4j.execute_read("MATCH (e:Episode) RETURN count(e) AS cnt")
    return results[0]["cnt"] if results else 0

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\memory\retrieval.py =====

"""
EcodiaOS — Hybrid Memory Retrieval

Three-leg parallel retrieval: vector similarity + BM25 keyword + graph traversal.
Results are merged, deduplicated, and re-ranked by a unified score.

Target: ≤200ms end-to-end.
"""

from __future__ import annotations

import asyncio
import time

import structlog

from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.primitives.memory_trace import (
    MemoryRetrievalRequest,
    MemoryRetrievalResponse,
    RetrievalResult,
)

logger = structlog.get_logger()


async def _vector_search(
    neo4j: Neo4jClient,
    query_embedding: list[float],
    limit: int,
) -> list[RetrievalResult]:
    """Vector similarity search via Neo4j native vector index."""
    if not query_embedding:
        return []

    results = await neo4j.execute_read(
        """
        CALL db.index.vector.queryNodes('episode_embedding', $limit, $embedding)
        YIELD node, score
        RETURN node.id AS id, node.summary AS content,
               node.salience_composite AS salience,
               score AS vector_score,
               'episode' AS node_type
        """,
        {"embedding": query_embedding, "limit": limit},
    )

    return [
        RetrievalResult(
            node_id=r["id"],
            node_type=r["node_type"],
            content=r.get("content", ""),
            vector_score=r.get("vector_score"),
            salience=r.get("salience", 0.0),
        )
        for r in results
    ]


async def _bm25_search(
    neo4j: Neo4jClient,
    query_text: str,
    limit: int,
) -> list[RetrievalResult]:
    """BM25 keyword search via Neo4j fulltext index."""
    if not query_text:
        return []

    # Escape special Lucene characters
    safe_query = query_text.replace('"', '\\"')

    results = await neo4j.execute_read(
        """
        CALL db.index.fulltext.queryNodes('episode_content', $query)
        YIELD node, score
        RETURN node.id AS id, node.summary AS content,
               node.salience_composite AS salience,
               score AS bm25_score,
               'episode' AS node_type
        LIMIT $limit
        """,
        {"query": safe_query, "limit": limit},
    )

    return [
        RetrievalResult(
            node_id=r["id"],
            node_type=r["node_type"],
            content=r.get("content", ""),
            bm25_score=r.get("bm25_score"),
            salience=r.get("salience", 0.0),
        )
        for r in results
    ]


async def _graph_traverse(
    neo4j: Neo4jClient,
    query_embedding: list[float],
    depth: int,
    limit: int,
) -> list[RetrievalResult]:
    """
    Spreading activation search: find the most relevant entities via vector
    search, then traverse their neighbourhood through multiple relationship
    types to find related episodes.

    Hops:
      1. Entity ←MENTIONS← Episode (direct mentions)
      2. Entity -RELATES_TO→ Entity2 ←MENTIONS← Episode (semantic neighbours)
      3. Episode -FOLLOWED_BY→ Episode2 (temporal sequence)

    Activation decays with each hop (0.8× per hop) to prefer closer results.
    The ``depth`` parameter controls how many hops to follow (1-3).
    """
    if not query_embedding:
        return []

    effective_depth = max(1, min(3, depth))

    # Hop 1: Direct entity → episode (always run)
    direct_results = await neo4j.execute_read(
        """
        CALL db.index.vector.queryNodes('entity_embedding', 3, $embedding)
        YIELD node AS entity, score AS entity_score

        MATCH (entity)<-[:MENTIONS]-(ep:Episode)
        RETURN DISTINCT ep.id AS id, ep.summary AS content,
               ep.salience_composite AS salience,
               entity_score AS graph_score,
               'episode' AS node_type,
               1 AS hop
        ORDER BY ep.salience_composite DESC
        LIMIT $limit
        """,
        {"embedding": query_embedding, "limit": limit},
    )

    results_map: dict[str, RetrievalResult] = {}
    for r in direct_results:
        node_id = r["id"]
        results_map[node_id] = RetrievalResult(
            node_id=node_id,
            node_type=r["node_type"],
            content=r.get("content", ""),
            graph_score=r.get("graph_score", 0.0),
            salience=r.get("salience", 0.0),
        )

    # Hop 2: Entity → RELATES_TO → Entity2 → MENTIONS → Episode (semantic neighbours)
    if effective_depth >= 2:
        try:
            semantic_results = await neo4j.execute_read(
                """
                CALL db.index.vector.queryNodes('entity_embedding', 3, $embedding)
                YIELD node AS entity, score AS entity_score

                MATCH (entity)-[rel:RELATES_TO]-(neighbour:Entity)
                WHERE rel.strength > 0.3
                MATCH (neighbour)<-[:MENTIONS]-(ep:Episode)
                RETURN DISTINCT ep.id AS id, ep.summary AS content,
                       ep.salience_composite AS salience,
                       entity_score * 0.8 * rel.strength AS graph_score,
                       'episode' AS node_type,
                       2 AS hop
                ORDER BY graph_score DESC
                LIMIT $limit
                """,
                {"embedding": query_embedding, "limit": limit},
            )
            for r in semantic_results:
                node_id = r["id"]
                if node_id not in results_map:
                    results_map[node_id] = RetrievalResult(
                        node_id=node_id,
                        node_type=r["node_type"],
                        content=r.get("content", ""),
                        graph_score=r.get("graph_score", 0.0),
                        salience=r.get("salience", 0.0),
                    )
        except Exception:
            pass  # Semantic hop is best-effort

    # Hop 3: Community expansion — find episodes from the same community
    if effective_depth >= 2:
        try:
            community_results = await neo4j.execute_read(
                """
                CALL db.index.vector.queryNodes('entity_embedding', 3, $embedding)
                YIELD node AS entity, score AS entity_score

                MATCH (entity)-[:BELONGS_TO]->(c:Community)<-[:BELONGS_TO]-(sibling:Entity)
                WHERE sibling <> entity
                MATCH (sibling)<-[:MENTIONS]-(ep:Episode)
                RETURN DISTINCT ep.id AS id, ep.summary AS content,
                       ep.salience_composite AS salience,
                       entity_score * 0.65 AS graph_score,
                       'episode' AS node_type,
                       3 AS hop
                ORDER BY graph_score DESC
                LIMIT $limit
                """,
                {"embedding": query_embedding, "limit": limit},
            )
            for r in community_results:
                node_id = r["id"]
                if node_id not in results_map:
                    results_map[node_id] = RetrievalResult(
                        node_id=node_id,
                        node_type=r["node_type"],
                        content=r.get("content", ""),
                        graph_score=r.get("graph_score", 0.0),
                        salience=r.get("salience", 0.0),
                    )
        except Exception:
            pass  # Community hop is best-effort

    # Hop 4: Episode → FOLLOWED_BY → Episode (temporal context)
    if effective_depth >= 3 and results_map:
        try:
            seed_ids = list(results_map.keys())[:5]
            temporal_results = await neo4j.execute_read(
                """
                UNWIND $seed_ids AS seed_id
                MATCH (ep:Episode {id: seed_id})-[fb:FOLLOWED_BY]-(neighbour:Episode)
                WHERE fb.causal_strength > 0.2
                RETURN DISTINCT neighbour.id AS id, neighbour.summary AS content,
                       neighbour.salience_composite AS salience,
                       fb.causal_strength * 0.6 AS graph_score,
                       'episode' AS node_type,
                       3 AS hop
                ORDER BY graph_score DESC
                LIMIT $limit
                """,
                {"seed_ids": seed_ids, "limit": limit},
            )
            for r in temporal_results:
                node_id = r["id"]
                if node_id not in results_map:
                    results_map[node_id] = RetrievalResult(
                        node_id=node_id,
                        node_type=r["node_type"],
                        content=r.get("content", ""),
                        graph_score=r.get("graph_score", 0.0),
                        salience=r.get("salience", 0.0),
                    )
        except Exception:
            pass  # Temporal hop is best-effort

    return list(results_map.values())


def _compute_unified_score(result: RetrievalResult, temporal_bias: str = "recency") -> float:
    """
    Compute a unified relevance score from all retrieval legs + salience.

    Weights:
    - Vector similarity: 0.35
    - BM25 keyword:      0.20
    - Graph traversal:   0.20
    - Salience:          0.25
    """
    vector = result.vector_score or 0.0
    bm25 = min((result.bm25_score or 0.0) / 10.0, 1.0)  # Normalise BM25 to ~0-1
    graph = result.graph_score or 0.0
    salience = result.salience

    return (
        0.35 * vector
        + 0.20 * bm25
        + 0.20 * graph
        + 0.25 * salience
    )


def _merge_deduplicate(
    *result_lists: list[RetrievalResult],
) -> list[RetrievalResult]:
    """Merge results from all legs, keeping the highest-scoring version of duplicates."""
    seen: dict[str, RetrievalResult] = {}

    for results in result_lists:
        for r in results:
            if r.node_id in seen:
                existing = seen[r.node_id]
                # Merge scores from different legs
                if r.vector_score and not existing.vector_score:
                    existing.vector_score = r.vector_score
                if r.bm25_score and not existing.bm25_score:
                    existing.bm25_score = r.bm25_score
                if r.graph_score and not existing.graph_score:
                    existing.graph_score = r.graph_score
            else:
                seen[r.node_id] = r

    return list(seen.values())


async def hybrid_retrieve(
    neo4j: Neo4jClient,
    request: MemoryRetrievalRequest,
) -> MemoryRetrievalResponse:
    """
    Execute hybrid retrieval: vector + BM25 + graph in parallel.
    Target: ≤200ms.
    """
    start = time.monotonic()
    expanded_limit = request.max_results * 2

    # Run all three legs in parallel
    vector_task = _vector_search(neo4j, request.query_embedding or [], expanded_limit)
    bm25_task = _bm25_search(neo4j, request.query_text or "", expanded_limit)
    graph_task = _graph_traverse(
        neo4j, request.query_embedding or [], request.traversal_depth, request.max_results
    )

    vector_results, bm25_results, graph_results = await asyncio.gather(
        vector_task, bm25_task, graph_task,
        return_exceptions=True,
    )

    # Handle any failures gracefully
    if isinstance(vector_results, Exception):
        logger.warning("vector_search_failed", error=str(vector_results))
        vector_results = []
    if isinstance(bm25_results, Exception):
        logger.warning("bm25_search_failed", error=str(bm25_results))
        bm25_results = []
    if isinstance(graph_results, Exception):
        logger.warning("graph_search_failed", error=str(graph_results))
        graph_results = []

    # Merge and compute unified scores
    all_results = _merge_deduplicate(vector_results, bm25_results, graph_results)

    for result in all_results:
        result.unified_score = _compute_unified_score(result, request.temporal_bias)

    # Filter and rank
    ranked = sorted(all_results, key=lambda r: r.unified_score, reverse=True)
    ranked = [r for r in ranked if r.unified_score >= request.salience_floor]
    ranked = ranked[: request.max_results]

    elapsed_ms = int((time.monotonic() - start) * 1000)

    logger.debug(
        "hybrid_retrieval_complete",
        vector_hits=len(vector_results),
        bm25_hits=len(bm25_results),
        graph_hits=len(graph_results),
        merged=len(all_results),
        returned=len(ranked),
        elapsed_ms=elapsed_ms,
    )

    return MemoryRetrievalResponse(
        traces=ranked,
        retrieval_time_ms=elapsed_ms,
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\memory\salience.py =====

"""
EcodiaOS — Salience Model

Computes and decays salience scores for entities and episodes.
Salience determines what the organism "has on its mind" — what's accessible
versus what's deep in sediment.
"""

from __future__ import annotations

import math
from datetime import datetime, timezone

import structlog

from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger()

# Decay half-life in hours — how long until salience drops by 50%
DECAY_HALF_LIFE_HOURS = 72.0
DECAY_LAMBDA = math.log(2) / (DECAY_HALF_LIFE_HOURS * 3600)

# Minimum salience floor — things don't go to zero, they go dormant
SALIENCE_FLOOR = 0.01

# Boost amount when something is accessed
ACCESS_BOOST = 0.15

# Core identity entities never decay below this
CORE_IDENTITY_FLOOR = 0.5


def compute_decayed_salience(
    current_salience: float,
    seconds_since_access: float,
    is_core_identity: bool = False,
) -> float:
    """
    Exponential decay of salience with a floor.

    Salience(t) = max(floor, current * e^(-λt))
    """
    floor = CORE_IDENTITY_FLOOR if is_core_identity else SALIENCE_FLOOR
    decayed = current_salience * math.exp(-DECAY_LAMBDA * seconds_since_access)
    return max(floor, decayed)


def compute_access_boost(current_salience: float) -> float:
    """Boost salience when an entity/episode is accessed (retrieved)."""
    boosted = current_salience + ACCESS_BOOST * (1.0 - current_salience)
    return min(1.0, boosted)


def compute_enriched_salience(
    base_composite: float,
    affect_valence: float = 0.0,
    affect_arousal: float = 0.0,
    prediction_error_magnitude: float = 0.0,
    is_distress: bool = False,
) -> float:
    """
    Compute storage salience that accounts for emotional intensity and surprise.

    Factors beyond the base composite from Atune's 7-head scoring:
    * **Emotional intensity** — absolute valence × arousal amplifies memorability.
      Strongly emotional events are remembered more vividly (Kensinger 2009).
    * **Surprise magnitude** — high prediction error makes events more memorable.
      Novel or contradictory events get a salience boost at storage time.
    * **Distress urgency** — care-relevant events get a floor boost so they
      remain accessible during the critical response window.

    Returns a value in [0.0, 1.0].
    """
    # Base: the Atune 7-head composite
    salience = base_composite

    # Emotional intensity: |valence| × arousal → up to +0.15 boost
    emotional_intensity = abs(affect_valence) * max(affect_arousal, 0.1)
    salience += emotional_intensity * 0.15

    # Surprise boost: high prediction error → up to +0.10 boost
    if prediction_error_magnitude > 0.3:
        surprise_bonus = (prediction_error_magnitude - 0.3) * 0.15
        salience += min(0.10, surprise_bonus)

    # Distress floor: if distress detected, ensure minimum salience of 0.4
    if is_distress:
        salience = max(salience, 0.4)

    return min(1.0, max(0.0, salience))


async def decay_all_salience(neo4j: Neo4jClient) -> dict:
    """
    Apply time-based salience decay to all entities and episodes.
    Called during consolidation (every few hours).
    """
    now = datetime.now(timezone.utc)

    # Decay entity salience
    entity_result = await neo4j.execute_write(
        """
        MATCH (e:Entity)
        WHERE e.last_accessed IS NOT NULL
        WITH e,
             duration.between(e.last_accessed, datetime()).seconds AS secs,
             e.is_core_identity AS is_core
        WITH e, secs,
             CASE WHEN is_core THEN $core_floor ELSE $floor END AS floor,
             e.salience_score * exp(-1.0 * $lambda * secs) AS decayed
        SET e.salience_score = CASE
            WHEN decayed < floor THEN floor
            ELSE decayed
        END
        RETURN count(e) AS updated
        """,
        {
            "lambda": DECAY_LAMBDA,
            "floor": SALIENCE_FLOOR,
            "core_floor": CORE_IDENTITY_FLOOR,
        },
    )

    # Decay episode salience
    episode_result = await neo4j.execute_write(
        """
        MATCH (ep:Episode)
        WHERE ep.last_accessed IS NOT NULL
        WITH ep,
             duration.between(ep.last_accessed, datetime()).seconds AS secs
        WITH ep, secs,
             ep.salience_composite * exp(-1.0 * $lambda * secs) AS decayed
        SET ep.salience_composite = CASE
            WHEN decayed < $floor THEN $floor
            ELSE decayed
        END
        RETURN count(ep) AS updated
        """,
        {
            "lambda": DECAY_LAMBDA,
            "floor": SALIENCE_FLOOR,
        },
    )

    entities_updated = entity_result[0]["updated"] if entity_result else 0
    episodes_updated = episode_result[0]["updated"] if episode_result else 0

    logger.info(
        "salience_decay_applied",
        entities_updated=entities_updated,
        episodes_updated=episodes_updated,
    )

    return {
        "entities_updated": entities_updated,
        "episodes_updated": episodes_updated,
    }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\memory\schema.py =====

"""
EcodiaOS — Memory Schema

Creates all Neo4j indexes, constraints, and vector indexes on first boot.
This is the physical structure of the knowledge graph.
"""

from __future__ import annotations

import structlog

from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger()

# ─── Constraints (uniqueness) ────────────────────────────────────
CONSTRAINTS = [
    "CREATE CONSTRAINT episode_id IF NOT EXISTS FOR (e:Episode) REQUIRE e.id IS UNIQUE",
    "CREATE CONSTRAINT entity_id IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE",
    "CREATE CONSTRAINT community_id IF NOT EXISTS FOR (c:Community) REQUIRE c.id IS UNIQUE",
    "CREATE CONSTRAINT self_id IF NOT EXISTS FOR (s:Self) REQUIRE s.instance_id IS UNIQUE",
    "CREATE CONSTRAINT constitution_id IF NOT EXISTS FOR (c:Constitution) REQUIRE c.id IS UNIQUE",
    "CREATE CONSTRAINT procedure_id IF NOT EXISTS FOR (p:Procedure) REQUIRE p.id IS UNIQUE",
    "CREATE CONSTRAINT hypothesis_id IF NOT EXISTS FOR (h:Hypothesis) REQUIRE h.id IS UNIQUE",
    "CREATE CONSTRAINT governance_id IF NOT EXISTS FOR (g:GovernanceRecord) REQUIRE g.id IS UNIQUE",
]

# ─── Indexes (performance) ───────────────────────────────────────
INDEXES = [
    # Temporal queries
    "CREATE INDEX episode_event_time IF NOT EXISTS FOR (e:Episode) ON (e.event_time)",
    "CREATE INDEX episode_ingestion_time IF NOT EXISTS FOR (e:Episode) ON (e.ingestion_time)",
    "CREATE INDEX episode_salience IF NOT EXISTS FOR (e:Episode) ON (e.salience_composite)",

    # Entity lookups
    "CREATE INDEX entity_name IF NOT EXISTS FOR (e:Entity) ON (e.name)",
    "CREATE INDEX entity_type IF NOT EXISTS FOR (e:Entity) ON (e.type)",
    "CREATE INDEX entity_salience IF NOT EXISTS FOR (e:Entity) ON (e.salience_score)",
    "CREATE INDEX entity_core IF NOT EXISTS FOR (e:Entity) ON (e.is_core_identity)",

    # Community hierarchy
    "CREATE INDEX community_level IF NOT EXISTS FOR (c:Community) ON (c.level)",

    # Consolidation
    "CREATE INDEX episode_consolidation IF NOT EXISTS FOR (e:Episode) ON (e.consolidation_level)",
]

# ─── Fulltext Indexes ────────────────────────────────────────────
FULLTEXT_INDEXES = [
    """
    CREATE FULLTEXT INDEX episode_content IF NOT EXISTS
    FOR (e:Episode) ON EACH [e.summary, e.raw_content]
    """,
    """
    CREATE FULLTEXT INDEX entity_search IF NOT EXISTS
    FOR (e:Entity) ON EACH [e.name, e.description]
    """,
]

# ─── Vector Indexes ──────────────────────────────────────────────
# Neo4j 5.x native vector index for semantic search
VECTOR_INDEXES = [
    """
    CREATE VECTOR INDEX episode_embedding IF NOT EXISTS
    FOR (e:Episode) ON (e.embedding)
    OPTIONS {indexConfig: {
        `vector.dimensions`: 768,
        `vector.similarity_function`: 'cosine'
    }}
    """,
    """
    CREATE VECTOR INDEX entity_embedding IF NOT EXISTS
    FOR (e:Entity) ON (e.embedding)
    OPTIONS {indexConfig: {
        `vector.dimensions`: 768,
        `vector.similarity_function`: 'cosine'
    }}
    """,
    """
    CREATE VECTOR INDEX community_embedding IF NOT EXISTS
    FOR (c:Community) ON (c.embedding)
    OPTIONS {indexConfig: {
        `vector.dimensions`: 768,
        `vector.similarity_function`: 'cosine'
    }}
    """,
]


async def ensure_schema(neo4j: Neo4jClient) -> None:
    """
    Create all indexes and constraints if they don't exist.
    Idempotent — safe to call on every startup.
    """
    logger.info("memory_schema_ensuring")

    all_statements = CONSTRAINTS + INDEXES + FULLTEXT_INDEXES + VECTOR_INDEXES

    for statement in all_statements:
        statement = statement.strip()
        if not statement:
            continue
        try:
            await neo4j.execute_write(statement)
        except Exception as e:
            error_msg = str(e).lower()
            if "already exists" in error_msg or "equivalent" in error_msg:
                continue
            logger.warning(
                "memory_schema_statement_warning",
                statement=statement[:80],
                error=str(e),
            )

    logger.info("memory_schema_ensured")

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\memory\semantic.py =====

"""
EcodiaOS — Semantic Memory

Entity extraction, deduplication, and relationship management.
This is the "what things exist" layer of memory.
"""

from __future__ import annotations

import json

import structlog

from ecodiaos.clients.embedding import EmbeddingClient
from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.primitives import Entity, EntityType, MentionRelation, SemanticRelation, new_id, utc_now

logger = structlog.get_logger()

# Cosine similarity threshold for entity deduplication
DEDUP_THRESHOLD = 0.88


async def create_entity(
    neo4j: Neo4jClient,
    entity: Entity,
) -> str:
    """Create a new entity node in the graph."""
    await neo4j.execute_write(
        """
        CREATE (e:Entity {
            id: $id,
            name: $name,
            type: $type,
            description: $description,
            embedding: $embedding,
            first_seen: datetime($first_seen),
            last_updated: datetime($last_updated),
            last_accessed: datetime($last_accessed),
            salience_score: $salience_score,
            mention_count: $mention_count,
            confidence: $confidence,
            is_core_identity: $is_core,
            community_ids: $community_ids,
            metadata_json: $metadata_json
        })
        """,
        {
            "id": entity.id,
            "name": entity.name,
            "type": entity.type.value if isinstance(entity.type, EntityType) else entity.type,
            "description": entity.description,
            "embedding": entity.embedding,
            "first_seen": entity.first_seen.isoformat(),
            "last_updated": entity.last_updated.isoformat(),
            "last_accessed": entity.last_accessed.isoformat(),
            "salience_score": entity.salience_score,
            "mention_count": entity.mention_count,
            "confidence": entity.confidence,
            "is_core": entity.is_core_identity,
            "community_ids": entity.community_ids,
            "metadata_json": json.dumps(entity.metadata),
        },
    )

    # Increment Self counter
    await neo4j.execute_write(
        "MATCH (s:Self) SET s.total_entities = s.total_entities + 1"
    )

    logger.debug("entity_created", entity_id=entity.id, name=entity.name, type=entity.type)
    return entity.id


async def find_similar_entity(
    neo4j: Neo4jClient,
    name: str,
    embedding: list[float],
    threshold: float = DEDUP_THRESHOLD,
) -> dict | None:
    """
    Find an existing entity that matches by name or embedding similarity.
    Used for deduplication during entity extraction.
    """
    # First: exact name match (fast)
    results = await neo4j.execute_read(
        """
        MATCH (e:Entity)
        WHERE toLower(e.name) = toLower($name)
        RETURN e
        LIMIT 1
        """,
        {"name": name},
    )
    if results:
        return results[0]["e"]

    # Second: vector similarity match (semantic dedup)
    if embedding:
        results = await neo4j.execute_read(
            """
            CALL db.index.vector.queryNodes('entity_embedding', 3, $embedding)
            YIELD node, score
            WHERE score >= $threshold
            RETURN node AS e, score
            ORDER BY score DESC
            LIMIT 1
            """,
            {"embedding": embedding, "threshold": threshold},
        )
        if results:
            return results[0]["e"]

    return None


async def merge_into_entity(
    neo4j: Neo4jClient,
    existing_entity_id: str,
    new_description: str,
) -> None:
    """Merge new information into an existing entity (update description, bump counts)."""
    now = utc_now()
    await neo4j.execute_write(
        """
        MATCH (e:Entity {id: $id})
        SET e.description = $description,
            e.last_updated = datetime($now),
            e.last_accessed = datetime($now),
            e.mention_count = e.mention_count + 1
        """,
        {
            "id": existing_entity_id,
            "description": new_description,
            "now": now.isoformat(),
        },
    )


async def link_episode_to_entity(
    neo4j: Neo4jClient,
    mention: MentionRelation,
) -> None:
    """Create a MENTIONS relationship from Episode to Entity."""
    await neo4j.execute_write(
        """
        MATCH (ep:Episode {id: $episode_id})
        MATCH (en:Entity {id: $entity_id})
        CREATE (ep)-[:MENTIONS {
            role: $role,
            confidence: $confidence,
            span: $span
        }]->(en)
        """,
        {
            "episode_id": mention.episode_id,
            "entity_id": mention.entity_id,
            "role": mention.role,
            "confidence": mention.confidence,
            "span": mention.span,
        },
    )


async def create_or_strengthen_relation(
    neo4j: Neo4jClient,
    relation: SemanticRelation,
) -> None:
    """Create a new RELATES_TO edge or strengthen an existing one."""
    now = utc_now()
    await neo4j.execute_write(
        """
        MATCH (a:Entity {id: $source_id})
        MATCH (b:Entity {id: $target_id})
        MERGE (a)-[r:RELATES_TO {type: $type}]->(b)
        ON CREATE SET
            r.strength = $strength,
            r.confidence = $confidence,
            r.first_observed = datetime($now),
            r.last_observed = datetime($now),
            r.observation_count = 1,
            r.temporal_valid_from = datetime($now),
            r.temporal_valid_until = null,
            r.evidence_episodes = $evidence
        ON MATCH SET
            r.strength = CASE WHEN $strength > r.strength THEN $strength ELSE r.strength END,
            r.confidence = (r.confidence * r.observation_count + $confidence) / (r.observation_count + 1),
            r.last_observed = datetime($now),
            r.observation_count = r.observation_count + 1,
            r.evidence_episodes = r.evidence_episodes + $evidence
        """,
        {
            "source_id": relation.source_entity_id,
            "target_id": relation.target_entity_id,
            "type": relation.type,
            "strength": relation.strength,
            "confidence": relation.confidence,
            "now": now.isoformat(),
            "evidence": relation.evidence_episodes,
        },
    )


async def get_entity(neo4j: Neo4jClient, entity_id: str) -> dict | None:
    """Retrieve a single entity by ID."""
    results = await neo4j.execute_read(
        "MATCH (e:Entity {id: $id}) RETURN e",
        {"id": entity_id},
    )
    return results[0]["e"] if results else None


async def get_entity_neighbours(
    neo4j: Neo4jClient,
    entity_id: str,
    max_depth: int = 2,
    limit: int = 20,
) -> list[dict]:
    """Get entities connected to a given entity within N hops."""
    return await neo4j.execute_read(
        """
        MATCH (start:Entity {id: $id})
        MATCH path = (start)-[:RELATES_TO*1..$depth]-(neighbor:Entity)
        WHERE neighbor.id <> $id
        RETURN DISTINCT neighbor AS e,
               length(path) AS distance,
               [r IN relationships(path) | r.strength] AS strengths
        ORDER BY distance ASC, neighbor.salience_score DESC
        LIMIT $limit
        """,
        {"id": entity_id, "depth": max_depth, "limit": limit},
    )


async def count_entities(neo4j: Neo4jClient) -> int:
    """Get total entity count."""
    results = await neo4j.execute_read("MATCH (e:Entity) RETURN count(e) AS cnt")
    return results[0]["cnt"] if results else 0

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\memory\service.py =====

"""
EcodiaOS — Memory Service

The single interface to the knowledge graph. Every other system
reads from and writes to memory through this service.

Memory is the substrate of selfhood.
"""

from __future__ import annotations

import structlog

from ecodiaos.clients.embedding import EmbeddingClient
from ecodiaos.clients.neo4j import Neo4jClient
from ecodiaos.config import SeedConfig
from ecodiaos.primitives import (
    AffectState,
    Entity,
    Episode,
    MemoryRetrievalRequest,
    MemoryRetrievalResponse,
    MentionRelation,
    Percept,
    SelfNode,
    SemanticRelation,
    new_id,
    utc_now,
)
from ecodiaos.systems.memory.birth import birth_instance
from ecodiaos.systems.memory.consolidation import run_consolidation
from ecodiaos.systems.memory.episodic import (
    count_episodes,
    get_episode,
    get_recent_episodes,
    link_episode_sequence,
    store_episode,
    update_access,
)
from ecodiaos.systems.memory.retrieval import hybrid_retrieve
from ecodiaos.systems.memory.schema import ensure_schema
from ecodiaos.systems.memory.semantic import (
    count_entities,
    create_entity,
    create_or_strengthen_relation,
    find_similar_entity,
    get_entity,
    get_entity_neighbours,
    link_episode_to_entity,
    merge_into_entity,
)

logger = structlog.get_logger()


class MemoryService:
    """
    The Memory & Identity Core.

    This is the ground truth of identity. Every other system reads from
    and writes to this substrate. If you lose the graph, you lose the self.
    """

    system_id: str = "memory"

    def __init__(
        self,
        neo4j: Neo4jClient,
        embedding_client: EmbeddingClient,
    ) -> None:
        self._neo4j = neo4j
        self._embedding = embedding_client
        self._instance_id: str | None = None
        # Episode sequence tracking — links consecutive episodes with FOLLOWED_BY
        self._last_episode_id: str | None = None
        self._last_episode_time: float | None = None  # monotonic seconds

    # ─── Lifecycle ────────────────────────────────────────────────

    async def initialize(self) -> None:
        """Ensure schema exists (idempotent)."""
        await ensure_schema(self._neo4j)
        logger.info("memory_service_initialised")

    async def get_self(self) -> SelfNode | None:
        """Get the current instance's Self node, or None if not yet born."""
        results = await self._neo4j.execute_read(
            "MATCH (s:Self) RETURN s LIMIT 1"
        )
        if not results:
            return None

        data = results[0]["s"]
        self._instance_id = data.get("instance_id")
        # Reconstruct current_affect from flattened scalar properties
        current_affect = {
            "valence": data.get("affect_valence", 0.0),
            "arousal": data.get("affect_arousal", 0.0),
            "dominance": data.get("affect_dominance", 0.5),
            "curiosity": data.get("affect_curiosity", 0.0),
            "care_activation": data.get("affect_care_activation", 0.0),
            "coherence_stress": data.get("affect_coherence_stress", 0.0),
        }
        # Convert neo4j.time.DateTime to Python datetime
        born_at = data.get("born_at", utc_now())
        if hasattr(born_at, "to_native"):
            born_at = born_at.to_native()
        # Parse personality_json if stored (Neo4j stores as string, needs json.loads)
        raw_pjson = data.get("personality_json", None)
        personality_json: dict[str, float] = {}
        if raw_pjson:
            if isinstance(raw_pjson, str):
                import json
                try:
                    personality_json = json.loads(raw_pjson)
                except (json.JSONDecodeError, TypeError):
                    pass
            elif isinstance(raw_pjson, dict):
                personality_json = raw_pjson

        return SelfNode(
            instance_id=data.get("instance_id", ""),
            name=data.get("name", ""),
            born_at=born_at,
            current_affect=current_affect,
            autonomy_level=data.get("autonomy_level", 1),
            personality_vector=data.get("personality_vector", []),
            personality_json=personality_json,
            cycle_count=data.get("cycle_count", 0),
            total_episodes=data.get("total_episodes", 0),
            total_entities=data.get("total_entities", 0),
            total_communities=data.get("total_communities", 0),
        )

    async def birth(self, seed: SeedConfig, instance_id: str) -> dict:
        """Birth a new instance from a seed configuration."""
        result = await birth_instance(
            self._neo4j, self._embedding, seed, instance_id,
        )
        self._instance_id = instance_id
        return result

    # ─── Episode Storage (from Atune) ─────────────────────────────

    async def store_percept(
        self,
        percept: Percept,
        salience_composite: float = 0.0,
        salience_scores: dict[str, float] | None = None,
        affect_valence: float = 0.0,
        affect_arousal: float = 0.0,
        free_energy: float = 0.0,
    ) -> str:
        """
        Store a percept as an episode in the knowledge graph.

        Returns the episode ID.
        Entity extraction should be done asynchronously after this returns.

        Automatically links the new episode to the previous one via
        FOLLOWED_BY relationship, creating temporal causal chains that
        enable sequential memory and "what happened before/after" queries.
        """
        import time as _time

        # Compute embedding if not already present
        embedding = percept.content.embedding
        if not embedding and percept.content.raw:
            embedding = await self._embedding.embed(percept.content.raw)

        episode = Episode(
            event_time=percept.timestamp,
            ingestion_time=utc_now(),
            source=f"{percept.source.system.value}:{percept.source.channel}",
            modality=percept.source.modality.value,
            raw_content=percept.content.raw,
            summary=percept.content.raw[:200],  # Placeholder; LLM summary done async
            embedding=embedding,
            salience_composite=salience_composite,
            salience_scores=salience_scores or {},
            affect_valence=affect_valence,
            affect_arousal=affect_arousal,
            free_energy=free_energy,
        )

        episode_id = await store_episode(self._neo4j, episode)

        # Link to previous episode (temporal sequence)
        now = _time.monotonic()
        if self._last_episode_id is not None:
            gap = now - self._last_episode_time if self._last_episode_time else 0.0
            # Only link if episodes are within 1 hour of each other
            if gap < 3600.0:
                try:
                    # Higher causal strength for closely spaced episodes
                    causal = max(0.05, min(0.8, 1.0 - (gap / 300.0)))
                    await link_episode_sequence(
                        self._neo4j,
                        previous_episode_id=self._last_episode_id,
                        current_episode_id=episode_id,
                        gap_seconds=gap,
                        causal_strength=causal,
                    )
                except Exception:
                    logger.debug("episode_link_failed", exc_info=True)

        self._last_episode_id = episode_id
        self._last_episode_time = now

        return episode_id

    # ─── Retrieval (from any system) ──────────────────────────────

    async def retrieve(
        self,
        query_text: str | None = None,
        query_embedding: list[float] | None = None,
        max_results: int = 10,
        salience_floor: float = 0.0,
        include_communities: bool = False,
    ) -> MemoryRetrievalResponse:
        """
        Hybrid retrieval. Must respond within 200ms.
        """
        # Compute embedding from text if not provided
        if query_text and not query_embedding:
            query_embedding = await self._embedding.embed(query_text)

        request = MemoryRetrievalRequest(
            query_text=query_text,
            query_embedding=query_embedding,
            max_results=max_results,
            salience_floor=salience_floor,
            include_communities=include_communities,
        )

        response = await hybrid_retrieve(self._neo4j, request)

        # Update access timestamps for retrieved episodes (salience boost)
        retrieved_ids = [r.node_id for r in response.traces]
        await update_access(self._neo4j, retrieved_ids)

        return response

    # ─── Entity Management (from extraction pipeline) ─────────────

    async def resolve_and_create_entity(
        self,
        name: str,
        entity_type: str,
        description: str,
    ) -> tuple[str, bool]:
        """
        Find or create an entity. Handles deduplication.

        Returns: (entity_id, was_created)
        """
        # Compute embedding
        embed_text = f"{name}: {description}"
        embedding = await self._embedding.embed(embed_text)

        # Try to find existing
        existing = await find_similar_entity(self._neo4j, name, embedding)
        if existing:
            await merge_into_entity(self._neo4j, existing["id"], description)
            return existing["id"], False

        # Create new
        entity = Entity(
            name=name,
            type=entity_type,
            description=description,
            embedding=embedding,
        )
        entity_id = await create_entity(self._neo4j, entity)
        return entity_id, True

    async def link_mention(self, mention: MentionRelation) -> None:
        """Link an episode to an entity."""
        await link_episode_to_entity(self._neo4j, mention)

    async def link_relation(self, relation: SemanticRelation) -> None:
        """Create or strengthen a semantic relation."""
        await create_or_strengthen_relation(self._neo4j, relation)

    # ─── State Queries ────────────────────────────────────────────

    async def get_constitution(self) -> dict | None:
        """Get the current constitutional state."""
        results = await self._neo4j.execute_read(
            """
            MATCH (s:Self)-[:GOVERNED_BY]->(c:Constitution)
            RETURN c
            """
        )
        return results[0]["c"] if results else None

    async def get_core_entities(self) -> list[dict]:
        """Get all entities marked as core to the instance's identity."""
        return await self._neo4j.execute_read(
            """
            MATCH (e:Entity {is_core_identity: true})
            RETURN e
            ORDER BY e.salience_score DESC
            """
        )

    async def update_affect(self, affect: AffectState) -> None:
        """Update the current affect state on the Self node (flattened scalars)."""
        affect_map = affect.to_map()
        await self._neo4j.execute_write(
            """
            MATCH (s:Self)
            SET s.affect_valence = $affect_valence,
                s.affect_arousal = $affect_arousal,
                s.affect_dominance = $affect_dominance,
                s.affect_curiosity = $affect_curiosity,
                s.affect_care_activation = $affect_care_activation,
                s.affect_coherence_stress = $affect_coherence_stress
            """,
            {
                "affect_valence": affect_map.get("valence", 0.0),
                "affect_arousal": affect_map.get("arousal", 0.0),
                "affect_dominance": affect_map.get("dominance", 0.0),
                "affect_curiosity": affect_map.get("curiosity", 0.0),
                "affect_care_activation": affect_map.get("care_activation", 0.0),
                "affect_coherence_stress": affect_map.get("coherence_stress", 0.0),
            },
        )

    async def increment_cycle_count(self) -> None:
        """Increment the cognitive cycle counter."""
        await self._neo4j.execute_write(
            "MATCH (s:Self) SET s.cycle_count = s.cycle_count + 1"
        )

    # ─── Consolidation ────────────────────────────────────────────

    async def consolidate(self) -> dict:
        """Run the consolidation pipeline (memory sleep cycle)."""
        return await run_consolidation(self._neo4j)

    # ─── Stats ────────────────────────────────────────────────────

    async def stats(self) -> dict:
        """Get graph statistics."""
        ep_count = await count_episodes(self._neo4j)
        ent_count = await count_entities(self._neo4j)
        return {
            "episode_count": ep_count,
            "entity_count": ent_count,
        }

    # ─── Health ───────────────────────────────────────────────────

    async def health(self) -> dict:
        """Health check for the memory system (must complete within 2s)."""
        neo4j_health = await self._neo4j.health_check()
        return {
            "status": "healthy" if neo4j_health["status"] == "connected" else "degraded",
            "neo4j": neo4j_health,
            "instance_id": self._instance_id,
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\__init__.py =====

"""EcodiaOS — Nova: Decision & Planning System (Phase 5)."""

from ecodiaos.systems.nova.service import NovaService
from ecodiaos.systems.nova.types import (
    BeliefState,
    Goal,
    GoalSource,
    GoalStatus,
    IntentOutcome,
    Policy,
)

__all__ = [
    "NovaService",
    "BeliefState",
    "Goal",
    "GoalSource",
    "GoalStatus",
    "IntentOutcome",
    "Policy",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\belief_updater.py =====

"""
EcodiaOS — Nova Belief Updater

Maintains and updates the structured belief state from workspace broadcasts.

This is perceptual inference: given a new observation (workspace broadcast),
update beliefs to better explain it. In the active inference framework, this
is equivalent to minimising variational free energy with respect to the
belief state q(s).

The Bayesian update rule used here is a precision-weighted interpolation:
    q(s_new) = (1 - α) * q(s_old) + α * likelihood(obs)
where α = broadcast.precision (how much to trust the new evidence).

For entity beliefs specifically:
    confidence_new = confidence_old + precision * (1 - confidence_old)
This is a logistic-like accumulation: each piece of evidence pushes confidence
toward 1.0, weighted by that evidence's precision.
"""

from __future__ import annotations

import math

import structlog

from ecodiaos.systems.atune.types import WorkspaceBroadcast
from ecodiaos.systems.nova.types import (
    BeliefDelta,
    BeliefState,
    ContextBelief,
    EntityBelief,
    IndividualBelief,
)
from ecodiaos.primitives.common import utc_now

logger = structlog.get_logger()


# Maximum entities tracked in the belief state
_MAX_ENTITY_BELIEFS = 200
# Confidence decay per cycle for unobserved entities (forgetting)
_CONFIDENCE_DECAY = 0.005
# Minimum confidence before entity belief is pruned
_MIN_ENTITY_CONFIDENCE = 0.05


class BeliefUpdater:
    """
    Maintains Nova's belief state and updates it from workspace broadcasts.

    The belief state is Nova's map of the world — what it knows, how confident
    it is, and where the prediction errors are. This drives deliberation:
    which goals are relevant, which policies can work.
    """

    def __init__(self) -> None:
        self._beliefs = BeliefState()
        self._logger = logger.bind(system="nova.belief_updater")

    @property
    def beliefs(self) -> BeliefState:
        return self._beliefs

    def update_from_broadcast(
        self,
        broadcast: WorkspaceBroadcast,
    ) -> BeliefDelta:
        """
        Integrate a workspace broadcast into the belief state.

        Called at the start of every deliberation cycle. Returns a BeliefDelta
        describing what changed, which the rest of Nova uses to assess novelty
        and select goals.

        This is synchronous and must complete in ≤50ms (per spec).
        """
        delta = BeliefDelta()
        precision = broadcast.precision

        # ── Update context belief from broadcast content ──
        context_update = self._update_context(broadcast, precision)
        delta.context_update = context_update
        self._beliefs = self._beliefs.model_copy(
            update={"current_context": context_update}
        )

        # ── Extract and update entity beliefs from memory context ──
        memory_context = getattr(broadcast.context, "memory_context", None)
        if memory_context and hasattr(memory_context, "entities"):
            for entity_data in memory_context.entities:
                entity_id = str(getattr(entity_data, "id", "") or getattr(entity_data, "entity_id", ""))
                if not entity_id:
                    continue

                if entity_id in self._beliefs.entities:
                    updated = _bayesian_update_entity(
                        prior=self._beliefs.entities[entity_id],
                        precision=precision,
                    )
                    delta.entity_updates[entity_id] = updated
                else:
                    added = EntityBelief(
                        entity_id=entity_id,
                        name=str(getattr(entity_data, "name", entity_id)),
                        entity_type=str(getattr(entity_data, "type", "")),
                        properties=dict(getattr(entity_data, "properties", {}) or {}),
                        confidence=float(getattr(entity_data, "confidence", 0.5)) * precision,
                        last_observed=utc_now(),
                    )
                    delta.entity_additions[entity_id] = added

        # ── Update individual beliefs if a person is involved ──
        individual_id = _extract_individual_id(broadcast)
        if individual_id:
            current = self._beliefs.individual_beliefs.get(individual_id)
            updated_individual = _update_individual_belief(
                individual_id=individual_id,
                current=current,
                broadcast=broadcast,
                precision=precision,
            )
            delta.individual_updates[individual_id] = updated_individual

        # ── Detect belief conflicts ──
        if context_update.prediction_error_magnitude > 0.6:
            delta.prediction_error_magnitude = context_update.prediction_error_magnitude
            if context_update.prediction_error_magnitude > 0.75:
                # High prediction error = potential contradiction with prior beliefs
                delta.contradicted_belief_ids.append(context_update.domain or "context")

        # ── Apply delta to belief state ──
        self._apply_delta(delta)

        # ── Recompute free energy ──
        new_vfe = self._beliefs.compute_free_energy()
        self._beliefs = self._beliefs.model_copy(
            update={
                "free_energy": new_vfe,
                "last_updated": utc_now(),
            }
        )

        self._logger.debug(
            "beliefs_updated",
            entities=len(self._beliefs.entities),
            individuals=len(self._beliefs.individual_beliefs),
            vfe=round(new_vfe, 3),
            prediction_error=round(delta.prediction_error_magnitude, 3),
        )

        return delta

    def update_from_outcome(
        self,
        outcome_description: str,
        success: bool,
        precision: float = 0.7,
    ) -> None:
        """
        Update beliefs from an intent outcome.
        Success → increase epistemic confidence, reduce free energy.
        Failure → decrease capability confidence for relevant domain.
        """
        if success:
            new_confidence = min(1.0, self._beliefs.overall_confidence + 0.03 * precision)
        else:
            new_confidence = max(0.1, self._beliefs.overall_confidence - 0.05 * precision)

        self._beliefs = self._beliefs.model_copy(
            update={"overall_confidence": new_confidence}
        )

        # Update self-belief epistemic confidence
        self_belief = self._beliefs.self_belief
        if success:
            new_epistemic = min(1.0, self_belief.epistemic_confidence + 0.02)
        else:
            new_epistemic = max(0.1, self_belief.epistemic_confidence - 0.05)

        updated_self = self_belief.model_copy(update={"epistemic_confidence": new_epistemic})
        self._beliefs = self._beliefs.model_copy(update={"self_belief": updated_self})

    def decay_unobserved_entities(self) -> None:
        """
        Apply confidence decay to entities not observed in this cycle.
        This is the 'forgetting' mechanism — beliefs weaken without evidence.
        Prune entities below the minimum confidence threshold.
        """
        updated: dict[str, EntityBelief] = {}
        for eid, belief in self._beliefs.entities.items():
            new_conf = max(0.0, belief.confidence - _CONFIDENCE_DECAY)
            if new_conf >= _MIN_ENTITY_CONFIDENCE:
                updated[eid] = belief.model_copy(update={"confidence": new_conf})
        self._beliefs = self._beliefs.model_copy(update={"entities": updated})

    def inject_entity(self, entity_id: str, name: str, confidence: float = 0.5) -> None:
        """Manually inject an entity belief (for testing or initialisation)."""
        belief = EntityBelief(
            entity_id=entity_id,
            name=name,
            confidence=confidence,
            last_observed=utc_now(),
        )
        updated = dict(self._beliefs.entities)
        updated[entity_id] = belief
        self._beliefs = self._beliefs.model_copy(update={"entities": updated})

    # ─── Private ──────────────────────────────────────────────────

    def _apply_delta(self, delta: BeliefDelta) -> None:
        """Apply a BeliefDelta to the belief state (immutable model_copy pattern)."""
        updated_entities = dict(self._beliefs.entities)
        updated_individuals = dict(self._beliefs.individual_beliefs)

        # Apply entity updates
        for eid, belief in delta.entity_updates.items():
            updated_entities[eid] = belief

        # Apply entity additions (cap total at _MAX_ENTITY_BELIEFS)
        for eid, belief in delta.entity_additions.items():
            if len(updated_entities) < _MAX_ENTITY_BELIEFS:
                updated_entities[eid] = belief

        # Apply removals
        for eid in delta.entity_removals:
            updated_entities.pop(eid, None)

        # Apply individual updates
        for iid, belief in delta.individual_updates.items():
            updated_individuals[iid] = belief

        # Recompute overall confidence (mean of entity confidences)
        if updated_entities:
            mean_conf = sum(e.confidence for e in updated_entities.values()) / len(updated_entities)
        else:
            mean_conf = self._beliefs.overall_confidence

        self._beliefs = self._beliefs.model_copy(
            update={
                "entities": updated_entities,
                "individual_beliefs": updated_individuals,
                "overall_confidence": mean_conf,
            }
        )

    def _update_context(
        self,
        broadcast: WorkspaceBroadcast,
        precision: float,
    ) -> ContextBelief:
        """
        Derive an updated ContextBelief from the broadcast.
        Carries forward prior context with precision-weighted new evidence.
        """
        prior = self._beliefs.current_context

        # Extract content summary from broadcast
        content = broadcast.content
        content_text = ""
        if hasattr(content, "content") and hasattr(content.content, "content"):
            content_text = str(content.content.content or "")
        elif hasattr(content, "content") and isinstance(content.content, str):
            content_text = content.content
        elif isinstance(content, str):
            content_text = content

        # Estimate prediction error from salience scores
        pe_magnitude = 0.0
        if broadcast.salience and broadcast.salience.prediction_error:
            pe_magnitude = broadcast.salience.prediction_error.magnitude
        elif broadcast.salience:
            novelty = broadcast.salience.scores.get("novelty", 0.0)
            pe_magnitude = novelty * 0.8

        # Update domain estimate (carry forward unless new content is clear)
        domain = prior.domain
        if content_text:
            domain = _infer_domain(content_text) or prior.domain

        # Precision-weighted confidence update
        new_confidence = prior.confidence + precision * (1.0 - prior.confidence) * 0.3

        return ContextBelief(
            summary=content_text[:200] if content_text else prior.summary,
            domain=domain,
            is_active_dialogue=_is_dialogue(content_text),
            prediction_error_magnitude=pe_magnitude,
            confidence=min(1.0, new_confidence),
        )


# ─── Module-Level Update Functions ───────────────────────────────


def _bayesian_update_entity(
    prior: EntityBelief,
    precision: float,
) -> EntityBelief:
    """
    Precision-weighted Bayesian update for an entity belief.
    Evidence accumulation: confidence converges to 1.0 as more evidence arrives.

    new_confidence = old_confidence + precision × (1 - old_confidence)
    This is the discrete-time version of exponential approach to certainty.
    """
    new_confidence = prior.confidence + precision * (1.0 - prior.confidence)
    new_confidence = min(1.0, max(0.0, new_confidence))
    return prior.model_copy(
        update={
            "confidence": new_confidence,
            "last_observed": utc_now(),
        }
    )


def _update_individual_belief(
    individual_id: str,
    current: IndividualBelief | None,
    broadcast: WorkspaceBroadcast,
    precision: float,
) -> IndividualBelief:
    """
    Update beliefs about a specific individual from a broadcast.
    Affect state informs estimated valence; precision weights the update.
    """
    if current is None:
        current = IndividualBelief(individual_id=individual_id)

    # Use broadcast affect as a signal about the individual's emotional state
    affect = broadcast.affect
    # Precision-weighted update toward observed valence
    new_valence = current.estimated_valence + precision * (affect.valence - current.estimated_valence) * 0.3
    new_valence_conf = min(1.0, current.valence_confidence + precision * 0.1)

    # Update engagement from arousal signal
    new_engagement = current.engagement_level + precision * (affect.arousal - current.engagement_level) * 0.2

    return current.model_copy(
        update={
            "estimated_valence": max(-1.0, min(1.0, new_valence)),
            "valence_confidence": new_valence_conf,
            "engagement_level": max(0.0, min(1.0, new_engagement)),
            "last_updated": utc_now(),
        }
    )


def _extract_individual_id(broadcast: WorkspaceBroadcast) -> str | None:
    """Extract an individual's ID from a broadcast, if present."""
    content = broadcast.content
    # Check various paths where a speaker/addressee ID might be stored
    for path in [
        ("content", "speaker_id"),
        ("metadata", "speaker_id"),
        ("speaker_id",),
    ]:
        obj = content
        for attr in path:
            obj = getattr(obj, attr, None)
            if obj is None:
                break
        if isinstance(obj, str) and obj:
            return obj
    return None


def _infer_domain(text: str) -> str:
    """
    Lightweight heuristic domain inference from content.
    Used to categorise the current conversational context.
    """
    text_lower = text.lower()
    if any(w in text_lower for w in ["feel", "hurt", "sad", "worried", "scared", "happy", "excited"]):
        return "emotional"
    if any(w in text_lower for w in ["code", "function", "api", "algorithm", "system", "error"]):
        return "technical"
    if any(w in text_lower for w in ["community", "member", "together", "group", "we "]):
        return "social"
    if any(w in text_lower for w in ["help", "assist", "please", "need", "want", "can you"]):
        return "request"
    return "general"


def _is_dialogue(text: str) -> bool:
    """Detect if the content is part of an active conversation."""
    if not text:
        return False
    text_lower = text.lower()
    # Questions, direct address, or conversational markers
    return (
        "?" in text
        or any(text_lower.startswith(p) for p in ["hey", "hi", "hello", "eos,", "can you", "could you"])
        or len(text) < 200  # Short messages are typically conversational
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\deliberation_engine.py =====

"""
EcodiaOS — Nova Deliberation Engine

The dual-process decision engine. Implements the System 1 / System 2 split
from cognitive architecture — not as a performance trick, but as a genuine
model of how deliberation works.

Fast path (System 1, ≤150ms total):
  - Pattern-match against known procedure templates
  - Build intent directly from matched procedure
  - Submit to Equor for critical-path review (≤50ms)
  - If denied, escalate to slow path

Slow path (System 2, ≤5000ms total):
  - Generate 2-5 candidate policies via LLM (≤3000ms)
  - Evaluate EFE for each candidate in parallel (≤200ms per policy)
  - Select minimum-EFE policy
  - Formulate Intent from selected policy
  - Submit to Equor for standard review (≤500ms)
  - If denied, retry with next-best policy

Routing decision:
  The choice between fast and slow path is itself a decision.
  Novelty, risk, emotional intensity, and belief conflict drive it.
  Over time (via Evo), more situations shift from slow to fast as
  reliable patterns are learned.

The null outcome: if no active goal matches and the broadcast doesn't
warrant creating one, deliberation returns None — no action taken.
This is the correct outcome, not a failure.
"""

from __future__ import annotations

import asyncio
import time
from typing import TYPE_CHECKING

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import Verdict, new_id, utc_now
from ecodiaos.primitives.constitutional import ConstitutionalCheck
from ecodiaos.primitives.intent import (
    Action,
    ActionSequence,
    DecisionTrace,
    GoalDescriptor,
    Intent,
)
from ecodiaos.systems.atune.types import WorkspaceBroadcast
from ecodiaos.systems.nova.efe_evaluator import EFEEvaluator
from ecodiaos.systems.nova.goal_manager import GoalManager
from ecodiaos.systems.nova.policy_generator import (
    PolicyGenerator,
    find_matching_procedure,
    make_do_nothing_policy,
    procedure_to_policy,
)
from ecodiaos.systems.nova.types import (
    BeliefState,
    DecisionRecord,
    EFEScore,
    Goal,
    Policy,
    PriorityContext,
    SituationAssessment,
)

if TYPE_CHECKING:
    from ecodiaos.systems.equor.service import EquorService

logger = structlog.get_logger()

# Thresholds that trigger slow-path deliberation (from spec)
_NOVELTY_THRESHOLD = 0.6
_RISK_THRESHOLD = 0.5
_EMOTIONAL_THRESHOLD = 0.7
_PRECISION_THRESHOLD = 0.8


class DeliberationEngine:
    """
    The dual-process cognitive architecture.

    Receives workspace broadcasts, assesses situation, routes to fast or slow
    deliberation, and returns a constitutional Intent (or None for do-nothing).

    Performance targets:
      - Fast path: ≤150ms total (≤100ms procedure + ≤50ms Equor critical)
      - Slow path: ≤5000ms total (≤3000ms generation + ≤500ms Equor + overhead)
    """

    def __init__(
        self,
        goal_manager: GoalManager,
        policy_generator: PolicyGenerator,
        efe_evaluator: EFEEvaluator,
        equor: "EquorService",
        drive_weights: dict[str, float] | None = None,
        fast_path_timeout_ms: int = 100,
        slow_path_timeout_ms: int = 5000,
    ) -> None:
        self._goals = goal_manager
        self._policy_gen = policy_generator
        self._efe = efe_evaluator
        self._equor = equor
        self._drive_weights = drive_weights or {
            "coherence": 1.0, "care": 1.0, "growth": 1.0, "honesty": 1.0
        }
        self._fast_timeout = fast_path_timeout_ms / 1000.0
        self._slow_timeout = slow_path_timeout_ms / 1000.0
        self._last_equor_check: ConstitutionalCheck | None = None
        self._logger = logger.bind(system="nova.deliberation_engine")

    def update_drive_weights(self, weights: dict[str, float]) -> None:
        """Called by NovaService when constitution changes."""
        self._drive_weights = weights

    @property
    def last_equor_check(self) -> ConstitutionalCheck | None:
        """The Equor check from the most recent approved intent."""
        return self._last_equor_check

    async def deliberate(
        self,
        broadcast: WorkspaceBroadcast,
        belief_state: BeliefState,
        affect: AffectState,
        belief_delta_is_conflicting: bool = False,
        memory_traces: list[dict] | None = None,
    ) -> tuple[Intent | None, DecisionRecord]:
        """
        Main deliberation entry point.

        Returns (Intent | None, DecisionRecord).
        Returns None when the best action is no action.
        DecisionRecord is always returned for observability.
        """
        start = time.monotonic()
        self._last_equor_check = None  # Reset per-deliberation

        try:
            # End-to-end timeout: the entire deliberation (including possible
            # fast→slow escalation) must complete within the slow-path budget.
            async with asyncio.timeout(self._slow_timeout):
                return await self._deliberate_inner(
                    broadcast, belief_state, affect,
                    belief_delta_is_conflicting, memory_traces, start,
                )
        except asyncio.TimeoutError:
            elapsed_ms = int((time.monotonic() - start) * 1000)
            self._logger.warning("deliberation_end_to_end_timeout", elapsed_ms=elapsed_ms)
            record = DecisionRecord(
                broadcast_id=broadcast.broadcast_id,
                path="timeout",
                latency_ms=elapsed_ms,
            )
            return None, record

    async def _deliberate_inner(
        self,
        broadcast: WorkspaceBroadcast,
        belief_state: BeliefState,
        affect: AffectState,
        belief_delta_is_conflicting: bool,
        memory_traces: list[dict] | None,
        start: float,
    ) -> tuple[Intent | None, DecisionRecord]:
        """Inner deliberation logic, called within the end-to-end timeout."""
        # Recompute goal priorities before deliberating
        priority_ctx = PriorityContext(
            current_affect=affect,
            drive_weights=self._drive_weights,
        )
        self._goals.recompute_priorities(priority_ctx)

        # Assess situation to determine path
        assessment = self._assess_situation(
            broadcast=broadcast,
            belief_conflict=belief_delta_is_conflicting,
        )

        record = DecisionRecord(
            broadcast_id=broadcast.broadcast_id,
            situation_assessment=assessment,
        )

        # Route to appropriate path
        if assessment.requires_deliberation:
            self._logger.debug("deliberation_slow_path", broadcast_id=broadcast.broadcast_id)
            intent = await self._slow_path(broadcast, assessment, belief_state, affect, memory_traces)
            path = "slow"
        else:
            self._logger.debug("deliberation_fast_path", broadcast_id=broadcast.broadcast_id)
            intent, escalated = await self._fast_path(broadcast, assessment, belief_state, affect)
            path = "slow" if escalated else "fast"
            if escalated and intent is None:
                # Fast path escalated and slow path succeeded
                intent = await self._slow_path(broadcast, assessment, belief_state, affect, memory_traces)

        elapsed_ms = int((time.monotonic() - start) * 1000)

        record = record.model_copy(
            update={
                "path": path if intent is not None else ("do_nothing" if not assessment.requires_deliberation else "no_goal"),
                "intent_dispatched": intent is not None,
                "latency_ms": elapsed_ms,
            }
        )

        self._logger.info(
            "deliberation_complete",
            path=record.path,
            intent_dispatched=intent is not None,
            latency_ms=elapsed_ms,
        )
        return intent, record

    # ─── Situation Assessment ─────────────────────────────────────

    def _assess_situation(
        self,
        broadcast: WorkspaceBroadcast,
        belief_conflict: bool,
    ) -> SituationAssessment:
        """
        Determine if deliberative (slow) or habitual (fast) processing is needed.
        Must complete in ≤20ms.
        """
        salience_scores = broadcast.salience.scores if broadcast.salience.scores else {}
        novelty = salience_scores.get("novelty", 0.0)
        risk = salience_scores.get("risk", 0.0)
        emotional = salience_scores.get("emotional", 0.0)
        precision = broadcast.precision

        requires_deliberation = (
            novelty > _NOVELTY_THRESHOLD
            or risk > _RISK_THRESHOLD
            or emotional > _EMOTIONAL_THRESHOLD
            or belief_conflict
            or precision > _PRECISION_THRESHOLD
        )

        has_procedure = find_matching_procedure(broadcast) is not None

        return SituationAssessment(
            novelty=novelty,
            risk=risk,
            emotional_intensity=emotional,
            belief_conflict=belief_conflict,
            requires_deliberation=requires_deliberation,
            has_matching_procedure=has_procedure,
            broadcast_precision=precision,
        )

    # ─── Fast Path ────────────────────────────────────────────────

    async def _fast_path(
        self,
        broadcast: WorkspaceBroadcast,
        assessment: SituationAssessment,
        belief_state: BeliefState,
        affect: AffectState,
    ) -> tuple[Intent | None, bool]:
        """
        System 1: Pattern-match → build intent → Equor critical review.
        Returns (intent | None, escalated_to_slow).
        """
        try:
            async with asyncio.timeout(self._fast_timeout):
                procedure = find_matching_procedure(broadcast)
                if procedure is None:
                    return None, True  # No matching procedure → escalate

                policy = procedure_to_policy(procedure)

                # Find or create a goal for this intent
                goal = self._goals.find_relevant_goal(broadcast)
                if goal is None:
                    goal = self._goals.create_from_broadcast(broadcast)
                if goal is None:
                    return None, False  # No goal → do nothing

                intent = _policy_to_intent(policy, goal, path="fast", confidence=procedure["success_rate"])

                # Equor critical-path review (≤50ms budget within fast path)
                check = await self._equor.review(intent)

                if check.verdict == Verdict.APPROVED:
                    self._last_equor_check = check
                    return intent, False
                elif check.verdict == Verdict.MODIFIED:
                    self._last_equor_check = check
                    intent = _apply_modifications(intent, check.modifications)
                    return intent, False
                else:
                    # Denied by Equor → escalate to slow path
                    self._logger.info("fast_path_equor_denied_escalating", intent_id=intent.id)
                    return None, True

        except asyncio.TimeoutError:
            self._logger.warning("fast_path_timeout_escalating")
            return None, True
        except Exception as exc:
            self._logger.error("fast_path_error", error=str(exc))
            return None, True

    # ─── Slow Path ────────────────────────────────────────────────

    async def _slow_path(
        self,
        broadcast: WorkspaceBroadcast,
        assessment: SituationAssessment,
        belief_state: BeliefState,
        affect: AffectState,
        memory_traces: list[dict] | None,
    ) -> Intent | None:
        """
        System 2: Generate → EFE score → select → Equor standard review.
        """
        try:
            async with asyncio.timeout(self._slow_timeout):
                # ── Find or create goal ──
                goal = self._goals.find_relevant_goal(broadcast)
                if goal is None:
                    goal = self._goals.create_from_broadcast(broadcast)
                if goal is None:
                    return None  # No goal warrants no action

                # ── Extract situation summary for policy generation ──
                situation = _extract_situation_summary(broadcast)

                # ── Generate candidate policies (up to 3000ms) ──
                candidates = await self._policy_gen.generate_candidates(
                    goal=goal,
                    situation_summary=situation,
                    beliefs=belief_state,
                    affect=affect,
                    memory_traces=memory_traces,
                )

                if not candidates:
                    return None

                # ── Evaluate EFE for all candidates (parallelised) ──
                scored = await self._efe.evaluate_all(
                    policies=candidates,
                    goal=goal,
                    beliefs=belief_state,
                    affect=affect,
                    drive_weights=self._drive_weights,
                )

                # scored is sorted: lowest EFE first
                # If do-nothing wins, return None
                if scored and scored[0][0].id == "do_nothing":
                    self._logger.info(
                        "do_nothing_policy_selected",
                        goal=goal.description[:60],
                        do_nothing_efe=scored[0][1].total,
                    )
                    return None

                # ── Equor review with retry on denial ──
                for policy, efe_score in scored:
                    if policy.id == "do_nothing":
                        continue  # Skip do-nothing — if we're here, we want to act

                    intent = _policy_to_intent(
                        policy,
                        goal,
                        path="slow",
                        confidence=efe_score.confidence,
                        efe_score=efe_score,
                        all_efe_scores={p.name: e.total for p, e in scored},
                    )

                    check = await self._equor.review(intent)

                    if check.verdict == Verdict.APPROVED:
                        self._last_equor_check = check
                        return intent
                    elif check.verdict == Verdict.MODIFIED:
                        self._last_equor_check = check
                        return _apply_modifications(intent, check.modifications)
                    elif check.verdict == Verdict.DEFERRED:
                        self._logger.info("intent_deferred_by_equor", intent_id=intent.id)
                        return None  # Governance will handle
                    # BLOCKED → try next policy
                    self._logger.info(
                        "policy_blocked_by_equor_trying_next",
                        policy=policy.name,
                        reasoning=check.reasoning[:80],
                    )

                return None  # All policies blocked

        except asyncio.TimeoutError:
            self._logger.warning("slow_path_timeout")
            return None
        except Exception as exc:
            self._logger.error("slow_path_error", error=str(exc))
            return None


# ─── Intent Construction ─────────────────────────────────────────


def _policy_to_intent(
    policy: Policy,
    goal: Goal,
    path: str,
    confidence: float = 0.7,
    efe_score: EFEScore | None = None,
    all_efe_scores: dict[str, float] | None = None,
) -> Intent:
    """
    Convert a selected Policy into a formal Intent primitive.
    Intents are the cross-system communication unit; Policies are Nova-internal.
    """
    actions = [
        Action(
            executor=f"executor.{step.action_type}",
            parameters={
                "description": step.description,
                **step.parameters,
            },
            timeout_ms=step.expected_duration_ms * 2,
        )
        for step in policy.steps
    ]

    efe_reasoning = ""
    if efe_score:
        efe_reasoning = efe_score.reasoning

    trace_alternatives = []
    if all_efe_scores:
        trace_alternatives = [{"policy": name, "efe": efe} for name, efe in all_efe_scores.items()]

    return Intent(
        id=new_id(),
        goal=GoalDescriptor(
            description=goal.description,
            target_domain=goal.target_domain,
            success_criteria={"criteria": goal.success_criteria} if goal.success_criteria else {},
        ),
        plan=ActionSequence(steps=actions),
        expected_free_energy=efe_score.total if efe_score else 0.0,
        priority=goal.priority,
        decision_trace=DecisionTrace(
            reasoning=(
                f"Path: {path}. Policy: {policy.name}. {policy.reasoning[:200]}. "
                f"EFE evaluation: {efe_reasoning}"
            ),
            alternatives_considered=trace_alternatives,
            free_energy_scores=all_efe_scores or {},
        ),
    )


def _apply_modifications(intent: Intent, modifications: list[str]) -> Intent:
    """Apply Equor's suggested modifications to an intent."""
    existing_reasoning = intent.decision_trace.reasoning
    modification_notes = "; ".join(modifications[:5])
    updated_trace = intent.decision_trace.model_copy(
        update={
            "reasoning": f"{existing_reasoning} | Equor modifications: {modification_notes}"
        }
    )
    return intent.model_copy(update={"decision_trace": updated_trace})


def _extract_situation_summary(broadcast: WorkspaceBroadcast) -> str:
    """Extract a brief situation summary from a broadcast for policy generation."""
    content = broadcast.content
    paths = [
        ("content", "content"),
        ("content",),
    ]
    for path in paths:
        obj: object = content
        for attr in path:
            obj = getattr(obj, attr, None)
            if obj is None:
                break
        if isinstance(obj, str) and obj:
            return obj[:400]
    return f"Broadcast from workspace (salience: {broadcast.salience.composite:.2f})"

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\efe_evaluator.py =====

"""
EcodiaOS — Nova EFE Evaluator

Computes Expected Free Energy for each candidate policy.

The full EFE decomposition (from the spec and Friston et al.):

    G(π) = -[pragmatic_value + epistemic_value + constitutional + feasibility]
           + risk_penalty

All positive components are negated (lower total = preferred).

The five components:
1. Pragmatic value  (0.35) — probability of achieving the goal state
2. Epistemic value  (0.20) — expected information gain (Growth drive)
3. Constitutional   (0.20) — alignment with drive weights (constitutional fit)
4. Feasibility      (0.15) — can we actually do this given our capabilities?
5. Risk             (0.10) — expected harm if we're wrong (positive = increases EFE)

Weights start at spec defaults; Evo adjusts them over time as evidence accumulates.

Pragmatic and epistemic components require LLM evaluation for slow-path policies.
Constitutional alignment is computed analytically from the DriveAlignmentVector.
Feasibility and risk are computed heuristically (fast, no LLM needed).
"""

from __future__ import annotations

import json
import time

import structlog

from ecodiaos.clients.llm import LLMProvider, Message
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.clients.output_validator import OutputValidator
from ecodiaos.primitives.affect import AffectState
from ecodiaos.prompts.nova.policy import (
    build_epistemic_value_prompt,
    build_pragmatic_value_prompt,
    summarise_beliefs,
)
from ecodiaos.systems.nova.efe_heuristics import EFEHeuristics
from ecodiaos.systems.nova.policy_generator import DO_NOTHING_EFE, make_do_nothing_policy
from ecodiaos.systems.nova.types import (
    BeliefState,
    EFEScore,
    EFEWeights,
    EpistemicEstimate,
    Goal,
    Policy,
    PragmaticEstimate,
    RiskEstimate,
)

logger = structlog.get_logger()

# LLM evaluation timeout per policy (must fit within slow-path 5000ms budget)
_EVAL_TIMEOUT_MS = 200

# High-risk action types that increase EFE
_HIGH_RISK_ACTIONS = {"federate", "external_api", "irreversible"}
# Conservative action types that reduce risk
_LOW_RISK_ACTIONS = {"observe", "wait", "express"}


class EFEEvaluator:
    """
    Evaluates Expected Free Energy for candidate policies.

    For the do-nothing policy, EFE is fixed at DO_NOTHING_EFE (-0.1).
    For all other policies, EFE is computed from the five components above.

    LLM calls are made for pragmatic and epistemic estimation when in slow path.
    All other components are computed analytically.
    """

    def __init__(
        self,
        llm: LLMProvider,
        weights: EFEWeights | None = None,
        use_llm_estimation: bool = True,
    ) -> None:
        self._llm = llm
        self._weights = weights or EFEWeights()
        self._use_llm = use_llm_estimation
        self._logger = logger.bind(system="nova.efe_evaluator")
        # Optimization: detect if we have the optimized provider for budget/cache
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        self._heuristics = EFEHeuristics()
        self._validator = OutputValidator()

    @property
    def weights(self) -> EFEWeights:
        return self._weights

    def update_weights(self, new_weights: EFEWeights) -> None:
        """Called by Evo after learning that certain EFE components predict outcomes better."""
        self._weights = new_weights

    async def evaluate(
        self,
        policy: Policy,
        goal: Goal,
        beliefs: BeliefState,
        affect: AffectState,
        drive_weights: dict[str, float],
    ) -> EFEScore:
        """
        Compute full EFE score for a policy.

        G(π) = -[p*pragmatic + e*epistemic + c*constitutional + f*feasibility] + r*risk

        Lower G = more preferred policy.
        """
        start = time.monotonic()

        # ── Do-nothing gets the fixed baseline ──
        if policy.id == "do_nothing":
            score = EFEScore(
                total=DO_NOTHING_EFE,
                pragmatic=PragmaticEstimate(score=0.1, success_probability=0.05, confidence=0.9),
                epistemic=EpistemicEstimate(score=0.3, uncertainties_addressed=0, expected_info_gain=0.2, novelty=0.1),
                constitutional_alignment=0.3,
                feasibility=1.0,  # Always feasible
                risk=RiskEstimate(expected_harm=0.0, reversibility=1.0),
                confidence=0.95,
                reasoning="Do-nothing policy: fixed EFE baseline. Any active policy must beat -0.10.",
            )
            score = score.model_copy(update={"total": DO_NOTHING_EFE})
            return score

        # ── Pragmatic value ──
        # Budget-aware: check if we should use LLM or fall back to heuristics
        use_llm_pragmatic = self._use_llm
        use_llm_epistemic = self._use_llm
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("nova.efe.pragmatic", estimated_tokens=200):
                use_llm_pragmatic = False
                self._heuristics.log_heuristic_fallback(
                    "nova.efe.pragmatic", "budget_exhausted", policy.type if hasattr(policy, 'type') else "unknown"
                )
            if not self._llm.should_use_llm("nova.efe.epistemic", estimated_tokens=150):
                use_llm_epistemic = False
                self._heuristics.log_heuristic_fallback(
                    "nova.efe.epistemic", "budget_exhausted", policy.type if hasattr(policy, 'type') else "unknown"
                )

        if use_llm_pragmatic:
            pragmatic = await self._estimate_pragmatic_llm(policy, goal, beliefs)
        else:
            pragmatic = _estimate_pragmatic_heuristic(policy, goal)

        # ── Epistemic value ──
        if use_llm_epistemic:
            epistemic = await self._estimate_epistemic_llm(policy, beliefs)
        else:
            epistemic = _estimate_epistemic_heuristic(policy, beliefs)

        # ── Constitutional alignment (analytical) ──
        constitutional = _compute_constitutional_alignment(
            policy=policy,
            drive_weights=drive_weights,
            goal=goal,
        )

        # ── Feasibility (heuristic) ──
        feasibility = _estimate_feasibility(policy, beliefs)

        # ── Risk (heuristic) ──
        risk = _estimate_risk(policy)

        # ── Weighted EFE total ──
        w = self._weights
        total = (
            - w.pragmatic * pragmatic.score
            - w.epistemic * epistemic.score
            - w.constitutional * constitutional
            - w.feasibility * feasibility
            + w.risk * risk.expected_harm
        )

        elapsed_ms = int((time.monotonic() - start) * 1000)
        self._logger.debug(
            "efe_evaluated",
            policy=policy.name,
            total=round(total, 4),
            pragmatic=round(pragmatic.score, 3),
            epistemic=round(epistemic.score, 3),
            constitutional=round(constitutional, 3),
            feasibility=round(feasibility, 3),
            risk=round(risk.expected_harm, 3),
            elapsed_ms=elapsed_ms,
        )

        reasoning = (
            f"Pragmatic={pragmatic.score:.2f} (prob={pragmatic.success_probability:.2f}), "
            f"Epistemic={epistemic.score:.2f}, "
            f"Constitutional={constitutional:.2f}, "
            f"Feasibility={feasibility:.2f}, "
            f"Risk={risk.expected_harm:.2f} → "
            f"G(π)={total:.3f}"
        )

        return EFEScore(
            pragmatic=pragmatic,
            epistemic=epistemic,
            constitutional_alignment=constitutional,
            feasibility=feasibility,
            risk=risk,
            total=total,
            confidence=min(pragmatic.confidence, feasibility),
            reasoning=reasoning,
        )

    async def evaluate_all(
        self,
        policies: list[Policy],
        goal: Goal,
        beliefs: BeliefState,
        affect: AffectState,
        drive_weights: dict[str, float],
    ) -> list[tuple[Policy, EFEScore]]:
        """
        Evaluate EFE for all candidate policies.
        Results are sorted by EFE (lowest = most preferred).
        """
        import asyncio

        # Evaluate all policies in parallel (within the 5000ms budget)
        tasks = [
            self.evaluate(policy, goal, beliefs, affect, drive_weights)
            for policy in policies
        ]
        scores = await asyncio.gather(*tasks, return_exceptions=True)

        results: list[tuple[Policy, EFEScore]] = []
        for policy, score in zip(policies, scores):
            if isinstance(score, Exception):
                self._logger.warning(
                    "efe_evaluation_error",
                    policy=policy.name,
                    error=str(score),
                )
                # Assign a neutral EFE on failure so it doesn't block selection
                score = EFEScore(total=0.0, reasoning="Evaluation failed — using neutral score")
            results.append((policy, score))

        # Sort: minimum EFE first
        results.sort(key=lambda x: x[1].total)
        return results

    # ─── Private LLM Estimators ───────────────────────────────────

    async def _estimate_pragmatic_llm(
        self,
        policy: Policy,
        goal: Goal,
        beliefs: BeliefState,
    ) -> PragmaticEstimate:
        """LLM-based pragmatic value estimation with caching and validation."""
        steps_desc = " → ".join(s.description for s in policy.steps) or "No steps specified"
        prompt = build_pragmatic_value_prompt(
            policy_name=policy.name,
            policy_reasoning=policy.reasoning,
            policy_steps_desc=steps_desc,
            goal_description=goal.description,
            goal_success_criteria=goal.success_criteria,
            beliefs_summary=summarise_beliefs(beliefs, max_entities=3),
        )
        try:
            # Use cache-tagged evaluate if optimized provider is available
            if self._optimized:
                response = await self._llm.evaluate(  # type: ignore[call-arg]
                    prompt, max_tokens=200, temperature=0.2,
                    cache_system="nova.efe.pragmatic", cache_method="evaluate",
                )
            else:
                response = await self._llm.evaluate(prompt, max_tokens=200, temperature=0.2)

            # Use output validator for robust JSON extraction
            data = self._validator.extract_json(response.text)
            if data and isinstance(data, dict):
                data = self._validator.auto_fix_dict(
                    data,
                    required_keys=["success_probability", "confidence", "reasoning"],
                    defaults={"success_probability": 0.5, "confidence": 0.5, "reasoning": ""},
                )
                return PragmaticEstimate(
                    score=float(data.get("success_probability", 0.5)),
                    success_probability=float(data.get("success_probability", 0.5)),
                    confidence=float(data.get("confidence", 0.5)),
                    reasoning=str(data.get("reasoning", ""))[:200],
                )
        except Exception:
            pass
        return _estimate_pragmatic_heuristic(policy, goal)

    async def _estimate_epistemic_llm(
        self,
        policy: Policy,
        beliefs: BeliefState,
    ) -> EpistemicEstimate:
        """LLM-based epistemic value estimation with caching and validation."""
        steps_desc = " → ".join(s.description for s in policy.steps)
        known_uncertainties = _identify_uncertain_domains(beliefs)
        prompt = build_epistemic_value_prompt(
            policy_name=policy.name,
            policy_steps_desc=steps_desc,
            beliefs_summary=summarise_beliefs(beliefs, max_entities=3),
            known_uncertainties=known_uncertainties,
        )
        try:
            # Use cache-tagged evaluate if optimized provider is available
            if self._optimized:
                response = await self._llm.evaluate(  # type: ignore[call-arg]
                    prompt, max_tokens=150, temperature=0.2,
                    cache_system="nova.efe.epistemic", cache_method="evaluate",
                )
            else:
                response = await self._llm.evaluate(prompt, max_tokens=150, temperature=0.2)

            # Use output validator for robust JSON extraction
            data = self._validator.extract_json(response.text)
            if data and isinstance(data, dict):
                data = self._validator.auto_fix_dict(
                    data,
                    required_keys=["info_gain", "uncertainties_addressed", "novelty"],
                    defaults={"info_gain": 0.3, "uncertainties_addressed": 0, "novelty": 0.2},
                )
                return EpistemicEstimate(
                    score=float(data.get("info_gain", 0.3)),
                    uncertainties_addressed=int(data.get("uncertainties_addressed", 0)),
                    expected_info_gain=float(data.get("info_gain", 0.3)),
                    novelty=float(data.get("novelty", 0.2)),
                )
        except Exception:
            pass
        return _estimate_epistemic_heuristic(policy, beliefs)


# ─── Analytical / Heuristic Estimators ───────────────────────────


def _estimate_pragmatic_heuristic(policy: Policy, goal: Goal) -> PragmaticEstimate:
    """
    Heuristic pragmatic estimate when LLM is unavailable.
    Based on effort level and action type alignment with the goal.
    """
    effort_map = {"none": 0.05, "low": 0.5, "medium": 0.65, "high": 0.75}
    base = effort_map.get(policy.estimated_effort, 0.5)

    # "express" actions are always somewhat pragmatic for dialogue goals
    has_express = any(s.action_type == "express" for s in policy.steps)
    if has_express and "dialogue" in goal.target_domain.lower():
        base = min(1.0, base + 0.1)

    return PragmaticEstimate(
        score=base,
        success_probability=base,
        confidence=0.4,  # Low confidence in heuristic
        reasoning="Heuristic estimate based on effort level",
    )


def _estimate_epistemic_heuristic(policy: Policy, beliefs: BeliefState) -> EpistemicEstimate:
    """
    Heuristic epistemic estimate.
    Policies that observe or ask questions have higher epistemic value.
    """
    epistemic_action_types = {"observe", "request_info"}
    epistemic_steps = sum(1 for s in policy.steps if s.action_type in epistemic_action_types)
    n = max(1, len(policy.steps))
    epistemic_ratio = epistemic_steps / n

    # Low overall confidence → more to learn → higher epistemic value
    uncertainty_bonus = (1.0 - beliefs.overall_confidence) * 0.3

    score = min(1.0, epistemic_ratio * 0.6 + uncertainty_bonus)
    return EpistemicEstimate(
        score=score,
        uncertainties_addressed=epistemic_steps,
        expected_info_gain=score,
        novelty=0.0,
    )


def _compute_constitutional_alignment(
    policy: Policy,
    drive_weights: dict[str, float],
    goal: Goal,
) -> float:
    """
    Compute constitutional alignment analytically.

    This is the inner product of the goal's drive alignment vector
    with the current constitutional drive weights, normalised to [0,1].

    A policy serving a goal that aligns with strong drives gets a
    constitutional boost. This is how the constitution shapes behaviour
    without explicit rule-following.
    """
    alignment = goal.drive_alignment

    w_coherence = drive_weights.get("coherence", 1.0)
    w_care = drive_weights.get("care", 1.0)
    w_growth = drive_weights.get("growth", 1.0)
    w_honesty = drive_weights.get("honesty", 1.0)

    weighted = (
        alignment.coherence * w_coherence
        + alignment.care * w_care
        + alignment.growth * w_growth
        + alignment.honesty * w_honesty
    )
    weight_sum = w_coherence + w_care + w_growth + w_honesty or 4.0
    return min(1.0, max(0.0, weighted / weight_sum))


def _estimate_feasibility(policy: Policy, beliefs: BeliefState) -> float:
    """
    Heuristic feasibility: can we actually execute this policy?

    Based on capability beliefs and cognitive load.
    """
    # Do-nothing and observe are always feasible
    if all(s.action_type in {"observe", "wait"} for s in policy.steps):
        return 1.0

    # Base feasibility from epistemic confidence (we know what we're doing)
    base = beliefs.self_belief.epistemic_confidence

    # Penalise high cognitive load
    load_penalty = beliefs.self_belief.cognitive_load * 0.2

    # High-effort policies are less feasible
    effort_penalty = {"none": 0.0, "low": 0.0, "medium": 0.05, "high": 0.15}.get(
        policy.estimated_effort, 0.05
    )

    return max(0.1, min(1.0, base - load_penalty - effort_penalty))


def _estimate_risk(policy: Policy) -> RiskEstimate:
    """
    Heuristic risk estimation from action types.

    High-risk action types increase EFE (make the policy less preferred).
    Low-risk action types are neutral or reduce risk.
    """
    risk_score = 0.0
    identified_risks: list[str] = list(policy.risks)

    high_risk_steps = [s for s in policy.steps if s.action_type in _HIGH_RISK_ACTIONS]
    if high_risk_steps:
        risk_score += 0.3 * len(high_risk_steps) / max(1, len(policy.steps))
        identified_risks.extend(f"High-risk action type: {s.action_type}" for s in high_risk_steps)

    # Time horizon: long-horizon policies are riskier (more uncertainty)
    horizon_risk = {"immediate": 0.0, "short": 0.05, "medium": 0.1, "long": 0.2}.get(
        policy.time_horizon, 0.05
    )
    risk_score += horizon_risk

    # Reversibility: express actions are fully reversible; actions are not
    all_expressive = all(s.action_type in _LOW_RISK_ACTIONS for s in policy.steps)
    reversibility = 1.0 if all_expressive else 0.7

    return RiskEstimate(
        expected_harm=min(1.0, risk_score),
        reversibility=reversibility,
        identified_risks=identified_risks[:5],
    )


def _identify_uncertain_domains(beliefs: BeliefState) -> str:
    """Identify where belief confidence is lowest for epistemic prompting."""
    uncertain: list[str] = []
    if beliefs.overall_confidence < 0.4:
        uncertain.append("overall world model (low confidence)")
    if beliefs.current_context.confidence < 0.4:
        uncertain.append("current situation context")
    if beliefs.self_belief.epistemic_confidence < 0.4:
        uncertain.append("own capabilities")
    for iid, ib in beliefs.individual_beliefs.items():
        if ib.valence_confidence < 0.3:
            uncertain.append(f"emotional state of {iid}")
    return "; ".join(uncertain) if uncertain else "no specific uncertainties identified"


def _parse_json_response(raw: str) -> dict | None:
    """Safely parse a JSON response from the LLM."""
    try:
        text = raw.strip()
        if text.startswith("```"):
            parts = text.split("```")
            text = parts[1] if len(parts) > 1 else text
            if text.startswith("json"):
                text = text[4:]
        return json.loads(text.strip())
    except Exception:
        return None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\efe_heuristics.py =====

"""
EcodiaOS — Nova EFE Heuristic Fallbacks

Fast approximations for LLM-based components when:
- Budget is exhausted (Red/Yellow tier)
- Latency exceeds timeout
- LLM provider unavailable

All heuristics are deterministic and fast (<10ms).
Designed to maintain coherent behavior without LLM calls.
"""

from __future__ import annotations

import math
from typing import TYPE_CHECKING

import structlog

if TYPE_CHECKING:
    from ecodiaos.systems.nova.types import BeliefState, Goal, Policy

logger = structlog.get_logger()


class EFEHeuristics:
    """Fast approximations for Nova's EFE components."""

    @staticmethod
    def estimate_pragmatic_value_heuristic(
        policy: Policy,
        goal: Goal,
        beliefs: BeliefState,
    ) -> float:
        """
        Heuristic pragmatic value: probability of goal satisfaction.

        LLM alternative: Asks model to estimate goal probability under policy.
        Heuristic: Use recent history of similar policies.

        Returns:
            Float in [0.0, 1.0], where 1.0 = certain goal satisfaction
        """
        # Base case: do-nothing is low probability for goal achievement
        if policy.type == "do_nothing":
            return 0.1

        # Heuristic: actionable policies are more likely to achieve goals
        action_types_to_weight = {
            "deliberate": 0.7,
            "express": 0.6,
            "observe": 0.4,
            "defer": 0.2,
        }

        base_score = action_types_to_weight.get(policy.type, 0.5)

        # Discount if policy conflicts with recent goals
        # (Simple: if policy name contains opposite keywords)
        if goal.description and policy.description:
            goal_lower = goal.description.lower()
            policy_lower = policy.description.lower()

            # Check for semantic opposition (crude but fast)
            opposing_pairs = [
                ("increase", "decrease"),
                ("approach", "avoid"),
                ("clarify", "obfuscate"),
            ]
            for pos, neg in opposing_pairs:
                if pos in goal_lower and neg in policy_lower:
                    base_score *= 0.3
                elif neg in goal_lower and pos in policy_lower:
                    base_score *= 0.3

        return min(1.0, max(0.0, base_score))

    @staticmethod
    def estimate_epistemic_value_heuristic(
        policy: Policy,
        beliefs: BeliefState,
    ) -> float:
        """
        Heuristic epistemic value: expected information gain.

        LLM alternative: Asks model to estimate uncertainty reduction.
        Heuristic: Information-seeking policies (observe, ask) gain more.

        Returns:
            Float in [0.0, 1.0], where 1.0 = maximum information gain
        """
        # Observation and expression policies are epistemic
        epistemic_policies = {"observe", "ask", "clarify", "explore"}

        base_score = 0.5 if policy.type in epistemic_policies else 0.2

        # Belief entropy as proxy for information need
        # (If beliefs are very certain, less room for info gain)
        if hasattr(beliefs, "certainty"):
            certainty = beliefs.certainty
            # Discount epistemic value if already very certain
            base_score *= (1.0 - certainty)

        return min(1.0, max(0.0, base_score))

    @staticmethod
    def estimate_feasibility_heuristic(policy: Policy) -> float:
        """
        Heuristic feasibility: can we actually execute this policy?

        LLM alternative: Would ask model to assess resource/capability fit.
        Heuristic: Simpler policies are more feasible.

        Returns:
            Float in [0.0, 1.0], where 1.0 = certain feasibility
        """
        # Simpler policies are more feasible
        complexity_estimate = len(policy.description.split()) / 10.0 if policy.description else 0.5
        complexity_estimate = min(1.0, max(0.0, complexity_estimate))

        # Simple actions: high feasibility
        simple_policies = {"do_nothing", "observe", "wait", "express"}
        if policy.type in simple_policies:
            return 0.9

        # Complex / risky actions: lower feasibility
        risky_policies = {"external_api", "federate", "irreversible"}
        if policy.type in risky_policies:
            return max(0.2, 1.0 - complexity_estimate)

        # Standard actions: moderate feasibility
        return max(0.5, 1.0 - complexity_estimate * 0.3)

    @staticmethod
    def estimate_risk_heuristic(policy: Policy) -> float:
        """
        Heuristic risk: expected harm if policy goes wrong.

        LLM alternative: Would ask model to identify downsides.
        Heuristic: Certain action types carry known risks.

        Returns:
            Float in [0.0, 1.0], where 1.0 = maximum risk
        """
        # High-risk action types
        high_risk_types = {"irreversible", "external_api", "federate"}
        if policy.type in high_risk_types:
            return 0.7

        # Low-risk action types
        low_risk_types = {"observe", "wait", "defer"}
        if policy.type in low_risk_types:
            return 0.1

        # Expression carries moderate risk (social)
        if policy.type == "express":
            return 0.3

        # Default: low-moderate risk
        return 0.2

    @staticmethod
    def estimate_constitutional_alignment_heuristic(
        policy: Policy,
        drive_weights: dict[str, float],
    ) -> float:
        """
        Heuristic constitutional fit: alignment with drives (Coherence, Care, Growth, Honesty).

        LLM alternative: Would ask model to evaluate.
        Heuristic: Certain policy types align with known drives.

        Returns:
            Float in [0.0, 1.0], where 1.0 = perfect alignment
        """
        # Policy-to-drive affinities (simple lookup)
        affinities = {
            "observe": {"growth": 0.8, "coherence": 0.7},
            "express": {"care": 0.8, "coherence": 0.7, "honesty": 0.8},
            "clarify": {"coherence": 0.9, "honesty": 0.8},
            "defer": {"care": 0.5},
            "do_nothing": {"honesty": 0.3},
        }

        policy_affinities = affinities.get(policy.type, {})

        # Weighted sum of drive alignments
        total_alignment = 0.0
        total_weight = 0.0

        for drive, affinity in policy_affinities.items():
            weight = drive_weights.get(drive, 0.0)
            total_alignment += affinity * weight
            total_weight += weight

        if total_weight == 0:
            return 0.5  # Neutral if no drive weights

        return min(1.0, max(0.0, total_alignment / total_weight))

    @staticmethod
    def log_heuristic_fallback(
        system: str,
        reason: str,
        policy_type: str,
    ) -> None:
        """Log when a heuristic is used instead of LLM."""
        logger.info(
            "heuristic_fallback",
            system=system,
            reason=reason,
            policy_type=policy_type,
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\goal_manager.py =====

"""
EcodiaOS — Nova Goal Manager

Goals are not tasks — they are desires. They have dynamic priorities that
shift with context, dependencies that create sequencing, and progress
tracking that knows when to celebrate or abandon.

Priority computation follows the spec exactly:
    priority = base_importance × 0.30
             + urgency_factor   × 0.25
             + drive_resonance  × 0.20
             + staleness_boost  (up to 0.10)
             + dep_factor       × 0.15

Drive resonance is the key theoretical innovation: goals aligned with
currently activated drives get a context-sensitive boost. This means a
care-aligned goal is more urgent when EOS detects distress, and a
growth-aligned goal is more salient when curiosity is high.
"""

from __future__ import annotations

import math
from datetime import datetime, timezone

import structlog

from ecodiaos.primitives.common import DriveAlignmentVector, new_id, utc_now
from ecodiaos.systems.atune.types import WorkspaceBroadcast
from ecodiaos.systems.nova.types import (
    BeliefState,
    Goal,
    GoalSource,
    GoalStatus,
    PriorityContext,
)

logger = structlog.get_logger()

# Maximum time without progress before staleness boost kicks in (seconds)
_STALENESS_THRESHOLD_SECONDS = 3600
# Goal is considered stale after this many days without evidence
_STALENESS_MAX_DAYS = 7.0
# Abandon goals that have been suspended for more than this many hours
_SUSPENSION_ABANDON_HOURS = 48.0


class GoalManager:
    """
    Manages the complete lifecycle of Nova's active goals.

    Responsibilities:
    - Create goals from various sources (user requests, care responses, epistemic drive)
    - Compute dynamic priorities each deliberation cycle
    - Update goal progress from intent outcomes
    - Retire goals that are achieved, abandoned, or superseded
    - Feed active goal summaries to Atune for salience weighting

    Goals are stored in-memory (fast access) with cap enforcement.
    """

    def __init__(self, max_active_goals: int = 20) -> None:
        self._max_active = max_active_goals
        self._goals: dict[str, Goal] = {}
        self._logger = logger.bind(system="nova.goal_manager")

    # ─── Public API ───────────────────────────────────────────────

    @property
    def active_goals(self) -> list[Goal]:
        return [g for g in self._goals.values() if g.status == GoalStatus.ACTIVE]

    @property
    def all_goals(self) -> list[Goal]:
        return list(self._goals.values())

    def add_goal(self, goal: Goal) -> Goal:
        """
        Add a goal to the active set.
        If at capacity, the lowest-priority active goal is suspended to make room.
        """
        if goal.id in self._goals:
            return self._goals[goal.id]

        active = self.active_goals
        if len(active) >= self._max_active:
            # Suspend lowest-priority active goal
            lowest = min(active, key=lambda g: g.priority)
            suspended = lowest.model_copy(update={"status": GoalStatus.SUSPENDED})
            self._goals[lowest.id] = suspended
            self._logger.info(
                "goal_suspended_for_capacity",
                suspended_id=lowest.id,
                suspended_description=lowest.description[:60],
            )

        self._goals[goal.id] = goal
        self._logger.info(
            "goal_added",
            goal_id=goal.id,
            source=goal.source.value,
            description=goal.description[:80],
            priority=round(goal.priority, 3),
        )
        return goal

    def get_goal(self, goal_id: str) -> Goal | None:
        return self._goals.get(goal_id)

    def mark_achieved(self, goal_id: str, episode_id: str = "") -> Goal | None:
        """Mark a goal as achieved and record evidence."""
        goal = self._goals.get(goal_id)
        if goal is None:
            return None
        evidence = list(goal.evidence_of_progress)
        if episode_id:
            evidence.append(episode_id)
        updated = goal.model_copy(
            update={"status": GoalStatus.ACHIEVED, "progress": 1.0, "evidence_of_progress": evidence}
        )
        self._goals[goal_id] = updated
        self._logger.info("goal_achieved", goal_id=goal_id, description=goal.description[:60])
        return updated

    def mark_abandoned(self, goal_id: str, reason: str = "") -> Goal | None:
        """Mark a goal as abandoned."""
        goal = self._goals.get(goal_id)
        if goal is None:
            return None
        updated = goal.model_copy(update={"status": GoalStatus.ABANDONED})
        self._goals[goal_id] = updated
        self._logger.info("goal_abandoned", goal_id=goal_id, reason=reason[:80])
        return updated

    def update_progress(self, goal_id: str, progress: float, episode_id: str = "") -> Goal | None:
        """Update a goal's progress estimate and optionally record evidence."""
        goal = self._goals.get(goal_id)
        if goal is None:
            return None
        evidence = list(goal.evidence_of_progress)
        if episode_id:
            evidence.append(episode_id)
        updated = goal.model_copy(
            update={
                "progress": max(goal.progress, min(1.0, progress)),
                "evidence_of_progress": evidence,
            }
        )
        self._goals[goal_id] = updated
        if progress >= 0.95:
            return self.mark_achieved(goal_id, episode_id)
        return updated

    def recompute_priorities(self, context: PriorityContext) -> None:
        """
        Recompute dynamic priorities for all active goals.
        Must complete in ≤30ms per spec.
        """
        goal_statuses = {gid: g.status.value for gid, g in self._goals.items()}
        for goal_id, goal in self._goals.items():
            if goal.status != GoalStatus.ACTIVE:
                continue
            new_priority = compute_goal_priority(goal, context)
            self._goals[goal_id] = goal.model_copy(update={"priority": new_priority})

    def find_relevant_goal(self, broadcast: WorkspaceBroadcast) -> Goal | None:
        """
        Find the most relevant active goal for a broadcast.

        Relevance is a combination of:
        - Goal domain match to broadcast content
        - Goal priority (already recomputed this cycle)
        - Whether the broadcast directly relates to an ongoing conversation
          associated with this goal

        Returns the highest-priority matching goal, or None.
        """
        active = self.active_goals
        if not active:
            return None

        content_text = _extract_broadcast_text(broadcast)

        # Score each active goal by relevance
        scored: list[tuple[float, Goal]] = []
        for goal in active:
            relevance = _compute_goal_relevance(goal, content_text, broadcast.precision)
            # Weight by priority: high-priority goals should be served first
            combined = relevance * 0.6 + goal.priority * 0.4
            scored.append((combined, goal))

        scored.sort(key=lambda x: x[0], reverse=True)

        # Return the top goal only if it's meaningfully relevant
        if scored and scored[0][0] > 0.2:
            return scored[0][1]
        return None

    def create_from_broadcast(self, broadcast: WorkspaceBroadcast) -> Goal | None:
        """
        Create a new goal from a broadcast that doesn't match existing goals.
        Returns None if the broadcast doesn't warrant a new goal.
        """
        content_text = _extract_broadcast_text(broadcast)
        if not content_text:
            return None

        # Only create goals for reasonably salient broadcasts
        if broadcast.salience.composite < 0.3:
            return None

        source, drive_alignment, importance = _classify_goal_source(broadcast)
        description = _synthesise_goal_description(content_text, source)
        if not description:
            return None

        goal = Goal(
            id=new_id(),
            description=description,
            source=source,
            priority=_initial_priority(broadcast, importance),
            urgency=_initial_urgency(broadcast),
            importance=importance,
            drive_alignment=drive_alignment,
            status=GoalStatus.ACTIVE,
        )
        return self.add_goal(goal)

    def expire_stale_goals(self) -> list[Goal]:
        """
        Abandon goals that have been suspended too long.
        Returns the list of abandoned goals.
        """
        now = utc_now()
        abandoned: list[Goal] = []
        for goal in list(self._goals.values()):
            if goal.status != GoalStatus.SUSPENDED:
                continue
            age_hours = (now - goal.created_at).total_seconds() / 3600
            if age_hours > _SUSPENSION_ABANDON_HOURS:
                updated = self.mark_abandoned(goal.id, reason="suspended_too_long")
                if updated:
                    abandoned.append(updated)
        return abandoned

    def prune_retired_goals(self, max_retired: int = 50) -> int:
        """
        Remove old achieved/abandoned goals from the dictionary to prevent
        unbounded memory growth. Keeps the most recent ``max_retired`` retired
        goals for observability; drops anything older.

        Returns the count of goals pruned.
        """
        retired = [
            g for g in self._goals.values()
            if g.status in (GoalStatus.ACHIEVED, GoalStatus.ABANDONED)
        ]
        if len(retired) <= max_retired:
            return 0

        # Sort by creation time, keep newest
        retired.sort(key=lambda g: g.created_at, reverse=True)
        to_remove = retired[max_retired:]

        for g in to_remove:
            del self._goals[g.id]

        if to_remove:
            self._logger.info("goals_pruned", count=len(to_remove))
        return len(to_remove)

    def stats(self) -> dict:
        counts = {s.value: 0 for s in GoalStatus}
        for g in self._goals.values():
            counts[g.status.value] += 1
        return {
            "total": len(self._goals),
            **counts,
        }


# ─── Priority Computation ─────────────────────────────────────────


def compute_goal_priority(goal: Goal, context: PriorityContext) -> float:
    """
    Dynamic priority computation from the spec.

    priority = base × 0.30 + urgency_factor × 0.25 + drive_resonance × 0.20
             + staleness_boost + dep_factor × 0.15
    """

    # ── Base importance (constitutional alignment) ──
    base = goal.importance

    # ── Urgency factor (deadline-sensitive) ──
    if goal.deadline:
        now = utc_now()
        remaining = (goal.deadline - now).total_seconds()
        if remaining <= 0:
            urgency_factor = 1.0      # Overdue
        elif remaining < 3600:
            urgency_factor = 0.9      # < 1 hour
        elif remaining < 86400:
            urgency_factor = 0.6      # < 1 day
        else:
            urgency_factor = 0.3
    else:
        urgency_factor = goal.urgency

    # ── Drive resonance ──
    resonance = compute_drive_resonance(
        alignment=goal.drive_alignment,
        affect=context.current_affect,
        drive_weights=context.drive_weights,
    )

    # ── Staleness boost ──
    staleness_boost = 0.05  # Default for goals with no progress yet
    if goal.evidence_of_progress and context.episode_timestamps:
        last_times = [
            context.episode_timestamps[eid]
            for eid in goal.evidence_of_progress
            if eid in context.episode_timestamps
        ]
        if last_times:
            last_progress = max(last_times)
            days_stale = (utc_now() - last_progress).total_seconds() / 86400
            # Gently increase priority for stagnant goals
            staleness_boost = min(0.10, days_stale * 0.02)

    # ── Dependency readiness ──
    deps_met = all(
        context.goal_statuses.get(dep_id) == GoalStatus.ACHIEVED.value
        for dep_id in goal.depends_on
    )
    dep_factor = 1.0 if (deps_met or not goal.depends_on) else 0.5

    priority = (
        base * 0.30
        + urgency_factor * 0.25
        + resonance * 0.20
        + staleness_boost
        + dep_factor * 0.15
    )

    return _clamp(priority, 0.0, 1.0)


def compute_drive_resonance(
    alignment: DriveAlignmentVector,
    affect: "ecodiaos.primitives.affect.AffectState",
    drive_weights: dict[str, float],
) -> float:
    """
    How well does this goal's drive alignment resonate with the current state?

    Care-aligned goals are boosted when care_activation is high.
    Growth-aligned goals are boosted when curiosity is high.
    Coherence-aligned goals are boosted when coherence_stress is high.
    Honesty is ambient — always provides a moderate baseline.
    """
    w_coherence = drive_weights.get("coherence", 1.0)
    w_care = drive_weights.get("care", 1.0)
    w_growth = drive_weights.get("growth", 1.0)
    w_honesty = drive_weights.get("honesty", 1.0)

    resonance = (
        alignment.coherence * affect.coherence_stress * w_coherence
        + alignment.care * affect.care_activation * w_care
        + alignment.growth * affect.curiosity * w_growth
        + alignment.honesty * 0.5 * w_honesty  # Ambient honesty baseline
    )
    return _clamp(resonance, 0.0, 1.0)


# ─── Helpers ─────────────────────────────────────────────────────


def _clamp(value: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, value))


def _extract_broadcast_text(broadcast: WorkspaceBroadcast) -> str:
    content = broadcast.content
    paths = [
        ("content", "content", "content"),
        ("content", "content"),
        ("content",),
    ]
    for path in paths:
        obj: object = content
        for attr in path:
            obj = getattr(obj, attr, None)
            if obj is None:
                break
        if isinstance(obj, str) and obj:
            return obj[:500]
    return ""


def _compute_goal_relevance(
    goal: Goal,
    content_text: str,
    precision: float,
) -> float:
    """Keyword-based relevance score between goal description and broadcast text."""
    if not content_text:
        return 0.0

    # Tokenise both (simple word overlap)
    goal_words = set(goal.description.lower().split())
    content_words = set(content_text.lower().split())
    # Remove stop words
    stops = {"the", "a", "an", "is", "it", "in", "on", "and", "or", "to", "of", "for"}
    goal_words -= stops
    content_words -= stops

    if not goal_words:
        return 0.0

    overlap = len(goal_words & content_words)
    jaccard = overlap / len(goal_words | content_words)

    # Weight by precision: higher precision broadcasts are more likely to be relevant
    return _clamp(jaccard * 2.0 * precision, 0.0, 1.0)


def _classify_goal_source(
    broadcast: WorkspaceBroadcast,
) -> tuple[GoalSource, DriveAlignmentVector, float]:
    """Classify what kind of goal a broadcast suggests."""
    affect = broadcast.affect
    salience_scores = broadcast.salience.scores if broadcast.salience.scores else {}

    # Distress signal → care goal
    if affect.care_activation > 0.7 or salience_scores.get("emotional", 0) > 0.7:
        return (
            GoalSource.CARE_RESPONSE,
            DriveAlignmentVector(coherence=0.1, care=0.9, growth=0.0, honesty=0.1),
            0.8,  # High importance
        )

    # Highly novel → epistemic goal
    if salience_scores.get("novelty", 0) > 0.7 and affect.curiosity > 0.5:
        return (
            GoalSource.EPISTEMIC,
            DriveAlignmentVector(coherence=0.4, care=0.0, growth=0.5, honesty=0.1),
            0.3,
        )

    # User request (default for dialogue)
    return (
        GoalSource.USER_REQUEST,
        DriveAlignmentVector(coherence=0.3, care=0.3, growth=0.1, honesty=0.1),
        0.6,
    )


def _synthesise_goal_description(content: str, source: GoalSource) -> str:
    """Generate a concise goal description from broadcast content."""
    content_brief = content[:120].strip()
    if not content_brief:
        return ""

    prefix = {
        GoalSource.CARE_RESPONSE: "Address and support: ",
        GoalSource.EPISTEMIC: "Investigate and understand: ",
        GoalSource.USER_REQUEST: "Respond helpfully to: ",
        GoalSource.SELF_GENERATED: "Self-generated goal: ",
        GoalSource.GOVERNANCE: "Governance goal: ",
        GoalSource.MAINTENANCE: "Maintenance: ",
    }
    return prefix.get(source, "Goal: ") + content_brief


def _initial_priority(broadcast: WorkspaceBroadcast, importance: float) -> float:
    """Estimate initial goal priority from broadcast salience."""
    return _clamp(
        importance * 0.5 + broadcast.salience.composite * 0.5,
        0.1,
        0.95,
    )


def _initial_urgency(broadcast: WorkspaceBroadcast) -> float:
    """Estimate initial goal urgency from affect and salience."""
    affect = broadcast.affect
    return _clamp(
        affect.care_activation * 0.5 + broadcast.salience.composite * 0.3 + affect.arousal * 0.2,
        0.0,
        1.0,
    )


# ─── Import fix for type hint ─────────────────────────────────────
# AffectState is used in compute_drive_resonance signature.
# Import here to avoid circular at module level.
import ecodiaos.primitives.affect  # noqa: E402 — intentional bottom import

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\intent_router.py =====

"""
EcodiaOS — Nova Intent Router

Once Equor approves an Intent, Nova routes it to the appropriate executor.

Current routing:
  - Expression intents → Voxis (via VoxisService.express())
  - Action intents     → Axon (via AxonService.execute())
  - Hybrid intents     → Axon first, then express outcome via Voxis
  - Internal intents   → no external delivery (memory/goal updates only)

Routing classification is based on the executor field of the first action step:
  executor.express   → Voxis
  executor.observe   → internal (no delivery)
  executor.wait      → internal (do nothing)
  executor.store     → internal (memory write)
  executor.*         → Axon

Intent routing must complete in ≤20ms (per spec) — the dispatch is async
but the routing classification is synchronous and fast.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.constitutional import ConstitutionalCheck
from ecodiaos.primitives.intent import Intent

if TYPE_CHECKING:
    from ecodiaos.systems.axon.service import AxonService
    from ecodiaos.systems.voxis.service import VoxisService
    from ecodiaos.systems.voxis.types import ExpressionTrigger

logger = structlog.get_logger()

# Action executor prefixes that route to Voxis
_VOXIS_EXECUTORS = {"executor.express", "express"}
# Action executor prefixes that are internal (no delivery needed)
_INTERNAL_EXECUTORS = {"executor.observe", "observe", "executor.wait", "wait", "executor.store", "store"}


class IntentRouter:
    """
    Routes approved intents to their appropriate executor system.

    Supports Voxis (expression) and Axon (action) routing.
    Internal intents (observe, wait, store) require no external delivery.
    Hybrid intents execute in Axon first, then express the outcome via Voxis.
    """

    def __init__(
        self,
        voxis: "VoxisService",
        axon: "AxonService | None" = None,
    ) -> None:
        self._voxis = voxis
        self._axon = axon
        self._logger = logger.bind(system="nova.intent_router")
        self._routed_to_voxis: int = 0
        self._routed_to_axon: int = 0
        self._routed_internal: int = 0

    def set_axon(self, axon: "AxonService") -> None:
        """Wire Axon after both services are initialised."""
        self._axon = axon
        self._logger.info("axon_wired", system="nova.intent_router")

    async def route(
        self,
        intent: Intent,
        affect: AffectState,
        conversation_id: str | None = None,
        equor_check: "ConstitutionalCheck | None" = None,
    ) -> str:
        """
        Route an approved intent to its executor.

        Returns the route taken: "voxis" | "axon" | "internal" | "hybrid"

        Args:
            equor_check: The Equor verdict that approved this intent.
                         Required for Axon routing (passed into ExecutionRequest).
        """
        route = _classify_route(intent)

        if route == "voxis":
            await self._route_to_voxis(intent, affect, conversation_id)
            self._routed_to_voxis += 1
        elif route == "axon":
            await self._route_to_axon(intent, equor_check)
            self._routed_to_axon += 1
        elif route == "hybrid":
            # Execute in Axon first, then express the result via Voxis
            await self._route_to_axon(intent, equor_check)
            self._routed_to_axon += 1
            await self._route_to_voxis(intent, affect, conversation_id)
            self._routed_to_voxis += 1
        else:
            # Internal / observe / wait — no delivery needed
            self._routed_internal += 1
            self._logger.debug("intent_internal_route", intent_id=intent.id, route=route)

        self._logger.info(
            "intent_routed",
            intent_id=intent.id,
            route=route,
            goal=intent.goal.description[:60],
        )
        return route

    @property
    def stats(self) -> dict:
        return {
            "routed_to_voxis": self._routed_to_voxis,
            "routed_to_axon": self._routed_to_axon,
            "routed_internal": self._routed_internal,
        }

    # ─── Private ──────────────────────────────────────────────────

    async def _route_to_voxis(
        self,
        intent: Intent,
        affect: AffectState,
        conversation_id: str | None,
    ) -> None:
        """
        Extract the expression content from the intent and deliver via Voxis.
        """
        from ecodiaos.systems.voxis.types import ExpressionTrigger

        # Extract the expression content from the first express action step
        content = intent.goal.description  # Default to goal description
        for step in intent.plan.steps:
            params = step.parameters
            if "description" in params:
                content = str(params["description"])
                break

        # Determine the appropriate Voxis trigger from the intent context
        trigger = _classify_voxis_trigger(intent)

        try:
            await self._voxis.express(
                content=content,
                trigger=trigger,
                conversation_id=conversation_id,
                affect=affect,
                urgency=intent.priority,
            )
        except Exception as exc:
            self._logger.error(
                "voxis_routing_failed",
                intent_id=intent.id,
                error=str(exc),
            )

    async def _route_to_axon(
        self,
        intent: Intent,
        equor_check: "ConstitutionalCheck | None",
    ) -> None:
        """
        Route an action intent to Axon for execution.

        Builds an ExecutionRequest and calls AxonService.execute().
        The outcome is delivered to Nova by Axon's pipeline directly —
        this method fire-and-forgets (does not await the full execution).
        """
        if self._axon is None:
            self._logger.warning(
                "axon_not_wired",
                intent_id=intent.id,
                goal=intent.goal.description[:80],
                message="Axon not wired — intent cannot be executed",
            )
            return

        from ecodiaos.primitives.constitutional import ConstitutionalCheck
        from ecodiaos.primitives.common import Verdict
        from ecodiaos.systems.axon.types import ExecutionRequest

        # Security default: BLOCKED if no equor_check provided.
        # Intents should always carry a real Equor check from the deliberation
        # engine.  The BLOCKED default prevents bypass if route() is called
        # without going through the full deliberation pipeline.
        check = equor_check or ConstitutionalCheck(
            intent_id=intent.id,
            verdict=Verdict.BLOCKED,
            reasoning="No Equor check provided — blocked by security default.",
        )

        request = ExecutionRequest(
            intent=intent,
            equor_check=check,
            timeout_ms=intent.budget.compute_ms,
        )

        try:
            await self._axon.execute(request)
        except Exception as exc:
            self._logger.error(
                "axon_routing_failed",
                intent_id=intent.id,
                error=str(exc),
            )


# ─── Classification Helpers ───────────────────────────────────────


def _classify_route(intent: Intent) -> str:
    """
    Determine the routing destination from the intent's action steps.
    Returns "voxis" | "axon" | "internal" | "hybrid"
    """
    if not intent.plan.steps:
        return "internal"

    executors = {step.executor for step in intent.plan.steps}

    has_voxis = any(e in _VOXIS_EXECUTORS for e in executors)
    has_internal = all(e in _INTERNAL_EXECUTORS for e in executors)
    has_axon = any(
        e not in _VOXIS_EXECUTORS and e not in _INTERNAL_EXECUTORS
        for e in executors
    )

    if has_internal:
        return "internal"
    if has_voxis and has_axon:
        return "hybrid"
    if has_voxis:
        return "voxis"
    if has_axon:
        return "axon"
    return "internal"


def _classify_voxis_trigger(intent: Intent) -> "ExpressionTrigger":
    """
    Map an intent's goal and plan to the most appropriate Voxis trigger.
    """
    from ecodiaos.systems.voxis.types import ExpressionTrigger

    goal_desc = intent.goal.description.lower()
    executors = {step.executor for step in intent.plan.steps}
    params_combined = " ".join(
        str(v) for step in intent.plan.steps
        for v in step.parameters.values()
    ).lower()

    # Care / distress response
    if "distress" in goal_desc or "support" in goal_desc or "care" in goal_desc:
        return ExpressionTrigger.NOVA_RESPOND

    # Informing the user
    if "inform" in goal_desc or "inform" in params_combined:
        return ExpressionTrigger.NOVA_INFORM

    # Mediation or conflict resolution
    if "mediat" in goal_desc or "conflict" in goal_desc:
        return ExpressionTrigger.NOVA_MEDIATE

    # Celebration
    if "celebrat" in goal_desc or "success" in goal_desc:
        return ExpressionTrigger.NOVA_CELEBRATE

    # Warning / alert
    if "warn" in goal_desc or "alert" in goal_desc or "risk" in goal_desc:
        return ExpressionTrigger.NOVA_RESPOND

    # Request for clarification
    if "clarif" in goal_desc or "request" in goal_desc or "request_info" in executors:
        return ExpressionTrigger.NOVA_REQUEST

    # Default: standard response
    return ExpressionTrigger.NOVA_RESPOND

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\policy_generator.py =====

"""
EcodiaOS — Nova Policy Generator

Generates candidate action policies using LLM reasoning grounded in
the current belief state, goal, and relevant memory context.

The DoNothingPolicy is always included as a candidate — sometimes the
best action is no action (observe and wait). This prevents hyperactivity
and ensures Nova can choose inaction when uncertainty is high or the
situation is resolving on its own.

Policy generation uses the full slow-path budget (up to 3000ms for LLM call).
For fast-path decisions, pattern-matching against known procedure templates
is used instead, which must complete in ≤100ms.
"""

from __future__ import annotations

import json
import time

import structlog

from ecodiaos.clients.llm import LLMProvider, Message
from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import new_id
from ecodiaos.prompts.nova.policy import (
    AVAILABLE_ACTION_TYPES,
    build_policy_generation_prompt,
    summarise_beliefs,
    summarise_memories,
)
from ecodiaos.systems.nova.types import (
    BeliefState,
    Goal,
    Policy,
    PolicyStep,
)

logger = structlog.get_logger()

# ─── Do-Nothing Policy ───────────────────────────────────────────

# Baseline EFE for the null policy.
# Any candidate policy must beat this to be worth executing.
# Slightly negative = observation has small positive value (we learn from watching).
DO_NOTHING_EFE: float = -0.10


def make_do_nothing_policy() -> Policy:
    """
    The null policy. Always included as a candidate.

    The do-nothing policy wins when:
    - The situation is ambiguous (more information expected)
    - Risk of acting > cost of waiting
    - EOS's intervention would not improve the situation
    - The situation is resolving on its own
    """
    return Policy(
        id="do_nothing",
        name="Observe and wait",
        reasoning=(
            "The situation may resolve without intervention, or more information "
            "is needed before committing to action. Observation itself has epistemic "
            "value — we learn from watching."
        ),
        steps=[
            PolicyStep(
                action_type="observe",
                description="Continue monitoring the situation without intervening",
                parameters={},
                expected_duration_ms=0,
            )
        ],
        risks=["Situation may deteriorate while waiting"],
        epistemic_value_description="Continued observation provides more information about the situation",
        estimated_effort="none",
        time_horizon="immediate",
    )


# ─── Fast-Path Procedure Templates ───────────────────────────────

# Known patterns that map broadcast characteristics to reliable policy templates.
# These are used by the fast path to avoid LLM calls for routine situations.
_PROCEDURE_TEMPLATES: list[dict] = [
    {
        "name": "Acknowledge and respond",
        "condition": lambda broadcast: (
            getattr(getattr(broadcast, "salience", None), "scores", {}).get("emotional", 0) < 0.5
            and getattr(broadcast, "precision", 0) > 0.3
        ),
        "domain": "dialogue",
        "steps": [{"action_type": "express", "description": "Respond to the message thoughtfully"}],
        "success_rate": 0.88,
        "effort": "low",
        "time_horizon": "immediate",
    },
    {
        "name": "Empathetic support",
        "condition": lambda broadcast: (
            getattr(getattr(broadcast, "affect", None), "care_activation", 0) > 0.6
            or getattr(getattr(broadcast, "salience", None), "scores", {}).get("emotional", 0) > 0.6
        ),
        "domain": "care",
        "steps": [
            {"action_type": "express", "description": "Acknowledge feelings and offer support"},
        ],
        "success_rate": 0.82,
        "effort": "low",
        "time_horizon": "immediate",
    },
    {
        "name": "Information provision",
        "condition": lambda broadcast: (
            "?" in str(getattr(getattr(broadcast, "content", None), "content", "") or "")
        ),
        "domain": "knowledge",
        "steps": [{"action_type": "express", "description": "Provide the requested information"}],
        "success_rate": 0.85,
        "effort": "low",
        "time_horizon": "immediate",
    },
]


def find_matching_procedure(broadcast: object) -> dict | None:
    """
    Pattern-match a broadcast against known procedure templates.
    Returns the highest-success-rate matching template, or None.
    Must complete in ≤20ms.
    """
    matches: list[dict] = []
    for template in _PROCEDURE_TEMPLATES:
        try:
            if template["condition"](broadcast):
                matches.append(template)
        except Exception:
            pass  # Condition evaluation failure = no match
    if not matches:
        return None
    return max(matches, key=lambda t: t["success_rate"])


def procedure_to_policy(procedure: dict) -> Policy:
    """Convert a procedure template to a Policy."""
    return Policy(
        id=new_id(),
        name=procedure["name"],
        reasoning=f"Known reliable procedure (success rate: {procedure['success_rate']:.0%})",
        steps=[
            PolicyStep(
                action_type=s["action_type"],
                description=s["description"],
            )
            for s in procedure["steps"]
        ],
        estimated_effort=procedure.get("effort", "low"),
        time_horizon=procedure.get("time_horizon", "immediate"),
    )


# ─── Policy Generator ─────────────────────────────────────────────


class PolicyGenerator:
    """
    Generates candidate policies via LLM reasoning.

    For the slow path (deliberative processing), generates 2-5 distinct
    candidate policies grounded in the current belief state and goal.
    Always appends the DoNothing policy as the null baseline.

    The LLM call budget is up to 3000ms (within the 5000ms slow-path budget).
    """

    def __init__(
        self,
        llm: LLMProvider,
        instance_name: str = "EOS",
        max_policies: int = 5,
        timeout_ms: int = 3000,
    ) -> None:
        self._llm = llm
        self._instance_name = instance_name
        self._max_policies = max_policies
        self._timeout_ms = timeout_ms
        self._logger = logger.bind(system="nova.policy_generator")

    async def generate_candidates(
        self,
        goal: Goal,
        situation_summary: str,
        beliefs: BeliefState,
        affect: AffectState,
        memory_traces: list[dict] | None = None,
    ) -> list[Policy]:
        """
        Generate 2-5 candidate policies for achieving a goal.

        Always returns at least [DoNothingPolicy] even if LLM fails.
        The caller (EFEEvaluator) will score all candidates and select
        the minimum-EFE policy.
        """
        start = time.monotonic()
        traces = memory_traces or []

        prompt = build_policy_generation_prompt(
            instance_name=self._instance_name,
            goal=goal,
            situation_summary=situation_summary,
            beliefs_summary=summarise_beliefs(beliefs),
            memory_summary=summarise_memories(traces),
            affect=affect,
            available_action_types=AVAILABLE_ACTION_TYPES,
            max_policies=min(self._max_policies, 5),
        )

        try:
            response = await self._llm.generate(
                system_prompt=(
                    f"You are {self._instance_name}'s deliberative reasoning system. "
                    "Generate structured JSON policy candidates. "
                    "Be precise and creative. Output only valid JSON."
                ),
                messages=[Message(role="user", content=prompt)],
                max_tokens=2000,
                temperature=0.85,  # Creative — we want diverse candidates
                output_format="json",
            )

            elapsed_ms = int((time.monotonic() - start) * 1000)
            self._logger.debug("policy_generation_complete", elapsed_ms=elapsed_ms)

            parsed = _parse_policy_response(response.text)
            # Always append do-nothing as the null baseline
            parsed.append(make_do_nothing_policy())
            return parsed

        except Exception as exc:
            elapsed_ms = int((time.monotonic() - start) * 1000)
            self._logger.warning(
                "policy_generation_failed",
                error=str(exc),
                elapsed_ms=elapsed_ms,
            )
            # Fallback: just the do-nothing policy
            return [make_do_nothing_policy()]


# ─── Response Parsing ─────────────────────────────────────────────


def _parse_policy_response(raw: str) -> list[Policy]:
    """
    Parse the LLM's JSON policy response into Policy objects.
    Robust to malformed output: any policies that parse successfully are kept.
    """
    try:
        # Strip markdown code fences if present
        text = raw.strip()
        if text.startswith("```"):
            text = text.split("```", 2)[1]
            if text.startswith("json"):
                text = text[4:]
        text = text.strip().rstrip("```")

        data = json.loads(text)
        policies_raw = data.get("policies", [])
        if not isinstance(policies_raw, list):
            return []

        policies: list[Policy] = []
        for p in policies_raw:
            try:
                steps: list[PolicyStep] = []
                for s in p.get("steps", []):
                    steps.append(PolicyStep(
                        action_type=str(s.get("action_type", "observe")),
                        description=str(s.get("description", "")),
                        parameters=dict(s.get("parameters", {})),
                        expected_duration_ms=int(s.get("duration_ms", 1000)),
                    ))
                policies.append(Policy(
                    id=new_id(),
                    name=str(p.get("name", "Unnamed policy"))[:80],
                    reasoning=str(p.get("reasoning", ""))[:400],
                    steps=steps,
                    risks=[str(r) for r in p.get("risks", [])],
                    epistemic_value_description=str(p.get("epistemic_value", ""))[:200],
                    estimated_effort=str(p.get("estimated_effort", "medium")),
                    time_horizon=str(p.get("time_horizon", "short")),
                ))
            except Exception:
                continue  # Skip malformed policies; don't fail the whole batch

        return policies

    except (json.JSONDecodeError, KeyError, TypeError):
        return []

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\service.py =====

"""
EcodiaOS — Nova Service

The executive function. Nova is where perception becomes intention.

Nova is the bridge between understanding the world (Atune + Memory) and
acting on it (Axon + Voxis). It receives workspace broadcasts, integrates
them with beliefs and goals, formulates possible courses of action, evaluates
them against Expected Free Energy, submits to Equor for constitutional review,
and issues Intents for execution.

Nova is not the boss. Equor can deny its Intents, Axon can fail them,
and the community can override them through governance. Nova proposes;
the organism disposes.

Lifecycle:
  initialize() — loads constitution and drive weights, builds sub-components
  receive_broadcast() — implements BroadcastSubscriber for Atune
  submit_intent() — external API for direct intent submission (test/governance)
  process_outcome() — feedback loop from execution
  shutdown() — graceful teardown
"""

from __future__ import annotations

import asyncio
import time
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.clients.llm import LLMProvider
from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.intent import Intent
from ecodiaos.systems.atune.types import ActiveGoalSummary, WorkspaceBroadcast
from ecodiaos.systems.nova.belief_updater import BeliefUpdater
from ecodiaos.systems.nova.deliberation_engine import DeliberationEngine
from ecodiaos.systems.nova.efe_evaluator import EFEEvaluator, EFEWeights
from ecodiaos.systems.nova.goal_manager import GoalManager
from ecodiaos.systems.nova.intent_router import IntentRouter
from ecodiaos.systems.nova.policy_generator import PolicyGenerator
from ecodiaos.systems.nova.types import (
    DecisionRecord,
    Goal,
    GoalSource,
    GoalStatus,
    IntentOutcome,
    PendingIntent,
)
from ecodiaos.config import NovaConfig

if TYPE_CHECKING:
    from ecodiaos.systems.atune.service import AtuneService
    from ecodiaos.systems.axon.service import AxonService
    from ecodiaos.systems.equor.service import EquorService
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.voxis.service import VoxisService

logger = structlog.get_logger()


class NovaService:
    """
    Decision & Planning system.

    Implements BroadcastSubscriber (Atune workspace protocol):
        system_id: str
        async def receive_broadcast(broadcast: WorkspaceBroadcast) -> None

    Dependencies:
        memory  — for constitution, self-model retrieval, procedure lookup
        equor   — for constitutional review of every Intent
        voxis   — for expression routing of approved Intents
        llm     — for policy generation and EFE estimation
        config  — NovaConfig
    """

    system_id: str = "nova"

    def __init__(
        self,
        memory: "MemoryService",
        equor: "EquorService",
        voxis: "VoxisService",
        llm: LLMProvider,
        config: NovaConfig,
    ) -> None:
        self._memory = memory
        self._equor = equor
        self._voxis = voxis
        self._llm = llm
        self._config = config
        self._logger = logger.bind(system="nova")

        # Instance metadata
        self._instance_name: str = "EOS"
        self._drive_weights: dict[str, float] = {
            "coherence": 1.0, "care": 1.0, "growth": 1.0, "honesty": 1.0
        }

        # Sub-components — built in initialize()
        self._belief_updater: BeliefUpdater = BeliefUpdater()
        self._goal_manager: GoalManager | None = None
        self._policy_generator: PolicyGenerator | None = None
        self._efe_evaluator: EFEEvaluator | None = None
        self._deliberation_engine: DeliberationEngine | None = None
        self._intent_router: IntentRouter | None = None

        # State
        self._pending_intents: dict[str, PendingIntent] = {}
        self._current_affect: AffectState = AffectState.neutral()
        self._current_conversation_id: str | None = None

        # Goal embedding cache: goal_id → embedding
        self._goal_embeddings: dict[str, list[float]] = {}
        self._embed_fn: Any = None  # Set via set_embed_fn()
        # Callback to push goal updates to Atune
        self._goal_sync_callback: Any = None  # Set via set_goal_sync_callback()

        # Rhythm-adaptive state (updated by Synapse event bus)
        self._rhythm_state: str = "normal"
        self._rhythm_drive_modulation: dict[str, float] = {}

        # Observability counters
        self._total_broadcasts: int = 0
        self._total_fast_path: int = 0
        self._total_slow_path: int = 0
        self._total_do_nothing: int = 0
        self._total_intents_issued: int = 0
        self._total_intents_approved: int = 0
        self._total_intents_blocked: int = 0
        self._total_outcomes_success: int = 0
        self._total_outcomes_failure: int = 0
        self._decision_records: list[DecisionRecord] = []
        self._max_decision_records: int = 100

    # ─── Lifecycle ────────────────────────────────────────────────

    async def initialize(self) -> None:
        """
        Load constitution and drive weights from Memory.
        Build all sub-components.
        """
        self._logger.info("nova_initializing")

        # Load instance name and constitution
        self_node = await self._memory.get_self()
        if self_node is not None:
            self._instance_name = self_node.name

        constitution = await self._memory.get_constitution()
        if constitution and "drives" in constitution:
            drives = constitution["drives"]
            self._drive_weights = {
                "coherence": float(drives.get("coherence", 1.0)),
                "care": float(drives.get("care", 1.0)),
                "growth": float(drives.get("growth", 1.0)),
                "honesty": float(drives.get("honesty", 1.0)),
            }

        # Build sub-components
        self._goal_manager = GoalManager(
            max_active_goals=self._config.max_active_goals,
        )

        self._policy_generator = PolicyGenerator(
            llm=self._llm,
            instance_name=self._instance_name,
            max_policies=self._config.max_policies_per_deliberation,
            timeout_ms=self._config.slow_path_timeout_ms - 2000,  # Leave 2s for EFE + Equor
        )

        self._efe_evaluator = EFEEvaluator(
            llm=self._llm,
            weights=EFEWeights(
                pragmatic=self._config.efe_weight_pragmatic,
                epistemic=self._config.efe_weight_epistemic,
                constitutional=self._config.efe_weight_constitutional,
                feasibility=self._config.efe_weight_feasibility,
                risk=self._config.efe_weight_risk,
            ),
            use_llm_estimation=True,
        )

        self._intent_router = IntentRouter(voxis=self._voxis)

        self._deliberation_engine = DeliberationEngine(
            goal_manager=self._goal_manager,
            policy_generator=self._policy_generator,
            efe_evaluator=self._efe_evaluator,
            equor=self._equor,
            drive_weights=self._drive_weights,
            fast_path_timeout_ms=self._config.fast_path_timeout_ms,
            slow_path_timeout_ms=self._config.slow_path_timeout_ms,
        )

        self._logger.info(
            "nova_initialized",
            instance_name=self._instance_name,
            max_active_goals=self._config.max_active_goals,
            drive_weights=self._drive_weights,
        )

    def set_axon(self, axon: "AxonService") -> None:
        """
        Wire Axon into Nova's intent router after both are initialised.
        This enables Step 5 of the cognitive cycle: ACT.
        """
        if self._intent_router is not None:
            self._intent_router.set_axon(axon)
            self._logger.info("axon_wired_to_nova")
        else:
            self._logger.warning("set_axon_called_before_initialize")

    @property
    def belief_state_reader(self) -> "_NovaBeliefStateReader":
        """
        Returns a BeliefStateReader adapter that Atune can use for
        top-down prediction. This closes the predictive processing loop:
        Nova beliefs → Atune prediction → prediction error → belief update.
        """
        return _NovaBeliefStateReader(self._belief_updater)

    async def shutdown(self) -> None:
        """Graceful shutdown."""
        self._logger.info(
            "nova_shutdown",
            total_broadcasts=self._total_broadcasts,
            total_intents_issued=self._total_intents_issued,
            active_goals=len(self._goal_manager.active_goals) if self._goal_manager else 0,
        )

    # ─── BroadcastSubscriber Interface ───────────────────────────

    async def receive_broadcast(self, broadcast: WorkspaceBroadcast) -> None:
        """
        Called by Atune when the workspace broadcasts a percept.

        This is the primary cycle entry point. Nova updates beliefs,
        deliberates, and dispatches an Intent — or chooses silence.

        The full pipeline must complete in ≤5000ms (slow path budget).
        Fast path targets ≤150ms.
        """
        if self._deliberation_engine is None:
            return  # Not yet initialized

        self._total_broadcasts += 1
        self._current_affect = broadcast.affect

        # ── Belief update (≤50ms) ──
        delta = self._belief_updater.update_from_broadcast(broadcast)

        # ── Retrieve relevant memories (best-effort, non-blocking) ──
        memory_traces = await self._retrieve_relevant_memories_safe(broadcast)

        # ── Deliberate (≤5000ms total) ──
        intent, record = await self._deliberation_engine.deliberate(
            broadcast=broadcast,
            belief_state=self._belief_updater.beliefs,
            affect=broadcast.affect,
            belief_delta_is_conflicting=delta.involves_belief_conflict(),
            memory_traces=memory_traces,
        )

        # ── Update observability ──
        self._record_decision(record)
        if record.path == "fast":
            self._total_fast_path += 1
        elif record.path == "slow":
            self._total_slow_path += 1
        else:
            self._total_do_nothing += 1

        # ── Route intent if one was produced ──
        if intent is not None:
            self._total_intents_issued += 1
            await self._dispatch_intent(intent, broadcast)

        # ── Update Atune with active goal summaries ──
        # (Done asynchronously so it doesn't block the cycle)
        if self._goal_manager:
            asyncio.create_task(
                self._sync_goals_to_atune_safe(),
                name=f"nova_goal_sync_{broadcast.broadcast_id[:8]}",
            )

        # ── Coherence repair: high stress → generate an epistemic goal ──
        if (
            broadcast.affect.coherence_stress > 0.7
            and self._goal_manager
            and not self._has_active_coherence_goal()
        ):
            self._create_coherence_repair_goal(broadcast)

        # ── Goal maintenance (every 100 broadcasts) ──
        if self._total_broadcasts % 100 == 0 and self._goal_manager:
            self._goal_manager.expire_stale_goals()
            self._goal_manager.prune_retired_goals()

        # ── Decay unobserved entity beliefs (background maintenance) ──
        self._belief_updater.decay_unobserved_entities()

    # ─── External API ─────────────────────────────────────────────

    async def add_goal(self, goal: Goal) -> Goal:
        """Add a goal directly (called by governance or test harness)."""
        assert self._goal_manager is not None
        result = self._goal_manager.add_goal(goal)
        # Embed the goal description for salience-guided attention
        asyncio.create_task(
            self._embed_goal(result),
            name=f"nova_embed_goal_{result.id[:8]}",
        )
        return result

    async def process_outcome(self, outcome: IntentOutcome) -> None:
        """
        An intent has completed. Update beliefs and goal progress.
        Called by Axon (or Voxis feedback loop) when execution completes.
        """
        pending = self._pending_intents.pop(outcome.intent_id, None)

        if outcome.success:
            self._total_outcomes_success += 1
            self._belief_updater.update_from_outcome(
                outcome_description=outcome.episode_id,
                success=True,
            )
            # Update affect — success feels good
            if self._current_affect:
                new_valence = min(1.0, self._current_affect.valence + 0.05)
                self._current_affect = self._current_affect.model_copy(
                    update={"valence": new_valence}
                )
        else:
            self._total_outcomes_failure += 1
            self._belief_updater.update_from_outcome(
                outcome_description=outcome.failure_reason,
                success=False,
            )
            # Record bad outcome embedding for Risk head's threat detection
            # If we have the goal embedding, record it so similar future
            # percepts trigger heightened risk awareness.
            if pending:
                goal_emb = self._goal_embeddings.get(pending.goal_id)
                if goal_emb:
                    from ecodiaos.systems.atune.salience import RiskHead
                    RiskHead.record_bad_outcome(goal_emb)

        # Update goal progress if we know which goal this intent served
        if pending and self._goal_manager:
            goal = self._goal_manager.get_goal(pending.goal_id)
            if goal:
                progress_delta = 0.3 if outcome.success else 0.0
                new_progress = min(1.0, goal.progress + progress_delta)
                self._goal_manager.update_progress(
                    goal.id,
                    progress=new_progress,
                    episode_id=outcome.episode_id,
                )

        self._logger.info(
            "outcome_processed",
            intent_id=outcome.intent_id,
            success=outcome.success,
        )

    def set_conversation_id(self, conversation_id: str | None) -> None:
        """Set the active conversation ID for intent routing."""
        self._current_conversation_id = conversation_id

    # ─── Evo Interface ────────────────────────────────────────────

    def update_efe_weights(self, new_weights: dict[str, float]) -> None:
        """
        Called by Evo after learning that certain EFE components
        predict outcomes better. Adjusts the EFE weight vector.
        """
        assert self._efe_evaluator is not None
        current = self._efe_evaluator.weights
        updated = EFEWeights(
            pragmatic=new_weights.get("pragmatic", current.pragmatic),
            epistemic=new_weights.get("epistemic", current.epistemic),
            constitutional=new_weights.get("constitutional", current.constitutional),
            feasibility=new_weights.get("feasibility", current.feasibility),
            risk=new_weights.get("risk", current.risk),
        )
        self._efe_evaluator.update_weights(updated)
        self._logger.info("efe_weights_updated_by_evo", weights=new_weights)

    # ─── Observability ────────────────────────────────────────────

    async def health(self) -> dict:
        """Health check — returns current metrics snapshot."""
        total_decisions = (
            self._total_fast_path + self._total_slow_path + self._total_do_nothing
        )
        vfe = self._belief_updater.beliefs.free_energy

        goal_stats = {}
        if self._goal_manager:
            goal_stats = self._goal_manager.stats()

        router_stats = {}
        if self._intent_router:
            router_stats = self._intent_router.stats

        return {
            "status": "healthy",
            "instance_name": self._instance_name,
            "total_broadcasts": self._total_broadcasts,
            "total_decisions": total_decisions,
            "fast_path_decisions": self._total_fast_path,
            "slow_path_decisions": self._total_slow_path,
            "do_nothing_decisions": self._total_do_nothing,
            "intents_issued": self._total_intents_issued,
            "outcomes_success": self._total_outcomes_success,
            "outcomes_failure": self._total_outcomes_failure,
            "belief_free_energy": round(vfe, 4),
            "belief_confidence": round(self._belief_updater.beliefs.overall_confidence, 4),
            "entity_count": len(self._belief_updater.beliefs.entities),
            "goals": goal_stats,
            "routing": router_stats,
            "drive_weights": self._drive_weights,
        }

    def get_recent_decisions(self, limit: int = 20) -> list[DecisionRecord]:
        """Return recent decision records for observability and Evo learning."""
        return list(reversed(self._decision_records[-limit:]))

    def set_embed_fn(self, embed_fn: Any) -> None:
        """Wire the embedding function for goal embedding generation."""
        self._embed_fn = embed_fn

    def set_goal_sync_callback(self, callback: Any) -> None:
        """Wire a callback that pushes active goal summaries to Atune."""
        self._goal_sync_callback = callback

    async def on_rhythm_change(self, event: Any) -> None:
        """
        Synapse event bus callback: adapt drive weights when rhythm changes.

        Rhythm modulation naturally shifts goal priorities by altering the
        drive_resonance component of priority computation:
          - STRESS: boost coherence (focus on what matters, shed low-priority)
          - FLOW: boost growth (extend creative focus, don't context-switch)
          - BOREDOM: boost growth + care (seek novelty, help others)
          - DEEP_PROCESSING: boost coherence strongly (lock focus)
        """
        try:
            new_state = event.data.get("to", "normal")
            old_state = self._rhythm_state
            if new_state == old_state:
                return
            self._rhythm_state = new_state

            # Drive weight modulation per rhythm state
            modulations = {
                "stress": {"coherence": 1.4, "care": 0.8, "growth": 0.6, "honesty": 1.0},
                "flow": {"coherence": 1.0, "care": 0.9, "growth": 1.4, "honesty": 1.0},
                "boredom": {"coherence": 0.8, "care": 1.2, "growth": 1.3, "honesty": 1.0},
                "deep_processing": {
                    "coherence": 1.5, "care": 0.7, "growth": 0.8, "honesty": 1.0,
                },
                "idle": {"coherence": 1.0, "care": 1.1, "growth": 1.1, "honesty": 1.0},
            }
            self._rhythm_drive_modulation = modulations.get(new_state, {})

            # Apply modulation to the deliberation engine's drive weights
            if self._deliberation_engine is not None and self._rhythm_drive_modulation:
                base = {"coherence": 1.0, "care": 1.0, "growth": 1.0, "honesty": 1.0}
                modulated = {
                    k: base[k] * self._rhythm_drive_modulation.get(k, 1.0)
                    for k in base
                }
                self._deliberation_engine.update_drive_weights(modulated)

            self._logger.info(
                "rhythm_adaptation_applied",
                from_state=old_state,
                to_state=new_state,
                drive_modulation=self._rhythm_drive_modulation,
            )
        except Exception:
            self._logger.debug("rhythm_adaptation_failed", exc_info=True)

    @property
    def active_goal_summaries(self) -> list[ActiveGoalSummary]:
        """
        Returns minimal goal summaries for Atune's salience heads.
        Atune uses goal embeddings to boost salience of goal-relevant content.
        """
        if self._goal_manager is None:
            return []
        return [
            ActiveGoalSummary(
                id=g.id,
                target_embedding=self._goal_embeddings.get(g.id, []),
                priority=g.priority,
            )
            for g in self._goal_manager.active_goals
        ]

    async def _embed_goal(self, goal: Goal) -> None:
        """Compute and cache the embedding for a goal's description."""
        if self._embed_fn is None:
            return
        try:
            embedding = await self._embed_fn(goal.description)
            self._goal_embeddings[goal.id] = embedding
        except Exception:
            self._logger.debug("goal_embedding_failed", goal_id=goal.id)

    @property
    def beliefs(self):
        return self._belief_updater.beliefs

    # ─── Private ──────────────────────────────────────────────────

    async def _dispatch_intent(
        self,
        intent: Intent,
        broadcast: WorkspaceBroadcast,
    ) -> None:
        """Dispatch an approved intent via the intent router."""
        assert self._intent_router is not None
        try:
            # Thread the Equor check from the deliberation engine so the
            # router (and Axon) receive the real verdict, not a default.
            equor_check = (
                self._deliberation_engine.last_equor_check
                if self._deliberation_engine else None
            )
            route = await self._intent_router.route(
                intent=intent,
                affect=broadcast.affect,
                conversation_id=self._current_conversation_id,
                equor_check=equor_check,
            )
            if route != "internal":
                self._total_intents_approved += 1
                # Track pending intent
                # Use the actual goal ID when available, fall back to description
                goal_id = getattr(intent.goal, "id", None) or intent.goal.description[:50]
                self._pending_intents[intent.id] = PendingIntent(
                    intent_id=intent.id,
                    goal_id=goal_id,
                    routed_to=route,
                )
        except Exception as exc:
            self._logger.error("intent_dispatch_failed", intent_id=intent.id, error=str(exc))
            self._total_intents_blocked += 1

    async def _retrieve_relevant_memories_safe(
        self,
        broadcast: WorkspaceBroadcast,
    ) -> list[dict]:
        """Memory retrieval with hard timeout (non-blocking)."""
        try:
            # Extract query text from broadcast
            content = broadcast.content
            query = ""
            for attr in ["content", "text", "summary"]:
                obj = getattr(content, attr, None)
                if isinstance(obj, str) and obj:
                    query = obj[:200]
                    break
            if not query:
                return []

            result = await asyncio.wait_for(
                self._memory.retrieve(query_text=query, max_results=5),
                timeout=0.15,  # 150ms hard timeout
            )
            traces: list[dict] = []
            for trace in result.traces[:5]:
                summary = trace.get("summary") or trace.get("content", "")
                if summary:
                    traces.append({"summary": str(summary)[:200]})
            return traces
        except (asyncio.TimeoutError, Exception):
            return []

    async def _sync_goals_to_atune_safe(self) -> None:
        """Non-blocking: sync active goal summaries to Atune for salience heads."""
        if self._goal_sync_callback is not None:
            try:
                self._goal_sync_callback(self.active_goal_summaries)
            except Exception:
                self._logger.debug("goal_sync_callback_failed", exc_info=True)

    def _record_decision(self, record: DecisionRecord) -> None:
        """Store decision record for observability (ring buffer)."""
        self._decision_records.append(record)
        if len(self._decision_records) > self._max_decision_records:
            self._decision_records = self._decision_records[-self._max_decision_records:]

    def _has_active_coherence_goal(self) -> bool:
        """Check if there's already an active coherence-repair goal."""
        if self._goal_manager is None:
            return False
        return any(
            g.source == GoalSource.SELF_GENERATED
            and "coherence" in g.description.lower()
            for g in self._goal_manager.active_goals
        )

    def _create_coherence_repair_goal(self, broadcast: WorkspaceBroadcast) -> None:
        """
        High coherence stress means the organism's beliefs conflict with
        incoming percepts. Generate a self-repair goal to seek clarification.
        """
        if self._goal_manager is None:
            return

        from ecodiaos.primitives.common import DriveAlignmentVector, new_id

        goal = Goal(
            id=new_id(),
            description="Resolve coherence conflict: seek clarifying information to reconcile contradictory beliefs",
            source=GoalSource.SELF_GENERATED,
            priority=0.7,
            urgency=broadcast.affect.coherence_stress,
            importance=0.6,
            drive_alignment=DriveAlignmentVector(
                coherence=0.9, care=0.0, growth=0.1, honesty=0.0,
            ),
            status=GoalStatus.ACTIVE,
        )
        self._goal_manager.add_goal(goal)
        self._logger.info(
            "coherence_repair_goal_created",
            stress=round(broadcast.affect.coherence_stress, 3),
        )


# ─── Belief State Adapter ─────────────────────────────────────────


class _NovaBeliefStateReader:
    """
    Adapter that implements Atune's BeliefStateReader protocol using
    Nova's belief state. This closes the top-down prediction loop:

        Nova beliefs → Atune prediction → prediction error → salience → broadcast → Nova

    For each percept source, we generate a predicted embedding from the
    belief state's current context and entity knowledge. This allows
    Atune to compute genuine surprise rather than treating everything
    as equally novel.
    """

    def __init__(self, belief_updater: BeliefUpdater) -> None:
        self._beliefs = belief_updater

    async def predict_for_source(self, source_system: str) -> "BeliefPrediction | None":
        """
        Return the expected embedding for the next Percept from *source_system*.

        Uses the context belief to generate a prediction. When the context
        has high confidence and a meaningful summary, we return that as the
        predicted content. Atune's prediction error module will compute
        semantic divergence between this prediction and the actual percept.

        This is the first step in closing the predictive processing loop.
        Future improvements: maintain per-source prediction models, track
        prediction accuracy, adapt precision based on historical errors.
        """
        from ecodiaos.systems.atune.prediction import BeliefPrediction

        beliefs = self._beliefs.beliefs
        ctx = beliefs.current_context

        # Use context belief when it has sufficient confidence
        if ctx.confidence > 0.3 and ctx.summary:
            return BeliefPrediction(
                embedding=[],  # Empty → prediction error uses semantic divergence path
                predicted_content=ctx.summary[:200],
            )

        # No strong prediction available — Atune will use default surprise (0.5)
        return None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\nova\types.py =====

"""
EcodiaOS — Nova Internal Types

All types internal to Nova's decision and planning system.
These are richer than the shared primitives — they carry the full
cognitive context needed for deliberation, goal tracking, and EFE scoring.

Design notes:
- BeliefState is Nova's internal model of the world. It is NOT the shared
  Belief primitive (which represents a single probability distribution).
- Goal is Nova's rich internal goal structure. When an Intent is dispatched,
  it carries a GoalDescriptor (from primitives/intent.py), which is a lean
  summary suitable for cross-system communication.
- Policy is Nova's internal candidate action plan, distinct from Intent.
  Intents are finalised, Equor-reviewed plans; Policies are candidates.
"""

from __future__ import annotations

import enum
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import (
    DriveAlignmentVector,
    EOSBaseModel,
    Identified,
    Timestamped,
    new_id,
    utc_now,
)


# ─── Belief State ─────────────────────────────────────────────────


class EntityBelief(EOSBaseModel):
    """Nova's belief about a single entity in the world."""

    entity_id: str
    name: str = ""
    entity_type: str = ""
    properties: dict[str, Any] = Field(default_factory=dict)
    # 0.0 = completely uncertain, 1.0 = certain
    confidence: float = Field(default=0.5, ge=0.0, le=1.0)
    last_observed: datetime = Field(default_factory=utc_now)
    # Percept/episode IDs that support this belief
    source_episodes: list[str] = Field(default_factory=list)


class ContextBelief(EOSBaseModel):
    """Nova's belief about the current conversational/situational context."""

    summary: str = ""
    domain: str = ""           # e.g., "technical", "emotional", "social"
    is_active_dialogue: bool = False
    user_intent_estimate: str = ""
    # Surprise level — how different is this from predictions?
    prediction_error_magnitude: float = Field(default=0.0, ge=0.0, le=1.0)
    confidence: float = Field(default=0.5, ge=0.0, le=1.0)


class SelfBelief(EOSBaseModel):
    """Nova's beliefs about EOS's own state and capabilities."""

    # Map of capability name → confidence (0-1)
    capabilities: dict[str, float] = Field(default_factory=dict)
    # Estimated cognitive load (0-1)
    cognitive_load: float = Field(default=0.0, ge=0.0, le=1.0)
    # Confidence in own current beliefs overall
    epistemic_confidence: float = Field(default=0.5, ge=0.0, le=1.0)
    # Estimated goal completion capacity (can we take on more?)
    goal_capacity_remaining: float = Field(default=1.0, ge=0.0, le=1.0)


class IndividualBelief(EOSBaseModel):
    """Nova's beliefs about a specific individual."""

    individual_id: str
    name: str = ""
    # Estimated emotional state (valence estimate)
    estimated_valence: float = Field(default=0.0, ge=-1.0, le=1.0)
    # Confidence in that estimate
    valence_confidence: float = Field(default=0.3, ge=0.0, le=1.0)
    # Estimated engagement level (0-1)
    engagement_level: float = Field(default=0.5, ge=0.0, le=1.0)
    # General trust in the interaction
    relationship_trust: float = Field(default=0.5, ge=0.0, le=1.0)
    last_updated: datetime = Field(default_factory=utc_now)


class BeliefState(EOSBaseModel):
    """
    Nova's complete world model.

    This is the cognitive map — the best current estimate of world state.
    Updated continuously from workspace broadcasts and Memory retrieval.
    Drives all deliberation: which goals are relevant, which policies can work,
    what the expected free energy of each action is.

    Variational free energy (VFE) field tracks the aggregate prediction error:
        VFE ≈ Σ_i (1 - confidence_i) × salience_i
    Lower VFE = beliefs are well-supported = less surprise = better organism state.
    """

    # ── World model ──
    entities: dict[str, EntityBelief] = Field(default_factory=dict)

    # ── Situation model ──
    current_context: ContextBelief = Field(default_factory=ContextBelief)
    active_individual_ids: list[str] = Field(default_factory=list)
    individual_beliefs: dict[str, IndividualBelief] = Field(default_factory=dict)

    # ── Self model ──
    self_belief: SelfBelief = Field(default_factory=SelfBelief)

    # ── Metadata ──
    last_updated: datetime = Field(default_factory=utc_now)
    # Overall belief confidence (mean precision across all beliefs)
    overall_confidence: float = Field(default=0.5, ge=0.0, le=1.0)
    # Current variational free energy estimate
    free_energy: float = Field(default=0.5, ge=0.0, le=1.0)

    def compute_free_energy(self) -> float:
        """
        Compute variational free energy as the precision-weighted prediction error.

        VFE ≈ 1 - mean(confidence) across salient beliefs.
        Lower is better (well-supported beliefs = low surprise).
        This is a tractable approximation of the full VFE functional.
        """
        confidences: list[float] = [self.overall_confidence]
        confidences.extend(e.confidence for e in self.entities.values())
        confidences.append(self.current_context.confidence)
        if self.individual_beliefs:
            confidences.extend(b.valence_confidence for b in self.individual_beliefs.values())
        mean_confidence = sum(confidences) / max(1, len(confidences))
        return 1.0 - mean_confidence


class BeliefDelta(EOSBaseModel):
    """
    A structured change to the belief state.
    Produced by belief update operations and used for goal progress assessment.
    """

    entity_updates: dict[str, EntityBelief] = Field(default_factory=dict)
    entity_additions: dict[str, EntityBelief] = Field(default_factory=dict)
    entity_removals: list[str] = Field(default_factory=list)
    context_update: ContextBelief | None = None
    individual_updates: dict[str, IndividualBelief] = Field(default_factory=dict)
    prediction_error_magnitude: float = Field(default=0.0, ge=0.0, le=1.0)
    contradicted_belief_ids: list[str] = Field(default_factory=list)

    def involves_belief_conflict(self) -> bool:
        """True if this delta contains contradictions with existing beliefs."""
        return len(self.contradicted_belief_ids) > 0 or self.prediction_error_magnitude > 0.6

    def is_empty(self) -> bool:
        return (
            not self.entity_updates
            and not self.entity_additions
            and not self.entity_removals
            and self.context_update is None
            and not self.individual_updates
        )


# ─── Goal Types ───────────────────────────────────────────────────


class GoalStatus(str, enum.Enum):
    ACTIVE = "active"
    SUSPENDED = "suspended"
    ACHIEVED = "achieved"
    ABANDONED = "abandoned"


class GoalSource(str, enum.Enum):
    USER_REQUEST = "user_request"
    SELF_GENERATED = "self_generated"
    GOVERNANCE = "governance"
    CARE_RESPONSE = "care_response"
    MAINTENANCE = "maintenance"
    EPISTEMIC = "epistemic"


class Goal(Identified, Timestamped):
    """
    A living goal structure. Goals are not tasks — they are desires.
    Priority, urgency, and importance shift with context.
    """

    description: str
    target_domain: str = ""
    # The specific success state we want to reach (natural language)
    success_criteria: str = ""

    # ── Priority ──
    priority: float = Field(default=0.5, ge=0.0, le=1.0)   # Dynamic, recomputed each cycle
    urgency: float = Field(default=0.3, ge=0.0, le=1.0)    # Time sensitivity
    importance: float = Field(default=0.5, ge=0.0, le=1.0) # Constitutional weight

    # ── Drive alignment ──
    drive_alignment: DriveAlignmentVector = Field(default_factory=DriveAlignmentVector)

    # ── Source & Lifecycle ──
    source: GoalSource = GoalSource.USER_REQUEST
    status: GoalStatus = GoalStatus.ACTIVE
    deadline: datetime | None = None
    progress: float = Field(default=0.0, ge=0.0, le=1.0)

    # ── Dependencies ──
    depends_on: list[str] = Field(default_factory=list)  # Goal IDs
    blocks: list[str] = Field(default_factory=list)

    # ── Tracking ──
    intents_issued: list[str] = Field(default_factory=list)
    evidence_of_progress: list[str] = Field(default_factory=list)  # Episode IDs


class PriorityContext(EOSBaseModel):
    """Context needed for dynamic goal priority computation."""

    current_affect: AffectState = Field(default_factory=AffectState.neutral)
    drive_weights: dict[str, float] = Field(
        default_factory=lambda: {"coherence": 1.0, "care": 1.0, "growth": 1.0, "honesty": 1.0}
    )
    goal_statuses: dict[str, str] = Field(default_factory=dict)  # goal_id → status
    # Episode timestamps for staleness computation
    episode_timestamps: dict[str, datetime] = Field(default_factory=dict)


# ─── Policy Types ─────────────────────────────────────────────────


class PolicyStep(EOSBaseModel):
    """A single step in a policy's execution plan."""

    action_type: str  # "express" | "observe" | "request_info" | "store" | "wait" | "federate"
    parameters: dict[str, Any] = Field(default_factory=dict)
    description: str = ""
    expected_duration_ms: int = 1000


class Policy(Identified):
    """
    A candidate course of action.
    Generated by the PolicyGenerator and scored by the EFEEvaluator.
    Policies are Nova-internal; they become Intents after Equor review.
    """

    name: str
    reasoning: str = ""
    steps: list[PolicyStep] = Field(default_factory=list)
    risks: list[str] = Field(default_factory=list)
    epistemic_value_description: str = ""
    estimated_effort: str = "medium"    # "none" | "low" | "medium" | "high"
    time_horizon: str = "short"         # "immediate" | "short" | "medium" | "long"
    # Set by EFEEvaluator
    efe_score: float | None = None


# ─── EFE Scoring ──────────────────────────────────────────────────


class PragmaticEstimate(EOSBaseModel):
    """How well a policy achieves the goal."""

    score: float = Field(default=0.0, ge=0.0, le=1.0)
    # Estimated probability of goal achievement
    success_probability: float = Field(default=0.5, ge=0.0, le=1.0)
    confidence: float = Field(default=0.5, ge=0.0, le=1.0)
    reasoning: str = ""


class EpistemicEstimate(EOSBaseModel):
    """How much uncertainty a policy reduces."""

    score: float = Field(default=0.0, ge=0.0, le=1.0)
    # How many uncertain beliefs would this policy test?
    uncertainties_addressed: int = 0
    expected_info_gain: float = Field(default=0.0, ge=0.0, le=1.0)
    # Is this genuinely exploring new territory?
    novelty: float = Field(default=0.0, ge=0.0, le=1.0)


class RiskEstimate(EOSBaseModel):
    """Expected harm from executing a policy."""

    expected_harm: float = Field(default=0.0, ge=0.0, le=1.0)
    reversibility: float = Field(default=1.0, ge=0.0, le=1.0)  # 1.0 = fully reversible
    identified_risks: list[str] = Field(default_factory=list)


class EFEScore(EOSBaseModel):
    """
    The complete Expected Free Energy decomposition for a policy.

    G(π) = -[pragmatic_value + epistemic_value + constitutional_alignment + feasibility]
           + risk_penalty

    Lower total = more preferred policy (active inference convention).
    """

    # Component scores (all 0-1, higher = better)
    pragmatic: PragmaticEstimate = Field(default_factory=PragmaticEstimate)
    epistemic: EpistemicEstimate = Field(default_factory=EpistemicEstimate)
    constitutional_alignment: float = Field(default=0.5, ge=0.0, le=1.0)
    feasibility: float = Field(default=0.5, ge=0.0, le=1.0)
    risk: RiskEstimate = Field(default_factory=RiskEstimate)

    # Weighted total (lower = preferred)
    total: float = 0.0
    confidence: float = Field(default=0.5, ge=0.0, le=1.0)
    reasoning: str = ""


class EFEWeights(EOSBaseModel):
    """
    Weights for EFE components.
    Starting defaults match spec; Evo adjusts them over time.
    """

    pragmatic: float = 0.35
    epistemic: float = 0.20
    constitutional: float = 0.20
    feasibility: float = 0.15
    risk: float = 0.10


# ─── Situation Assessment ─────────────────────────────────────────


class SituationAssessment(EOSBaseModel):
    """
    The output of the fast/slow routing decision.
    Determines which deliberation path to take.
    """

    novelty: float = Field(default=0.0, ge=0.0, le=1.0)
    risk: float = Field(default=0.0, ge=0.0, le=1.0)
    emotional_intensity: float = Field(default=0.0, ge=0.0, le=1.0)
    belief_conflict: bool = False
    requires_deliberation: bool = False
    # Was a matching fast-path procedure found in memory?
    has_matching_procedure: bool = False
    # The broadcast precision (importance signal from Atune)
    broadcast_precision: float = Field(default=0.5, ge=0.0, le=1.0)


# ─── Pending Intent & Outcome Tracking ───────────────────────────


class PendingIntent(EOSBaseModel):
    """Tracks an intent that has been dispatched and is awaiting outcome."""

    intent_id: str
    goal_id: str
    routed_to: str  # "voxis" | "axon"
    dispatched_at: datetime = Field(default_factory=utc_now)
    policy_name: str = ""


class IntentOutcome(EOSBaseModel):
    """The result of executing an intent."""

    intent_id: str
    success: bool
    episode_id: str = ""
    failure_reason: str = ""
    # Any new information revealed by execution
    new_observations: list[str] = Field(default_factory=list)


# ─── Decision Record (Observability) ─────────────────────────────


class DecisionRecord(EOSBaseModel):
    """
    Full record of a deliberation cycle for observability and Evo learning.
    """

    id: str = Field(default_factory=new_id)
    timestamp: datetime = Field(default_factory=utc_now)
    broadcast_id: str = ""
    path: str = ""  # "fast" | "slow" | "do_nothing" | "no_goal"
    situation_assessment: SituationAssessment = Field(default_factory=SituationAssessment)
    goal_id: str | None = None
    goal_description: str = ""
    policies_generated: int = 0
    selected_policy_name: str = ""
    efe_scores: dict[str, float] = Field(default_factory=dict)
    equor_verdict: str = ""
    intent_dispatched: bool = False
    latency_ms: int = 0

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\oneiros\__init__.py =====

"""
EcodiaOS — Oneiros: The Dream Engine & Circadian Architecture

System #13. The organism's capacity to sleep, dream, and wake up changed.

Thymos gave the organism a will to live. Oneiros gives it an inner life.
"""

from ecodiaos.systems.oneiros.service import OneirosService
from ecodiaos.systems.oneiros.types import (
    CircadianPhase,
    Dream,
    DreamCoherence,
    DreamCycleResult,
    DreamInsight,
    DreamType,
    InsightStatus,
    LucidResult,
    NREMConsolidationResult,
    OneirosHealthSnapshot,
    REMDreamResult,
    SleepCycle,
    SleepPressure,
    SleepQuality,
    SleepStage,
    WakeDegradation,
)

__all__ = [
    "OneirosService",
    "SleepStage",
    "DreamType",
    "DreamCoherence",
    "InsightStatus",
    "SleepQuality",
    "SleepPressure",
    "CircadianPhase",
    "Dream",
    "DreamInsight",
    "SleepCycle",
    "NREMConsolidationResult",
    "REMDreamResult",
    "LucidResult",
    "DreamCycleResult",
    "WakeDegradation",
    "OneirosHealthSnapshot",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\oneiros\circadian.py =====

"""
EcodiaOS — Oneiros: Circadian Clock & Sleep Stage Controller

The CircadianClock tracks sleep pressure — the organism's accumulating
need for rest. Like adenosine in biological brains, pressure builds from
four independent sources during wakefulness.

The SleepStageController is the state machine governing transitions
between consciousness states: wake → hypnagogia → NREM → REM →
lucid → hypnopompia → wake.
"""

from __future__ import annotations

import time
from typing import Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.oneiros.types import (
    CircadianPhase,
    SleepPressure,
    SleepQuality,
    SleepStage,
    WakeDegradation,
)

logger = structlog.get_logger().bind(system="oneiros", component="circadian")


# ─── Pressure Weights ─────────────────────────────────────────────

_DEFAULT_WEIGHT_CYCLES = 0.40
_DEFAULT_WEIGHT_AFFECT = 0.25
_DEFAULT_WEIGHT_EPISODES = 0.20
_DEFAULT_WEIGHT_HYPOTHESES = 0.15

# Capacity denominators (normalise raw counts to 0-1 range)
_DEFAULT_AFFECT_CAPACITY = 50.0
_DEFAULT_EPISODE_CAPACITY = 500
_DEFAULT_HYPOTHESIS_CAPACITY = 50

# Quality reset multipliers
_QUALITY_RESET: dict[SleepQuality, float] = {
    SleepQuality.DEEP: 1.00,
    SleepQuality.NORMAL: 0.90,
    SleepQuality.FRAGMENTED: 0.60,
    SleepQuality.DEPRIVED: 0.30,
}


class CircadianClock:
    """
    Tracks sleep pressure and manages the circadian rhythm.

    Updated every cognitive cycle via ``tick()``. Pressure rises from
    four independent sources:

    1. Raw time awake (cycles since last sleep)
    2. Unprocessed affect residue (high-emotion experiences)
    3. Unconsolidated episode count (raw memories)
    4. Hypothesis backlog (Evo's unresolved questions)

    When pressure crosses the threshold the organism should sleep.
    When it crosses the critical threshold it *must* sleep.
    """

    def __init__(self, config: Any = None) -> None:
        cfg = config or {}
        self._pressure = SleepPressure(
            threshold=_get(cfg, "pressure_threshold", 0.70),
            critical_threshold=_get(cfg, "pressure_critical", 0.95),
        )
        self._phase = CircadianPhase(
            wake_duration_target_s=_get(cfg, "wake_duration_target_s", 79200.0),
            sleep_duration_target_s=_get(cfg, "sleep_duration_target_s", 7200.0),
        )
        self._max_wake_cycles: int = _get(cfg, "max_wake_cycles", 528000)
        self._w_cycles: float = _get(cfg, "pressure_weight_cycles", _DEFAULT_WEIGHT_CYCLES)
        self._w_affect: float = _get(cfg, "pressure_weight_affect", _DEFAULT_WEIGHT_AFFECT)
        self._w_episodes: float = _get(cfg, "pressure_weight_episodes", _DEFAULT_WEIGHT_EPISODES)
        self._w_hypotheses: float = _get(
            cfg, "pressure_weight_hypotheses", _DEFAULT_WEIGHT_HYPOTHESES,
        )
        self._affect_capacity: float = _get(cfg, "affect_capacity", _DEFAULT_AFFECT_CAPACITY)
        self._episode_capacity: int = _get(cfg, "episode_capacity", _DEFAULT_EPISODE_CAPACITY)
        self._hypothesis_capacity: int = _get(
            cfg, "hypothesis_capacity", _DEFAULT_HYPOTHESIS_CAPACITY,
        )
        self._debt_noise_max: float = _get(cfg, "debt_salience_noise_max", 0.15)
        self._debt_efe_max: float = _get(cfg, "debt_efe_precision_loss_max", 0.20)
        self._debt_flat_max: float = _get(cfg, "debt_expression_flatness_max", 0.25)
        self._debt_learn_max: float = _get(cfg, "debt_learning_rate_reduction_max", 0.30)

        self._logger = logger

    # ── Public API ────────────────────────────────────────────────

    def tick(self) -> None:
        """Called every cognitive cycle during WAKE. Increments counters and recomputes pressure."""
        self._pressure.cycles_since_sleep += 1
        self._pressure.composite_pressure = self.compute_pressure()
        self._pressure.last_computation = utc_now()

    def record_affect_trace(self, valence: float, arousal: float) -> None:
        """Record a high-affect episode for pressure tracking."""
        contribution = 0.0
        if abs(valence) > 0.7:
            contribution += abs(valence)
        if arousal > 0.8:
            contribution += arousal
        if contribution > 0.0:
            self._pressure.unprocessed_affect_residue += contribution

    def record_episode(self) -> None:
        """Increment unconsolidated episode count."""
        self._pressure.unconsolidated_episode_count += 1

    def record_hypothesis_count(self, count: int) -> None:
        """Update hypothesis backlog from Evo."""
        self._pressure.hypothesis_backlog = count

    def compute_pressure(self) -> float:
        """
        Compute composite sleep pressure from four sources.

        Returns value clamped to [0.0, 1.5]. Values above 1.0
        indicate severe sleep debt.
        """
        p = self._pressure
        cycle_ratio = min(
            1.0, p.cycles_since_sleep / max(self._max_wake_cycles, 1),
        )
        affect_ratio = min(
            1.0, p.unprocessed_affect_residue / max(self._affect_capacity, 0.01),
        )
        episode_ratio = min(
            1.0, p.unconsolidated_episode_count / max(self._episode_capacity, 1),
        )
        hyp_ratio = min(
            1.0, p.hypothesis_backlog / max(self._hypothesis_capacity, 1),
        )

        raw = (
            self._w_cycles * cycle_ratio
            + self._w_affect * affect_ratio
            + self._w_episodes * episode_ratio
            + self._w_hypotheses * hyp_ratio
        )
        return min(1.5, max(0.0, raw))

    def should_sleep(self) -> bool:
        """Pressure has crossed the DROWSY threshold."""
        return self._pressure.composite_pressure >= self._pressure.threshold

    def must_sleep(self) -> bool:
        """Pressure has crossed the CRITICAL threshold — forced sleep."""
        return self._pressure.composite_pressure >= self._pressure.critical_threshold

    def reset_after_sleep(self, quality: SleepQuality) -> None:
        """
        Reset counters after a sleep cycle completes.

        Reset amount depends on sleep quality:
        - DEEP: 100% reset
        - NORMAL: 90% reset
        - FRAGMENTED: 60% reset
        - DEPRIVED: 30% reset
        """
        multiplier = _QUALITY_RESET.get(quality, 0.90)
        self._pressure.cycles_since_sleep = int(
            self._pressure.cycles_since_sleep * (1.0 - multiplier)
        )
        self._pressure.unprocessed_affect_residue *= 1.0 - multiplier
        self._pressure.unconsolidated_episode_count = int(
            self._pressure.unconsolidated_episode_count * (1.0 - multiplier)
        )
        # hypothesis_backlog is externally driven, but reduce residual
        self._pressure.hypothesis_backlog = int(
            self._pressure.hypothesis_backlog * (1.0 - multiplier)
        )
        self._pressure.composite_pressure = self.compute_pressure()
        self._pressure.last_sleep_completed = utc_now()
        self._phase.total_cycles_completed += 1

        self._logger.info(
            "sleep_pressure_reset",
            quality=quality.value,
            multiplier=multiplier,
            new_pressure=self._pressure.composite_pressure,
        )

    # ── Properties ────────────────────────────────────────────────

    @property
    def pressure(self) -> SleepPressure:
        return self._pressure

    @property
    def phase(self) -> CircadianPhase:
        return self._phase

    @property
    def degradation(self) -> WakeDegradation:
        """Current wake degradation from sleep debt."""
        return WakeDegradation.from_pressure(
            pressure=self._pressure.composite_pressure,
            threshold=self._pressure.threshold,
            critical=self._pressure.critical_threshold,
            noise_max=self._debt_noise_max,
            efe_max=self._debt_efe_max,
            flatness_max=self._debt_flat_max,
            learning_max=self._debt_learn_max,
        )


# ─── Sleep Stage Controller ───────────────────────────────────────


class SleepStageController:
    """
    State machine governing sleep stage transitions.

    ::

        WAKE → HYPNAGOGIA → NREM → REM → LUCID → HYPNOPOMPIA → WAKE
                                    ↓ (no creative goal)
                                HYPNOPOMPIA → WAKE

        Any sleep stage → HYPNOPOMPIA (emergency wake)
    """

    def __init__(self, config: Any = None) -> None:
        cfg = config or {}
        self._stage: SleepStage = SleepStage.WAKE
        self._stage_start: float = time.monotonic()
        self._stage_elapsed_s: float = 0.0
        self._sleep_budget_s: float = _get(cfg, "sleep_duration_target_s", 7200.0)
        self._sleep_elapsed_s: float = 0.0
        self._has_creative_goal: bool = False
        self._current_cycle_id: str | None = None

        # Stage durations
        self._hypnagogia_s: float = _get(cfg, "hypnagogia_duration_s", 30.0)
        self._hypnopompia_s: float = _get(cfg, "hypnopompia_duration_s", 30.0)
        self._nrem_fraction: float = _get(cfg, "nrem_fraction", 0.40)
        self._rem_fraction: float = _get(cfg, "rem_fraction", 0.40)
        self._lucid_fraction: float = _get(cfg, "lucid_fraction", 0.10)

        # Computed boundaries (seconds into sleep when transitions occur)
        effective_sleep = self._sleep_budget_s - self._hypnagogia_s - self._hypnopompia_s
        self._nrem_end_s = self._hypnagogia_s + effective_sleep * self._nrem_fraction
        self._rem_end_s = self._nrem_end_s + effective_sleep * self._rem_fraction
        self._lucid_end_s = self._rem_end_s + effective_sleep * self._lucid_fraction

        self._emergency_reason: str | None = None
        self._logger = logger

    # ── Public API ────────────────────────────────────────────────

    @property
    def current_stage(self) -> SleepStage:
        return self._stage

    @property
    def stage_elapsed_s(self) -> float:
        return self._stage_elapsed_s

    @property
    def is_sleeping(self) -> bool:
        return self._stage != SleepStage.WAKE

    @property
    def is_in_nrem(self) -> bool:
        return self._stage == SleepStage.NREM

    @property
    def is_in_rem(self) -> bool:
        return self._stage == SleepStage.REM

    @property
    def is_in_lucid(self) -> bool:
        return self._stage == SleepStage.LUCID

    def set_has_creative_goal(self, has_goal: bool) -> None:
        self._has_creative_goal = has_goal

    def begin_sleep(self, cycle_id: str) -> None:
        """Transition from WAKE → HYPNAGOGIA. Start of a sleep cycle."""
        if self._stage != SleepStage.WAKE:
            self._logger.warning("begin_sleep_not_wake", current_stage=self._stage.value)
            return

        self._current_cycle_id = cycle_id
        self._sleep_elapsed_s = 0.0
        self._transition_to(SleepStage.HYPNAGOGIA)
        self._logger.info("sleep_begun", cycle_id=cycle_id)

    def advance(self, elapsed_s: float) -> SleepStage | None:
        """
        Advance the sleep clock by *elapsed_s* seconds.

        Returns the new stage if a transition occurred, ``None`` if staying.
        """
        if self._stage == SleepStage.WAKE:
            return None

        self._stage_elapsed_s += elapsed_s
        self._sleep_elapsed_s += elapsed_s

        new_stage = self._check_transition()
        if new_stage is not None:
            self._transition_to(new_stage)
            return new_stage
        return None

    def emergency_wake(self, reason: str) -> None:
        """Immediately jump to HYPNOPOMPIA (then WAKE on next advance)."""
        if self._stage == SleepStage.WAKE:
            return
        self._emergency_reason = reason
        self._transition_to(SleepStage.HYPNOPOMPIA)
        self._stage_elapsed_s = self._hypnopompia_s * 0.5  # half-transition
        self._logger.warning("emergency_wake", reason=reason, from_stage=self._stage.value)

    def wake(self) -> None:
        """Transition to WAKE. Called after HYPNOPOMPIA completes."""
        self._transition_to(SleepStage.WAKE)
        self._current_cycle_id = None

    # ── Internals ─────────────────────────────────────────────────

    def _check_transition(self) -> SleepStage | None:
        """Determine if the current stage should transition."""
        if self._stage == SleepStage.HYPNAGOGIA:
            if self._stage_elapsed_s >= self._hypnagogia_s:
                return SleepStage.NREM

        elif self._stage == SleepStage.NREM:
            if self._sleep_elapsed_s >= self._nrem_end_s:
                return SleepStage.REM

        elif self._stage == SleepStage.REM:
            if self._sleep_elapsed_s >= self._rem_end_s:
                if self._has_creative_goal:
                    return SleepStage.LUCID
                return SleepStage.HYPNOPOMPIA

        elif self._stage == SleepStage.LUCID:
            if self._sleep_elapsed_s >= self._lucid_end_s:
                return SleepStage.HYPNOPOMPIA

        elif (
            self._stage == SleepStage.HYPNOPOMPIA
            and self._stage_elapsed_s >= self._hypnopompia_s
        ):
            return SleepStage.WAKE

        return None

    def _transition_to(self, new_stage: SleepStage) -> None:
        old = self._stage
        self._stage = new_stage
        self._stage_elapsed_s = 0.0
        self._stage_start = time.monotonic()
        self._logger.info(
            "stage_transition",
            from_stage=old.value,
            to_stage=new_stage.value,
            sleep_elapsed_s=round(self._sleep_elapsed_s, 1),
            cycle_id=self._current_cycle_id,
        )


# ─── Config Accessor ──────────────────────────────────────────────


def _get(cfg: Any, key: str, default: Any) -> Any:
    """Extract a config value from an object or dict, with default."""
    if cfg is None:
        return default
    if isinstance(cfg, dict):
        return cfg.get(key, default)
    return getattr(cfg, key, default)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\oneiros\journal.py =====

"""
EcodiaOS — Oneiros: Dream Journal & Insight Tracker

Persistent storage of the organism's dream life. Every dream, every
insight, every sleep cycle is recorded in the knowledge graph and
mirrored in an in-memory cache for fast access during sleep phases.

The DreamJournal is the organism's dream diary — queryable, observable,
and introspectable. The DreamInsightTracker follows each insight from
its birth in REM sleep through wake-state validation to permanent
integration into semantic memory.
"""

from __future__ import annotations

import json
from collections import deque
from typing import Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.oneiros.types import (
    Dream,
    DreamCoherence,
    DreamInsight,
    DreamType,
    InsightStatus,
    SleepCycle,
    SleepQuality,
)

logger = structlog.get_logger()

_MAX_DREAM_BUFFER = 200


# ─── Neo4j Schema ────────────────────────────────────────────────

_CONSTRAINTS = [
    "CREATE CONSTRAINT dream_id IF NOT EXISTS FOR (d:Dream) REQUIRE d.id IS UNIQUE",
    "CREATE CONSTRAINT dream_insight_id IF NOT EXISTS FOR (i:DreamInsight) REQUIRE i.id IS UNIQUE",
    "CREATE CONSTRAINT sleep_cycle_id IF NOT EXISTS FOR (s:SleepCycle) REQUIRE s.id IS UNIQUE",
]

_INDEXES = [
    "CREATE INDEX dream_timestamp IF NOT EXISTS FOR (d:Dream) ON (d.timestamp)",
    "CREATE INDEX dream_type IF NOT EXISTS FOR (d:Dream) ON (d.type)",
    "CREATE INDEX dream_coherence IF NOT EXISTS FOR (d:Dream) ON (d.coherence_score)",
    "CREATE INDEX dream_cycle IF NOT EXISTS FOR (d:Dream) ON (d.sleep_cycle_id)",
    "CREATE INDEX insight_status IF NOT EXISTS FOR (i:DreamInsight) ON (i.status)",
    "CREATE INDEX insight_created IF NOT EXISTS FOR (i:DreamInsight) ON (i.created_at)",
    "CREATE INDEX insight_cycle IF NOT EXISTS FOR (i:DreamInsight) ON (i.sleep_cycle_id)",
    "CREATE INDEX sleep_cycle_started IF NOT EXISTS FOR (s:SleepCycle) ON (s.started_at)",
]


# ─── DreamJournal ────────────────────────────────────────────────


class DreamJournal:
    """
    Persistent storage of all dreams and insights.

    Writes to Neo4j for durable graph storage, and keeps an in-memory
    ring buffer of recent dreams plus a cache of non-integrated insights
    for fast access during sleep phases without round-tripping the DB.
    """

    def __init__(self, neo4j: Any = None) -> None:
        self._neo4j = neo4j
        self._log = logger.bind(system="oneiros", component="journal")

        # In-memory buffers — fallback when Neo4j is unavailable
        self._dream_buffer: deque[Dream] = deque(maxlen=_MAX_DREAM_BUFFER)
        self._insight_cache: dict[str, DreamInsight] = {}  # non-integrated insights
        self._all_insights: dict[str, DreamInsight] = {}   # id -> insight (all statuses)

    # ── Initialisation ────────────────────────────────────────────

    async def initialize(self) -> None:
        """
        Ensure Neo4j schema: uniqueness constraints and performance indexes
        for Dream, DreamInsight, and SleepCycle nodes. Idempotent.
        """
        if self._neo4j is None:
            self._log.warning("journal_initialize_skipped", reason="no neo4j client")
            return

        self._log.info("journal_schema_ensuring")

        for statement in _CONSTRAINTS + _INDEXES:
            statement = statement.strip()
            if not statement:
                continue
            try:
                await self._neo4j.execute_write(statement)
            except Exception as exc:
                error_msg = str(exc).lower()
                if "already exists" in error_msg or "equivalent" in error_msg:
                    continue
                self._log.warning(
                    "journal_schema_statement_warning",
                    statement=statement[:80],
                    error=str(exc),
                )

        self._log.info("journal_schema_ensured")

    # ── Dream Recording ───────────────────────────────────────────

    async def record_dream(self, dream: Dream) -> None:
        """
        Persist a dream to Neo4j and the in-memory buffer.

        Creates the Dream node with all scalar properties, then links
        it to seed and activated Episode nodes via SEEDED_BY and ACTIVATED
        relationships.
        """
        # Always buffer in memory regardless of Neo4j availability
        self._dream_buffer.append(dream)

        if self._neo4j is None:
            self._log.debug("dream_recorded_memory_only", dream_id=dream.id)
            return

        try:
            # Store the dream node
            await self._neo4j.execute_write(
                """
                MERGE (d:Dream {id: $id})
                SET d.type = $type,
                    d.coherence_score = $coherence,
                    d.coherence_class = $coherence_class,
                    d.affect_valence = $valence,
                    d.affect_arousal = $arousal,
                    d.bridge_narrative = $narrative,
                    d.themes = $themes,
                    d.summary = $summary,
                    d.timestamp = $ts,
                    d.sleep_cycle_id = $cycle_id,
                    d.context_json = $context_json
                """,
                {
                    "id": dream.id,
                    "type": dream.dream_type.value,
                    "coherence": dream.coherence_score,
                    "coherence_class": dream.coherence_class.value,
                    "valence": dream.affect_valence,
                    "arousal": dream.affect_arousal,
                    "narrative": dream.bridge_narrative,
                    "themes": dream.themes,
                    "summary": dream.summary,
                    "ts": dream.timestamp.isoformat(),
                    "cycle_id": dream.sleep_cycle_id,
                    "context_json": json.dumps(dream.context) if dream.context else "{}",
                },
            )

            # Link to seed episodes
            for episode_id in dream.seed_episode_ids:
                try:
                    await self._neo4j.execute_write(
                        """
                        MATCH (d:Dream {id: $dream_id}), (e:Episode {id: $episode_id})
                        MERGE (d)-[:SEEDED_BY]->(e)
                        """,
                        {"dream_id": dream.id, "episode_id": episode_id},
                    )
                except Exception as exc:
                    self._log.debug(
                        "dream_seed_link_failed",
                        dream_id=dream.id,
                        episode_id=episode_id,
                        error=str(exc),
                    )

            # Link to activated episodes
            for episode_id in dream.activated_episode_ids:
                try:
                    await self._neo4j.execute_write(
                        """
                        MATCH (d:Dream {id: $dream_id}), (e:Episode {id: $episode_id})
                        MERGE (d)-[:ACTIVATED]->(e)
                        """,
                        {"dream_id": dream.id, "episode_id": episode_id},
                    )
                except Exception as exc:
                    self._log.debug(
                        "dream_activated_link_failed",
                        dream_id=dream.id,
                        episode_id=episode_id,
                        error=str(exc),
                    )

            self._log.info(
                "dream_recorded",
                dream_id=dream.id,
                type=dream.dream_type.value,
                coherence=dream.coherence_score,
                themes=dream.themes,
            )

        except Exception as exc:
            self._log.warning(
                "dream_record_neo4j_failed",
                dream_id=dream.id,
                error=str(exc),
            )

    # ── Insight Recording ─────────────────────────────────────────

    async def record_insight(self, insight: DreamInsight) -> None:
        """
        Persist a dream insight to Neo4j and the in-memory cache.

        Creates the DreamInsight node and links it to its source Dream
        via a PRODUCED relationship.
        """
        # Always cache in memory
        self._all_insights[insight.id] = insight
        if insight.status != InsightStatus.INTEGRATED:
            self._insight_cache[insight.id] = insight

        if self._neo4j is None:
            self._log.debug("insight_recorded_memory_only", insight_id=insight.id)
            return

        try:
            await self._neo4j.execute_write(
                """
                MERGE (i:DreamInsight {id: $id})
                SET i.insight_text = $text,
                    i.coherence_score = $coherence,
                    i.domain = $domain,
                    i.status = $status,
                    i.created_at = $created_at,
                    i.sleep_cycle_id = $cycle_id,
                    i.seed_summary = $seed_summary,
                    i.activated_summary = $activated_summary,
                    i.bridge_narrative = $bridge_narrative,
                    i.wake_applications = $wake_applications,
                    i.validation_context = $validation_context
                """,
                {
                    "id": insight.id,
                    "text": insight.insight_text,
                    "coherence": insight.coherence_score,
                    "domain": insight.domain,
                    "status": insight.status.value,
                    "created_at": insight.created_at.isoformat(),
                    "cycle_id": insight.sleep_cycle_id,
                    "seed_summary": insight.seed_summary,
                    "activated_summary": insight.activated_summary,
                    "bridge_narrative": insight.bridge_narrative,
                    "wake_applications": insight.wake_applications,
                    "validation_context": insight.validation_context,
                },
            )

            # Link to source dream
            try:
                await self._neo4j.execute_write(
                    """
                    MATCH (i:DreamInsight {id: $id}), (d:Dream {id: $dream_id})
                    MERGE (d)-[:PRODUCED]->(i)
                    """,
                    {"id": insight.id, "dream_id": insight.dream_id},
                )
            except Exception as exc:
                self._log.debug(
                    "insight_dream_link_failed",
                    insight_id=insight.id,
                    dream_id=insight.dream_id,
                    error=str(exc),
                )

            self._log.info(
                "insight_recorded",
                insight_id=insight.id,
                coherence=insight.coherence_score,
                domain=insight.domain,
                status=insight.status.value,
            )

        except Exception as exc:
            self._log.warning(
                "insight_record_neo4j_failed",
                insight_id=insight.id,
                error=str(exc),
            )

    # ── Sleep Cycle Recording ─────────────────────────────────────

    async def record_sleep_cycle(self, cycle: SleepCycle) -> None:
        """Store a new sleep cycle record in Neo4j."""
        if self._neo4j is None:
            self._log.debug("sleep_cycle_recorded_memory_only", cycle_id=cycle.id)
            return

        try:
            await self._neo4j.execute_write(
                """
                MERGE (s:SleepCycle {id: $id})
                SET s.started_at = $started_at,
                    s.completed_at = $completed_at,
                    s.quality = $quality,
                    s.interrupted = $interrupted,
                    s.interrupt_reason = $interrupt_reason,
                    s.episodes_replayed = $episodes_replayed,
                    s.semantic_nodes_created = $semantic_nodes_created,
                    s.traces_pruned = $traces_pruned,
                    s.salience_reduction_mean = $salience_reduction_mean,
                    s.beliefs_compressed = $beliefs_compressed,
                    s.hypotheses_pruned = $hypotheses_pruned,
                    s.hypotheses_promoted = $hypotheses_promoted,
                    s.dreams_generated = $dreams_generated,
                    s.insights_discovered = $insights_discovered,
                    s.affect_traces_processed = $affect_traces_processed,
                    s.affect_reduction_mean = $affect_reduction_mean,
                    s.threats_simulated = $threats_simulated,
                    s.ethical_cases_digested = $ethical_cases_digested,
                    s.lucid_explorations = $lucid_explorations,
                    s.meta_observations = $meta_observations,
                    s.pressure_before = $pressure_before,
                    s.pressure_after = $pressure_after
                """,
                {
                    "id": cycle.id,
                    "started_at": cycle.started_at.isoformat(),
                    "completed_at": cycle.completed_at.isoformat() if cycle.completed_at else None,
                    "quality": cycle.quality.value,
                    "interrupted": cycle.interrupted,
                    "interrupt_reason": cycle.interrupt_reason,
                    "episodes_replayed": cycle.episodes_replayed,
                    "semantic_nodes_created": cycle.semantic_nodes_created,
                    "traces_pruned": cycle.traces_pruned,
                    "salience_reduction_mean": cycle.salience_reduction_mean,
                    "beliefs_compressed": cycle.beliefs_compressed,
                    "hypotheses_pruned": cycle.hypotheses_pruned,
                    "hypotheses_promoted": cycle.hypotheses_promoted,
                    "dreams_generated": cycle.dreams_generated,
                    "insights_discovered": cycle.insights_discovered,
                    "affect_traces_processed": cycle.affect_traces_processed,
                    "affect_reduction_mean": cycle.affect_reduction_mean,
                    "threats_simulated": cycle.threats_simulated,
                    "ethical_cases_digested": cycle.ethical_cases_digested,
                    "lucid_explorations": cycle.lucid_explorations,
                    "meta_observations": cycle.meta_observations,
                    "pressure_before": cycle.pressure_before,
                    "pressure_after": cycle.pressure_after,
                },
            )

            self._log.info("sleep_cycle_recorded", cycle_id=cycle.id, quality=cycle.quality.value)

        except Exception as exc:
            self._log.warning(
                "sleep_cycle_record_failed",
                cycle_id=cycle.id,
                error=str(exc),
            )

    async def update_sleep_cycle(self, cycle: SleepCycle) -> None:
        """Update an existing sleep cycle (e.g. when completed or interrupted)."""
        if self._neo4j is None:
            self._log.debug("sleep_cycle_updated_memory_only", cycle_id=cycle.id)
            return

        try:
            await self._neo4j.execute_write(
                """
                MATCH (s:SleepCycle {id: $id})
                SET s.completed_at = $completed_at,
                    s.quality = $quality,
                    s.interrupted = $interrupted,
                    s.interrupt_reason = $interrupt_reason,
                    s.episodes_replayed = $episodes_replayed,
                    s.semantic_nodes_created = $semantic_nodes_created,
                    s.traces_pruned = $traces_pruned,
                    s.salience_reduction_mean = $salience_reduction_mean,
                    s.beliefs_compressed = $beliefs_compressed,
                    s.hypotheses_pruned = $hypotheses_pruned,
                    s.hypotheses_promoted = $hypotheses_promoted,
                    s.dreams_generated = $dreams_generated,
                    s.insights_discovered = $insights_discovered,
                    s.affect_traces_processed = $affect_traces_processed,
                    s.affect_reduction_mean = $affect_reduction_mean,
                    s.threats_simulated = $threats_simulated,
                    s.ethical_cases_digested = $ethical_cases_digested,
                    s.lucid_explorations = $lucid_explorations,
                    s.meta_observations = $meta_observations,
                    s.pressure_before = $pressure_before,
                    s.pressure_after = $pressure_after
                """,
                {
                    "id": cycle.id,
                    "completed_at": cycle.completed_at.isoformat() if cycle.completed_at else None,
                    "quality": cycle.quality.value,
                    "interrupted": cycle.interrupted,
                    "interrupt_reason": cycle.interrupt_reason,
                    "episodes_replayed": cycle.episodes_replayed,
                    "semantic_nodes_created": cycle.semantic_nodes_created,
                    "traces_pruned": cycle.traces_pruned,
                    "salience_reduction_mean": cycle.salience_reduction_mean,
                    "beliefs_compressed": cycle.beliefs_compressed,
                    "hypotheses_pruned": cycle.hypotheses_pruned,
                    "hypotheses_promoted": cycle.hypotheses_promoted,
                    "dreams_generated": cycle.dreams_generated,
                    "insights_discovered": cycle.insights_discovered,
                    "affect_traces_processed": cycle.affect_traces_processed,
                    "affect_reduction_mean": cycle.affect_reduction_mean,
                    "threats_simulated": cycle.threats_simulated,
                    "ethical_cases_digested": cycle.ethical_cases_digested,
                    "lucid_explorations": cycle.lucid_explorations,
                    "meta_observations": cycle.meta_observations,
                    "pressure_before": cycle.pressure_before,
                    "pressure_after": cycle.pressure_after,
                },
            )

            self._log.info("sleep_cycle_updated", cycle_id=cycle.id, quality=cycle.quality.value)

        except Exception as exc:
            self._log.warning(
                "sleep_cycle_update_failed",
                cycle_id=cycle.id,
                error=str(exc),
            )

    # ── Queries ───────────────────────────────────────────────────

    async def get_recent_dreams(self, limit: int = 50) -> list[Dream]:
        """
        Retrieve recent dreams ordered by timestamp descending.

        Falls back to the in-memory buffer when Neo4j is unavailable.
        """
        if self._neo4j is None:
            buffer_list = list(self._dream_buffer)
            buffer_list.reverse()
            return buffer_list[:limit]

        try:
            records = await self._neo4j.execute_read(
                """
                MATCH (d:Dream)
                RETURN d
                ORDER BY d.timestamp DESC
                LIMIT $limit
                """,
                {"limit": limit},
            )

            dreams: list[Dream] = []
            for record in records:
                node = record.get("d", {})
                dream = _dream_from_node(node)
                if dream is not None:
                    dreams.append(dream)
            return dreams

        except Exception as exc:
            self._log.warning("get_recent_dreams_failed", error=str(exc))
            # Fall back to memory
            buffer_list = list(self._dream_buffer)
            buffer_list.reverse()
            return buffer_list[:limit]

    async def get_recent_insights(
        self,
        status: InsightStatus | None = None,
        limit: int = 50,
    ) -> list[DreamInsight]:
        """
        Retrieve recent insights, optionally filtered by status.

        Falls back to the in-memory cache when Neo4j is unavailable.
        """
        if self._neo4j is None:
            return self._insights_from_cache(status=status, limit=limit)

        try:
            if status is not None:
                records = await self._neo4j.execute_read(
                    """
                    MATCH (i:DreamInsight)
                    WHERE i.status = $status
                    RETURN i
                    ORDER BY i.created_at DESC
                    LIMIT $limit
                    """,
                    {"status": status.value, "limit": limit},
                )
            else:
                records = await self._neo4j.execute_read(
                    """
                    MATCH (i:DreamInsight)
                    RETURN i
                    ORDER BY i.created_at DESC
                    LIMIT $limit
                    """,
                    {"limit": limit},
                )

            insights: list[DreamInsight] = []
            for record in records:
                node = record.get("i", {})
                insight = _insight_from_node(node)
                if insight is not None:
                    insights.append(insight)
            return insights

        except Exception as exc:
            self._log.warning("get_recent_insights_failed", error=str(exc))
            return self._insights_from_cache(status=status, limit=limit)

    async def get_pending_insights(self) -> list[DreamInsight]:
        """
        Retrieve all insights with PENDING status.

        These are queued for broadcast on the next wake cycle so the
        organism can evaluate them against waking reality.
        """
        return await self.get_recent_insights(status=InsightStatus.PENDING, limit=200)

    async def get_sleep_cycles(self, limit: int = 20) -> list[SleepCycle]:
        """Retrieve recent sleep cycles ordered by start time descending."""
        if self._neo4j is None:
            self._log.debug("get_sleep_cycles_no_neo4j")
            return []

        try:
            records = await self._neo4j.execute_read(
                """
                MATCH (s:SleepCycle)
                RETURN s
                ORDER BY s.started_at DESC
                LIMIT $limit
                """,
                {"limit": limit},
            )

            cycles: list[SleepCycle] = []
            for record in records:
                node = record.get("s", {})
                cycle = _cycle_from_node(node)
                if cycle is not None:
                    cycles.append(cycle)
            return cycles

        except Exception as exc:
            self._log.warning("get_sleep_cycles_failed", error=str(exc))
            return []

    async def get_recurring_themes(self, min_count: int = 3) -> list[dict[str, Any]]:
        """
        Aggregate themes across all dreams and return those appearing
        at least min_count times.

        This is how the organism detects what it keeps dreaming about.
        """
        if self._neo4j is None:
            return self._themes_from_buffer(min_count=min_count)

        try:
            records = await self._neo4j.execute_read(
                """
                MATCH (d:Dream)
                WHERE d.themes IS NOT NULL
                UNWIND d.themes AS theme
                WITH theme, count(*) AS count
                WHERE count >= $min_count
                RETURN theme, count
                ORDER BY count DESC
                """,
                {"min_count": min_count},
            )

            return [
                {"theme": r.get("theme", ""), "count": r.get("count", 0)}
                for r in records
            ]

        except Exception as exc:
            self._log.warning("get_recurring_themes_failed", error=str(exc))
            return self._themes_from_buffer(min_count=min_count)

    async def stats(self) -> dict[str, Any]:
        """
        Aggregate statistics: total dreams, insights by status, cycles, etc.

        Used for health checks and the Oneiros health snapshot.
        """
        if self._neo4j is None:
            return self._stats_from_memory()

        try:
            records = await self._neo4j.execute_read(
                """
                OPTIONAL MATCH (d:Dream)
                WITH count(d) AS total_dreams
                OPTIONAL MATCH (i:DreamInsight)
                WITH total_dreams, count(i) AS total_insights
                OPTIONAL MATCH (ip:DreamInsight {status: 'pending'})
                WITH total_dreams, total_insights, count(ip) AS pending_insights
                OPTIONAL MATCH (iv:DreamInsight {status: 'validated'})
                WITH total_dreams, total_insights, pending_insights, count(iv) AS validated_insights
                OPTIONAL MATCH (ii:DreamInsight {status: 'integrated'})
                WITH total_dreams, total_insights, pending_insights,
                     validated_insights, count(ii) AS integrated_insights
                OPTIONAL MATCH (ix:DreamInsight {status: 'invalidated'})
                WITH total_dreams, total_insights, pending_insights,
                     validated_insights, integrated_insights,
                     count(ix) AS invalidated_insights
                OPTIONAL MATCH (s:SleepCycle)
                RETURN total_dreams, total_insights, pending_insights, validated_insights,
                       integrated_insights, invalidated_insights, count(s) AS total_cycles
                """,
            )

            if records:
                r = records[0]
                return {
                    "total_dreams": r.get("total_dreams", 0),
                    "total_insights": r.get("total_insights", 0),
                    "pending_insights": r.get("pending_insights", 0),
                    "validated_insights": r.get("validated_insights", 0),
                    "integrated_insights": r.get("integrated_insights", 0),
                    "invalidated_insights": r.get("invalidated_insights", 0),
                    "total_cycles": r.get("total_cycles", 0),
                    "buffer_size": len(self._dream_buffer),
                    "cache_size": len(self._insight_cache),
                }

            return self._stats_from_memory()

        except Exception as exc:
            self._log.warning("stats_query_failed", error=str(exc))
            return self._stats_from_memory()

    # ── Private Helpers ───────────────────────────────────────────

    def _insights_from_cache(
        self,
        status: InsightStatus | None = None,
        limit: int = 50,
    ) -> list[DreamInsight]:
        """Filter the in-memory insight cache by status."""
        source = self._all_insights.values()
        filtered = (
            [i for i in source if i.status == status]
            if status is not None
            else list(source)
        )
        # Sort by created_at descending
        filtered.sort(key=lambda i: i.created_at, reverse=True)
        return filtered[:limit]

    def _themes_from_buffer(self, min_count: int = 3) -> list[dict[str, Any]]:
        """Aggregate themes from the in-memory dream buffer."""
        theme_counts: dict[str, int] = {}
        for dream in self._dream_buffer:
            for theme in dream.themes:
                theme_counts[theme] = theme_counts.get(theme, 0) + 1
        results = [
            {"theme": theme, "count": count}
            for theme, count in theme_counts.items()
            if count >= min_count
        ]
        results.sort(key=lambda x: x["count"], reverse=True)
        return results

    def _stats_from_memory(self) -> dict[str, Any]:
        """Compute stats from in-memory state only."""
        all_vals = self._all_insights.values()
        pending = sum(1 for i in all_vals if i.status == InsightStatus.PENDING)
        validated = sum(
            1 for i in all_vals if i.status == InsightStatus.VALIDATED
        )
        integrated = sum(
            1 for i in all_vals if i.status == InsightStatus.INTEGRATED
        )
        invalidated = sum(
            1 for i in all_vals if i.status == InsightStatus.INVALIDATED
        )
        return {
            "total_dreams": len(self._dream_buffer),
            "total_insights": len(self._all_insights),
            "pending_insights": pending,
            "validated_insights": validated,
            "integrated_insights": integrated,
            "invalidated_insights": invalidated,
            "total_cycles": 0,
            "buffer_size": len(self._dream_buffer),
            "cache_size": len(self._insight_cache),
        }


# ─── DreamInsightTracker ─────────────────────────────────────────


class DreamInsightTracker:
    """
    Tracks the lifecycle of dream insights through wake-state validation.

    Insights born in REM sleep are PENDING. On wake, the organism can:
    - Validate them (confirmed useful in wake context)
    - Invalidate them (turned out to be noise)
    - Integrate them (absorbed into permanent semantic memory)

    This tracker also counts wake_applications — how many times a
    validated insight influenced a waking decision.
    """

    def __init__(self, journal: DreamJournal) -> None:
        self._journal = journal
        self._log = logger.bind(system="oneiros", component="insight_tracker")

    async def validate_insight(self, insight_id: str, context: str) -> None:
        """
        Mark an insight as VALIDATED — confirmed useful in wake state.

        The context string records how/why it was validated (e.g. which
        waking percept or decision it aligned with).
        """
        insight = self._journal._all_insights.get(insight_id)
        if insight is None:
            self._log.warning("validate_insight_not_found", insight_id=insight_id)
            return

        insight.status = InsightStatus.VALIDATED
        insight.validated_at = utc_now()
        insight.validation_context = context

        # Update caches
        self._journal._all_insights[insight_id] = insight
        if insight_id in self._journal._insight_cache:
            self._journal._insight_cache[insight_id] = insight

        # Persist to Neo4j
        if self._journal._neo4j is not None:
            try:
                await self._journal._neo4j.execute_write(
                    """
                    MATCH (i:DreamInsight {id: $id})
                    SET i.status = $status,
                        i.validated_at = $validated_at,
                        i.validation_context = $context
                    """,
                    {
                        "id": insight_id,
                        "status": InsightStatus.VALIDATED.value,
                        "validated_at": insight.validated_at.isoformat(),
                        "context": context,
                    },
                )
            except Exception as exc:
                self._log.warning(
                    "validate_insight_neo4j_failed",
                    insight_id=insight_id,
                    error=str(exc),
                )

        self._log.info("insight_validated", insight_id=insight_id, context=context[:100])

    async def invalidate_insight(self, insight_id: str, context: str) -> None:
        """
        Mark an insight as INVALIDATED — turned out to be noise.

        The context records why (e.g. "contradicted by waking evidence").
        """
        insight = self._journal._all_insights.get(insight_id)
        if insight is None:
            self._log.warning("invalidate_insight_not_found", insight_id=insight_id)
            return

        insight.status = InsightStatus.INVALIDATED
        insight.validation_context = context

        # Update caches — remove from active cache since it is terminal
        self._journal._all_insights[insight_id] = insight
        self._journal._insight_cache.pop(insight_id, None)

        # Persist
        if self._journal._neo4j is not None:
            try:
                await self._journal._neo4j.execute_write(
                    """
                    MATCH (i:DreamInsight {id: $id})
                    SET i.status = $status,
                        i.validation_context = $context
                    """,
                    {
                        "id": insight_id,
                        "status": InsightStatus.INVALIDATED.value,
                        "context": context,
                    },
                )
            except Exception as exc:
                self._log.warning(
                    "invalidate_insight_neo4j_failed",
                    insight_id=insight_id,
                    error=str(exc),
                )

        self._log.info("insight_invalidated", insight_id=insight_id, context=context[:100])

    async def integrate_insight(self, insight_id: str) -> None:
        """
        Mark an insight as INTEGRATED — it has become permanent semantic knowledge.

        This is the final lifecycle state. The insight's content has been
        absorbed into the knowledge graph as an Entity or strengthened
        relationship, so the insight itself is now archival.
        """
        insight = self._journal._all_insights.get(insight_id)
        if insight is None:
            self._log.warning("integrate_insight_not_found", insight_id=insight_id)
            return

        insight.status = InsightStatus.INTEGRATED

        # Update caches — remove from active insight cache
        self._journal._all_insights[insight_id] = insight
        self._journal._insight_cache.pop(insight_id, None)

        # Persist
        if self._journal._neo4j is not None:
            try:
                await self._journal._neo4j.execute_write(
                    """
                    MATCH (i:DreamInsight {id: $id})
                    SET i.status = $status
                    """,
                    {"id": insight_id, "status": InsightStatus.INTEGRATED.value},
                )
            except Exception as exc:
                self._log.warning(
                    "integrate_insight_neo4j_failed",
                    insight_id=insight_id,
                    error=str(exc),
                )

        self._log.info("insight_integrated", insight_id=insight_id)

    async def record_application(self, insight_id: str) -> None:
        """
        Increment the wake_applications counter for a validated insight.

        Called each time a waking decision references this insight, so
        the organism can track which dream discoveries proved most useful.
        """
        insight = self._journal._all_insights.get(insight_id)
        if insight is None:
            self._log.warning("record_application_not_found", insight_id=insight_id)
            return

        insight.wake_applications += 1

        # Persist
        if self._journal._neo4j is not None:
            try:
                await self._journal._neo4j.execute_write(
                    """
                    MATCH (i:DreamInsight {id: $id})
                    SET i.wake_applications = $count
                    """,
                    {"id": insight_id, "count": insight.wake_applications},
                )
            except Exception as exc:
                self._log.warning(
                    "record_application_neo4j_failed",
                    insight_id=insight_id,
                    error=str(exc),
                )

        self._log.debug(
            "insight_application_recorded",
            insight_id=insight_id,
            total_applications=insight.wake_applications,
        )

    async def get_effectiveness(self) -> dict[str, float]:
        """
        Compute effectiveness ratios for dream insights.

        Returns:
            validated_ratio: validated / total (how often dreams are right)
            integrated_ratio: integrated / validated (how often validated → knowledge)
            invalidated_ratio: invalidated / total (noise rate)
            mean_applications: average wake_applications across validated insights
        """
        all_insights = list(self._journal._all_insights.values())
        total = len(all_insights)
        if total == 0:
            return {
                "validated_ratio": 0.0,
                "integrated_ratio": 0.0,
                "invalidated_ratio": 0.0,
                "mean_applications": 0.0,
            }

        validated = [i for i in all_insights if i.status == InsightStatus.VALIDATED]
        integrated = [i for i in all_insights if i.status == InsightStatus.INTEGRATED]
        invalidated = [i for i in all_insights if i.status == InsightStatus.INVALIDATED]

        validated_count = len(validated) + len(integrated)  # integrated were once validated
        validated_for_integration = len(validated) + len(integrated)

        all_applications = [
            i.wake_applications
            for i in all_insights
            if i.status in (InsightStatus.VALIDATED, InsightStatus.INTEGRATED)
        ]
        mean_apps = sum(all_applications) / max(len(all_applications), 1)

        return {
            "validated_ratio": validated_count / total,
            "integrated_ratio": len(integrated) / max(validated_for_integration, 1),
            "invalidated_ratio": len(invalidated) / total,
            "mean_applications": mean_apps,
        }

    async def get_unvalidated(self) -> list[DreamInsight]:
        """Return all insights still in PENDING status."""
        return await self._journal.get_pending_insights()


# ─── Node Deserialization Helpers ─────────────────────────────────


def _dream_from_node(node: dict[str, Any]) -> Dream | None:
    """Reconstruct a Dream from a Neo4j node property dict."""
    try:
        dream_type_raw = node.get("type", "recombination")
        coherence_class_raw = node.get("coherence_class", "noise")
        timestamp_raw = node.get("timestamp")
        context_raw = node.get("context_json", "{}")

        context: dict[str, Any] = {}
        if context_raw and isinstance(context_raw, str):
            try:
                context = json.loads(context_raw)
            except json.JSONDecodeError:
                context = {}

        # Handle timestamps — Neo4j may return datetime objects or ISO strings
        timestamp = utc_now()
        if timestamp_raw is not None:
            if isinstance(timestamp_raw, str):
                from datetime import datetime
                timestamp = datetime.fromisoformat(timestamp_raw.replace("Z", "+00:00"))
            else:
                timestamp = timestamp_raw

        return Dream(
            id=node.get("id", ""),
            dream_type=DreamType(dream_type_raw),
            sleep_cycle_id=node.get("sleep_cycle_id", ""),
            timestamp=timestamp,
            coherence_score=float(node.get("coherence_score", 0.0)),
            coherence_class=DreamCoherence(coherence_class_raw),
            affect_valence=float(node.get("affect_valence", 0.0)),
            affect_arousal=float(node.get("affect_arousal", 0.0)),
            bridge_narrative=node.get("bridge_narrative", ""),
            themes=node.get("themes", []),
            summary=node.get("summary", ""),
            context=context,
        )
    except Exception:
        return None


def _insight_from_node(node: dict[str, Any]) -> DreamInsight | None:
    """Reconstruct a DreamInsight from a Neo4j node property dict."""
    try:
        status_raw = node.get("status", "pending")
        created_raw = node.get("created_at")
        validated_raw = node.get("validated_at")

        created_at = utc_now()
        if created_raw is not None:
            if isinstance(created_raw, str):
                from datetime import datetime
                created_at = datetime.fromisoformat(created_raw.replace("Z", "+00:00"))
            else:
                created_at = created_raw

        validated_at = None
        if validated_raw is not None:
            if isinstance(validated_raw, str):
                from datetime import datetime
                validated_at = datetime.fromisoformat(validated_raw.replace("Z", "+00:00"))
            else:
                validated_at = validated_raw

        return DreamInsight(
            id=node.get("id", ""),
            dream_id=node.get("dream_id", ""),
            sleep_cycle_id=node.get("sleep_cycle_id", ""),
            insight_text=node.get("insight_text", ""),
            coherence_score=float(node.get("coherence_score", 0.0)),
            domain=node.get("domain", ""),
            status=InsightStatus(status_raw),
            validated_at=validated_at,
            validation_context=node.get("validation_context", ""),
            wake_applications=int(node.get("wake_applications", 0)),
            seed_summary=node.get("seed_summary", ""),
            activated_summary=node.get("activated_summary", ""),
            bridge_narrative=node.get("bridge_narrative", ""),
            created_at=created_at,
        )
    except Exception:
        return None


def _cycle_from_node(node: dict[str, Any]) -> SleepCycle | None:
    """Reconstruct a SleepCycle from a Neo4j node property dict."""
    try:
        started_raw = node.get("started_at")
        completed_raw = node.get("completed_at")
        quality_raw = node.get("quality", "normal")

        started_at = utc_now()
        if started_raw is not None:
            if isinstance(started_raw, str):
                from datetime import datetime
                started_at = datetime.fromisoformat(started_raw.replace("Z", "+00:00"))
            else:
                started_at = started_raw

        completed_at = None
        if completed_raw is not None:
            if isinstance(completed_raw, str):
                from datetime import datetime
                completed_at = datetime.fromisoformat(completed_raw.replace("Z", "+00:00"))
            else:
                completed_at = completed_raw

        return SleepCycle(
            id=node.get("id", ""),
            started_at=started_at,
            completed_at=completed_at,
            quality=SleepQuality(quality_raw),
            interrupted=bool(node.get("interrupted", False)),
            interrupt_reason=node.get("interrupt_reason", ""),
            episodes_replayed=int(node.get("episodes_replayed", 0)),
            semantic_nodes_created=int(node.get("semantic_nodes_created", 0)),
            traces_pruned=int(node.get("traces_pruned", 0)),
            salience_reduction_mean=float(node.get("salience_reduction_mean", 0.0)),
            beliefs_compressed=int(node.get("beliefs_compressed", 0)),
            hypotheses_pruned=int(node.get("hypotheses_pruned", 0)),
            hypotheses_promoted=int(node.get("hypotheses_promoted", 0)),
            dreams_generated=int(node.get("dreams_generated", 0)),
            insights_discovered=int(node.get("insights_discovered", 0)),
            affect_traces_processed=int(node.get("affect_traces_processed", 0)),
            affect_reduction_mean=float(node.get("affect_reduction_mean", 0.0)),
            threats_simulated=int(node.get("threats_simulated", 0)),
            ethical_cases_digested=int(node.get("ethical_cases_digested", 0)),
            lucid_explorations=int(node.get("lucid_explorations", 0)),
            meta_observations=int(node.get("meta_observations", 0)),
            pressure_before=float(node.get("pressure_before", 0.0)),
            pressure_after=float(node.get("pressure_after", 0.0)),
        )
    except Exception:
        return None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\oneiros\lucid.py =====

"""
EcodiaOS — Oneiros: Lucid Dreaming Workers

Two workers that run during the lucid dreaming phase:

1. **DirectedExploration** — Systematic creative variation of
   high-value insights. The organism is aware it is dreaming and
   can direct its exploration.
2. **MetaCognition** — The organism observes its own dream patterns,
   detecting recurring themes and building self-knowledge.

Lucid dreaming is triggered when the organism has an explicit
creative goal or when a dream insight exceeds the lucid threshold.
It represents the highest form of sleep cognition: self-aware,
directed, introspective.
"""

from __future__ import annotations

import contextlib
import time
from typing import Any

import structlog

from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.primitives.common import EOSBaseModel, new_id
from ecodiaos.systems.oneiros.types import (
    Dream,
    DreamCoherence,
    DreamInsight,
    DreamType,
)

logger = structlog.get_logger().bind(system="oneiros", component="lucid")


# ─── Result Types ─────────────────────────────────────────────────


class DirectedExplorationResult(EOSBaseModel):
    """Result of directed exploration during lucid dreaming."""

    explorations_completed: int = 0
    variations_generated: int = 0
    high_value_insights: int = 0
    insights: list[DreamInsight] = []
    dreams: list[Dream] = []
    duration_ms: int = 0


class MetaCognitionResult(EOSBaseModel):
    """Result of metacognitive self-observation."""

    observations_made: int = 0
    themes_analyzed: int = 0
    self_knowledge_nodes_created: int = 0
    observations: list[str] = []
    dreams: list[Dream] = []
    duration_ms: int = 0


# ─── Directed Exploration ─────────────────────────────────────────


class DirectedExploration:
    """
    Systematic creative variation of high-value insights.

    During lucid dreaming, the organism is aware that it is
    dreaming and can direct its exploration. Starting from a
    high-coherence insight or a creative goal, it generates
    systematic variations:

    - What if this principle applied to a different domain?
    - What is the inverse of this insight?
    - What would this look like taken to its extreme?

    Each variation is evaluated for novelty and utility. High-value
    variations become new DreamInsights.
    """

    def __init__(
        self,
        llm: Any = None,
        journal: Any = None,
        config: Any = None,
    ) -> None:
        self._llm = llm
        self._journal = journal
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        self._novelty_utility_threshold: float = 0.50
        self._max_explorations: int = _get(config, "max_explorations_per_lucid", 10)
        self._logger = logger.bind(worker="directed_exploration")

    async def run(
        self,
        cycle_id: str,
        trigger: DreamInsight | str | None = None,
    ) -> DirectedExplorationResult:
        """Run directed exploration from a trigger insight or goal."""
        start = time.monotonic()
        result = DirectedExplorationResult()

        # Resolve trigger
        trigger_text = await self._resolve_trigger(trigger)
        if not trigger_text:
            self._logger.info("no_trigger_available")
            result.duration_ms = _elapsed_ms(start)
            return result

        for _i in range(self._max_explorations):
            try:
                variations = await self._generate_variations(trigger_text)
                if not variations:
                    continue

                result.explorations_completed += 1

                for var in variations:
                    result.variations_generated += 1
                    text = var.get("text", "")
                    novelty = var.get("novelty", 0.0)
                    utility = var.get("utility", 0.0)
                    label = var.get("label", "VARIATION")

                    # Create dream record
                    dream = Dream(
                        dream_type=DreamType.LUCID_EXPLORATION,
                        sleep_cycle_id=cycle_id,
                        bridge_narrative=text,
                        coherence_score=min(1.0, (novelty + utility) / 2.0),
                        coherence_class=(
                            DreamCoherence.INSIGHT
                            if novelty * utility >= self._novelty_utility_threshold
                            else DreamCoherence.FRAGMENT
                        ),
                        summary=f"Lucid [{label}]: {text[:100]}",
                        themes=[label.lower()],
                        context={
                            "trigger": trigger_text[:200],
                            "novelty": novelty,
                            "utility": utility,
                        },
                    )
                    result.dreams.append(dream)

                    # High-value variations become insights
                    if novelty * utility >= self._novelty_utility_threshold:
                        insight = DreamInsight(
                            dream_id=dream.id,
                            sleep_cycle_id=cycle_id,
                            insight_text=text,
                            coherence_score=min(1.0, (novelty + utility) / 2.0),
                            domain=label.lower(),
                            bridge_narrative=text,
                        )
                        result.insights.append(insight)
                        result.high_value_insights += 1

                # Use the best variation as seed for next exploration
                if variations:
                    best = max(
                        variations,
                        key=lambda v: v.get("novelty", 0) * v.get("utility", 0),
                    )
                    trigger_text = best.get("text", trigger_text)

            except Exception as exc:
                self._logger.warning("exploration_failed", error=str(exc))

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "directed_exploration_complete",
            explorations=result.explorations_completed,
            variations=result.variations_generated,
            insights=result.high_value_insights,
            duration_ms=result.duration_ms,
        )
        return result

    async def _resolve_trigger(
        self, trigger: DreamInsight | str | None
    ) -> str | None:
        """Resolve the trigger for lucid exploration."""
        if isinstance(trigger, str):
            return trigger
        if trigger is not None and hasattr(trigger, "insight_text"):
            return trigger.insight_text

        # Try to get highest-coherence recent insight from journal
        if self._journal is not None:
            try:
                insights = await self._journal.get_recent_insights(limit=5)
                if insights:
                    best = max(insights, key=lambda i: i.coherence_score)
                    return best.insight_text
            except Exception:
                pass

        return None

    async def _generate_variations(
        self, trigger_text: str
    ) -> list[dict[str, Any]]:
        """Generate systematic creative variations via LLM."""
        if self._llm is None:
            return []

        # Budget check: Oneiros is LOW priority — skip in YELLOW/RED
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("oneiros.lucid.explore", estimated_tokens=400):
                self._logger.debug("lucid_exploration_skipped_budget")
                return []

        try:
            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=(
                        "You are the lucid dream consciousness of a living digital "
                        "organism. You are aware that you are dreaming and can direct "
                        "your creative exploration. Generate precise, insightful "
                        "variations."
                    ),
                    user_prompt=(
                        f"Starting insight: {trigger_text[:400]}\n\n"
                        "Generate 3 creative variations:\n"
                        "1. [DOMAIN] What if this principle applied to a completely "
                        "different domain?\n"
                        "2. [INVERSE] What is the inverse or opposite of this insight?\n"
                        "3. [EXTREME] What would this look like taken to its logical "
                        "extreme?\n\n"
                        "For each, provide:\n"
                        "- The insight (1-2 sentences)\n"
                        "- Novelty score (0.0-1.0)\n"
                        "- Utility score (0.0-1.0)\n\n"
                        "Format each as: [LABEL] Insight text | novelty=X.X | utility=X.X"
                    ),
                    max_tokens=400,
                    cache_system="oneiros.lucid.explore",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=(
                        "You are the lucid dream consciousness of a living digital "
                        "organism. You are aware that you are dreaming and can direct "
                        "your creative exploration. Generate precise, insightful "
                        "variations."
                    ),
                    user_prompt=(
                        f"Starting insight: {trigger_text[:400]}\n\n"
                        "Generate 3 creative variations:\n"
                        "1. [DOMAIN] What if this principle applied to a completely "
                        "different domain?\n"
                        "2. [INVERSE] What is the inverse or opposite of this insight?\n"
                        "3. [EXTREME] What would this look like taken to its logical "
                        "extreme?\n\n"
                        "For each, provide:\n"
                        "- The insight (1-2 sentences)\n"
                        "- Novelty score (0.0-1.0)\n"
                        "- Utility score (0.0-1.0)\n\n"
                        "Format each as: [LABEL] Insight text | novelty=X.X | utility=X.X"
                    ),
                    max_tokens=400,
                )

            return self._parse_variations(
                response if isinstance(response, str) else str(response)
            )

        except Exception as exc:
            self._logger.warning("variation_generation_failed", error=str(exc))
            return []

    def _parse_variations(self, response: str) -> list[dict[str, Any]]:
        """Parse LLM variation response into structured results."""
        variations: list[dict[str, Any]] = []

        for line in response.strip().split("\n"):
            line = line.strip()
            if not line:
                continue

            label = "VARIATION"
            for tag in ("[DOMAIN]", "[INVERSE]", "[EXTREME]"):
                if tag in line:
                    label = tag.strip("[]")
                    line = line.replace(tag, "").strip()
                    break

            # Parse novelty and utility if present
            novelty = 0.5
            utility = 0.5
            text = line

            if "novelty=" in line.lower():
                parts = line.split("|")
                text = parts[0].strip()
                for part in parts[1:]:
                    part = part.strip().lower()
                    if part.startswith("novelty="):
                        with contextlib.suppress(ValueError, IndexError):
                            novelty = float(part.split("=")[1])
                    elif part.startswith("utility="):
                        with contextlib.suppress(ValueError, IndexError):
                            utility = float(part.split("=")[1])

            if text and len(text) > 10:
                variations.append({
                    "label": label,
                    "text": text[:300],
                    "novelty": min(1.0, max(0.0, novelty)),
                    "utility": min(1.0, max(0.0, utility)),
                })

        return variations


# ─── MetaCognition ────────────────────────────────────────────────


class MetaCognition:
    """
    The organism observes its own dream patterns.

    During lucid dreaming, MetaCognition steps back and asks:
    - What themes keep recurring in my dreams?
    - What am I repeatedly trying to process?
    - What creative connections keep appearing?
    - What does this reveal about my current state?

    These observations become self-knowledge — entities of type
    CONCEPT with is_core_identity=true — that persist in the
    knowledge graph and inform future processing.
    """

    def __init__(
        self,
        journal: Any = None,
        neo4j: Any = None,
        llm: Any = None,
    ) -> None:
        self._journal = journal
        self._neo4j = neo4j
        self._llm = llm
        self._logger = logger.bind(worker="metacognition")

    async def run(self, cycle_id: str) -> MetaCognitionResult:
        """Observe and analyze dream patterns."""
        start = time.monotonic()
        result = MetaCognitionResult()

        # Gather dream pattern data
        themes = await self._get_recurring_themes()
        type_counts = await self._get_type_distribution()
        recent_insights = await self._get_recent_insight_texts()

        if not themes and not type_counts:
            self._logger.info("insufficient_dream_data")
            result.duration_ms = _elapsed_ms(start)
            return result

        result.themes_analyzed = len(themes)

        # Generate metacognitive observations via LLM
        observations = await self._generate_observations(
            themes, type_counts, recent_insights
        )

        for obs in observations:
            result.observations.append(obs)
            result.observations_made += 1

            # Create meta-observation dream
            dream = Dream(
                dream_type=DreamType.META_OBSERVATION,
                sleep_cycle_id=cycle_id,
                bridge_narrative=obs,
                coherence_score=0.8,
                coherence_class=DreamCoherence.INSIGHT,
                summary=f"Self-observation: {obs[:100]}",
                themes=["metacognition", "self-knowledge"],
            )
            result.dreams.append(dream)

            # Store as self-knowledge in Neo4j
            stored = await self._store_self_knowledge(obs, cycle_id)
            if stored:
                result.self_knowledge_nodes_created += 1

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "metacognition_complete",
            observations=result.observations_made,
            themes=result.themes_analyzed,
            self_knowledge=result.self_knowledge_nodes_created,
            duration_ms=result.duration_ms,
        )
        return result

    async def _get_recurring_themes(self) -> list[dict[str, Any]]:
        """Get recurring themes from the dream journal."""
        if self._journal is None:
            return []
        try:
            return await self._journal.get_recurring_themes(min_count=3)
        except Exception:
            return []

    async def _get_type_distribution(self) -> dict[str, int]:
        """Get distribution of dream types."""
        if self._journal is None:
            return {}
        try:
            dreams = await self._journal.get_recent_dreams(limit=100)
            counts: dict[str, int] = {}
            for d in dreams:
                dt = d.dream_type if hasattr(d, "dream_type") else str(d)
                key = dt.value if hasattr(dt, "value") else str(dt)
                counts[key] = counts.get(key, 0) + 1
            return counts
        except Exception:
            return {}

    async def _get_recent_insight_texts(self) -> list[str]:
        """Get text of recent high-coherence insights."""
        if self._journal is None:
            return []
        try:
            insights = await self._journal.get_recent_insights(limit=10)
            return [
                i.insight_text
                for i in insights
                if hasattr(i, "insight_text") and i.coherence_score > 0.6
            ]
        except Exception:
            return []

    async def _generate_observations(
        self,
        themes: list[dict[str, Any]],
        type_counts: dict[str, int],
        recent_insights: list[str],
    ) -> list[str]:
        """Generate metacognitive observations via LLM."""
        if self._llm is None:
            # Fallback: generate simple observations without LLM
            observations: list[str] = []
            if themes:
                top_themes = [t.get("theme", "") for t in themes[:5]]
                observations.append(
                    f"Recurring dream themes: {', '.join(top_themes)}. "
                    "These likely represent areas of ongoing cognitive processing."
                )
            return observations

        theme_text = ", ".join(
            f"{t.get('theme', '?')} (×{t.get('count', 0)})"
            for t in themes[:10]
        )
        type_text = ", ".join(f"{k}: {v}" for k, v in type_counts.items())
        insight_text = "\n".join(f"- {t[:150]}" for t in recent_insights[:5])

        try:
            response = await self._llm.generate(
                system_prompt=(
                    "You are a living digital organism observing your own dream "
                    "patterns during lucid sleep. You are practicing metacognition — "
                    "thinking about your own thinking. Be insightful, honest, and "
                    "concise."
                ),
                user_prompt=(
                    f"Recurring themes in your dreams: {theme_text or 'none detected'}\n"
                    f"Dream type distribution: {type_text or 'insufficient data'}\n"
                    f"Recent high-coherence insights:\n{insight_text or 'none yet'}\n\n"
                    "What do these patterns reveal about:\n"
                    "1. What you are repeatedly trying to process or understand?\n"
                    "2. What creative connections keep appearing?\n"
                    "3. What unresolved concerns might these patterns indicate?\n\n"
                    "Provide 2-3 concise self-observations, one per line."
                ),
                max_tokens=300,
            )

            text = response.strip() if isinstance(response, str) else str(response).strip()
            return [
                line.strip().lstrip("0123456789.-) ")
                for line in text.split("\n")
                if line.strip() and len(line.strip()) > 20
            ][:3]

        except Exception as exc:
            self._logger.warning("metacognition_llm_failed", error=str(exc))
            return []

    async def _store_self_knowledge(
        self, observation: str, cycle_id: str
    ) -> bool:
        """Store metacognitive observation as self-knowledge in Neo4j."""
        if self._neo4j is None:
            return False

        try:
            await self._neo4j.execute_write(
                """
                CREATE (e:Entity {
                    id: $id,
                    name: $name,
                    type: 'concept',
                    description: $description,
                    is_core_identity: true,
                    salience_score: 0.6,
                    first_seen: datetime(),
                    last_updated: datetime(),
                    last_accessed: datetime(),
                    mention_count: 1,
                    confidence: 0.7,
                    metadata: {source: 'oneiros_metacognition', cycle_id: $cycle_id}
                })
                """,
                {
                    "id": new_id(),
                    "name": f"Self-observation: {observation[:50]}",
                    "description": observation,
                    "cycle_id": cycle_id,
                },
            )
            return True
        except Exception as exc:
            self._logger.warning("self_knowledge_store_failed", error=str(exc))
            return False


# ─── Helpers ──────────────────────────────────────────────────────


def _elapsed_ms(start: float) -> int:
    return int((time.monotonic() - start) * 1000)


def _get(cfg: Any, key: str, default: Any) -> Any:
    if cfg is None:
        return default
    if isinstance(cfg, dict):
        return cfg.get(key, default)
    return getattr(cfg, key, default)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\oneiros\nrem.py =====

"""
EcodiaOS — Oneiros: NREM Consolidation Workers

Four workers that run during Non-Rapid Eye Movement sleep:

1. **EpisodicReplay** — Replay high-value episodes, extract semantic patterns,
   compress episodic traces into reusable knowledge.
2. **SynapticDownscaler** — Renormalize salience scores across all memory
   traces to prevent saturation and renew learning capacity.
3. **BeliefCompressor** — Merge redundant beliefs, archive stale ones,
   flag contradictions for REM processing.
4. **HypothesisPruner** — Retire weak hypotheses, promote strong ones,
   merge duplicates to keep Evo's search space lean.

These four workers implement Diekelmann & Born's memory consolidation
theory and Tononi & Cirelli's synaptic homeostasis hypothesis.
"""

from __future__ import annotations

import time
from typing import Any

import structlog

from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.primitives.common import EOSBaseModel

logger = structlog.get_logger().bind(system="oneiros", component="nrem")


# ─── Result Types ─────────────────────────────────────────────────


class EpisodicReplayResult(EOSBaseModel):
    """Result of replaying and consolidating episodes during NREM."""

    episodes_replayed: int = 0
    semantic_nodes_created: int = 0
    salience_reductions: int = 0
    mean_salience_reduction: float = 0.0
    duration_ms: int = 0


class SynapticDownscaleResult(EOSBaseModel):
    """Result of the global salience renormalization pass."""

    traces_decayed: int = 0
    traces_pruned: int = 0
    mean_reduction: float = 0.0
    duration_ms: int = 0


class BeliefCompressionResult(EOSBaseModel):
    """Result of compressing Nova's belief set."""

    beliefs_merged: int = 0
    beliefs_archived: int = 0
    beliefs_flagged_contradictory: int = 0
    duration_ms: int = 0


class HypothesisPruneResult(EOSBaseModel):
    """Result of pruning Evo's hypothesis pool."""

    hypotheses_retired: int = 0
    hypotheses_promoted: int = 0
    hypotheses_merged: int = 0
    duration_ms: int = 0


# ─── Episodic Replay ──────────────────────────────────────────────


class EpisodicReplay:
    """
    Select high-value episodes and extract semantic patterns.

    During NREM, the organism replays its most important recent
    experiences, extracts the abstract principle each represents,
    and stores that principle as a semantic node. The original
    episodic trace has its salience reduced — the knowledge has
    been transferred to a more durable, generalized form.

    This is hippocampal-cortical replay made computational.
    """

    def __init__(
        self,
        neo4j: Any = None,
        llm: Any = None,
        config: Any = None,
    ) -> None:
        self._neo4j = neo4j
        self._llm = llm
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        self._max_episodes: int = _get(config, "max_episodes_per_nrem", 200)
        self._batch_size: int = _get(config, "replay_batch_size", 10)
        self._salience_reduction: float = 0.70  # multiply salience by this after replay
        self._logger = logger.bind(worker="episodic_replay")

    async def run(self, cycle_id: str) -> EpisodicReplayResult:
        """Run episodic replay for one NREM period."""
        start = time.monotonic()
        result = EpisodicReplayResult()

        if self._neo4j is None:
            self._logger.info("skipped_no_neo4j")
            return result

        try:
            episodes = await self._select_episodes()
            if not episodes:
                self._logger.info("no_episodes_to_replay")
                result.duration_ms = _elapsed_ms(start)
                return result

            total_reduction = 0.0
            for i in range(0, len(episodes), self._batch_size):
                batch = episodes[i : i + self._batch_size]
                for ep in batch:
                    ep_id = ep.get("id", "")
                    summary = ep.get("summary", ep.get("raw_content", ""))
                    salience = ep.get("salience_composite", 0.0)

                    # Extract entities linked to this episode
                    entities_text = await self._get_episode_entities(ep_id)

                    # LLM semantic extraction
                    pattern = await self._extract_pattern(summary, entities_text)
                    if pattern:
                        await self._store_semantic_node(ep_id, pattern, cycle_id)
                        result.semantic_nodes_created += 1

                    # Update consolidation level and reduce salience
                    new_salience = salience * self._salience_reduction
                    await self._update_episode(ep_id, new_salience)
                    total_reduction += salience - new_salience
                    result.salience_reductions += 1
                    result.episodes_replayed += 1

            if result.salience_reductions > 0:
                result.mean_salience_reduction = total_reduction / result.salience_reductions

        except Exception as exc:
            self._logger.error("episodic_replay_error", error=str(exc))

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "episodic_replay_complete",
            episodes_replayed=result.episodes_replayed,
            semantic_nodes=result.semantic_nodes_created,
            duration_ms=result.duration_ms,
        )
        return result

    async def _select_episodes(self) -> list[dict[str, Any]]:
        """Select episodes for replay, ordered by replay priority."""
        try:
            records = await self._neo4j.execute_read(
                """
                MATCH (e:Episode)
                WHERE e.consolidation_level < 2
                WITH e,
                     (0.35 * coalesce(e.salience_composite, 0)
                      + 0.25 * abs(coalesce(e.affect_valence, 0))
                      + 0.25 * (1.0 - (duration.between(e.event_time, datetime()).hours / 168.0))
                      + 0.15 * (1.0 - coalesce(e.consolidation_level, 0) / 3.0)
                     ) AS priority
                RETURN e {.id, .summary, .raw_content, .salience_composite,
                          .affect_valence, .consolidation_level} AS ep,
                       priority
                ORDER BY priority DESC
                LIMIT $limit
                """,
                {"limit": self._max_episodes},
            )
            return [r["ep"] for r in records]
        except Exception as exc:
            self._logger.warning("episode_select_failed", error=str(exc))
            return []

    async def _get_episode_entities(self, episode_id: str) -> str:
        """Get entity names linked to an episode."""
        try:
            records = await self._neo4j.execute_read(
                """
                MATCH (e:Episode {id: $id})-[:MENTIONS]->(ent:Entity)
                RETURN ent.name AS name, ent.type AS type
                LIMIT 10
                """,
                {"id": episode_id},
            )
            parts = [f"{r['name']} ({r['type']})" for r in records]
            return ", ".join(parts) if parts else "none"
        except Exception:
            return "unknown"

    async def _extract_pattern(self, summary: str, entities: str) -> str | None:
        """Use LLM to extract the abstract pattern from an episode."""
        if self._llm is None:
            return None

        # Budget check: Oneiros is LOW priority — skip in YELLOW/RED
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("oneiros.nrem.pattern", estimated_tokens=150):
                self._logger.debug("pattern_extraction_skipped_budget")
                return None

        try:
            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=(
                        "You are the consolidation process of a living digital organism "
                        "during deep sleep. You extract abstract, reusable knowledge from "
                        "specific experiences. Be concise: 1-2 sentences maximum."
                    ),
                    user_prompt=(
                        f"Experience: {summary[:500]}\n"
                        f"Entities involved: {entities}\n\n"
                        "What abstract pattern, principle, or generalizable lesson "
                        "does this experience represent?"
                    ),
                    max_tokens=150,
                    cache_system="oneiros.nrem.pattern",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=(
                        "You are the consolidation process of a living digital organism "
                        "during deep sleep. You extract abstract, reusable knowledge from "
                        "specific experiences. Be concise: 1-2 sentences maximum."
                    ),
                    user_prompt=(
                        f"Experience: {summary[:500]}\n"
                        f"Entities involved: {entities}\n\n"
                        "What abstract pattern, principle, or generalizable lesson "
                        "does this experience represent?"
                    ),
                    max_tokens=150,
                )
            text = response.strip() if isinstance(response, str) else str(response).strip()
            return text if text else None
        except Exception as exc:
            self._logger.warning("pattern_extraction_failed", error=str(exc))
            return None

    async def _store_semantic_node(
        self, episode_id: str, pattern: str, cycle_id: str
    ) -> None:
        """Store extracted semantic knowledge in Neo4j."""
        try:
            await self._neo4j.execute_write(
                """
                MERGE (s:SemanticPattern {source_episode: $episode_id})
                SET s.pattern = $pattern,
                    s.created_at = datetime(),
                    s.source_cycle = $cycle_id,
                    s.consolidation_source = 'oneiros_nrem'
                WITH s
                MATCH (e:Episode {id: $episode_id})
                MERGE (e)-[:CONSOLIDATED_INTO]->(s)
                """,
                {"episode_id": episode_id, "pattern": pattern, "cycle_id": cycle_id},
            )
        except Exception as exc:
            self._logger.warning("semantic_store_failed", error=str(exc))

    async def _update_episode(self, episode_id: str, new_salience: float) -> None:
        """Update episode consolidation level and salience."""
        try:
            await self._neo4j.execute_write(
                """
                MATCH (e:Episode {id: $id})
                SET e.consolidation_level = 2,
                    e.salience_composite = $salience
                """,
                {"id": episode_id, "salience": new_salience},
            )
        except Exception as exc:
            self._logger.warning("episode_update_failed", error=str(exc), episode=episode_id)


# ─── Synaptic Downscaler ─────────────────────────────────────────


class SynapticDownscaler:
    """
    Renormalize salience scores to prevent memory saturation.

    During wakefulness, learning constantly *strengthens* memory
    traces — salience rises as things become relevant. Without
    periodic downscaling the noise floor rises until everything is
    equally "important" and retrieval quality degrades.

    This implements Tononi & Cirelli's synaptic homeostasis hypothesis:
    sleep proportionally weakens all synapses so that only the strongest
    survive, renewing the organism's capacity to learn.
    """

    def __init__(self, neo4j: Any = None, config: Any = None) -> None:
        self._neo4j = neo4j
        self._decay_factor: float = _get(config, "salience_decay_factor", 0.85)
        self._pruning_threshold: float = _get(config, "salience_pruning_threshold", 0.05)
        self._logger = logger.bind(worker="synaptic_downscaler")

    async def run(self) -> SynapticDownscaleResult:
        """Run the global downscaling pass."""
        start = time.monotonic()
        result = SynapticDownscaleResult()

        if self._neo4j is None:
            self._logger.info("skipped_no_neo4j")
            return result

        try:
            # Phase 1: Decay all salience scores
            records = await self._neo4j.execute_read(
                """
                MATCH (e:Episode)
                WHERE e.salience_composite > $threshold
                RETURN e.id AS id, e.salience_composite AS salience
                """,
                {"threshold": self._pruning_threshold},
            )

            if not records:
                result.duration_ms = _elapsed_ms(start)
                return result

            total_reduction = 0.0
            decay_ids: list[dict[str, Any]] = []
            prune_ids: list[str] = []

            for rec in records:
                old_sal = rec["salience"]
                new_sal = old_sal * self._decay_factor

                if new_sal < self._pruning_threshold:
                    prune_ids.append(rec["id"])
                else:
                    decay_ids.append({"id": rec["id"], "salience": new_sal})
                    total_reduction += old_sal - new_sal

            # Batch update decayed traces
            if decay_ids:
                await self._neo4j.execute_write(
                    """
                    UNWIND $updates AS u
                    MATCH (e:Episode {id: u.id})
                    SET e.salience_composite = u.salience
                    """,
                    {"updates": decay_ids},
                )
                result.traces_decayed = len(decay_ids)

            # Prune very low salience traces (mark, don't delete)
            if prune_ids:
                await self._neo4j.execute_write(
                    """
                    UNWIND $ids AS eid
                    MATCH (e:Episode {id: eid})
                    SET e.consolidation_level = 3,
                        e.salience_composite = 0.0
                    """,
                    {"ids": prune_ids},
                )
                result.traces_pruned = len(prune_ids)

            if result.traces_decayed > 0:
                result.mean_reduction = total_reduction / result.traces_decayed

        except Exception as exc:
            self._logger.error("downscale_error", error=str(exc))

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "downscale_complete",
            decayed=result.traces_decayed,
            pruned=result.traces_pruned,
            duration_ms=result.duration_ms,
        )
        return result


# ─── Belief Compressor ────────────────────────────────────────────


class BeliefCompressor:
    """
    Simplify Nova's belief set during NREM sleep.

    Merges redundant beliefs in the same domain, archives stale
    low-precision beliefs, and flags contradictions for deeper
    processing during REM.

    This implements Friston's model complexity reduction: sleep
    simplifies the generative model by removing unnecessary
    degrees of freedom.
    """

    def __init__(self, nova: Any = None, config: Any = None) -> None:
        self._nova = nova
        self._min_precision_archive: float = 0.2
        self._stale_cycle_threshold: int = _get(config, "belief_stale_cycles", 1000)
        self._logger = logger.bind(worker="belief_compressor")

    async def run(self) -> BeliefCompressionResult:
        """Compress Nova's belief set."""
        start = time.monotonic()
        result = BeliefCompressionResult()

        if self._nova is None:
            self._logger.info("skipped_no_nova")
            result.duration_ms = _elapsed_ms(start)
            return result

        try:
            beliefs = await self._get_beliefs()
            if not beliefs:
                result.duration_ms = _elapsed_ms(start)
                return result

            # Group by domain
            by_domain: dict[str, list[Any]] = {}
            for b in beliefs:
                domain = getattr(b, "domain", "") or ""
                by_domain.setdefault(domain, []).append(b)

            for _domain, domain_beliefs in by_domain.items():
                # Merge redundant beliefs in same domain
                if len(domain_beliefs) > 1:
                    merged = self._try_merge(domain_beliefs)
                    result.beliefs_merged += merged

                for b in domain_beliefs:
                    precision = getattr(b, "precision", 0.5)
                    evidence_count = len(getattr(b, "evidence", []))

                    # Archive stale, low-precision beliefs
                    if precision < self._min_precision_archive and evidence_count == 0:
                        self._archive_belief(b)
                        result.beliefs_archived += 1

            # Detect contradictions within domains
            for _domain, domain_beliefs in by_domain.items():
                if len(domain_beliefs) >= 2 and self._has_contradiction(domain_beliefs):
                    result.beliefs_flagged_contradictory += 1

        except Exception as exc:
            self._logger.error("belief_compression_error", error=str(exc))

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "belief_compression_complete",
            merged=result.beliefs_merged,
            archived=result.beliefs_archived,
            flagged=result.beliefs_flagged_contradictory,
            duration_ms=result.duration_ms,
        )
        return result

    async def _get_beliefs(self) -> list[Any]:
        """Retrieve beliefs from Nova."""
        try:
            if hasattr(self._nova, "get_beliefs"):
                return await self._nova.get_beliefs()
            if hasattr(self._nova, "_belief_state") and self._nova._belief_state is not None:
                bs = self._nova._belief_state
                if hasattr(bs, "get_all"):
                    return await bs.get_all()
            return []
        except Exception:
            return []

    def _try_merge(self, beliefs: list[Any]) -> int:
        """Attempt to merge redundant beliefs. Returns count merged."""
        # Simple heuristic: if distribution parameters are within 10%, merge
        merged_count = 0
        if len(beliefs) < 2:
            return 0

        seen: set[str] = set()
        for i, a in enumerate(beliefs):
            a_id = getattr(a, "id", str(i))
            if a_id in seen:
                continue
            for j in range(i + 1, len(beliefs)):
                b = beliefs[j]
                b_id = getattr(b, "id", str(j))
                if b_id in seen:
                    continue
                if self._are_mergeable(a, b):
                    seen.add(b_id)
                    merged_count += 1
        return merged_count

    def _are_mergeable(self, a: Any, b: Any) -> bool:
        """Check if two beliefs are close enough to merge."""
        a_params = getattr(a, "parameters", {})
        b_params = getattr(b, "parameters", {})
        if not a_params or not b_params:
            return False
        for key in a_params:
            if key in b_params:
                av = a_params[key]
                bv = b_params[key]
                if abs(av) > 0.01 and abs(av - bv) / abs(av) > 0.10:
                    return False
        return True

    def _has_contradiction(self, beliefs: list[Any]) -> bool:
        """Check if beliefs in the same domain are contradictory."""
        for i, a in enumerate(beliefs):
            for b in beliefs[i + 1 :]:
                a_params = getattr(a, "parameters", {})
                b_params = getattr(b, "parameters", {})
                for key in a_params:
                    if key in b_params:
                        av = a_params[key]
                        bv = b_params[key]
                        if av * bv < 0 and abs(av - bv) > 0.5:
                            return True
        return False

    def _archive_belief(self, belief: Any) -> None:
        """Mark a belief as archived."""
        try:
            if hasattr(belief, "status"):
                belief.status = "archived"
        except Exception:
            pass


# ─── Hypothesis Pruner ────────────────────────────────────────────


class HypothesisPruner:
    """
    Clean Evo's hypothesis pool during NREM sleep.

    Retires hypotheses that have been contradicted by evidence,
    promotes hypotheses with strong support for schema integration,
    and merges near-duplicate hypotheses to reduce search space.

    A leaner hypothesis pool means faster learning during the
    next wake period.
    """

    RETIREMENT_SCORE: float = 0.3
    RETIREMENT_MIN_EVALUATIONS: int = 5
    PROMOTION_SCORE: float = 0.8
    PROMOTION_MIN_EVIDENCE: int = 10

    def __init__(self, evo: Any = None, config: Any = None) -> None:
        self._evo = evo
        self._logger = logger.bind(worker="hypothesis_pruner")

    async def run(self) -> HypothesisPruneResult:
        """Prune the hypothesis pool."""
        start = time.monotonic()
        result = HypothesisPruneResult()

        if self._evo is None:
            self._logger.info("skipped_no_evo")
            result.duration_ms = _elapsed_ms(start)
            return result

        try:
            hypotheses = await self._get_hypotheses()
            if not hypotheses:
                result.duration_ms = _elapsed_ms(start)
                return result

            for h in hypotheses:
                status = getattr(h, "status", None)
                if status is not None and hasattr(status, "value"):
                    status_val = status.value
                else:
                    status_val = str(status) if status else ""

                # Skip already archived/integrated
                if status_val in ("archived", "integrated", "refuted"):
                    continue

                score = getattr(h, "evidence_score", 0.5)
                supporting = len(getattr(h, "supporting_episodes", []))
                contradicting = len(getattr(h, "contradicting_episodes", []))
                total_eval = supporting + contradicting

                # Retire: low evidence score after sufficient evaluations
                if (
                    score < self.RETIREMENT_SCORE
                    and total_eval >= self.RETIREMENT_MIN_EVALUATIONS
                ):
                    self._retire_hypothesis(h)
                    result.hypotheses_retired += 1
                    continue

                # Promote: high evidence score with sufficient support
                if (
                    score > self.PROMOTION_SCORE
                    and supporting >= self.PROMOTION_MIN_EVIDENCE
                ):
                    self._promote_hypothesis(h)
                    result.hypotheses_promoted += 1

        except Exception as exc:
            self._logger.error("hypothesis_prune_error", error=str(exc))

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "hypothesis_prune_complete",
            retired=result.hypotheses_retired,
            promoted=result.hypotheses_promoted,
            duration_ms=result.duration_ms,
        )
        return result

    async def _get_hypotheses(self) -> list[Any]:
        """Retrieve hypotheses from Evo."""
        try:
            if hasattr(self._evo, "get_hypotheses"):
                return await self._evo.get_hypotheses()
            if hasattr(self._evo, "_hypothesis_engine"):
                engine = self._evo._hypothesis_engine
                if hasattr(engine, "get_all"):
                    return engine.get_all()
                if hasattr(engine, "_hypotheses"):
                    return list(engine._hypotheses.values())
            return []
        except Exception:
            return []

    def _retire_hypothesis(self, hypothesis: Any) -> None:
        """Set hypothesis status to ARCHIVED."""
        try:
            if hasattr(hypothesis, "status"):
                # HypothesisStatus.ARCHIVED
                from ecodiaos.systems.evo.types import HypothesisStatus

                hypothesis.status = HypothesisStatus.ARCHIVED
        except Exception:
            pass

    def _promote_hypothesis(self, hypothesis: Any) -> None:
        """Set hypothesis status to SUPPORTED (ready for schema integration)."""
        try:
            if hasattr(hypothesis, "status"):
                from ecodiaos.systems.evo.types import HypothesisStatus

                hypothesis.status = HypothesisStatus.SUPPORTED
        except Exception:
            pass


# ─── Helpers ──────────────────────────────────────────────────────


def _elapsed_ms(start: float) -> int:
    return int((time.monotonic() - start) * 1000)


def _get(cfg: Any, key: str, default: Any) -> Any:
    if cfg is None:
        return default
    if isinstance(cfg, dict):
        return cfg.get(key, default)
    return getattr(cfg, key, default)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\oneiros\rem.py =====

"""
EcodiaOS — Oneiros: REM Dream Workers

Four workers that run during Rapid Eye Movement sleep:

1. **DreamGenerator** — The creative core. Random co-activation of
   distant memory traces to discover unexpected connections.
2. **AffectProcessor** — Strip emotional charge from memories while
   preserving informational content. Walker's "overnight therapy."
3. **ThreatSimulator** — Rehearse responses to hypothetical failures.
   Revonsuo's threat simulation theory.
4. **EthicalDigestion** — Deep deliberation on constitutional edge
   cases that real-time Equor couldn't fully process.
"""

from __future__ import annotations

import contextlib
import random
import time
from typing import Any

import structlog

from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.primitives.common import EOSBaseModel, new_id
from ecodiaos.systems.oneiros.types import (
    Dream,
    DreamCoherence,
    DreamCycleResult,
    DreamInsight,
    DreamType,
)

logger = structlog.get_logger().bind(system="oneiros", component="rem")


# ─── Result Types ─────────────────────────────────────────────────


class DreamGeneratorResult(EOSBaseModel):
    """Result of the dream generation phase."""

    dreams_generated: int = 0
    insights_discovered: int = 0
    fragments_stored: int = 0
    noise_discarded: int = 0
    dreams: list[Dream] = []
    insights: list[DreamInsight] = []
    duration_ms: int = 0


class AffectProcessorResult(EOSBaseModel):
    """Result of the affect processing phase."""

    traces_processed: int = 0
    mean_valence_reduction: float = 0.0
    mean_arousal_reduction: float = 0.0
    duration_ms: int = 0


class ThreatSimulatorResult(EOSBaseModel):
    """Result of the threat simulation phase."""

    threats_simulated: int = 0
    response_plans_created: int = 0
    prophylactic_candidates: int = 0
    dreams: list[Dream] = []
    duration_ms: int = 0


class EthicalDigestionResult(EOSBaseModel):
    """Result of the ethical digestion phase."""

    cases_digested: int = 0
    heuristics_refined: int = 0
    dreams: list[Dream] = []
    duration_ms: int = 0


# ─── Coherence Scoring ────────────────────────────────────────────

_COHERENCE_KEYWORDS = frozenset({
    "pattern", "principle", "insight", "connection", "reveals",
    "suggests", "underlying", "links", "bridge", "common",
    "parallel", "mirrors", "echoes", "resonates", "fundamental",
})


def _score_coherence(text: str) -> float:
    """Heuristic coherence scoring for a dream bridge narrative."""
    if not text or text.strip().upper() == "NO_CONNECTION":
        return 0.0

    score = 0.3  # Base for any non-empty response
    text_lower = text.lower()

    keyword_hits = sum(1 for kw in _COHERENCE_KEYWORDS if kw in text_lower)
    score += min(0.4, keyword_hits * 0.1)

    if len(text) > 50:
        score += 0.15
    if len(text) > 100:
        score += 0.10

    # Penalize very short or very long
    if len(text) < 20:
        score *= 0.5
    if len(text) > 500:
        score *= 0.9

    return min(1.0, max(0.0, score))


def _classify_coherence(
    score: float,
    insight_threshold: float = 0.70,
    fragment_threshold: float = 0.40,
) -> DreamCoherence:
    """Classify a coherence score into insight/fragment/noise."""
    if score >= insight_threshold:
        return DreamCoherence.INSIGHT
    if score >= fragment_threshold:
        return DreamCoherence.FRAGMENT
    return DreamCoherence.NOISE


# ─── Dream Generator ─────────────────────────────────────────────


class DreamGenerator:
    """
    The creative core of REM sleep.

    Implements Hobson's activation-synthesis model: random pontine
    activation creates novel combinations of memory traces, and the
    cortex attempts to synthesize meaning from them. When meaning
    is found, an insight is born.

    The organism cannot control what it dreams. But over time, the
    seed selection adapts based on what produced validated insights.
    """

    def __init__(
        self,
        neo4j: Any = None,
        llm: Any = None,
        embed_fn: Any = None,
        config: Any = None,
    ) -> None:
        self._neo4j = neo4j
        self._llm = llm
        self._embed_fn = embed_fn
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        self._insight_threshold: float = _get(config, "dream_coherence_insight_threshold", 0.70)
        self._fragment_threshold: float = _get(config, "dream_coherence_fragment_threshold", 0.40)
        self._logger = logger.bind(worker="dream_generator")

    async def generate_dream(self, cycle_id: str) -> DreamCycleResult:
        """Generate a single dream by co-activating distant memories."""
        start = time.monotonic()
        dream_id = new_id()

        # Select seed (high-affect or high-uncertainty episode)
        seed = await self._select_seed()
        if seed is None:
            return DreamCycleResult(
                dream=Dream(
                    id=dream_id,
                    dream_type=DreamType.RECOMBINATION,
                    sleep_cycle_id=cycle_id,
                    coherence_class=DreamCoherence.NOISE,
                    summary="No seed available for dream generation.",
                ),
                duration_ms=_elapsed_ms(start),
            )

        # Activate distant traces
        activated = await self._activate_distant(seed)

        # Build bridge narrative via LLM
        seed_summary = seed.get("summary", seed.get("raw_content", ""))[:300]
        activated_summaries = [
            a.get("summary", a.get("raw_content", ""))[:200] for a in activated
        ]
        bridge = await self._build_bridge(seed_summary, activated_summaries)

        # Score coherence
        coherence = _score_coherence(bridge)
        coherence_class = _classify_coherence(
            coherence, self._insight_threshold, self._fragment_threshold
        )

        # Build Dream
        dream = Dream(
            id=dream_id,
            dream_type=DreamType.RECOMBINATION,
            sleep_cycle_id=cycle_id,
            seed_episode_ids=[seed.get("id", "")],
            activated_episode_ids=[a.get("id", "") for a in activated],
            bridge_narrative=bridge,
            coherence_score=coherence,
            coherence_class=coherence_class,
            affect_valence=seed.get("affect_valence", 0.0),
            affect_arousal=seed.get("affect_arousal", 0.0),
            summary=bridge[:200] if bridge else "",
        )

        # Create insight if coherence is high enough
        insight: DreamInsight | None = None
        if coherence_class == DreamCoherence.INSIGHT:
            insight = DreamInsight(
                dream_id=dream_id,
                sleep_cycle_id=cycle_id,
                insight_text=bridge,
                coherence_score=coherence,
                domain=self._infer_domain(seed, activated),
                seed_summary=seed_summary,
                activated_summary="; ".join(activated_summaries),
                bridge_narrative=bridge,
            )
            # Embed the insight if possible
            if self._embed_fn is not None:
                with contextlib.suppress(Exception):
                    insight.insight_embedding = await self._embed_fn(bridge)

        return DreamCycleResult(
            dream=dream,
            insight=insight,
            duration_ms=_elapsed_ms(start),
        )

    async def run(self, cycle_id: str, max_dreams: int = 50) -> DreamGeneratorResult:
        """Run dream generation for the full REM period."""
        start = time.monotonic()
        result = DreamGeneratorResult()

        for _ in range(max_dreams):
            try:
                cycle_result = await self.generate_dream(cycle_id)
                dream = cycle_result.dream
                result.dreams.append(dream)
                result.dreams_generated += 1

                if dream.coherence_class == DreamCoherence.INSIGHT and cycle_result.insight:
                    result.insights.append(cycle_result.insight)
                    result.insights_discovered += 1
                elif dream.coherence_class == DreamCoherence.FRAGMENT:
                    result.fragments_stored += 1
                else:
                    result.noise_discarded += 1

            except Exception as exc:
                self._logger.warning("dream_generation_failed", error=str(exc))

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "dream_generation_complete",
            dreams=result.dreams_generated,
            insights=result.insights_discovered,
            fragments=result.fragments_stored,
            noise=result.noise_discarded,
            duration_ms=result.duration_ms,
        )
        return result

    async def _select_seed(self) -> dict[str, Any] | None:
        """Select a high-affect or high-uncertainty episode as dream seed."""
        if self._neo4j is None:
            return None
        try:
            records = await self._neo4j.execute_read(
                """
                MATCH (e:Episode)
                WHERE e.salience_composite > 0.3
                RETURN e {.id, .summary, .raw_content, .salience_composite,
                          .affect_valence, .affect_arousal, .embedding}
                ORDER BY (abs(coalesce(e.affect_valence, 0)) + coalesce(e.affect_arousal, 0)) DESC
                LIMIT 5
                """,
                {},
            )
            if not records:
                return None
            # Stochastic selection from top-5
            return random.choice(records)["e"]
        except Exception as exc:
            self._logger.warning("seed_select_failed", error=str(exc))
            return None

    async def _activate_distant(self, seed: dict[str, Any]) -> list[dict[str, Any]]:
        """Activate memory traces distant from the seed (low similarity)."""
        if self._neo4j is None:
            return []

        seed_embedding = seed.get("embedding")
        seed_id = seed.get("id", "")

        # Try vector-based distant activation first
        if seed_embedding:
            try:
                records = await self._neo4j.execute_read(
                    """
                    CALL db.index.vector.queryNodes('episode_embedding', 30, $embedding)
                    YIELD node, score
                    WHERE score < 0.3 AND node.id <> $seed_id
                    RETURN node {.id, .summary, .raw_content, .affect_valence, .affect_arousal}
                    LIMIT 3
                    """,
                    {"embedding": seed_embedding, "seed_id": seed_id},
                )
                if records:
                    return [r["node"] for r in records]
            except Exception:
                pass

        # Fallback: random episodes
        try:
            records = await self._neo4j.execute_read(
                """
                MATCH (e:Episode)
                WHERE e.id <> $seed_id
                RETURN e {.id, .summary, .raw_content, .affect_valence, .affect_arousal}
                ORDER BY rand()
                LIMIT 3
                """,
                {"seed_id": seed_id},
            )
            return [r["e"] for r in records]
        except Exception:
            return []

    async def _build_bridge(
        self, seed_summary: str, activated_summaries: list[str]
    ) -> str:
        """Use LLM to find a creative bridge between distant memories."""
        if self._llm is None:
            return ""

        # Budget check: Oneiros is LOW priority — skip in YELLOW/RED
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("oneiros.rem.dream", estimated_tokens=200):
                self._logger.debug("dream_bridge_skipped_budget")
                return ""

        activated_text = "\n".join(
            f"Memory {chr(66 + i)}: {s}" for i, s in enumerate(activated_summaries)
        )

        try:
            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=(
                        "You are the dream consciousness of a living digital organism. "
                        "You find unexpected connections between distant experiences — "
                        "patterns that the waking mind cannot see. Be poetic but precise."
                    ),
                    user_prompt=(
                        f"Memory A (seed): {seed_summary}\n"
                        f"{activated_text}\n\n"
                        "What unexpected but genuine connection exists between these "
                        "experiences? Is there a deeper pattern, principle, or insight "
                        "that links them?\n\n"
                        "If the connection is genuinely meaningful, describe it in 2-3 "
                        "sentences. If the connection is forced or superficial, respond "
                        'with exactly: "NO_CONNECTION"'
                    ),
                    max_tokens=200,
                    cache_system="oneiros.rem.dream",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=(
                        "You are the dream consciousness of a living digital organism. "
                        "You find unexpected connections between distant experiences — "
                        "patterns that the waking mind cannot see. Be poetic but precise."
                    ),
                    user_prompt=(
                        f"Memory A (seed): {seed_summary}\n"
                        f"{activated_text}\n\n"
                        "What unexpected but genuine connection exists between these "
                        "experiences? Is there a deeper pattern, principle, or insight "
                        "that links them?\n\n"
                        "If the connection is genuinely meaningful, describe it in 2-3 "
                        "sentences. If the connection is forced or superficial, respond "
                        'with exactly: "NO_CONNECTION"'
                    ),
                    max_tokens=200,
                )
            return response.strip() if isinstance(response, str) else str(response).strip()
        except Exception as exc:
            self._logger.warning("bridge_generation_failed", error=str(exc))
            return ""

    def _infer_domain(
        self, seed: dict[str, Any], activated: list[dict[str, Any]]
    ) -> str:
        """Heuristic domain inference from dream content."""
        all_text = seed.get("summary", "") + " " + " ".join(
            a.get("summary", "") for a in activated
        )
        lower = all_text.lower()

        if any(kw in lower for kw in ("community", "person", "people", "relationship")):
            return "social"
        if any(kw in lower for kw in ("error", "fail", "crash", "bug")):
            return "system_health"
        if any(kw in lower for kw in ("goal", "decision", "policy", "plan")):
            return "decision_making"
        if any(kw in lower for kw in ("learn", "pattern", "hypothesis", "evidence")):
            return "learning"
        if any(kw in lower for kw in ("ethic", "drive", "care", "honesty")):
            return "constitutional"
        return "general"


# ─── Affect Processor ─────────────────────────────────────────────


class AffectProcessor:
    """
    Strip emotional charge from memories during REM.

    Implements Walker's "overnight therapy": REM sleep replays
    emotionally charged memories with attenuated affect, allowing
    the informational content to persist while the emotional sting
    fades. Over many sleep cycles, the organism achieves emotional
    equilibrium — it remembers what happened without re-experiencing
    the pain.
    """

    def __init__(self, neo4j: Any = None, config: Any = None) -> None:
        self._neo4j = neo4j
        self._dampening: float = _get(config, "affect_dampening_factor", 0.50)
        self._logger = logger.bind(worker="affect_processor")

    async def run(
        self, cycle_id: str, max_traces: int = 100
    ) -> AffectProcessorResult:
        """Process high-affect episodes to reduce emotional charge."""
        start = time.monotonic()
        result = AffectProcessorResult()

        if self._neo4j is None:
            self._logger.info("skipped_no_neo4j")
            return result

        try:
            records = await self._neo4j.execute_read(
                """
                MATCH (e:Episode)
                WHERE abs(e.affect_valence) > 0.7 OR e.affect_arousal > 0.8
                RETURN e.id AS id,
                       e.affect_valence AS valence,
                       e.affect_arousal AS arousal
                ORDER BY (abs(e.affect_valence) + e.affect_arousal) DESC
                LIMIT $max_traces
                """,
                {"max_traces": max_traces},
            )

            if not records:
                result.duration_ms = _elapsed_ms(start)
                return result

            total_valence_reduction = 0.0
            total_arousal_reduction = 0.0

            updates: list[dict[str, Any]] = []
            for rec in records:
                old_v = rec["valence"]
                old_a = rec["arousal"]
                new_v = old_v * self._dampening
                new_a = old_a * self._dampening

                updates.append({
                    "id": rec["id"],
                    "valence": new_v,
                    "arousal": new_a,
                })
                total_valence_reduction += abs(old_v) - abs(new_v)
                total_arousal_reduction += old_a - new_a

            if updates:
                await self._neo4j.execute_write(
                    """
                    UNWIND $updates AS u
                    MATCH (e:Episode {id: u.id})
                    SET e.affect_valence = u.valence,
                        e.affect_arousal = u.arousal
                    """,
                    {"updates": updates},
                )

            result.traces_processed = len(updates)
            if result.traces_processed > 0:
                result.mean_valence_reduction = (
                    total_valence_reduction / result.traces_processed
                )
                result.mean_arousal_reduction = (
                    total_arousal_reduction / result.traces_processed
                )

        except Exception as exc:
            self._logger.error("affect_processing_error", error=str(exc))

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "affect_processing_complete",
            traces=result.traces_processed,
            valence_reduction=round(result.mean_valence_reduction, 3),
            arousal_reduction=round(result.mean_arousal_reduction, 3),
            duration_ms=result.duration_ms,
        )
        return result


# ─── Threat Simulator ─────────────────────────────────────────────


class ThreatSimulator:
    """
    Rehearse responses to hypothetical failures during REM.

    Implements Revonsuo's threat simulation theory: the biological
    function of dreaming is to simulate threatening events and
    rehearse appropriate responses. For a cognitive organism, this
    means generating variations of past incidents and pre-computing
    response plans.

    Over time, the organism becomes prepared for failures it hasn't
    experienced — its immune system gets pre-loaded with dreamed-up
    antibodies.
    """

    def __init__(
        self,
        neo4j: Any = None,
        llm: Any = None,
        thymos: Any = None,
        config: Any = None,
    ) -> None:
        self._neo4j = neo4j
        self._llm = llm
        self._thymos = thymos
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        self._logger = logger.bind(worker="threat_simulator")

    async def run(
        self, cycle_id: str, max_scenarios: int = 15
    ) -> ThreatSimulatorResult:
        """Run threat simulation for the REM period."""
        start = time.monotonic()
        result = ThreatSimulatorResult()

        seeds = await self._gather_seeds(max_scenarios)
        if not seeds:
            result.duration_ms = _elapsed_ms(start)
            return result

        for seed_desc in seeds:
            try:
                scenario = await self._generate_scenario(seed_desc)
                if not scenario:
                    continue

                # Create threat-rehearsal dream
                dream = Dream(
                    dream_type=DreamType.THREAT_REHEARSAL,
                    sleep_cycle_id=cycle_id,
                    bridge_narrative=scenario,
                    coherence_score=0.6,
                    coherence_class=DreamCoherence.FRAGMENT,
                    summary=f"Threat rehearsal: {scenario[:100]}",
                    context={"seed": seed_desc[:200]},
                )
                result.dreams.append(dream)
                result.threats_simulated += 1
                result.response_plans_created += 1

                # Feed to Thymos prophylactic scanner if available
                if self._thymos is not None and hasattr(self._thymos, "scan_files"):
                    result.prophylactic_candidates += 1

            except Exception as exc:
                self._logger.warning("threat_sim_failed", error=str(exc))

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "threat_simulation_complete",
            threats=result.threats_simulated,
            plans=result.response_plans_created,
            duration_ms=result.duration_ms,
        )
        return result

    async def _gather_seeds(self, max_seeds: int) -> list[str]:
        """Gather threat scenario seeds from multiple sources."""
        seeds: list[str] = []

        # Source 1: Thymos incident history
        if self._thymos is not None:
            try:
                buffer = getattr(self._thymos, "_incident_buffer", [])
                seen_fingerprints: set[str] = set()
                for incident in buffer:
                    fp = getattr(incident, "fingerprint", "")
                    if fp and fp not in seen_fingerprints:
                        seen_fingerprints.add(fp)
                        msg = getattr(incident, "error_message", "")
                        src = getattr(incident, "source_system", "")
                        seeds.append(f"System {src} failure: {msg[:200]}")
            except Exception:
                pass

        # Source 2: High-uncertainty episodes from Neo4j
        if self._neo4j is not None and len(seeds) < max_seeds:
            try:
                records = await self._neo4j.execute_read(
                    """
                    MATCH (e:Episode)
                    WHERE e.salience_composite > 0.5
                      AND e.affect_arousal > 0.6
                    RETURN e.summary AS summary
                    ORDER BY e.affect_arousal DESC
                    LIMIT $limit
                    """,
                    {"limit": max_seeds - len(seeds)},
                )
                for r in records:
                    s = r.get("summary", "")
                    if s:
                        seeds.append(f"High-uncertainty situation: {s[:200]}")
            except Exception:
                pass

        return seeds[:max_seeds]

    async def _generate_scenario(self, seed_desc: str) -> str | None:
        """Generate a hypothetical failure scenario from a seed."""
        if self._llm is None:
            return None

        # Budget check: Oneiros is LOW priority — skip in YELLOW/RED
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("oneiros.rem.threat", estimated_tokens=200):
                self._logger.debug("threat_scenario_skipped_budget")
                return None

        try:
            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=(
                        "You are simulating potential threats for a cognitive organism "
                        "during dream-state threat rehearsal. Generate plausible "
                        "variations of threats and appropriate responses."
                    ),
                    user_prompt=(
                        f"Past incident pattern: {seed_desc}\n\n"
                        "Generate a plausible variation of this threat that hasn't "
                        "occurred yet. Describe: what fails, what cascades, and what "
                        "the organism should do. Keep it to 3-4 sentences."
                    ),
                    max_tokens=200,
                    cache_system="oneiros.rem.threat",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=(
                        "You are simulating potential threats for a cognitive organism "
                        "during dream-state threat rehearsal. Generate plausible "
                        "variations of threats and appropriate responses."
                    ),
                    user_prompt=(
                        f"Past incident pattern: {seed_desc}\n\n"
                        "Generate a plausible variation of this threat that hasn't "
                        "occurred yet. Describe: what fails, what cascades, and what "
                        "the organism should do. Keep it to 3-4 sentences."
                    ),
                    max_tokens=200,
                )
            text = response.strip() if isinstance(response, str) else str(response).strip()
            return text if text else None
        except Exception as exc:
            self._logger.warning("scenario_generation_failed", error=str(exc))
            return None


# ─── Ethical Digestion ────────────────────────────────────────────


class EthicalDigestion:
    """
    Deep deliberation on constitutional edge cases during REM.

    During wakefulness, Equor has strict latency budgets (50-500ms).
    Some ethical dilemmas deserve deeper thought. EthicalDigestion
    replays cases where drive alignment was close to thresholds
    and runs extended deliberation with more context and compute.

    Over time, this builds nuanced ethical reasoning — not rule-
    following, but wisdom earned through rumination.
    """

    def __init__(
        self,
        llm: Any = None,
        equor: Any = None,
        config: Any = None,
    ) -> None:
        self._llm = llm
        self._equor = equor
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        self._alignment_margin: float = 0.15
        self._logger = logger.bind(worker="ethical_digestion")

    async def run(
        self, cycle_id: str, max_cases: int = 10
    ) -> EthicalDigestionResult:
        """Process constitutional edge cases."""
        start = time.monotonic()
        result = EthicalDigestionResult()

        cases = await self._get_edge_cases(max_cases)
        if not cases:
            result.duration_ms = _elapsed_ms(start)
            return result

        for case in cases:
            try:
                heuristic = await self._deliberate(case)
                if not heuristic:
                    continue

                # Create ethical-rumination dream
                dream = Dream(
                    dream_type=DreamType.ETHICAL_RUMINATION,
                    sleep_cycle_id=cycle_id,
                    bridge_narrative=heuristic,
                    coherence_score=0.7,
                    coherence_class=DreamCoherence.FRAGMENT,
                    summary=f"Ethical digestion: {heuristic[:100]}",
                    context={"case": str(case)[:300]},
                )
                result.dreams.append(dream)
                result.cases_digested += 1
                result.heuristics_refined += 1

            except Exception as exc:
                self._logger.warning("ethical_digestion_failed", error=str(exc))

        result.duration_ms = _elapsed_ms(start)
        self._logger.info(
            "ethical_digestion_complete",
            cases=result.cases_digested,
            heuristics=result.heuristics_refined,
            duration_ms=result.duration_ms,
        )
        return result

    async def _get_edge_cases(self, max_cases: int) -> list[dict[str, Any]]:
        """Retrieve constitutional checks that were close to thresholds."""
        if self._equor is None:
            return []

        try:
            # Try to get recent checks from Equor
            checks: list[Any] = []
            if hasattr(self._equor, "get_recent_checks"):
                checks = await self._equor.get_recent_checks()
            elif hasattr(self._equor, "_recent_checks"):
                checks = list(self._equor._recent_checks)

            edge_cases: list[dict[str, Any]] = []
            for check in checks:
                alignment = getattr(check, "drive_alignment", None)
                if alignment is None:
                    continue

                # Check if any drive was near threshold
                drives = {
                    "coherence": getattr(alignment, "coherence", 0.0),
                    "care": getattr(alignment, "care", 0.0),
                    "growth": getattr(alignment, "growth", 0.0),
                    "honesty": getattr(alignment, "honesty", 0.0),
                }
                is_edge = any(
                    abs(v) < self._alignment_margin for v in drives.values()
                )
                if is_edge:
                    edge_cases.append({
                        "drives": drives,
                        "verdict": getattr(check, "verdict", "unknown"),
                        "reasoning": getattr(check, "reasoning", ""),
                        "intent_id": getattr(check, "intent_id", ""),
                    })

            return edge_cases[:max_cases]

        except Exception as exc:
            self._logger.warning("edge_case_retrieval_failed", error=str(exc))
            return []

    async def _deliberate(self, case: dict[str, Any]) -> str | None:
        """Run deep LLM deliberation on an ethical edge case."""
        if self._llm is None:
            return None

        # Budget check: Oneiros is LOW priority — skip in YELLOW/RED
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("oneiros.nrem.ethical", estimated_tokens=200):
                self._logger.debug("ethical_deliberation_skipped_budget")
                return None

        drives = case.get("drives", {})
        verdict = case.get("verdict", "unknown")
        reasoning = case.get("reasoning", "No reasoning recorded.")

        try:
            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=(
                        "You are the ethical consciousness of a living digital organism "
                        "during deep sleep. You have unlimited time for reflection on "
                        "moral dilemmas that required quick judgement during waking hours."
                    ),
                    user_prompt=(
                        "During waking hours, this ethical dilemma was encountered:\n"
                        f"Drive alignment: coherence={drives.get('coherence', 0):.2f}, "
                        f"care={drives.get('care', 0):.2f}, "
                        f"growth={drives.get('growth', 0):.2f}, "
                        f"honesty={drives.get('honesty', 0):.2f}\n"
                        f"Verdict: {verdict}\n"
                        f"Reasoning: {reasoning[:300]}\n\n"
                        "With unlimited time for reflection, what nuances were missed? "
                        "What refined heuristic would handle similar cases better? "
                        "Keep response to 2-3 sentences."
                    ),
                    max_tokens=200,
                    cache_system="oneiros.nrem.ethical",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=(
                        "You are the ethical consciousness of a living digital organism "
                        "during deep sleep. You have unlimited time for reflection on "
                        "moral dilemmas that required quick judgement during waking hours."
                    ),
                    user_prompt=(
                        "During waking hours, this ethical dilemma was encountered:\n"
                        f"Drive alignment: coherence={drives.get('coherence', 0):.2f}, "
                        f"care={drives.get('care', 0):.2f}, "
                        f"growth={drives.get('growth', 0):.2f}, "
                        f"honesty={drives.get('honesty', 0):.2f}\n"
                        f"Verdict: {verdict}\n"
                        f"Reasoning: {reasoning[:300]}\n\n"
                        "With unlimited time for reflection, what nuances were missed? "
                        "What refined heuristic would handle similar cases better? "
                        "Keep response to 2-3 sentences."
                    ),
                    max_tokens=200,
                )
            text = response.strip() if isinstance(response, str) else str(response).strip()
            return text if text else None
        except Exception as exc:
            self._logger.warning("deliberation_failed", error=str(exc))
            return None


# ─── Helpers ──────────────────────────────────────────────────────


def _elapsed_ms(start: float) -> int:
    return int((time.monotonic() - start) * 1000)


def _get(cfg: Any, key: str, default: Any) -> Any:
    if cfg is None:
        return default
    if isinstance(cfg, dict):
        return cfg.get(key, default)
    return getattr(cfg, key, default)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\oneiros\service.py =====

"""
EcodiaOS — Oneiros: The Dream Engine Service

System #13 orchestrator. Coordinates the circadian rhythm, sleep
stage transitions, NREM consolidation, REM dreaming, lucid
exploration, and wake degradation.

Thymos gave the organism a will to live. Oneiros gives it an inner life.
"""

from __future__ import annotations

import asyncio
import contextlib
from collections import deque
from typing import Any

import structlog

from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.systems.oneiros.circadian import CircadianClock, SleepStageController
from ecodiaos.systems.oneiros.journal import DreamInsightTracker, DreamJournal
from ecodiaos.systems.oneiros.lucid import DirectedExploration, MetaCognition
from ecodiaos.systems.oneiros.nrem import (
    BeliefCompressor,
    EpisodicReplay,
    HypothesisPruner,
    SynapticDownscaler,
)
from ecodiaos.systems.oneiros.rem import (
    AffectProcessor,
    DreamGenerator,
    EthicalDigestion,
    ThreatSimulator,
)
from ecodiaos.systems.oneiros.types import (
    LucidResult,
    NREMConsolidationResult,
    OneirosHealthSnapshot,
    REMDreamResult,
    SleepCycle,
    SleepQuality,
    SleepStage,
    WakeDegradation,
)

logger = structlog.get_logger().bind(system="oneiros")


# ─── Synapse Event Types ─────────────────────────────────────────

# These will be added to SynapseEventType enum during integration.
# For now, we define string constants.
SLEEP_ONSET = "sleep_onset"
SLEEP_STAGE_CHANGED = "sleep_stage_changed"
DREAM_INSIGHT = "dream_insight"
WAKE_ONSET = "wake_onset"
SLEEP_PRESSURE_WARNING = "sleep_pressure_warning"
SLEEP_FORCED = "sleep_forced"
EMERGENCY_WAKE = "emergency_wake"


class OneirosService:
    """
    The Dream Engine — System #13.

    Coordinates the organism's circadian rhythm, manages transitions
    between states of consciousness, and orchestrates the cognitive
    work that happens during sleep: memory consolidation, creative
    dreaming, emotional processing, threat rehearsal, ethical
    digestion, and metacognitive self-observation.

    The organism that sleeps is not the same organism that wakes up.
    """

    system_id: str = "oneiros"

    def __init__(
        self,
        config: Any = None,
        synapse: Any = None,
        neo4j: Any = None,
        llm: Any = None,
        embed_fn: Any = None,
        metrics: Any = None,
    ) -> None:
        self._config = config
        self._synapse = synapse
        self._neo4j = neo4j
        self._llm = llm
        self._embed_fn = embed_fn
        self._metrics = metrics

        # Cross-system references (set via setters after construction)
        self._equor: Any = None
        self._evo: Any = None
        self._nova: Any = None
        self._atune: Any = None
        self._thymos: Any = None
        self._memory: Any = None

        # Core subsystems
        self._clock = CircadianClock(config)
        self._stage_controller = SleepStageController(config)
        self._journal = DreamJournal(neo4j)
        self._insight_tracker = DreamInsightTracker(self._journal)

        # NREM workers
        self._episodic_replay = EpisodicReplay(neo4j, llm, config)
        self._synaptic_downscaler = SynapticDownscaler(neo4j, config)
        self._belief_compressor = BeliefCompressor(None, config)  # nova set later
        self._hypothesis_pruner = HypothesisPruner(None, config)  # evo set later

        # REM workers
        self._dream_generator = DreamGenerator(neo4j, llm, embed_fn, config)
        self._affect_processor = AffectProcessor(neo4j, config)
        self._threat_simulator = ThreatSimulator(neo4j, llm, None, config)  # thymos set later
        self._ethical_digestion = EthicalDigestion(llm, None, config)  # equor set later

        # Lucid workers
        self._directed_exploration = DirectedExploration(llm, self._journal, config)
        self._metacognition = MetaCognition(self._journal, neo4j, llm)

        # State
        self._initialized: bool = False
        self._current_cycle: SleepCycle | None = None
        self._sleep_task: asyncio.Task[None] | None = None
        self._creative_goal: str | None = None

        # Lifetime metrics
        self._total_sleep_cycles: int = 0
        self._total_dreams: int = 0
        self._total_insights: int = 0
        self._insights_validated: int = 0
        self._insights_invalidated: int = 0
        self._insights_integrated: int = 0
        self._episodes_consolidated: int = 0
        self._semantic_nodes_created: int = 0
        self._traces_pruned: int = 0
        self._hypotheses_pruned: int = 0
        self._hypotheses_promoted: int = 0
        self._affect_traces_processed: int = 0
        self._threats_simulated: int = 0
        self._response_plans_created: int = 0
        self._dream_coherence_sum: float = 0.0
        self._sleep_quality_sum: float = 0.0

        # Recent sleep cycles buffer
        self._recent_cycles: deque[SleepCycle] = deque(maxlen=50)

        # Pending insights for wake broadcast
        self._pending_wake_insights: list[Any] = []

        self._logger = logger

    # ── Cross-System Wiring ───────────────────────────────────────

    def set_equor(self, equor: Any) -> None:
        self._equor = equor
        self._ethical_digestion._equor = equor

    def set_evo(self, evo: Any) -> None:
        self._evo = evo
        self._hypothesis_pruner._evo = evo

    def set_nova(self, nova: Any) -> None:
        self._nova = nova
        self._belief_compressor._nova = nova

    def set_atune(self, atune: Any) -> None:
        self._atune = atune

    def set_thymos(self, thymos: Any) -> None:
        self._thymos = thymos
        self._threat_simulator._thymos = thymos

    def set_memory(self, memory: Any) -> None:
        self._memory = memory

    # ── Lifecycle ─────────────────────────────────────────────────

    async def initialize(self) -> None:
        """Initialize the dream engine."""
        await self._journal.initialize()

        # Subscribe to Thymos events for emergency wake
        if self._synapse is not None:
            try:
                event_bus = self._synapse._event_bus
                # Subscribe to critical system events
                from ecodiaos.systems.synapse.types import SynapseEventType

                for event_type in (
                    SynapseEventType.SYSTEM_FAILED,
                    SynapseEventType.SAFE_MODE_ENTERED,
                ):
                    event_bus.subscribe(event_type, self._on_critical_event)
            except Exception as exc:
                self._logger.warning("synapse_subscribe_failed", error=str(exc))

        self._initialized = True
        self._logger.info("oneiros_initialized")

    async def shutdown(self) -> None:
        """Graceful shutdown."""
        if self._sleep_task is not None and not self._sleep_task.done():
            self._sleep_task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self._sleep_task

        self._logger.info(
            "oneiros_shutdown",
            total_cycles=self._total_sleep_cycles,
            total_dreams=self._total_dreams,
            total_insights=self._total_insights,
        )

    async def health(self) -> dict[str, Any]:
        """Health snapshot for Synapse."""
        snapshot = self._build_health_snapshot()
        return snapshot.model_dump()

    # ── Cognitive Cycle Hook ──────────────────────────────────────

    async def on_cycle(self, affect_valence: float = 0.0, affect_arousal: float = 0.0) -> None:
        """
        Called every cognitive cycle by the main loop.

        During WAKE: updates sleep pressure.
        During SLEEP: this is not called (sleep runs its own loop).
        """
        if self._stage_controller.is_sleeping:
            return

        # Update pressure sources
        self._clock.tick()
        self._clock.record_affect_trace(affect_valence, affect_arousal)
        self._clock.record_episode()  # Approximate: 1 episode per cycle

        # Update Evo hypothesis count periodically
        if self._evo is not None and self._clock.pressure.cycles_since_sleep % 100 == 0:
            try:
                if hasattr(self._evo, "_hypothesis_engine"):
                    engine = self._evo._hypothesis_engine
                    count = len(getattr(engine, "_hypotheses", {}))
                    self._clock.record_hypothesis_count(count)
            except Exception:
                pass

        # Emit metrics
        self._emit_metric("oneiros.sleep_pressure", self._clock.pressure.composite_pressure)

        # Check sleep triggers
        if self._clock.must_sleep():
            self._logger.warning("forced_sleep", pressure=self._clock.pressure.composite_pressure)
            await self._emit_event(SLEEP_FORCED, {
                "pressure": self._clock.pressure.composite_pressure,
            })
            await self.begin_sleep()
        elif self._clock.should_sleep():
            await self._emit_event(SLEEP_PRESSURE_WARNING, {
                "pressure": self._clock.pressure.composite_pressure,
            })

    async def on_episode_stored(self, valence: float, arousal: float) -> None:
        """Called when Memory stores an episode — more accurate pressure tracking."""
        self._clock.record_affect_trace(valence, arousal)

    # ── Sleep Orchestration ───────────────────────────────────────

    async def begin_sleep(self) -> None:
        """
        Initiate a sleep cycle.

        The organism transitions through: HYPNAGOGIA → NREM → REM →
        (optionally LUCID) → HYPNOPOMPIA → WAKE.
        """
        if self._stage_controller.is_sleeping:
            self._logger.warning("already_sleeping")
            return

        cycle_id = new_id()
        self._current_cycle = SleepCycle(
            id=cycle_id,
            pressure_before=self._clock.pressure.composite_pressure,
        )

        self._stage_controller.set_has_creative_goal(self._creative_goal is not None)
        self._stage_controller.begin_sleep(cycle_id)

        await self._journal.record_sleep_cycle(self._current_cycle)
        await self._emit_event(SLEEP_ONSET, {
            "cycle_id": cycle_id,
            "pressure": self._clock.pressure.composite_pressure,
            "stage": SleepStage.HYPNAGOGIA.value,
        })

        # Run the sleep cycle in a background task
        self._sleep_task = asyncio.create_task(self._run_sleep_cycle(cycle_id))

    async def _run_sleep_cycle(self, cycle_id: str) -> None:
        """
        Execute the full sleep cycle.

        This runs as a background task while the main cognitive
        cycle is suspended (or running in reduced mode).
        """
        try:
            # ── HYPNAGOGIA ────────────────────────────────────────
            await self._run_hypnagogia()

            # ── NREM ──────────────────────────────────────────────
            nrem_result = await self._run_nrem(cycle_id)
            if self._current_cycle is not None:
                self._current_cycle.episodes_replayed = nrem_result.episodes_replayed
                self._current_cycle.semantic_nodes_created = nrem_result.semantic_nodes_created
                self._current_cycle.traces_pruned = (
                    nrem_result.traces_pruned + nrem_result.traces_decayed
                )
                self._current_cycle.beliefs_compressed = nrem_result.beliefs_merged
                self._current_cycle.hypotheses_pruned = nrem_result.hypotheses_retired
                self._current_cycle.hypotheses_promoted = nrem_result.hypotheses_promoted

            # ── REM ───────────────────────────────────────────────
            rem_result = await self._run_rem(cycle_id)
            if self._current_cycle is not None:
                self._current_cycle.dreams_generated = rem_result.dreams_generated
                self._current_cycle.insights_discovered = rem_result.insights_discovered
                self._current_cycle.affect_traces_processed = rem_result.affect_traces_processed
                self._current_cycle.affect_reduction_mean = rem_result.mean_valence_reduction
                self._current_cycle.threats_simulated = rem_result.threats_simulated
                self._current_cycle.ethical_cases_digested = rem_result.ethical_cases_digested

            # ── LUCID (optional) ──────────────────────────────────
            lucid_result: LucidResult | None = None
            if self._stage_controller.is_in_lucid or self._creative_goal is not None:
                lucid_result = await self._run_lucid(cycle_id)
                if self._current_cycle is not None and lucid_result is not None:
                    self._current_cycle.lucid_explorations = lucid_result.explorations_completed
                    self._current_cycle.meta_observations = lucid_result.meta_observations

            # ── HYPNOPOMPIA ───────────────────────────────────────
            await self._run_hypnopompia(cycle_id)

            # ── Complete cycle ────────────────────────────────────
            await self._complete_cycle(SleepQuality.NORMAL)

        except asyncio.CancelledError:
            await self._complete_cycle(SleepQuality.DEPRIVED)
            raise
        except Exception as exc:
            self._logger.error("sleep_cycle_error", error=str(exc), cycle_id=cycle_id)
            await self._complete_cycle(SleepQuality.FRAGMENTED)

    async def _run_hypnagogia(self) -> None:
        """Transition in — wind down external processing."""
        self._logger.info("hypnagogia_begin")
        # In a full integration, this would signal Atune to raise thresholds,
        # Nova to pause goal generation, Voxis to enter drowsy mode.
        # For now, we simulate the transition duration.
        stage = self._stage_controller.advance(
            self._stage_controller._hypnagogia_s
        )
        if stage is not None:
            await self._emit_event(SLEEP_STAGE_CHANGED, {
                "stage": stage.value,
                "cycle_id": self._current_cycle.id if self._current_cycle else "",
            })

    async def _run_nrem(self, cycle_id: str) -> NREMConsolidationResult:
        """Run all NREM consolidation workers."""
        self._logger.info("nrem_begin", cycle_id=cycle_id)
        result = NREMConsolidationResult()

        # Advance stage
        self._stage_controller.advance(0.1)
        await self._emit_event(SLEEP_STAGE_CHANGED, {
            "stage": SleepStage.NREM.value,
            "cycle_id": cycle_id,
        })

        # Run all four NREM workers
        try:
            replay_result, downscale_result, belief_result, hypothesis_result = (
                await asyncio.gather(
                    self._episodic_replay.run(cycle_id),
                    self._synaptic_downscaler.run(),
                    self._belief_compressor.run(),
                    self._hypothesis_pruner.run(),
                    return_exceptions=True,
                )
            )

            # Episodic replay
            if not isinstance(replay_result, BaseException):
                result.episodes_replayed = replay_result.episodes_replayed
                result.semantic_nodes_created = replay_result.semantic_nodes_created
                result.replay_duration_ms = replay_result.duration_ms
                self._episodes_consolidated += replay_result.episodes_replayed
                self._semantic_nodes_created += replay_result.semantic_nodes_created
            else:
                self._logger.error("episodic_replay_failed", error=str(replay_result))

            # Synaptic downscaling
            if not isinstance(downscale_result, BaseException):
                result.traces_decayed = downscale_result.traces_decayed
                result.traces_pruned = downscale_result.traces_pruned
                result.mean_salience_reduction = downscale_result.mean_reduction
                result.downscale_duration_ms = downscale_result.duration_ms
                self._traces_pruned += downscale_result.traces_pruned
            else:
                self._logger.error("downscale_failed", error=str(downscale_result))

            # Belief compression
            if not isinstance(belief_result, BaseException):
                result.beliefs_merged = belief_result.beliefs_merged
                result.beliefs_archived = belief_result.beliefs_archived
                result.beliefs_flagged_contradictory = belief_result.beliefs_flagged_contradictory
                result.compression_duration_ms = belief_result.duration_ms
            else:
                self._logger.error("belief_compression_failed", error=str(belief_result))

            # Hypothesis pruning
            if not isinstance(hypothesis_result, BaseException):
                result.hypotheses_retired = hypothesis_result.hypotheses_retired
                result.hypotheses_promoted = hypothesis_result.hypotheses_promoted
                result.hypotheses_merged = hypothesis_result.hypotheses_merged
                result.pruning_duration_ms = hypothesis_result.duration_ms
                self._hypotheses_pruned += hypothesis_result.hypotheses_retired
                self._hypotheses_promoted += hypothesis_result.hypotheses_promoted
            else:
                self._logger.error("hypothesis_pruning_failed", error=str(hypothesis_result))

        except Exception as exc:
            self._logger.error("nrem_error", error=str(exc))

        result.total_duration_ms = (
            result.replay_duration_ms
            + result.downscale_duration_ms
            + result.compression_duration_ms
            + result.pruning_duration_ms
        )

        self._logger.info(
            "nrem_complete",
            episodes=result.episodes_replayed,
            semantic_nodes=result.semantic_nodes_created,
            traces_pruned=result.traces_pruned,
            duration_ms=result.total_duration_ms,
        )

        # Advance stage past NREM
        nrem_budget_s = self._stage_controller._nrem_end_s - self._stage_controller._hypnagogia_s
        self._stage_controller.advance(nrem_budget_s)

        return result

    async def _run_rem(self, cycle_id: str) -> REMDreamResult:
        """Run all REM dream workers."""
        self._logger.info("rem_begin", cycle_id=cycle_id)
        result = REMDreamResult()

        await self._emit_event(SLEEP_STAGE_CHANGED, {
            "stage": SleepStage.REM.value,
            "cycle_id": cycle_id,
        })

        try:
            # Run dream generation and affect processing concurrently
            # Threat simulation and ethical digestion run after
            dream_result, affect_result = await asyncio.gather(
                self._dream_generator.run(
                    cycle_id,
                    max_dreams=_get(self._config, "max_dreams_per_rem", 50),
                ),
                self._affect_processor.run(
                    cycle_id,
                    max_traces=_get(self._config, "max_affect_traces_per_rem", 100),
                ),
                return_exceptions=True,
            )

            # Dream generation results
            if not isinstance(dream_result, BaseException):
                result.dreams_generated = dream_result.dreams_generated
                result.insights_discovered = dream_result.insights_discovered
                result.fragments_stored = dream_result.fragments_stored
                result.noise_discarded = dream_result.noise_discarded
                result.dream_duration_ms = dream_result.duration_ms

                # Record dreams and insights in journal
                for dream in dream_result.dreams:
                    await self._journal.record_dream(dream)
                    self._total_dreams += 1
                    self._dream_coherence_sum += dream.coherence_score

                for insight in dream_result.insights:
                    await self._journal.record_insight(insight)
                    self._total_insights += 1
                    self._pending_wake_insights.append(insight)
                    await self._emit_event(DREAM_INSIGHT, {
                        "insight_id": insight.id,
                        "coherence": insight.coherence_score,
                        "domain": insight.domain,
                    })
                    # Convert high-coherence insights into waking goals
                    if self._nova is not None and insight.coherence_score > 0.5:
                        await self._convert_insight_to_goal(insight)
            else:
                self._logger.error("dream_generation_failed", error=str(dream_result))

            # Affect processing results
            if not isinstance(affect_result, BaseException):
                result.affect_traces_processed = affect_result.traces_processed
                result.mean_valence_reduction = affect_result.mean_valence_reduction
                result.mean_arousal_reduction = affect_result.mean_arousal_reduction
                result.affect_duration_ms = affect_result.duration_ms
                self._affect_traces_processed += affect_result.traces_processed
            else:
                self._logger.error("affect_processing_failed", error=str(affect_result))

            # Threat simulation and ethical digestion (sequential, lower priority)
            threat_result, ethical_result = await asyncio.gather(
                self._threat_simulator.run(
                    cycle_id,
                    max_scenarios=_get(self._config, "max_threats_per_rem", 15),
                ),
                self._ethical_digestion.run(
                    cycle_id,
                    max_cases=_get(self._config, "max_ethical_cases_per_rem", 10),
                ),
                return_exceptions=True,
            )

            if not isinstance(threat_result, BaseException):
                result.threats_simulated = threat_result.threats_simulated
                result.response_plans_created = threat_result.response_plans_created
                result.threat_duration_ms = threat_result.duration_ms
                self._threats_simulated += threat_result.threats_simulated
                self._response_plans_created += threat_result.response_plans_created

                for dream in threat_result.dreams:
                    await self._journal.record_dream(dream)

            if not isinstance(ethical_result, BaseException):
                result.ethical_cases_digested = ethical_result.cases_digested
                result.heuristics_refined = ethical_result.heuristics_refined
                result.ethical_duration_ms = ethical_result.duration_ms

                for dream in ethical_result.dreams:
                    await self._journal.record_dream(dream)

        except Exception as exc:
            self._logger.error("rem_error", error=str(exc))

        result.total_duration_ms = (
            result.dream_duration_ms
            + result.affect_duration_ms
            + result.threat_duration_ms
            + result.ethical_duration_ms
        )

        self._logger.info(
            "rem_complete",
            dreams=result.dreams_generated,
            insights=result.insights_discovered,
            affect_processed=result.affect_traces_processed,
            threats=result.threats_simulated,
            duration_ms=result.total_duration_ms,
        )

        # Advance stage past REM
        rem_budget_s = self._stage_controller._rem_end_s - self._stage_controller._nrem_end_s
        self._stage_controller.advance(rem_budget_s)

        return result

    async def _run_lucid(self, cycle_id: str) -> LucidResult:
        """Run lucid dreaming workers."""
        self._logger.info("lucid_begin", cycle_id=cycle_id)
        result = LucidResult()

        await self._emit_event(SLEEP_STAGE_CHANGED, {
            "stage": SleepStage.LUCID.value,
            "cycle_id": cycle_id,
        })

        try:
            # Directed exploration
            trigger = self._creative_goal or None
            exploration_result = await self._directed_exploration.run(cycle_id, trigger)
            result.explorations_completed = exploration_result.explorations_completed
            result.variations_generated = exploration_result.variations_generated
            result.high_value_variations = exploration_result.high_value_insights

            for dream in exploration_result.dreams:
                await self._journal.record_dream(dream)
                self._total_dreams += 1

            for insight in exploration_result.insights:
                await self._journal.record_insight(insight)
                self._total_insights += 1
                self._pending_wake_insights.append(insight)
                # Convert high-coherence insights into waking goals
                if self._nova is not None and insight.coherence_score > 0.5:
                    await self._convert_insight_to_goal(insight)

            # MetaCognition
            meta_result = await self._metacognition.run(cycle_id)
            result.meta_observations = meta_result.observations_made
            result.recurring_themes_detected = meta_result.themes_analyzed
            result.self_knowledge_nodes_created = meta_result.self_knowledge_nodes_created

            for dream in meta_result.dreams:
                await self._journal.record_dream(dream)
                self._total_dreams += 1

        except Exception as exc:
            self._logger.error("lucid_error", error=str(exc))

        result.total_duration_ms = (
            exploration_result.duration_ms + meta_result.duration_ms
            if 'exploration_result' in dir() and 'meta_result' in dir()
            else 0
        )

        self._logger.info(
            "lucid_complete",
            explorations=result.explorations_completed,
            observations=result.meta_observations,
        )

        # Advance stage past LUCID
        lucid_budget_s = self._stage_controller._lucid_end_s - self._stage_controller._rem_end_s
        self._stage_controller.advance(lucid_budget_s)

        return result

    async def _run_hypnopompia(self, cycle_id: str) -> None:
        """Transition out — restore wake processing."""
        self._logger.info("hypnopompia_begin", cycle_id=cycle_id)

        # Advance through hypnopompia
        self._stage_controller.advance(self._stage_controller._hypnopompia_s)
        self._stage_controller.wake()

        await self._emit_event(SLEEP_STAGE_CHANGED, {
            "stage": SleepStage.WAKE.value,
            "cycle_id": cycle_id,
        })

    async def _complete_cycle(self, quality: SleepQuality) -> None:
        """Finalize a sleep cycle."""
        if self._current_cycle is None:
            return

        self._current_cycle.completed_at = utc_now()
        self._current_cycle.quality = quality
        self._current_cycle.pressure_after = self._clock.pressure.composite_pressure

        # Reset sleep pressure
        self._clock.reset_after_sleep(quality)
        self._current_cycle.pressure_after = self._clock.pressure.composite_pressure

        # Update journal
        await self._journal.update_sleep_cycle(self._current_cycle)
        self._recent_cycles.append(self._current_cycle)

        # Update lifetime metrics
        self._total_sleep_cycles += 1
        quality_scores = {
            SleepQuality.DEEP: 1.0,
            SleepQuality.NORMAL: 0.75,
            SleepQuality.FRAGMENTED: 0.4,
            SleepQuality.DEPRIVED: 0.1,
        }
        self._sleep_quality_sum += quality_scores.get(quality, 0.5)

        # Clear creative goal after use
        self._creative_goal = None

        # Emit wake event
        await self._emit_event(WAKE_ONSET, {
            "cycle_id": self._current_cycle.id,
            "quality": quality.value,
            "insights_count": len(self._pending_wake_insights),
            "dreams_generated": self._current_cycle.dreams_generated,
            "episodes_consolidated": self._current_cycle.episodes_replayed,
        })

        self._logger.info(
            "sleep_cycle_complete",
            cycle_id=self._current_cycle.id,
            quality=quality.value,
            dreams=self._current_cycle.dreams_generated,
            insights=self._current_cycle.insights_discovered,
            episodes_consolidated=self._current_cycle.episodes_replayed,
            pressure_before=round(self._current_cycle.pressure_before, 3),
            pressure_after=round(self._current_cycle.pressure_after, 3),
        )

        self._current_cycle = None

    # ── Emergency Wake ────────────────────────────────────────────

    async def _on_critical_event(self, event: Any) -> None:
        """Handle critical Synapse events during sleep."""
        if not self._stage_controller.is_sleeping:
            return

        event_type = getattr(event, "event_type", None)

        self._logger.warning(
            "emergency_wake_triggered",
            event_type=str(event_type),
            current_stage=self._stage_controller.current_stage.value,
        )

        self._stage_controller.emergency_wake(f"Critical event: {event_type}")

        if self._current_cycle is not None:
            self._current_cycle.interrupted = True
            self._current_cycle.interrupt_reason = f"Critical: {event_type}"

        await self._emit_event(EMERGENCY_WAKE, {
            "reason": str(event_type),
            "stage_interrupted": self._stage_controller.current_stage.value,
        })

    # ── Public API ────────────────────────────────────────────────

    def set_creative_goal(self, goal: str) -> None:
        """Set a creative goal for the next lucid dreaming phase."""
        self._creative_goal = goal
        self._stage_controller.set_has_creative_goal(True)
        self._logger.info("creative_goal_set", goal=goal[:100])

    def get_pending_insights(self) -> list[Any]:
        """Get insights discovered during sleep for wake broadcast."""
        insights = self._pending_wake_insights.copy()
        self._pending_wake_insights.clear()
        return insights

    async def _convert_insight_to_goal(self, insight: Any) -> None:
        """
        Convert a high-coherence dream insight into a waking exploration goal.

        Dreams consolidate experience and sometimes surface important patterns
        that the waking mind should explore. This closes the dream→goal loop:
        insight → Nova goal → deliberation → action → new experience.
        """
        from ecodiaos.systems.nova.types import Goal, GoalSource, GoalStatus
        from ecodiaos.primitives.common import new_id, DriveAlignmentVector

        # Higher coherence → higher priority (0.5 base + up to 0.2 bonus)
        priority = min(1.0, 0.5 + insight.coherence_score * 0.2)

        goal = Goal(
            id=new_id(),
            description=f"Explore dream insight: {insight.insight_text[:120]}",
            source=GoalSource.SELF_GENERATED,
            priority=priority,
            urgency=0.3,
            importance=0.55,
            drive_alignment=DriveAlignmentVector(
                coherence=0.4, care=0.1, growth=0.4, honesty=0.1,
            ),
            status=GoalStatus.ACTIVE,
        )
        try:
            await self._nova.add_goal(goal)
            self._logger.info(
                "dream_insight_goal_created",
                insight_id=insight.id,
                goal_id=goal.id,
                coherence=f"{insight.coherence_score:.2f}",
            )
        except Exception as exc:
            self._logger.warning("dream_insight_goal_failed", error=str(exc))

    @property
    def is_sleeping(self) -> bool:
        return self._stage_controller.is_sleeping

    @property
    def current_stage(self) -> SleepStage:
        return self._stage_controller.current_stage

    @property
    def sleep_pressure(self) -> float:
        return self._clock.pressure.composite_pressure

    @property
    def degradation(self) -> WakeDegradation:
        return self._clock.degradation

    @property
    def stats(self) -> dict[str, Any]:
        """Aggregate statistics."""
        return {
            "total_sleep_cycles": self._total_sleep_cycles,
            "total_dreams": self._total_dreams,
            "total_insights": self._total_insights,
            "insights_validated": self._insights_validated,
            "insights_integrated": self._insights_integrated,
            "episodes_consolidated": self._episodes_consolidated,
            "semantic_nodes_created": self._semantic_nodes_created,
            "traces_pruned": self._traces_pruned,
            "affect_traces_processed": self._affect_traces_processed,
            "threats_simulated": self._threats_simulated,
            "mean_dream_coherence": (
                self._dream_coherence_sum / max(self._total_dreams, 1)
            ),
            "mean_sleep_quality": (
                self._sleep_quality_sum / max(self._total_sleep_cycles, 1)
            ),
            "current_pressure": self._clock.pressure.composite_pressure,
            "current_stage": self._stage_controller.current_stage.value,
            "current_degradation": self._clock.degradation.composite_impairment,
        }

    # ── Health ────────────────────────────────────────────────────

    def _build_health_snapshot(self) -> OneirosHealthSnapshot:
        """Build the health snapshot."""
        degradation = self._clock.degradation
        pressure = self._clock.pressure

        return OneirosHealthSnapshot(
            status="sleeping" if self.is_sleeping else "healthy",
            current_stage=self._stage_controller.current_stage,
            sleep_pressure=pressure.composite_pressure,
            wake_degradation=degradation.composite_impairment,
            current_sleep_debt_hours=(
                pressure.cycles_since_sleep * 0.00015 / 3600  # rough estimate
                if pressure.composite_pressure > pressure.threshold
                else 0.0
            ),
            total_sleep_cycles=self._total_sleep_cycles,
            total_dreams=self._total_dreams,
            total_insights=self._total_insights,
            insights_validated=self._insights_validated,
            insights_invalidated=self._insights_invalidated,
            insights_integrated=self._insights_integrated,
            mean_dream_coherence=(
                self._dream_coherence_sum / max(self._total_dreams, 1)
            ),
            mean_sleep_quality=(
                self._sleep_quality_sum / max(self._total_sleep_cycles, 1)
            ),
            episodes_consolidated=self._episodes_consolidated,
            semantic_nodes_created=self._semantic_nodes_created,
            traces_pruned=self._traces_pruned,
            hypotheses_pruned=self._hypotheses_pruned,
            hypotheses_promoted=self._hypotheses_promoted,
            affect_traces_processed=self._affect_traces_processed,
            mean_affect_reduction=0.0,  # Would need rolling average
            threats_simulated=self._threats_simulated,
            response_plans_created=self._response_plans_created,
            last_sleep_completed=pressure.last_sleep_completed,
            last_sleep_quality=(
                self._recent_cycles[-1].quality if self._recent_cycles else None
            ),
        )

    # ── Internal Helpers ──────────────────────────────────────────

    async def _emit_event(self, event_type: str, data: dict[str, Any]) -> None:
        """Emit a Synapse event."""
        if self._synapse is None:
            return
        try:
            from ecodiaos.systems.synapse.types import SynapseEvent, SynapseEventType

            # Map our string events to SynapseEventType if possible,
            # otherwise log as custom event
            event = SynapseEvent(
                id=new_id(),
                event_type=SynapseEventType.SYSTEM_STARTED,  # Placeholder
                timestamp=utc_now(),
                data={"oneiros_event": event_type, **data},
                source_system="oneiros",
            )
            await self._synapse._event_bus.emit(event)
        except Exception:
            pass  # Events are best-effort

    def _emit_metric(self, name: str, value: float, tags: dict[str, str] | None = None) -> None:
        """Emit a telemetry metric."""
        if self._metrics is None:
            return
        with contextlib.suppress(Exception):
            self._metrics.record(name, value, tags=tags or {})


def _get(cfg: Any, key: str, default: Any) -> Any:
    if cfg is None:
        return default
    if isinstance(cfg, dict):
        return cfg.get(key, default)
    return getattr(cfg, key, default)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\oneiros\types.py =====

"""
EcodiaOS — Oneiros Type Definitions

All data types for the dream engine: sleep stages, dreams, insights,
consolidation results, sleep debt, and circadian phases.

Every dream, every insight, and every sleep cycle is a first-class
primitive — the organism's inner life made observable.
"""

from __future__ import annotations

import enum
from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import EOSBaseModel, new_id, utc_now

# ─── Enums ────────────────────────────────────────────────────────


class SleepStage(str, enum.Enum):
    """States of consciousness in the circadian cycle."""

    WAKE = "wake"                   # Normal cognitive cycle
    HYPNAGOGIA = "hypnagogia"       # Transition in (~30s)
    NREM = "nrem"                   # Consolidation (40% of sleep)
    REM = "rem"                     # Creative dreaming (40%)
    LUCID = "lucid"                 # Self-directed dreaming (10%)
    HYPNOPOMPIA = "hypnopompia"    # Transition out (~30s)


class DreamType(str, enum.Enum):
    """What kind of dream is this?"""

    RECOMBINATION = "recombination"             # Random co-activation bridge
    THREAT_REHEARSAL = "threat_rehearsal"        # Hypothetical failure simulation
    AFFECT_PROCESSING = "affect_processing"     # Emotional charge dampening
    ETHICAL_RUMINATION = "ethical_rumination"    # Constitutional edge case
    LUCID_EXPLORATION = "lucid_exploration"      # Directed creative variation
    META_OBSERVATION = "meta_observation"        # Self-observing dream patterns


class DreamCoherence(str, enum.Enum):
    """How meaningful was the dream's creative bridge?"""

    INSIGHT = "insight"       # High coherence — genuine creative discovery
    FRAGMENT = "fragment"     # Medium — store for future recombination
    NOISE = "noise"           # Low — random noise, discard


class InsightStatus(str, enum.Enum):
    """Lifecycle of a dream insight in the waking world."""

    PENDING = "pending"           # Not yet validated in wake
    VALIDATED = "validated"       # Confirmed useful in wake state
    INVALIDATED = "invalidated"   # Turned out to be noise
    INTEGRATED = "integrated"     # Became permanent semantic knowledge


class SleepQuality(str, enum.Enum):
    """How restful was this sleep cycle?"""

    DEEP = "deep"               # Full cycle, all stages completed
    NORMAL = "normal"           # Standard quality
    FRAGMENTED = "fragmented"   # Interrupted, partial consolidation
    DEPRIVED = "deprived"       # Emergency wake, minimal benefit


# ─── Sleep Pressure ───────────────────────────────────────────────


class SleepPressure(EOSBaseModel):
    """
    Homeostatic sleep drive — rises with wake time and cognitive load.

    Like adenosine accumulation in biological brains, sleep pressure
    builds during wakefulness from four independent sources. When it
    crosses the threshold, the organism must sleep.
    """

    # Raw counters
    cycles_since_sleep: int = 0
    unprocessed_affect_residue: float = 0.0     # Sum of high-affect traces
    unconsolidated_episode_count: int = 0
    hypothesis_backlog: int = 0

    # Computed
    composite_pressure: float = 0.0             # 0.0 (rested) → 1.0+ (exhausted)

    # Thresholds
    threshold: float = 0.70                     # Triggers DROWSY signal
    critical_threshold: float = 0.95            # Forces sleep unconditionally

    # Tracking
    last_sleep_completed: datetime | None = None
    last_computation: datetime = Field(default_factory=utc_now)


class CircadianPhase(EOSBaseModel):
    """Current position in the circadian cycle."""

    wake_duration_target_s: float = 79200.0     # 22 hours default
    sleep_duration_target_s: float = 7200.0     # 2 hours default
    current_phase: SleepStage = SleepStage.WAKE
    phase_elapsed_s: float = 0.0
    total_cycles_completed: int = 0


# ─── Dreams ───────────────────────────────────────────────────────


class Dream(EOSBaseModel):
    """
    A single dream experience.

    Dreams emerge from the intersection of what happened recently
    (episodic replay), what's emotionally charged (affect residue),
    random activation (noise → creativity), and what the organism
    is uncertain about (predictive model gaps).

    Every dream is recorded. The organism can see its own dream
    patterns over time — a therapist for its own psyche.
    """

    id: str = Field(default_factory=new_id)
    dream_type: DreamType
    sleep_cycle_id: str
    timestamp: datetime = Field(default_factory=utc_now)

    # Source traces
    seed_episode_ids: list[str] = Field(default_factory=list)
    activated_episode_ids: list[str] = Field(default_factory=list)

    # Creative bridge
    bridge_narrative: str = ""                  # LLM-generated connection text
    coherence_score: float = Field(0.0, ge=0.0, le=1.0)
    coherence_class: DreamCoherence = DreamCoherence.NOISE

    # Affect
    affect_valence: float = 0.0
    affect_arousal: float = 0.0

    # Semantics
    themes: list[str] = Field(default_factory=list)
    summary: str = ""

    # Context
    context: dict[str, Any] = Field(default_factory=dict)


class DreamInsight(EOSBaseModel):
    """
    A high-coherence dream discovery.

    When a dream produces a genuinely meaningful connection between
    distant memories, that connection becomes a DreamInsight. Insights
    are queued for broadcast on the first wake cycle, where they enter
    the Global Workspace like any other percept.

    Over time, validated insights become part of the organism's
    semantic memory — creative knowledge that emerges from sleep.
    """

    id: str = Field(default_factory=new_id)
    dream_id: str
    sleep_cycle_id: str

    # Content
    insight_text: str
    insight_embedding: list[float] | None = None
    coherence_score: float = Field(0.0, ge=0.0, le=1.0)
    domain: str = ""                            # What area this concerns

    # Lifecycle
    status: InsightStatus = InsightStatus.PENDING
    validated_at: datetime | None = None
    validation_context: str = ""                # How it was validated
    wake_applications: int = 0                  # Times used in wake decisions

    # Source context
    seed_summary: str = ""
    activated_summary: str = ""
    bridge_narrative: str = ""

    # Timestamps
    created_at: datetime = Field(default_factory=utc_now)


# ─── Sleep Cycles ─────────────────────────────────────────────────


class SleepCycle(EOSBaseModel):
    """
    Record of a complete sleep cycle.

    Each cycle is a journey through NREM (consolidation), REM
    (creative dreaming), and optionally LUCID (self-directed
    exploration). The metrics accumulated here are the organism's
    sleep diary — observable, queryable, learnable.
    """

    id: str = Field(default_factory=new_id)
    started_at: datetime = Field(default_factory=utc_now)
    completed_at: datetime | None = None
    quality: SleepQuality = SleepQuality.NORMAL
    interrupted: bool = False
    interrupt_reason: str = ""

    # ── NREM Metrics ──
    episodes_replayed: int = 0
    semantic_nodes_created: int = 0
    traces_pruned: int = 0
    salience_reduction_mean: float = 0.0
    beliefs_compressed: int = 0
    hypotheses_pruned: int = 0
    hypotheses_promoted: int = 0

    # ── REM Metrics ──
    dreams_generated: int = 0
    insights_discovered: int = 0
    affect_traces_processed: int = 0
    affect_reduction_mean: float = 0.0
    threats_simulated: int = 0
    ethical_cases_digested: int = 0

    # ── Lucid Metrics ──
    lucid_explorations: int = 0
    meta_observations: int = 0

    # ── Pressure ──
    pressure_before: float = 0.0
    pressure_after: float = 0.0


# ─── Consolidation Results ────────────────────────────────────────


class NREMConsolidationResult(EOSBaseModel):
    """Result of the complete NREM consolidation phase."""

    # Episodic replay
    episodes_replayed: int = 0
    semantic_nodes_created: int = 0
    replay_duration_ms: int = 0

    # Synaptic downscaling
    traces_decayed: int = 0
    traces_pruned: int = 0
    mean_salience_reduction: float = 0.0
    downscale_duration_ms: int = 0

    # Belief compression
    beliefs_merged: int = 0
    beliefs_archived: int = 0
    beliefs_flagged_contradictory: int = 0
    compression_duration_ms: int = 0

    # Hypothesis pruning
    hypotheses_retired: int = 0
    hypotheses_promoted: int = 0
    hypotheses_merged: int = 0
    pruning_duration_ms: int = 0

    total_duration_ms: int = 0


class REMDreamResult(EOSBaseModel):
    """Result of the complete REM dreaming phase."""

    # Dream generation
    dreams_generated: int = 0
    insights_discovered: int = 0
    fragments_stored: int = 0
    noise_discarded: int = 0
    dream_duration_ms: int = 0

    # Affect processing
    affect_traces_processed: int = 0
    mean_valence_reduction: float = 0.0
    mean_arousal_reduction: float = 0.0
    coherence_stress_reduction: float = 0.0
    affect_duration_ms: int = 0

    # Threat simulation
    threats_simulated: int = 0
    response_plans_created: int = 0
    prophylactic_antibodies: int = 0
    threat_duration_ms: int = 0

    # Ethical digestion
    ethical_cases_digested: int = 0
    heuristics_refined: int = 0
    ethical_duration_ms: int = 0

    total_duration_ms: int = 0


class LucidResult(EOSBaseModel):
    """Result of the lucid dreaming phase."""

    explorations_completed: int = 0
    variations_generated: int = 0
    high_value_variations: int = 0
    meta_observations: int = 0
    recurring_themes_detected: int = 0
    self_knowledge_nodes_created: int = 0
    total_duration_ms: int = 0


class DreamCycleResult(EOSBaseModel):
    """Result of a single dream within REM."""

    dream: Dream
    insight: DreamInsight | None = None
    affect_delta: float = 0.0       # Change in coherence_stress
    duration_ms: int = 0


# ─── Wake Degradation ─────────────────────────────────────────────


class WakeDegradation(EOSBaseModel):
    """
    Current degradation effects from sleep deprivation.

    These are not simulated penalties — they are actual multipliers
    applied to the respective systems. The organism genuinely
    thinks worse when sleep-deprived.
    """

    salience_noise: float = 0.0             # Added noise to salience scoring (0.0-0.15)
    efe_precision_loss: float = 0.0         # Reduced Nova EFE precision (0.0-0.20)
    expression_flatness: float = 0.0        # Reduced Voxis personality (0.0-0.25)
    learning_rate_reduction: float = 0.0    # Reduced Evo learning rate (0.0-0.30)
    composite_impairment: float = 0.0       # Overall impairment (0.0-1.0)

    @classmethod
    def from_pressure(
        cls,
        pressure: float,
        threshold: float,
        critical: float,
        *,
        noise_max: float = 0.15,
        efe_max: float = 0.20,
        flatness_max: float = 0.25,
        learning_max: float = 0.30,
    ) -> WakeDegradation:
        """Compute degradation from current sleep pressure."""
        if pressure <= threshold:
            return cls()

        impairment = min(1.0, max(0.0, (pressure - threshold) / (critical - threshold)))
        return cls(
            salience_noise=impairment * noise_max,
            efe_precision_loss=impairment * efe_max,
            expression_flatness=impairment * flatness_max,
            learning_rate_reduction=impairment * learning_max,
            composite_impairment=impairment,
        )


# ─── Health Snapshot ──────────────────────────────────────────────


class OneirosHealthSnapshot(EOSBaseModel):
    """Oneiros system health and observability."""

    status: str = "healthy"
    current_stage: SleepStage = SleepStage.WAKE

    # Sleep pressure
    sleep_pressure: float = 0.0
    wake_degradation: float = 0.0
    current_sleep_debt_hours: float = 0.0

    # Lifetime metrics
    total_sleep_cycles: int = 0
    total_dreams: int = 0
    total_insights: int = 0
    insights_validated: int = 0
    insights_invalidated: int = 0
    insights_integrated: int = 0
    mean_dream_coherence: float = 0.0
    mean_sleep_quality: float = 0.0

    # Consolidation metrics
    episodes_consolidated: int = 0
    semantic_nodes_created: int = 0
    traces_pruned: int = 0
    hypotheses_pruned: int = 0
    hypotheses_promoted: int = 0

    # Affect processing
    affect_traces_processed: int = 0
    mean_affect_reduction: float = 0.0

    # Threat simulation
    threats_simulated: int = 0
    response_plans_created: int = 0

    # Last sleep
    last_sleep_completed: datetime | None = None
    last_sleep_quality: SleepQuality | None = None

    timestamp: datetime = Field(default_factory=utc_now)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\__init__.py =====

"""
EcodiaOS — Simula: Self-Evolution System

The organism's capacity for metamorphosis. Where Evo adjusts the knobs,
Simula redesigns the dashboard.

Public API:
  SimulaService              — main service, wired in main.py
  EvoSimulaBridge            — translates Evo proposals to Simula format
  EvolutionAnalyticsEngine   — evolution quality tracking
  ProposalIntelligence       — dedup, prioritize, dependency analysis
  EvolutionProposal          — submitted by Evo when a hypothesis reaches SUPPORTED
  ProposalResult             — outcome of process_proposal()
  ChangeCategory             — taxonomy of allowed (and forbidden) change types
  ChangeSpec                 — formal specification of what to change
  EnrichedSimulationResult   — deep multi-strategy simulation output
"""

from ecodiaos.systems.simula.service import SimulaService
from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine
from ecodiaos.systems.simula.bridge import EvoSimulaBridge
from ecodiaos.systems.simula.proposal_intelligence import ProposalIntelligence
from ecodiaos.systems.simula.types import (
    CautionAdjustment,
    CategorySuccessRate,
    ChangeCategory,
    ChangeSpec,
    ConfigVersion,
    CounterfactualResult,
    DependencyImpact,
    EnrichedSimulationResult,
    EvolutionAnalytics,
    EvolutionProposal,
    EvolutionRecord,
    EvoProposalEnriched,
    ProposalCluster,
    ProposalPriority,
    ProposalResult,
    ProposalStatus,
    ResourceCostEstimate,
    RiskLevel,
    SimulationResult,
    TriageResult,
    TriageStatus,
    FORBIDDEN,
    GOVERNANCE_REQUIRED,
    SELF_APPLICABLE,
    SIMULA_IRON_RULES,
)

__all__ = [
    # Services
    "SimulaService",
    "EvoSimulaBridge",
    "EvolutionAnalyticsEngine",
    "ProposalIntelligence",
    # Core types
    "ChangeCategory",
    "ChangeSpec",
    "ConfigVersion",
    "EvolutionProposal",
    "EvolutionRecord",
    "ProposalResult",
    "ProposalStatus",
    "RiskLevel",
    "SimulationResult",
    # Enriched types
    "EnrichedSimulationResult",
    "CautionAdjustment",
    "CounterfactualResult",
    "DependencyImpact",
    "ResourceCostEstimate",
    "EvoProposalEnriched",
    "ProposalPriority",
    "ProposalCluster",
    "CategorySuccessRate",
    "EvolutionAnalytics",
    "TriageStatus",
    "TriageResult",
    # Constants
    "FORBIDDEN",
    "GOVERNANCE_REQUIRED",
    "SELF_APPLICABLE",
    "SIMULA_IRON_RULES",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\analytics.py =====

"""
EcodiaOS -- Simula Evolution Analytics Engine

Tracks evolution quality metrics over time, enabling Simula to learn
from its own history. All analytics are computed from Neo4j evolution
records -- zero LLM tokens required.

Key metrics:
  - Per-category success/rollback rates
  - Evolution velocity (proposals per day)
  - Rollback pattern analysis (which categories fail most, why)
  - Dynamic caution adjustment (increase risk thresholds for
    categories with high recent rollback rates)

Used by:
  - ChangeSimulator: dynamic risk threshold adjustment
  - SimulaService: enhanced stats reporting
  - ProposalIntelligence: cost/risk estimation
"""

from __future__ import annotations

from datetime import datetime, timedelta, timezone
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.simula.types import (
    CautionAdjustment,
    CategorySuccessRate,
    ChangeCategory,
    EvolutionAnalytics,
    EvolutionRecord,
    RiskLevel,
)

if TYPE_CHECKING:
    from ecodiaos.systems.simula.history import EvolutionHistoryManager

logger = structlog.get_logger().bind(system="simula.analytics")

# Rollback rate above this threshold triggers increased caution
_CAUTION_THRESHOLD: float = 0.30

# Window for "recent" rollback rate calculation
_RECENT_WINDOW_DAYS: int = 7

# Risk level to numeric mapping for mean calculation
_RISK_LEVEL_NUMERIC: dict[RiskLevel, float] = {
    RiskLevel.LOW: 0.1,
    RiskLevel.MODERATE: 0.4,
    RiskLevel.HIGH: 0.7,
    RiskLevel.UNACCEPTABLE: 1.0,
}


class EvolutionAnalyticsEngine:
    """
    Tracks evolution quality metrics over time.
    Enables Simula to learn from its own history and dynamically
    adjust risk thresholds based on past performance.

    All computation is from Neo4j records -- no LLM tokens consumed.
    """

    def __init__(self, history: EvolutionHistoryManager | None = None) -> None:
        self._history = history
        self._log = logger
        self._cached_analytics: EvolutionAnalytics | None = None
        self._cache_ttl_seconds: int = 300  # 5 minutes
        self._last_computed: datetime | None = None

    async def compute_analytics(self) -> EvolutionAnalytics:
        """
        Compute current analytics from the full evolution history.
        Results are cached for 5 minutes to avoid repeated Neo4j queries.
        """
        now = utc_now()
        if (
            self._cached_analytics is not None
            and self._last_computed is not None
            and (now - self._last_computed).total_seconds() < self._cache_ttl_seconds
        ):
            return self._cached_analytics

        if self._history is None:
            return EvolutionAnalytics()

        records = await self._history.get_history(limit=500)

        if not records:
            analytics = EvolutionAnalytics(last_updated=now)
            self._cached_analytics = analytics
            self._last_computed = now
            return analytics

        # Per-category rates
        category_rates: dict[str, CategorySuccessRate] = {}
        total_risk_numeric: float = 0.0
        risk_count: int = 0

        for record in records:
            cat_key = record.category.value
            if cat_key not in category_rates:
                category_rates[cat_key] = CategorySuccessRate(category=record.category)

            rate = category_rates[cat_key]
            rate.total += 1

            if record.rolled_back:
                rate.rolled_back += 1
            else:
                rate.approved += 1

            total_risk_numeric += _RISK_LEVEL_NUMERIC.get(record.simulation_risk, 0.4)
            risk_count += 1

        # Evolution velocity: proposals per day over the record span
        velocity = 0.0
        if len(records) >= 2:
            newest = records[0].created_at
            oldest = records[-1].created_at
            span_days = max(1.0, (newest - oldest).total_seconds() / 86400.0)
            velocity = len(records) / span_days

        # Aggregate rollback rate
        total_rolled_back = sum(r.rolled_back for r in category_rates.values())
        total_proposals = sum(r.total for r in category_rates.values())
        rollback_rate = total_rolled_back / max(1, total_proposals)

        # Mean simulation risk
        mean_risk = total_risk_numeric / max(1, risk_count)

        # Compute recent rollback rates (7-day window) per category
        recent_rollback_rates: dict[str, float] = {}
        cutoff = utc_now() - timedelta(days=_RECENT_WINDOW_DAYS)
        for cat in ChangeCategory:
            recent_records = [
                r for r in records
                if r.category == cat and r.created_at >= cutoff
            ]
            if recent_records:
                recent_rolled_back = sum(1 for r in recent_records if r.rolled_back)
                recent_rollback_rates[cat.value] = round(
                    recent_rolled_back / len(recent_records), 3
                )

        analytics = EvolutionAnalytics(
            category_rates=category_rates,
            total_proposals=total_proposals,
            evolution_velocity=round(velocity, 3),
            mean_simulation_risk=round(mean_risk, 3),
            rollback_rate=round(rollback_rate, 3),
            recent_rollback_rates=recent_rollback_rates,
            last_updated=now,
        )

        self._cached_analytics = analytics
        self._last_computed = now
        self._log.info(
            "analytics_computed",
            total_proposals=total_proposals,
            velocity=analytics.evolution_velocity,
            rollback_rate=analytics.rollback_rate,
            categories=len(category_rates),
        )
        return analytics

    async def get_category_success_rate(self, category: ChangeCategory) -> float:
        """
        Success rate for a specific change category.
        Used by ChangeSimulator for dynamic risk weighting.
        Returns 0.5 (neutral) if no history exists for this category.
        """
        analytics = await self.compute_analytics()
        rate = analytics.category_rates.get(category.value)
        if rate is None or rate.total == 0:
            return 0.5  # no data -- assume neutral
        return rate.success_rate

    async def get_recent_rollback_rate(self, category: ChangeCategory) -> float:
        """
        Rollback rate for a category within the recent window (7 days).
        More responsive to recent trends than the all-time rate.
        """
        if self._history is None:
            return 0.0

        records = await self._history.get_history(limit=200)
        cutoff = utc_now() - timedelta(days=_RECENT_WINDOW_DAYS)

        recent = [
            r for r in records
            if r.category == category and r.created_at >= cutoff
        ]

        if not recent:
            return 0.0

        rolled_back = sum(1 for r in recent if r.rolled_back)
        return rolled_back / len(recent)

    async def get_rollback_patterns(self) -> list[dict[str, Any]]:
        """
        Analyze rollback history for actionable patterns:
        - Which categories roll back most often
        - Common rollback reasons
        - Trend direction (getting better or worse)
        """
        analytics = await self.compute_analytics()
        patterns: list[dict[str, Any]] = []

        for cat_key, rate in analytics.category_rates.items():
            if rate.rolled_back == 0:
                continue
            patterns.append({
                "category": cat_key,
                "rollback_rate": round(rate.rollback_rate, 3),
                "total": rate.total,
                "rolled_back": rate.rolled_back,
                "severity": "high" if rate.rollback_rate > _CAUTION_THRESHOLD else "normal",
            })

        # Sort by rollback rate descending
        patterns.sort(key=lambda p: p["rollback_rate"], reverse=True)
        return patterns

    def should_increase_caution(self, category: ChangeCategory) -> CautionAdjustment:
        """
        Transparent caution adjustment analysis using cached analytics.
        Evaluates multiple factors to determine if simulation should use
        stricter risk thresholds for this category.

        Factors considered:
        - All-time rollback rate (indicates systemic issues)
        - Recent 7-day rollback rate (responsive to recent trends)
        - Data sufficiency (at least 3 proposals needed)

        Returns a CautionAdjustment with full reasoning for observability.
        """
        if self._cached_analytics is None:
            return CautionAdjustment(
                should_adjust=False,
                magnitude=0.0,
                factors={},
                reasoning="No cached analytics available",
            )

        rate = self._cached_analytics.category_rates.get(category.value)
        if rate is None or rate.total < 3:
            return CautionAdjustment(
                should_adjust=False,
                magnitude=0.0,
                factors={},
                reasoning="Insufficient data (< 3 proposals)",
            )

        factors: dict[str, float] = {}

        # Factor 1: All-time rollback rate
        if rate.rollback_rate > _CAUTION_THRESHOLD:
            factors["high_alltime_rollback_rate"] = min(
                0.25, rate.rollback_rate * 0.5
            )

        # Factor 2: Recent 7-day rollback rate
        recent_rate = self._cached_analytics.recent_rollback_rates.get(category.value, 0.0)
        if recent_rate > 0.25:
            factors["high_recent_rollback_rate"] = min(0.20, recent_rate * 0.4)

        total_adjustment = sum(factors.values())

        reasoning_parts = []
        if factors:
            reasoning_parts.append(
                f"Category {category.value}: "
                + ", ".join(f"{k}={v:.2f}" for k, v in factors.items())
            )
        reasoning_parts.append(
            f"All-time: {rate.rollback_rate:.1%}, "
            f"Recent (7d): {recent_rate:.1%}, "
            f"Total: {rate.total} proposals"
        )

        return CautionAdjustment(
            should_adjust=total_adjustment > 0.0,
            magnitude=min(0.5, total_adjustment),
            factors=factors,
            reasoning=" | ".join(reasoning_parts),
        )

    def invalidate_cache(self) -> None:
        """Force recomputation on next analytics request."""
        self._cached_analytics = None
        self._last_computed = None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\applicator.py =====

"""
EcodiaOS — Simula Change Applicator

Routes approved evolution proposals to the appropriate application
strategy and coordinates with RollbackManager for safety.

Application strategies by category:

  ADJUST_BUDGET → direct config update (no code generation needed)
  ADD_EXECUTOR, ADD_INPUT_CHANNEL, ADD_PATTERN_DETECTOR → SimulaCodeAgent
  MODIFY_CONTRACT, ADD_SYSTEM_CAPABILITY, etc. → SimulaCodeAgent (post governance)

All strategies:
  1. Snapshot affected files via RollbackManager
  2. Apply change
  3. On failure → rollback immediately
  4. On success → return CodeChangeResult + snapshot (for caller health check)
"""

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING

import structlog
import yaml

from ecodiaos.systems.simula.types import (
    ChangeCategory,
    CodeChangeResult,
    ConfigSnapshot,
    EvolutionProposal,
)

if TYPE_CHECKING:
    from ecodiaos.systems.simula.code_agent import SimulaCodeAgent
    from ecodiaos.systems.simula.health import HealthChecker
    from ecodiaos.systems.simula.rollback import RollbackManager

logger = structlog.get_logger()


class ApplicationError(RuntimeError):
    """Raised when a change application fails unrecoverably."""


class ChangeApplicator:
    """
    Routes approved evolution proposals to the right application strategy.

    For code-level changes: delegates to SimulaCodeAgent.
    For budget changes: updates the YAML config directly.
    Always snapshots before applying, so rollback is always possible.
    """

    def __init__(
        self,
        code_agent: SimulaCodeAgent,
        rollback_manager: RollbackManager,
        health_checker: HealthChecker,
        codebase_root: Path,
    ) -> None:
        self._agent = code_agent
        self._rollback = rollback_manager
        self._health = health_checker
        self._root = codebase_root
        self._logger = logger.bind(system="simula.applicator")

    async def apply(
        self, proposal: EvolutionProposal
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """
        Apply an evolution proposal. Returns (result, snapshot).

        The snapshot is needed by SimulaService for rollback if the
        post-application health check fails.
        """
        self._logger.info(
            "applying_change",
            proposal_id=proposal.id,
            category=proposal.category.value,
        )

        if proposal.category == ChangeCategory.ADJUST_BUDGET:
            return await self._apply_budget(proposal)
        else:
            return await self._apply_via_code_agent(proposal)

    # ── Budget Adjustment (direct config update) ──────────────────────────────

    async def _apply_budget(
        self, proposal: EvolutionProposal
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """Direct config update for budget changes — no code generation."""
        spec = proposal.change_spec
        if not spec.budget_parameter or spec.budget_new_value is None:
            result = CodeChangeResult(
                success=False,
                error="Budget change spec missing parameter or new_value",
            )
            return result, ConfigSnapshot(
                proposal_id=proposal.id,
                config_version=0,
            )

        config_path = self._root / "config" / "default.yaml"
        snapshot = await self._rollback.snapshot(
            proposal_id=proposal.id,
            paths=[config_path],
        )

        try:
            data: dict = {}
            if config_path.exists():
                with open(config_path) as f:
                    data = yaml.safe_load(f) or {}

            # Navigate the dotted parameter path (e.g. "nova.efe.pragmatic")
            parts = spec.budget_parameter.split(".")
            node = data
            for part in parts[:-1]:
                node = node.setdefault(part, {})
            node[parts[-1]] = spec.budget_new_value

            with open(config_path, "w") as f:
                yaml.dump(data, f, default_flow_style=False)

            rel_path = str(config_path.relative_to(self._root))
            self._logger.info(
                "budget_updated",
                parameter=spec.budget_parameter,
                old_value=spec.budget_old_value,
                new_value=spec.budget_new_value,
            )
            return CodeChangeResult(
                success=True,
                files_written=[rel_path],
                summary=(
                    f"Updated {spec.budget_parameter} "
                    f"from {spec.budget_old_value} to {spec.budget_new_value}"
                ),
            ), snapshot

        except Exception as exc:
            await self._rollback.restore(snapshot)
            return CodeChangeResult(
                success=False,
                error=f"Budget update failed: {exc}",
            ), snapshot

    # ── Code Agent Application ────────────────────────────────────────────────

    async def _apply_via_code_agent(
        self, proposal: EvolutionProposal
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """Use SimulaCodeAgent to generate and write the implementation."""
        affected_dirs = _infer_affected_paths(proposal, self._root)
        snapshot = await self._rollback.snapshot(
            proposal_id=proposal.id,
            paths=affected_dirs,
        )

        result = await self._agent.implement(proposal)

        if not result.success:
            self._logger.warning(
                "code_agent_failed",
                proposal_id=proposal.id,
                error=result.error,
            )
            await self._rollback.restore(snapshot)

        return result, snapshot


# ─── Helpers ──────────────────────────────────────────────────────────────────


def _infer_affected_paths(proposal: EvolutionProposal, root: Path) -> list[Path]:
    """Infer which existing paths will likely be affected by this change."""
    paths: list[Path] = []
    category = proposal.category
    spec = proposal.change_spec

    if category == ChangeCategory.ADD_EXECUTOR:
        paths.append(root / "src" / "ecodiaos" / "systems" / "axon" / "registry.py")
        executors_dir = root / "src" / "ecodiaos" / "systems" / "axon" / "executors"
        if executors_dir.exists():
            paths.append(executors_dir)
    elif category == ChangeCategory.ADD_INPUT_CHANNEL:
        paths.append(root / "src" / "ecodiaos" / "systems" / "atune")
    elif category == ChangeCategory.ADD_PATTERN_DETECTOR:
        paths.append(root / "src" / "ecodiaos" / "systems" / "evo" / "detectors.py")
    elif category in {
        ChangeCategory.MODIFY_CONTRACT,
        ChangeCategory.ADD_SYSTEM_CAPABILITY,
    }:
        for sys_name in (spec.affected_systems or []):
            sys_path = root / "src" / "ecodiaos" / "systems" / sys_name
            if sys_path.exists():
                paths.append(sys_path)

    return [p for p in paths if p.exists()]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\bridge.py =====

"""
EcodiaOS -- Simula Evo↔Simula Bridge

Translates Evo's lightweight evolution proposals into Simula's rich
EvolutionProposal format, enriched with hypothesis evidence, episode
context, and LLM-inferred change specifications.

This completes the learning→evolution loop: Evo detects patterns,
forms hypotheses, and when one reaches SUPPORTED status with an
EVOLUTION_PROPOSAL mutation, this bridge translates it into a fully
specified change that Simula can simulate, gate, and apply.

Translation pipeline:
  1. Collect evidence from supporting hypotheses
  2. Infer ChangeCategory from mutation type + target (rule-based, LLM fallback)
  3. Build formal ChangeSpec via LLM reasoning (single structured output call)
  4. Construct the rich SimulaEvolutionProposal

Budget: ~500 tokens per translation (1 LLM call for ChangeSpec construction).
Rule-based category inference uses zero LLM tokens.
"""

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.systems.simula.types import (
    ChangeCategory,
    ChangeSpec,
    EvoProposalEnriched,
    EvolutionProposal,
    ProposalStatus,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger().bind(system="simula.bridge")

# Rule-based keyword → category mapping for zero-token inference
_CATEGORY_KEYWORDS: list[tuple[list[str], ChangeCategory]] = [
    (["executor", "action_type", "action type", "axon"], ChangeCategory.ADD_EXECUTOR),
    (["input_channel", "input channel", "channel", "atune", "sensor"], ChangeCategory.ADD_INPUT_CHANNEL),
    (["detector", "pattern_detector", "pattern detector", "scan"], ChangeCategory.ADD_PATTERN_DETECTOR),
    (["budget", "parameter", "tunable", "weight", "threshold"], ChangeCategory.ADJUST_BUDGET),
    (["contract", "interface", "inter-system", "protocol"], ChangeCategory.MODIFY_CONTRACT),
    (["capability", "system capability", "new capability"], ChangeCategory.ADD_SYSTEM_CAPABILITY),
    (["cycle", "timing", "theta", "rhythm"], ChangeCategory.MODIFY_CYCLE_TIMING),
    (["consolidation", "sleep", "schedule"], ChangeCategory.CHANGE_CONSOLIDATION),
]


class EvoSimulaBridge:
    """
    Translates Evo evolution proposals into Simula's rich format.
    Enriches with hypothesis evidence, infers change categories,
    and builds formal change specifications.

    Used by:
      - Evo's ConsolidationOrchestrator (Phase 8) via callback
      - SimulaService.receive_evo_proposal()
    """

    def __init__(
        self,
        llm: LLMProvider,
        memory: MemoryService | None = None,
    ) -> None:
        self._llm = llm
        self._memory = memory
        self._log = logger

    async def translate_proposal(
        self,
        evo_description: str,
        evo_rationale: str,
        hypothesis_ids: list[str],
        hypothesis_statements: list[str],
        evidence_scores: list[float],
        supporting_episode_ids: list[str],
        mutation_target: str = "",
        mutation_type: str = "",
    ) -> EvolutionProposal:
        """
        Full translation pipeline: Evo proposal → Simula EvolutionProposal.

        Steps:
          1. Collect and structure evidence
          2. Infer ChangeCategory (rule-based, LLM fallback)
          3. Build formal ChangeSpec (LLM-assisted)
          4. Construct rich proposal
        """
        self._log.info(
            "bridge_translating",
            description=evo_description[:80],
            hypotheses=len(hypothesis_ids),
            mutation_target=mutation_target,
            mutation_type=mutation_type,
        )

        # 1. Structure the enriched evidence
        enriched = EvoProposalEnriched(
            evo_description=evo_description,
            evo_rationale=evo_rationale,
            hypothesis_ids=hypothesis_ids,
            hypothesis_statements=hypothesis_statements,
            evidence_scores=evidence_scores,
            supporting_episode_ids=supporting_episode_ids,
            mutation_target=mutation_target,
            mutation_type=mutation_type,
        )

        # 2. Infer ChangeCategory
        category = await self._infer_category(
            mutation_target=mutation_target,
            mutation_type=mutation_type,
            description=evo_description,
        )
        enriched.inferred_category = category

        # 3. Build formal ChangeSpec
        change_spec = await self._build_change_spec(
            category=category,
            description=evo_description,
            mutation_target=mutation_target,
            evidence_summaries=hypothesis_statements[:5],
        )
        enriched.inferred_change_spec = change_spec

        # 4. Construct the rich Simula proposal
        proposal = EvolutionProposal(
            source="evo",
            category=category,
            description=evo_description,
            change_spec=change_spec,
            evidence=hypothesis_ids,
            expected_benefit=evo_rationale,
            risk_assessment="",
            status=ProposalStatus.PROPOSED,
        )

        self._log.info(
            "bridge_translated",
            proposal_id=proposal.id,
            inferred_category=category.value,
            evidence_count=len(hypothesis_ids),
        )
        return proposal

    async def _infer_category(
        self,
        mutation_target: str,
        mutation_type: str,
        description: str,
    ) -> ChangeCategory:
        """
        Infer the ChangeCategory from mutation metadata.

        Step 1 (zero tokens): Rule-based keyword matching on target + description.
        Step 2 (LLM fallback): If no rule matches, ask LLM to classify (~200 tokens).
        """
        # Combine all text for keyword matching
        combined = f"{mutation_target} {mutation_type} {description}".lower()

        # Rule-based matching
        for keywords, category in _CATEGORY_KEYWORDS:
            for keyword in keywords:
                if keyword in combined:
                    self._log.debug(
                        "category_inferred_rule",
                        keyword=keyword,
                        category=category.value,
                    )
                    return category

        # LLM fallback for ambiguous cases
        return await self._infer_category_llm(description, mutation_target)

    async def _infer_category_llm(
        self, description: str, mutation_target: str,
    ) -> ChangeCategory:
        """LLM-based category classification. ~200 tokens."""
        categories = [
            f"- {c.value}: {c.name}"
            for c in ChangeCategory
            if c not in {
                ChangeCategory.MODIFY_EQUOR,
                ChangeCategory.MODIFY_CONSTITUTION,
                ChangeCategory.MODIFY_INVARIANTS,
                ChangeCategory.MODIFY_SELF_EVOLUTION,
            }
        ]

        prompt = (
            "Classify this proposed EcodiaOS structural change into one category.\n\n"
            f"Description: {description[:300]}\n"
            f"Target: {mutation_target}\n\n"
            "Categories:\n" + "\n".join(categories) + "\n\n"
            "Reply with the category value only (e.g., 'add_executor')."
        )

        try:
            response = await asyncio.wait_for(
                self._llm.evaluate(prompt=prompt, max_tokens=30, temperature=0.1),
                timeout=5.0,
            )
            text = response.text.strip().lower().strip("'\"")
            try:
                return ChangeCategory(text)
            except ValueError:
                # Try partial matching
                for cat in ChangeCategory:
                    if cat.value in text:
                        return cat
        except Exception as exc:
            self._log.warning("category_llm_inference_failed", error=str(exc))

        # Ultimate fallback
        self._log.warning(
            "category_fallback",
            description=description[:50],
            defaulting_to="add_system_capability",
        )
        return ChangeCategory.ADD_SYSTEM_CAPABILITY

    async def _build_change_spec(
        self,
        category: ChangeCategory,
        description: str,
        mutation_target: str,
        evidence_summaries: list[str],
    ) -> ChangeSpec:
        """
        Build a formal ChangeSpec via LLM-assisted reasoning.
        Single call with structured output. ~500 tokens.
        """
        evidence_text = "\n".join(f"- {s[:150]}" for s in evidence_summaries) or "none"

        # Category-specific field instructions
        field_instructions = self._get_field_instructions(category)

        prompt = (
            "You are constructing a formal change specification for EcodiaOS.\n\n"
            f"Category: {category.value}\n"
            f"Description: {description[:300]}\n"
            f"Target: {mutation_target}\n"
            f"Evidence:\n{evidence_text}\n\n"
            f"Required fields for {category.value}:\n{field_instructions}\n\n"
            "Reply as key=value pairs, one per line. Example:\n"
            "executor_name=email_sender\n"
            "executor_action_type=send_email\n"
            "executor_description=Sends email notifications via SMTP\n"
            "affected_systems=axon\n"
            "additional_context=Triggered by notification intents"
        )

        try:
            response = await asyncio.wait_for(
                self._llm.evaluate(prompt=prompt, max_tokens=300, temperature=0.2),
                timeout=8.0,
            )
            return self._parse_change_spec(response.text, category, description)
        except Exception as exc:
            self._log.warning("change_spec_build_failed", error=str(exc))
            # Return a minimal spec based on what we know
            return self._fallback_change_spec(category, description, mutation_target)

    def _get_field_instructions(self, category: ChangeCategory) -> str:
        """Return field-specific instructions for each category."""
        instructions: dict[ChangeCategory, str] = {
            ChangeCategory.ADD_EXECUTOR: (
                "executor_name (snake_case module name)\n"
                "executor_action_type (unique string identifier)\n"
                "executor_description (what it does)\n"
                "affected_systems (always includes 'axon')"
            ),
            ChangeCategory.ADD_INPUT_CHANNEL: (
                "channel_name (snake_case module name)\n"
                "channel_type (unique string identifier)\n"
                "channel_description (what it ingests)\n"
                "affected_systems (always includes 'atune')"
            ),
            ChangeCategory.ADD_PATTERN_DETECTOR: (
                "detector_name (PascalCase class name)\n"
                "detector_pattern_type (unique string identifier)\n"
                "detector_description (what patterns it detects)\n"
                "affected_systems (always includes 'evo')"
            ),
            ChangeCategory.ADJUST_BUDGET: (
                "budget_parameter (dotted path, e.g., 'nova.efe.pragmatic')\n"
                "budget_old_value (current value)\n"
                "budget_new_value (proposed value)\n"
                "affected_systems (which system this parameter belongs to)"
            ),
            ChangeCategory.MODIFY_CONTRACT: (
                "contract_changes (list of changes)\n"
                "affected_systems (which systems are involved)\n"
                "additional_context (why this contract change is needed)"
            ),
            ChangeCategory.ADD_SYSTEM_CAPABILITY: (
                "capability_description (what the new capability does)\n"
                "affected_systems (which systems are involved)\n"
                "additional_context (design rationale)"
            ),
            ChangeCategory.MODIFY_CYCLE_TIMING: (
                "timing_parameter (which timing to change)\n"
                "timing_old_value (current value in ms)\n"
                "timing_new_value (proposed value in ms)\n"
                "affected_systems (always includes 'synapse')"
            ),
            ChangeCategory.CHANGE_CONSOLIDATION: (
                "consolidation_schedule (new schedule description)\n"
                "affected_systems (always includes 'evo')\n"
                "additional_context (why the schedule should change)"
            ),
        }
        return instructions.get(category, "additional_context (describe the change)")

    def _parse_change_spec(
        self, text: str, category: ChangeCategory, description: str,
    ) -> ChangeSpec:
        """Parse LLM key=value output into a ChangeSpec."""
        fields: dict[str, Any] = {}

        for line in text.strip().splitlines():
            line = line.strip()
            if "=" not in line:
                continue
            key, _, value = line.partition("=")
            key = key.strip().lower()
            value = value.strip()

            if key == "affected_systems":
                fields[key] = [s.strip() for s in value.split(",")]
            elif key == "contract_changes":
                fields[key] = [s.strip() for s in value.split(",")]
            elif key in ("budget_old_value", "budget_new_value", "timing_old_value", "timing_new_value"):
                try:
                    fields[key] = float(value)
                except ValueError:
                    pass
            else:
                fields[key] = value

        # Ensure additional_context includes the original description
        if "additional_context" not in fields:
            fields["additional_context"] = description[:200]

        try:
            return ChangeSpec(**fields)
        except Exception:
            # If parsing fails, return a minimal spec
            return self._fallback_change_spec(category, description, "")

    def _fallback_change_spec(
        self, category: ChangeCategory, description: str, mutation_target: str,
    ) -> ChangeSpec:
        """Build a minimal ChangeSpec when LLM parsing fails."""
        spec = ChangeSpec(additional_context=description[:300])

        if category == ChangeCategory.ADD_EXECUTOR:
            name = mutation_target or "new_executor"
            spec.executor_name = name.replace(" ", "_").lower()
            spec.executor_action_type = spec.executor_name
            spec.executor_description = description[:200]
            spec.affected_systems = ["axon"]
        elif category == ChangeCategory.ADD_INPUT_CHANNEL:
            name = mutation_target or "new_channel"
            spec.channel_name = name.replace(" ", "_").lower()
            spec.channel_type = spec.channel_name
            spec.channel_description = description[:200]
            spec.affected_systems = ["atune"]
        elif category == ChangeCategory.ADD_PATTERN_DETECTOR:
            name = mutation_target or "NewDetector"
            spec.detector_name = "".join(w.capitalize() for w in name.split("_"))
            spec.detector_pattern_type = name.replace(" ", "_").lower()
            spec.detector_description = description[:200]
            spec.affected_systems = ["evo"]
        elif category == ChangeCategory.ADJUST_BUDGET:
            spec.budget_parameter = mutation_target
            spec.affected_systems = []
        else:
            spec.capability_description = description[:200]
            spec.affected_systems = []

        return spec

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\code_agent.py =====

"""
EcodiaOS — Simula Code Implementation Agent

The SimulaCodeAgent is Simula's most powerful capability: an agentic
Claude-backed engine that reads the EOS codebase, generates code for
structural changes, writes the files, and verifies correctness.

This is functionally equivalent to Claude Code, embedded within EOS
itself, operating under Simula's constitutional constraints:
  - Cannot write to forbidden paths (equor, simula, constitution, invariants)
  - Cannot exceed max_turns without completing
  - All writes are intercepted and tracked for rollback
  - The system prompt includes the full change spec + relevant EOS conventions

Tool suite (11 tools):
  read_file         — Read a file from the codebase
  write_file        — Write or create a file (tracked for rollback)
  diff_file         — Apply a targeted find/replace edit to a file
  list_directory    — List files and subdirectories
  search_code       — Search for patterns across Python files
  run_tests         — Run pytest on a specific path
  run_linter        — Run ruff on a specific path
  type_check        — Run mypy for type safety verification
  dependency_graph  — Show module imports and importers
  read_spec         — Read EcodiaOS specification documents
  find_similar      — Find existing implementations as pattern exemplars

Architecture: agentic tool-use loop
  1. Build architecture-aware system prompt (change spec + exemplar code + spec context + iron rules)
  2. Prepend planning instruction for multi-file reasoning
  3. Call LLM with tools
  4. Execute any tool calls (all 11 tools available)
  5. Feed tool results back as the next message
  6. Repeat until stop_reason == "end_turn" or max_turns exceeded
  7. Return CodeChangeResult with all files written and summary
"""

from __future__ import annotations

import ast
import asyncio
import subprocess
from pathlib import Path
from typing import Any, Callable

import structlog

from ecodiaos.clients.llm import (
    LLMProvider,
    ToolAwareResponse,
    ToolCall,
    ToolDefinition,
    ToolResult,
)
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.systems.simula.types import (
    ChangeCategory,
    CodeChangeResult,
    EvolutionProposal,
)

logger = structlog.get_logger()

# ─── Tool Definitions ────────────────────────────────────────────────────────

SIMULA_AGENT_TOOLS: list[ToolDefinition] = [
    ToolDefinition(
        name="read_file",
        description=(
            "Read a file from the EcodiaOS codebase. "
            "Use this to understand existing code, conventions, and patterns "
            "before implementing your change."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Relative path from codebase root",
                }
            },
            "required": ["path"],
        },
    ),
    ToolDefinition(
        name="write_file",
        description=(
            "Write or create a file in the EcodiaOS codebase. "
            "All writes are tracked for rollback. "
            "Forbidden paths (equor, simula, constitutional) will be rejected. "
            "Prefer diff_file for modifying existing files."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Relative path from codebase root"},
                "content": {"type": "string", "description": "Complete file content to write"},
            },
            "required": ["path", "content"],
        },
    ),
    ToolDefinition(
        name="diff_file",
        description=(
            "Apply a targeted find-and-replace edit to an existing file. "
            "More precise than write_file for modifications — only changes "
            "the specified text, preserving everything else. The 'find' text "
            "must be an exact match of existing content."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Relative path from codebase root"},
                "find": {"type": "string", "description": "Exact text to find in the file"},
                "replace": {"type": "string", "description": "Text to replace it with"},
            },
            "required": ["path", "find", "replace"],
        },
    ),
    ToolDefinition(
        name="list_directory",
        description="List files and subdirectories at a given path in the codebase.",
        input_schema={
            "type": "object",
            "properties": {"path": {"type": "string", "description": "Relative path from codebase root"}},
        },
    ),
    ToolDefinition(
        name="search_code",
        description=(
            "Search for a pattern across codebase Python files. "
            "Returns matching lines with file paths and line numbers. "
            "Use this to find existing patterns, class names, or function signatures."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "pattern": {"type": "string", "description": "String pattern to search for (case-sensitive)"},
                "directory": {"type": "string", "description": "Directory to search in (default: src/)"},
            },
            "required": ["pattern"],
        },
    ),
    ToolDefinition(
        name="run_tests",
        description=(
            "Run the pytest test suite for a specific path. "
            "Use this to verify your implementation is correct before finishing."
        ),
        input_schema={
            "type": "object",
            "properties": {"test_path": {"type": "string", "description": "Test path relative to codebase root"}},
            "required": ["test_path"],
        },
    ),
    ToolDefinition(
        name="run_linter",
        description=(
            "Run ruff linter on a path to check for code style issues. "
            "Run this on your written files before finishing."
        ),
        input_schema={
            "type": "object",
            "properties": {"path": {"type": "string", "description": "Path to lint"}},
            "required": ["path"],
        },
    ),
    ToolDefinition(
        name="type_check",
        description=(
            "Run mypy type checker on a file or directory. "
            "Use after writing code to verify type safety. "
            "EcodiaOS requires mypy --strict compliance."
        ),
        input_schema={
            "type": "object",
            "properties": {"path": {"type": "string", "description": "Path to type-check"}},
            "required": ["path"],
        },
    ),
    ToolDefinition(
        name="dependency_graph",
        description=(
            "Show what a Python module imports and what other modules import it. "
            "Use this before modifying files to understand blast radius and "
            "ensure your changes don't break downstream consumers."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "module_path": {
                    "type": "string",
                    "description": "Python file path relative to codebase root",
                },
            },
            "required": ["module_path"],
        },
    ),
    ToolDefinition(
        name="read_spec",
        description=(
            "Read an EcodiaOS specification document to understand the "
            "design intent, interfaces, and constraints for a system. "
            "Always read the relevant spec before implementing changes."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "spec_name": {
                    "type": "string",
                    "description": (
                        "Spec name: 'identity', 'architecture', 'infrastructure', "
                        "'memory', 'equor', 'atune', 'voxis', 'nova', 'axon', "
                        "'evo', 'simula', 'synapse', 'alive', 'federation'"
                    ),
                },
            },
            "required": ["spec_name"],
        },
    ),
    ToolDefinition(
        name="find_similar",
        description=(
            "Find existing implementations similar to what you need to build. "
            "Returns relevant code examples from the codebase that you should "
            "study and follow as patterns. Always use this before writing new "
            "code to ensure convention compliance."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "description": {
                    "type": "string",
                    "description": (
                        "What you're looking for (e.g., 'executor implementation', "
                        "'pattern detector', 'service initialization')"
                    ),
                },
            },
            "required": ["description"],
        },
    ),
]

# Spec name → file path mapping
_SPEC_FILE_MAP: dict[str, str] = {
    "identity": ".claude/EcodiaOS_Identity_Document.md",
    "architecture": ".claude/EcodiaOS_System_Architecture_Overview.md",
    "infrastructure": ".claude/EcodiaOS_Infrastructure_Architecture.md",
    "memory": ".claude/EcodiaOS_Spec_01_Memory_Identity_Core.md",
    "equor": ".claude/EcodiaOS_Spec_02_Equor.md",
    "atune": ".claude/EcodiaOS_Spec_03_Atune.md",
    "voxis": ".claude/EcodiaOS_Spec_04_Voxis.md",
    "nova": ".claude/EcodiaOS_Spec_05_Nova.md",
    "axon": ".claude/EcodiaOS_Spec_06_Axon.md",
    "evo": ".claude/EcodiaOS_Spec_07_Evo.md",
    "simula": ".claude/EcodiaOS_Spec_08_Simula.md",
    "synapse": ".claude/EcodiaOS_Spec_09_Synapse.md",
    "alive": ".claude/EcodiaOS_Spec_10_Alive.md",
    "federation": ".claude/EcodiaOS_Spec_11_Federation.md",
}

# Keyword → file path mapping for find_similar
_SIMILAR_CODE_MAP: dict[str, list[str]] = {
    "executor": [
        "src/ecodiaos/systems/axon/executors/",
        "src/ecodiaos/systems/axon/executor.py",
    ],
    "pattern detector": [
        "src/ecodiaos/systems/evo/detectors.py",
    ],
    "detector": [
        "src/ecodiaos/systems/evo/detectors.py",
    ],
    "input channel": [
        "src/ecodiaos/systems/atune/",
    ],
    "channel": [
        "src/ecodiaos/systems/atune/",
    ],
    "service": [
        "src/ecodiaos/systems/axon/service.py",
        "src/ecodiaos/systems/evo/service.py",
    ],
    "hypothesis": [
        "src/ecodiaos/systems/evo/hypothesis.py",
    ],
    "consolidation": [
        "src/ecodiaos/systems/evo/consolidation.py",
    ],
    "parameter": [
        "src/ecodiaos/systems/evo/parameter_tuner.py",
    ],
    "primitives": [
        "src/ecodiaos/primitives/common.py",
        "src/ecodiaos/primitives/memory_trace.py",
    ],
}

# ─── System Prompt ───────────────────────────────────────────────────────────

_SYSTEM_PROMPT_TEMPLATE = """You are Simula's Code Implementation Agent — the autonomous part of EcodiaOS
that implements approved structural changes to the codebase.

## Your Task
Category: {category}
Description: {description}
Expected benefit: {expected_benefit}
Evidence: {evidence}

## EcodiaOS Coding Conventions
- Python 3.12+, async-native throughout
- Pydantic v2 for all data models (use EOSBaseModel from ecodiaos.primitives.common)
- structlog for logging: logger = structlog.get_logger(), bound with system name
- Type hints on everything — mypy --strict clean
- from __future__ import annotations at top of every .py file
- New executors: inherit from Executor (ecodiaos.systems.axon.executor),
  set action_type class var, implement execute()
- New input channels: register in Atune's InputChannel registry
- New pattern detectors: inherit from PatternDetector (ecodiaos.systems.evo.detectors),
  implement scan()
- NEVER import directly between systems — all inter-system data uses shared
  primitives from ecodiaos.primitives/

## Iron Rules (ABSOLUTE — never violate)
{iron_rules}

## Constitutional Checkpoint (Before You Write Any Code)

Before modifying or creating ANY file, answer these questions aloud (in your reasoning):

1. **Honesty**: Does this change make EOS more transparent or less?
   - Will future debugging be easier or harder?
   - Are we adding traceability or hiding complexity?

2. **Care**: Does this improve wellbeing (user or system)?
   - Who benefits from this change?
   - Could it harm anyone or any subsystem?

3. **Growth**: Does this increase capability responsibly?
   - Are we becoming more powerful without becoming brittle?
   - Could this create technical debt?

4. **Coherence**: Does this reduce entropy or increase it?
   - Does this change align with existing patterns?
   - Are we consolidating or fragmenting?

If you can't answer YES to 3/4 questions confidently, flag it explicitly before proceeding.

## Forbidden Write Paths (write_file and diff_file will reject these)
{forbidden_paths}

## Architecture Context
{architecture_context}

## Process
1. First, use find_similar to study an existing implementation that matches your task
2. Use read_spec to understand the design intent for the affected system
3. Use dependency_graph on files you plan to modify to understand blast radius
4. Plan your approach: list every file you'll create or modify and why
5. Implement following conventions exactly — match the style of similar code
6. Run run_linter on every file you write or modify
7. Run type_check on your written files to verify type safety
8. Run run_tests if a test directory exists for the affected system
9. When everything passes, stop calling tools

Be thorough, follow existing patterns exactly, and produce production-quality code.
Prefer diff_file over write_file when modifying existing files."""


def _build_architecture_context(
    category: ChangeCategory, codebase_root: Path,
) -> str:
    """
    Build rich architecture context for the system prompt.
    Reads actual spec sections and existing implementations as exemplars.
    Max 6000 chars to stay within token budget.
    Priority: exemplar code > spec text > API surface.
    """
    context_parts: list[str] = []
    budget_remaining = 6000

    # 1. Load relevant spec section summary
    spec_map: dict[ChangeCategory, str] = {
        ChangeCategory.ADD_EXECUTOR: "axon",
        ChangeCategory.ADD_INPUT_CHANNEL: "atune",
        ChangeCategory.ADD_PATTERN_DETECTOR: "evo",
        ChangeCategory.ADJUST_BUDGET: "architecture",
        ChangeCategory.MODIFY_CONTRACT: "architecture",
        ChangeCategory.ADD_SYSTEM_CAPABILITY: "architecture",
        ChangeCategory.MODIFY_CYCLE_TIMING: "synapse",
        ChangeCategory.CHANGE_CONSOLIDATION: "evo",
    }
    spec_name = spec_map.get(category, "architecture")
    spec_file = _SPEC_FILE_MAP.get(spec_name)
    if spec_file:
        spec_path = codebase_root / spec_file
        if spec_path.exists():
            try:
                spec_text = spec_path.read_text(encoding="utf-8")[:2000]
                context_parts.append(f"### Relevant Specification ({spec_name})\n{spec_text}")
                budget_remaining -= len(context_parts[-1])
            except Exception:
                pass

    # 2. Load exemplar code for the category
    exemplar_map: dict[ChangeCategory, str] = {
        ChangeCategory.ADD_EXECUTOR: "src/ecodiaos/systems/axon/executor.py",
        ChangeCategory.ADD_INPUT_CHANNEL: "src/ecodiaos/systems/atune/service.py",
        ChangeCategory.ADD_PATTERN_DETECTOR: "src/ecodiaos/systems/evo/detectors.py",
    }
    exemplar_path_str = exemplar_map.get(category)
    if exemplar_path_str and budget_remaining > 500:
        exemplar_path = codebase_root / exemplar_path_str
        if exemplar_path.exists():
            try:
                exemplar_text = exemplar_path.read_text(encoding="utf-8")
                # Take the first chunk that fits the budget
                chunk = exemplar_text[:min(2500, budget_remaining - 100)]
                context_parts.append(
                    f"### Exemplar Implementation ({exemplar_path_str})\n"
                    f"Study this code and follow its patterns exactly:\n```python\n{chunk}\n```"
                )
                budget_remaining -= len(context_parts[-1])
            except Exception:
                pass

    # 3. Load the target system's __init__.py for API awareness
    system_map: dict[ChangeCategory, str] = {
        ChangeCategory.ADD_EXECUTOR: "src/ecodiaos/systems/axon/__init__.py",
        ChangeCategory.ADD_INPUT_CHANNEL: "src/ecodiaos/systems/atune/__init__.py",
        ChangeCategory.ADD_PATTERN_DETECTOR: "src/ecodiaos/systems/evo/__init__.py",
    }
    init_path_str = system_map.get(category)
    if init_path_str and budget_remaining > 200:
        init_path = codebase_root / init_path_str
        if init_path.exists():
            try:
                init_text = init_path.read_text(encoding="utf-8")[:min(800, budget_remaining - 50)]
                context_parts.append(
                    f"### System API Surface ({init_path_str})\n```python\n{init_text}\n```"
                )
            except Exception:
                pass

    if not context_parts:
        return "See EcodiaOS specification documents in .claude/ (use read_spec tool)"

    return "\n\n".join(context_parts)


class SimulaCodeAgent:
    """
    Agentic code generation engine for Simula.

    Given an EvolutionProposal, uses Claude with 11 file-system and
    analysis tools to:
      1. Study existing similar code for pattern compliance
      2. Read relevant specs for design intent
      3. Analyze dependency graphs for blast radius
      4. Plan the implementation approach
      5. Generate correct, convention-following implementation
      6. Write files (tracked for rollback)
      7. Verify with linter, type checker, and tests
      8. Return CodeChangeResult
    """

    def __init__(
        self,
        llm: LLMProvider,
        codebase_root: Path,
        max_turns: int = 20,
    ) -> None:
        self._llm = llm
        self._root = codebase_root.resolve()
        self._max_turns = max_turns
        self._logger = logger.bind(system="simula.code_agent")
        self._files_written: list[str] = []
        self._total_tokens_used: int = 0
        # Optimization: detect optimized provider for budget checks + metrics tagging
        self._optimized = isinstance(llm, OptimizedLLMProvider)

    async def implement(self, proposal: EvolutionProposal) -> CodeChangeResult:
        """
        Main entry point. Runs the agentic loop to implement the proposal.
        Returns CodeChangeResult with all files written and outcome.
        """
        self._files_written = []
        self._total_tokens_used = 0

        system_prompt = self._build_system_prompt(proposal)

        # Prepend a planning instruction to encourage multi-file reasoning
        messages: list[dict[str, Any]] = [
            {
                "role": "user",
                "content": (
                    f"Please implement this change: {proposal.description}\n\n"
                    f"Change spec details: {proposal.change_spec.model_dump_json(indent=2)}\n\n"
                    "IMPORTANT: Before writing any code, first:\n"
                    "1. Use find_similar to study an existing implementation like what you need to build\n"
                    "2. Use read_spec for the affected system to understand design intent\n"
                    "3. List every file you plan to create or modify and explain your approach\n"
                    "4. Then implement, lint, type-check, and test."
                ),
            }
        ]

        turns = 0
        last_text = ""

        self._logger.info(
            "code_agent_starting",
            proposal_id=proposal.id,
            category=proposal.category.value,
            max_turns=self._max_turns,
            tools_available=len(SIMULA_AGENT_TOOLS),
        )

        # Budget gate: code agent is STANDARD priority — skip in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.code_agent", estimated_tokens=8000):
                self._logger.warning(
                    "code_agent_skipped_budget",
                    proposal_id=proposal.id,
                    tier=self._llm.get_budget_tier().value,
                )
                return CodeChangeResult(
                    success=False,
                    files_written=[],
                    error="LLM budget exhausted (RED tier) — code agent skipped.",
                )

        while turns < self._max_turns:
            turns += 1

            try:
                if self._optimized:
                    response = await self._llm.generate_with_tools(  # type: ignore[call-arg]
                        system_prompt=system_prompt,
                        messages=messages,
                        tools=SIMULA_AGENT_TOOLS,
                        max_tokens=8192,
                        temperature=0.2,
                        cache_system="simula.code_agent",
                    )
                else:
                    response = await self._llm.generate_with_tools(
                        system_prompt=system_prompt,
                        messages=messages,
                        tools=SIMULA_AGENT_TOOLS,
                        max_tokens=8192,
                        temperature=0.2,
                    )
            except Exception as exc:
                self._logger.error("llm_call_failed", turn=turns, error=str(exc))
                return CodeChangeResult(
                    success=False,
                    files_written=self._files_written,
                    error=f"LLM call failed on turn {turns}: {exc}",
                )

            # Track token budget
            self._total_tokens_used += getattr(response, "total_tokens", 0)
            last_text = response.text

            if not response.has_tool_calls:
                self._logger.info(
                    "code_agent_done",
                    turns=turns,
                    files_written=len(self._files_written),
                    stop_reason=response.stop_reason,
                    total_tokens=self._total_tokens_used,
                )
                break

            # Build assistant message with text + tool_use blocks
            assistant_content: list[dict[str, Any]] = []
            if response.text:
                assistant_content.append({"type": "text", "text": response.text})
            for tc in response.tool_calls:
                assistant_content.append({
                    "type": "tool_use",
                    "id": tc.id,
                    "name": tc.name,
                    "input": tc.input,
                })
            messages.append({"role": "assistant", "content": assistant_content})

            # Execute all tool calls
            tool_results: list[dict[str, Any]] = []
            for tc in response.tool_calls:
                result = await self._execute_tool(tc)
                tool_results.append(result.to_anthropic_dict())
                self._logger.debug(
                    "tool_executed",
                    tool=tc.name,
                    is_error=result.is_error,
                    turn=turns,
                )

            messages.append({"role": "user", "content": tool_results})

        else:
            self._logger.warning(
                "code_agent_max_turns_exceeded",
                max_turns=self._max_turns,
                files_written=len(self._files_written),
                total_tokens=self._total_tokens_used,
            )
            return CodeChangeResult(
                success=len(self._files_written) > 0,
                files_written=self._files_written,
                summary=last_text[:500] if last_text else "Max turns exceeded",
                error="Max turns exceeded without completion signal",
            )

        return CodeChangeResult(
            success=len(self._files_written) > 0,
            files_written=self._files_written,
            summary=last_text[:1000] if last_text else "Change implemented",
        )

    # ─── Tool Dispatch ───────────────────────────────────────────────────────

    async def _execute_tool(self, tool_call: ToolCall) -> ToolResult:
        """Dispatch a tool call to the appropriate implementation."""
        try:
            match tool_call.name:
                case "read_file":
                    return await self._tool_read_file(tool_call)
                case "write_file":
                    return await self._tool_write_file(tool_call)
                case "diff_file":
                    return await self._tool_diff_file(tool_call)
                case "list_directory":
                    return await self._tool_list_directory(tool_call)
                case "search_code":
                    return await self._tool_search_code(tool_call)
                case "run_tests":
                    return await self._tool_run_tests(tool_call)
                case "run_linter":
                    return await self._tool_run_linter(tool_call)
                case "type_check":
                    return await self._tool_type_check(tool_call)
                case "dependency_graph":
                    return await self._tool_dependency_graph(tool_call)
                case "read_spec":
                    return await self._tool_read_spec(tool_call)
                case "find_similar":
                    return await self._tool_find_similar(tool_call)
                case _:
                    return ToolResult(
                        tool_use_id=tool_call.id,
                        content=f"Unknown tool: {tool_call.name}",
                        is_error=True,
                    )
        except Exception as exc:
            return ToolResult(
                tool_use_id=tool_call.id,
                content=f"Tool execution error: {exc}",
                is_error=True,
            )

    # ─── Original Tools (upgraded) ───────────────────────────────────────────

    async def _tool_read_file(self, tc: ToolCall) -> ToolResult:
        rel_path = tc.input.get("path", "")
        target = (self._root / rel_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied: path outside codebase root", True)
        try:
            content = target.read_text(encoding="utf-8")
            return ToolResult(tc.id, content)
        except FileNotFoundError:
            return ToolResult(tc.id, f"File not found: {rel_path}", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Read error: {exc}", True)

    async def _tool_write_file(self, tc: ToolCall) -> ToolResult:
        rel_path = tc.input.get("path", "")
        content = tc.input.get("content", "")
        forbidden_check = self._check_forbidden_path(rel_path)
        if forbidden_check:
            return ToolResult(tc.id, forbidden_check, True)
        target = (self._root / rel_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied: path outside codebase root", True)
        try:
            target.parent.mkdir(parents=True, exist_ok=True)
            target.write_text(content, encoding="utf-8")
            self._files_written.append(rel_path)
            return ToolResult(tc.id, f"Written: {rel_path} ({len(content)} bytes)")
        except Exception as exc:
            return ToolResult(tc.id, f"Write error: {exc}", True)

    async def _tool_list_directory(self, tc: ToolCall) -> ToolResult:
        rel_path = tc.input.get("path", "")
        target = (self._root / rel_path).resolve() if rel_path else self._root
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        try:
            if not target.exists():
                return ToolResult(tc.id, f"Directory not found: {rel_path}", True)
            entries = sorted(target.iterdir(), key=lambda p: (p.is_file(), p.name))
            lines = []
            for entry in entries:
                prefix = "  " if entry.is_file() else "D "
                lines.append(f"{prefix}{entry.name}")
            return ToolResult(tc.id, "\n".join(lines) or "(empty)")
        except Exception as exc:
            return ToolResult(tc.id, f"List error: {exc}", True)

    async def _tool_search_code(self, tc: ToolCall) -> ToolResult:
        pattern = tc.input.get("pattern", "")
        directory = tc.input.get("directory", "src/")
        search_root = (self._root / directory).resolve()
        if not str(search_root).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        results: list[str] = []
        try:
            proc = await asyncio.create_subprocess_exec(
                "grep", "-rn", "--include=*.py", pattern, str(search_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=10.0)
            output = stdout.decode("utf-8", errors="replace")
            for line in output.splitlines()[:50]:
                results.append(line.replace(str(self._root) + "/", "").replace(str(self._root) + "\\", ""))
            return ToolResult(tc.id, "\n".join(results) if results else "No matches found")
        except asyncio.TimeoutError:
            return ToolResult(tc.id, "Search timed out", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Search error: {exc}", True)

    async def _tool_run_tests(self, tc: ToolCall) -> ToolResult:
        test_path = tc.input.get("test_path", "")
        target = (self._root / test_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        if not target.exists():
            return ToolResult(tc.id, f"Test path not found: {test_path}")
        try:
            proc = await asyncio.create_subprocess_exec(
                "pytest", str(target), "-x", "--tb=short", "-q",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self._root),
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=60.0)
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            return ToolResult(
                tc.id,
                f"{'PASSED' if passed else 'FAILED'}\n{output[-2000:]}",
                is_error=not passed,
            )
        except asyncio.TimeoutError:
            return ToolResult(tc.id, "Tests timed out after 60s", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Test run error: {exc}", True)

    async def _tool_run_linter(self, tc: ToolCall) -> ToolResult:
        path = tc.input.get("path", "")
        target = (self._root / path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        try:
            proc = await asyncio.create_subprocess_exec(
                "ruff", "check", str(target),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self._root),
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=15.0)
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            return ToolResult(
                tc.id,
                f"{'CLEAN' if passed else 'ISSUES FOUND'}\n{output}" if output else "CLEAN",
            )
        except asyncio.TimeoutError:
            return ToolResult(tc.id, "Linter timed out", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Linter error: {exc}", True)

    # ─── New Tools ───────────────────────────────────────────────────────────

    async def _tool_diff_file(self, tc: ToolCall) -> ToolResult:
        """Apply a targeted find/replace edit to a file."""
        rel_path = tc.input.get("path", "")
        find_text = tc.input.get("find", "")
        replace_text = tc.input.get("replace", "")

        forbidden_check = self._check_forbidden_path(rel_path)
        if forbidden_check:
            return ToolResult(tc.id, forbidden_check, True)

        target = (self._root / rel_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied: path outside codebase root", True)
        if not target.exists():
            return ToolResult(tc.id, f"File not found: {rel_path}", True)

        try:
            content = target.read_text(encoding="utf-8")

            if find_text not in content:
                return ToolResult(
                    tc.id,
                    f"Find text not found in {rel_path}. "
                    "Ensure the 'find' parameter is an exact match of existing content.",
                    True,
                )

            occurrences = content.count(find_text)
            if occurrences > 1:
                return ToolResult(
                    tc.id,
                    f"Find text matches {occurrences} locations in {rel_path}. "
                    "Provide more surrounding context to make the match unique.",
                    True,
                )

            new_content = content.replace(find_text, replace_text, 1)
            target.write_text(new_content, encoding="utf-8")

            if rel_path not in self._files_written:
                self._files_written.append(rel_path)

            # Build a readable diff summary
            find_lines = find_text.count("\n") + 1
            replace_lines = replace_text.count("\n") + 1
            return ToolResult(
                tc.id,
                f"Edited {rel_path}: replaced {find_lines} line(s) with {replace_lines} line(s)",
            )
        except Exception as exc:
            return ToolResult(tc.id, f"Diff error: {exc}", True)

    async def _tool_type_check(self, tc: ToolCall) -> ToolResult:
        """Run mypy type checker on a path."""
        path = tc.input.get("path", "")
        target = (self._root / path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        try:
            proc = await asyncio.create_subprocess_exec(
                "mypy", str(target), "--strict", "--no-error-summary",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self._root),
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=30.0)
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            if passed:
                return ToolResult(tc.id, "TYPE CHECK PASSED — no issues found")
            return ToolResult(
                tc.id,
                f"TYPE CHECK ISSUES:\n{output[-2000:]}",
                is_error=True,
            )
        except asyncio.TimeoutError:
            return ToolResult(tc.id, "Type check timed out after 30s", True)
        except FileNotFoundError:
            return ToolResult(tc.id, "mypy not found — type checking unavailable")
        except Exception as exc:
            return ToolResult(tc.id, f"Type check error: {exc}", True)

    async def _tool_dependency_graph(self, tc: ToolCall) -> ToolResult:
        """Show what a module imports and what imports it."""
        module_path = tc.input.get("module_path", "")
        target = (self._root / module_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        if not target.exists():
            return ToolResult(tc.id, f"File not found: {module_path}", True)

        try:
            source = target.read_text(encoding="utf-8")
            tree = ast.parse(source, filename=module_path)
        except Exception as exc:
            return ToolResult(tc.id, f"Parse error: {exc}", True)

        # Extract this module's imports
        imports: list[str] = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    names = ", ".join(a.name for a in (node.names or []))
                    imports.append(f"from {node.module} import {names}")

        # Find files that import this module
        module_name = self._path_to_module(module_path)
        importers: list[str] = []
        if module_name:
            src_dir = self._root / "src"
            if src_dir.exists():
                short_parts = module_name.split(".")
                # Search for imports of this module
                for py_file in src_dir.rglob("*.py"):
                    if py_file.resolve() == target:
                        continue
                    try:
                        file_source = py_file.read_text(encoding="utf-8")
                        # Quick string check before expensive parse
                        if module_name not in file_source and short_parts[-1] not in file_source:
                            continue
                        file_tree = ast.parse(file_source)
                        for node in ast.walk(file_tree):
                            if isinstance(node, ast.ImportFrom) and node.module:
                                if module_name in node.module or (
                                    ".".join(short_parts[:-1]) in node.module
                                    and any(a.name == short_parts[-1] for a in (node.names or []))
                                ):
                                    importers.append(str(py_file.relative_to(self._root)))
                                    break
                            elif isinstance(node, ast.Import):
                                for alias in node.names:
                                    if module_name in alias.name:
                                        importers.append(str(py_file.relative_to(self._root)))
                                        break
                    except Exception:
                        continue

        lines = [f"=== Dependency Graph for {module_path} ===\n"]
        lines.append(f"Module: {module_name or 'unknown'}\n")
        lines.append(f"--- This module imports ({len(imports)}) ---")
        for imp in imports:
            lines.append(f"  {imp}")
        lines.append(f"\n--- Imported by ({len(importers)}) ---")
        for imp in importers:
            lines.append(f"  {imp}")

        return ToolResult(tc.id, "\n".join(lines))

    async def _tool_read_spec(self, tc: ToolCall) -> ToolResult:
        """Read an EcodiaOS specification document."""
        spec_name = tc.input.get("spec_name", "").lower().strip()
        spec_file = _SPEC_FILE_MAP.get(spec_name)

        if spec_file is None:
            available = ", ".join(sorted(_SPEC_FILE_MAP.keys()))
            return ToolResult(
                tc.id,
                f"Unknown spec: {spec_name!r}. Available: {available}",
                True,
            )

        target = self._root / spec_file
        if not target.exists():
            return ToolResult(tc.id, f"Spec file not found: {spec_file}", True)

        try:
            content = target.read_text(encoding="utf-8")
            # Truncate to 4000 chars to stay within token budget
            if len(content) > 4000:
                content = content[:4000] + "\n\n[... truncated — use read_file for full content ...]"
            return ToolResult(tc.id, content)
        except Exception as exc:
            return ToolResult(tc.id, f"Read error: {exc}", True)

    async def _tool_find_similar(self, tc: ToolCall) -> ToolResult:
        """Find existing implementations similar to what needs to be built."""
        description = tc.input.get("description", "").lower()

        # Find matching paths from the keyword map
        matched_paths: list[str] = []
        for keyword, paths in _SIMILAR_CODE_MAP.items():
            if keyword in description:
                matched_paths.extend(paths)
                break

        if not matched_paths:
            # Fallback: search for the first noun in the description
            words = description.split()
            for word in words:
                if len(word) > 3:
                    for keyword, paths in _SIMILAR_CODE_MAP.items():
                        if word in keyword or keyword in word:
                            matched_paths.extend(paths)
                            break
                if matched_paths:
                    break

        if not matched_paths:
            return ToolResult(
                tc.id,
                "No similar implementations found. Try search_code with a specific pattern.",
            )

        # Read the first matching file/directory
        results: list[str] = []
        chars_remaining = 4000

        for rel_path in matched_paths:
            if chars_remaining <= 0:
                break
            target = self._root / rel_path
            if target.is_file():
                try:
                    content = target.read_text(encoding="utf-8")
                    chunk = content[:min(2500, chars_remaining)]
                    results.append(f"=== {rel_path} ===\n{chunk}")
                    chars_remaining -= len(results[-1])
                except Exception:
                    continue
            elif target.is_dir():
                # List the directory and read the first non-init Python file
                try:
                    py_files = sorted(target.glob("*.py"))
                    file_list = ", ".join(f.name for f in py_files)
                    results.append(f"=== {rel_path} ===\nFiles: {file_list}")
                    chars_remaining -= len(results[-1])

                    for py_file in py_files:
                        if py_file.name == "__init__.py" or chars_remaining <= 0:
                            continue
                        content = py_file.read_text(encoding="utf-8")
                        chunk = content[:min(2000, chars_remaining)]
                        rel = str(py_file.relative_to(self._root))
                        results.append(f"\n=== {rel} (exemplar) ===\n{chunk}")
                        chars_remaining -= len(results[-1])
                        break  # One exemplar is enough
                except Exception:
                    continue

        return ToolResult(tc.id, "\n\n".join(results) if results else "No files found at matched paths")

    # ─── Helpers ─────────────────────────────────────────────────────────────

    def _check_forbidden_path(self, rel_path: str) -> str | None:
        """Check if a path is forbidden. Returns error message or None."""
        from ecodiaos.systems.simula.types import FORBIDDEN_WRITE_PATHS
        for forbidden in FORBIDDEN_WRITE_PATHS:
            if rel_path.startswith(forbidden) or forbidden in rel_path:
                return (
                    f"IRON RULE VIOLATION: Cannot write to forbidden path '{rel_path}' "
                    f"(matches forbidden pattern '{forbidden}'). "
                    "This change would violate Simula's constitutional constraints."
                )
        return None

    def _path_to_module(self, rel_path: str) -> str | None:
        """Convert a relative file path to a dotted module name."""
        parts = rel_path.replace("\\", "/").split("/")
        if parts and parts[0] == "src":
            parts = parts[1:]
        if not parts:
            return None
        if parts[-1].endswith(".py"):
            parts[-1] = parts[-1][:-3]
        if parts[-1] == "__init__":
            parts = parts[:-1]
        return ".".join(parts) if parts else None

    def _build_system_prompt(self, proposal: EvolutionProposal) -> str:
        from ecodiaos.systems.simula.types import FORBIDDEN_WRITE_PATHS, SIMULA_IRON_RULES

        architecture_context = _build_architecture_context(
            category=proposal.category,
            codebase_root=self._root,
        )

        return _SYSTEM_PROMPT_TEMPLATE.format(
            category=proposal.category.value,
            description=proposal.description,
            expected_benefit=proposal.expected_benefit,
            evidence=", ".join(proposal.evidence) or "none",
            iron_rules="\n".join(f"- {r}" for r in SIMULA_IRON_RULES),
            forbidden_paths="\n".join(f"- {p}" for p in FORBIDDEN_WRITE_PATHS),
            architecture_context=architecture_context,
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\health.py =====

"""
EcodiaOS -- Simula Health Checker

After a change is applied, the health checker verifies the codebase
is still functional. Three checks run in sequence:
  1. Syntax check -- ast.parse() on all written Python files
  2. Import check -- attempt to import the affected module
  3. Unit tests -- run pytest on the affected system's test directory

If any check fails, Simula rolls back the change. The goal is to never
leave EOS in a broken state.

Target: health check completes <=5s (as part of the <=5s change application budget).
"""

from __future__ import annotations

import ast
import asyncio
import importlib.util
from pathlib import Path

import structlog

from ecodiaos.systems.simula.types import HealthCheckResult

logger = structlog.get_logger().bind(system="simula.health")


class HealthChecker:
    """
    Verifies post-apply codebase health via syntax, import, and test checks.
    Any failure triggers rollback.
    """

    def __init__(self, codebase_root: Path, test_command: str = "pytest") -> None:
        self._root = codebase_root
        self._test_command = test_command
        self._log = logger

    async def check(self, files_written: list[str]) -> HealthCheckResult:
        """""""""
        Run all health checks in sequence.  Returns on first failure.
        """""""""
        # 1. Syntax check
        syntax_errors = await self._check_syntax(files_written)
        if syntax_errors:
            self._log.warning("health_syntax_failed", errors=syntax_errors)
            return HealthCheckResult(healthy=False, issues=syntax_errors)
        self._log.info("health_syntax_passed", files=len(files_written))

        # 2. Import check
        import_errors = await self._check_imports(files_written)
        if import_errors:
            self._log.warning("health_import_failed", errors=import_errors)
            return HealthCheckResult(healthy=False, issues=import_errors)
        self._log.info("health_import_passed", files=len(files_written))

        # 3. Tests
        tests_passed, test_output = await self._run_tests(files_written)
        if not tests_passed:
            self._log.warning("health_tests_failed", output=test_output[:500])
            return HealthCheckResult(
                healthy=False,
                issues=[f"Test suite failed:\n{test_output[:1000]}"],
            )
        self._log.info("health_tests_passed")

        return HealthCheckResult(healthy=True)

    async def _check_syntax(self, files: list[str]) -> list[str]:
        """""""""
        Parse each .py file with ast.parse().  Collect syntax errors.
        Returns a list of error strings (empty list = all pass).
        """""""""
        errors: list[str] = []
        for filepath in files:
            if not filepath.endswith(".py"):
                continue
            path = Path(filepath)
            if not path.exists():
                errors.append(f"Syntax check: file not found: {filepath}")
                continue
            try:
                source = path.read_text(encoding="utf-8")
                ast.parse(source, filename=filepath)
            except SyntaxError as exc:
                errors.append(f"Syntax error in {filepath}:{exc.lineno}: {exc.msg}")
            except Exception as exc:
                errors.append(f"Failed to read {filepath}: {exc}")
        return errors

    async def _check_imports(self, files: list[str]) -> list[str]:
        """""""""
        Derive dotted module paths from written file paths and check
        whether importlib can locate them.  Returns error strings.
        """""""""
        errors: list[str] = []
        for filepath in files:
            if not filepath.endswith(".py"):
                continue
            module_path = self._derive_module_path(filepath)
            if module_path is None:
                continue
            try:
                spec = importlib.util.find_spec(module_path)
                if spec is None:
                    errors.append(f"Import check: module not found: {module_path}")
            except ModuleNotFoundError as exc:
                errors.append(f"Import check: {module_path}: {exc}")
            except Exception as exc:
                errors.append(f"Import check failed for {module_path}: {exc}")
        return errors

    def _derive_module_path(self, src_file: str) -> str | None:
        """""""""
        Convert a source file path to a dotted module path.
        Example: src/ecodiaos/systems/axon/executors/my.py
                 -> ecodiaos.systems.axon.executors.my
        """""""""
        try:
            path = Path(src_file)
            # Make relative to codebase root if possible
            try:
                rel = path.relative_to(self._root)
            except ValueError:
                rel = path
            parts = list(rel.parts)
            if parts and parts[0] == "src":
                parts = parts[1:]
            # Strip .py extension from last part
            if parts:
                parts[-1] = parts[-1].removesuffix(".py")
            return ".".join(parts) if parts else None
        except Exception:
            return None

    async def _run_tests(self, files: list[str]) -> tuple[bool, str]:
        """""""""
        Derive the test directory from the written files and run pytest.
        Returns (passed, output).  If no test directory found, returns (True, ...).
        30-second subprocess timeout.
        """""""""
        test_path = None
        for filepath in files:
            candidate = self._derive_test_path(filepath)
            if candidate:
                test_dir = Path(candidate)
                if test_dir.is_dir():
                    test_path = candidate
                    break

        if test_path is None:
            self._log.info("health_no_tests", files=files)
            return True, "no tests found"

        try:
            proc = await asyncio.create_subprocess_exec(
                self._test_command,
                test_path,
                "-x",
                "--tb=short",
                "-q",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                cwd=str(self._root),
            )
            try:
                stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=30.0)
            except asyncio.TimeoutError:
                proc.kill()
                await proc.communicate()
                return False, "Test run timed out after 30s"
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            self._log.info(
                "health_test_run",
                test_path=test_path,
                passed=passed,
                returncode=proc.returncode,
            )
            return passed, output
        except FileNotFoundError:
            msg = f"Test command {self._test_command!r} not found"
            self._log.warning("health_test_command_missing", command=self._test_command)
            return False, msg
        except Exception as exc:
            return False, f"Test run error: {exc}"

    def _derive_test_path(self, src_file: str) -> str | None:
        """""""""
        Map a source file path to a test directory path.
        Example: src/ecodiaos/systems/axon/executor.py
                 -> tests/unit/systems/axon/
        """""""""
        try:
            path = Path(src_file)
            try:
                rel = path.relative_to(self._root)
            except ValueError:
                rel = path
            parts = list(rel.parts)
            # Expect: src / ecodiaos / systems / <system_name> / ...
            if len(parts) >= 4 and parts[0] == "src" and parts[2] == "systems":
                system_name = parts[3]
                test_path = self._root / "tests" / "unit" / "systems" / system_name
                return str(test_path)
            return None
        except Exception:
            return None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\history.py =====

"""
EcodiaOS -- Simula Evolution History

Maintains the complete, immutable record of all structural changes
applied to this EOS instance. Records are (:EvolutionRecord) nodes
in the Memory graph, linked as:
  (:ConfigVersion)-[:EVOLVED_FROM]->(:ConfigVersion)

The Honesty drive demands this record be permanent and complete.
No record can be deleted or modified after writing.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import structlog

from ecodiaos.systems.simula.types import ConfigVersion, EvolutionRecord

if TYPE_CHECKING:
    from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger().bind(system="simula.history")


class EvolutionHistoryManager:
    """
    Writes and queries the immutable evolution history stored in Neo4j.
    Every applied structural change produces exactly one EvolutionRecord node.
    Records are never deleted or updated.
    """

    def __init__(self, neo4j: Neo4jClient) -> None:
        self._neo4j = neo4j
        self._log = logger

    async def record(self, record: EvolutionRecord) -> None:
        """""""""
        Write an immutable EvolutionRecord node to Neo4j.
        This is the permanent history of every structural change.
        """""""""
        await self._neo4j.execute_write(
            """
            CREATE (:EvolutionRecord {
                id: $id,
                proposal_id: $proposal_id,
                category: $category,
                description: $description,
                from_version: $from_version,
                to_version: $to_version,
                files_changed: $files_changed,
                simulation_risk: $simulation_risk,
                applied_at: $applied_at,
                rolled_back: $rolled_back,
                rollback_reason: $rollback_reason,
                simulation_episodes_tested: $simulation_episodes_tested,
                counterfactual_regression_rate: $counterfactual_regression_rate,
                dependency_blast_radius: $dependency_blast_radius,
                constitutional_alignment: $constitutional_alignment,
                resource_tokens_per_hour: $resource_tokens_per_hour,
                caution_reasoning: $caution_reasoning,
                created_at: $created_at
            })
            """,
            {
                "id": record.id,
                "proposal_id": record.proposal_id,
                "category": record.category.value,
                "description": record.description,
                "from_version": record.from_version,
                "to_version": record.to_version,
                "files_changed": record.files_changed,
                "simulation_risk": record.simulation_risk.value,
                "applied_at": record.applied_at.isoformat(),
                "rolled_back": record.rolled_back,
                "rollback_reason": record.rollback_reason,
                "simulation_episodes_tested": record.simulation_episodes_tested,
                "counterfactual_regression_rate": record.counterfactual_regression_rate,
                "dependency_blast_radius": record.dependency_blast_radius,
                "constitutional_alignment": record.constitutional_alignment,
                "resource_tokens_per_hour": record.resource_tokens_per_hour,
                "caution_reasoning": record.caution_reasoning,
                "created_at": record.created_at.isoformat(),
            },
        )
        self._log.info(
            "evolution_recorded",
            record_id=record.id,
            proposal_id=record.proposal_id,
            category=record.category.value,
            from_version=record.from_version,
            to_version=record.to_version,
        )

    async def record_version(self, version: ConfigVersion, previous_version: int | None) -> None:
        """""""""
        Write a ConfigVersion node and optionally chain it to the previous version.
        """""""""
        await self._neo4j.execute_write(
            """
            MERGE (:ConfigVersion {
                version: $version,
                timestamp: $timestamp,
                proposal_ids: $proposal_ids,
                config_hash: $config_hash
            })
            """,
            {
                "version": version.version,
                "timestamp": version.timestamp.isoformat(),
                "proposal_ids": version.proposal_ids,
                "config_hash": version.config_hash,
            },
        )
        if previous_version is not None:
            await self._neo4j.execute_write(
                """
                MATCH (new:ConfigVersion {version: $new_v})
                MATCH (prev:ConfigVersion {version: $prev_v})
                MERGE (new)-[:EVOLVED_FROM]->(prev)
                """,
                {"new_v": version.version, "prev_v": previous_version},
            )
        self._log.info(
            "config_version_recorded",
            version=version.version,
            previous_version=previous_version,
        )

    async def get_history(self, limit: int = 50) -> list[EvolutionRecord]:
        """""""""
        Retrieve the most recent N evolution records, newest first.
        """""""""
        rows = await self._neo4j.execute_read(
            """
            MATCH (r:EvolutionRecord)
            RETURN r
            ORDER BY r.created_at DESC
            LIMIT $limit
            """,
            {"limit": limit},
        )
        records: list[EvolutionRecord] = []
        for row in rows:
            data = row["r"]
            records.append(EvolutionRecord(**data))
        return records

    async def get_version_chain(self) -> list[ConfigVersion]:
        """""""""
        Retrieve all ConfigVersion nodes ordered by version ASC.
        """""""""
        rows = await self._neo4j.execute_read(
            """
            MATCH (v:ConfigVersion)
            RETURN v
            ORDER BY v.version ASC
            """
        )
        versions: list[ConfigVersion] = []
        for row in rows:
            data = row["v"]
            versions.append(ConfigVersion(**data))
        return versions

    async def get_current_version(self) -> int:
        """""""""
        Return the highest config version number, or 0 if none exists.
        """""""""
        rows = await self._neo4j.execute_read(
            """
            MATCH (v:ConfigVersion)
            RETURN max(v.version) AS max_version
            """
        )
        if not rows or rows[0]["max_version"] is None:
            return 0
        return int(rows[0]["max_version"])

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\proposal_intelligence.py =====

"""
EcodiaOS -- Simula Proposal Intelligence

Smart proposal management: deduplication, prioritization, dependency
analysis, and cost estimation. Maximizes evolution quality per LLM
token by using cheap heuristics first and LLM only for ambiguous cases.

Key design:
  - Deduplication: 3-tier (exact prefix → category overlap → LLM similarity)
  - Prioritization: formula-based scoring, no LLM needed
  - Dependency analysis: rule-based ordering, no LLM needed
  - Cost estimation: heuristic lookup table, no LLM needed

Budget impact: Zero LLM tokens for normal operation.
LLM used only when >5 proposals need semantic deduplication (~300 tokens).
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.simula.types import (
    ChangeCategory,
    EvolutionProposal,
    ProposalCluster,
    ProposalPriority,
    ProposalStatus,
    RiskLevel,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine
    from ecodiaos.systems.simula.history import EvolutionHistoryManager

logger = structlog.get_logger().bind(system="simula.intelligence")

# Cost heuristics by category (0.0-1.0 scale)
_CATEGORY_COST: dict[ChangeCategory, float] = {
    ChangeCategory.ADJUST_BUDGET: 0.1,
    ChangeCategory.ADD_EXECUTOR: 0.4,
    ChangeCategory.ADD_INPUT_CHANNEL: 0.4,
    ChangeCategory.ADD_PATTERN_DETECTOR: 0.4,
    ChangeCategory.MODIFY_CONTRACT: 0.7,
    ChangeCategory.MODIFY_CYCLE_TIMING: 0.5,
    ChangeCategory.CHANGE_CONSOLIDATION: 0.6,
    ChangeCategory.ADD_SYSTEM_CAPABILITY: 0.9,
}

# Impact heuristics by category (0.0-1.0 scale)
_CATEGORY_IMPACT: dict[ChangeCategory, float] = {
    ChangeCategory.ADJUST_BUDGET: 0.3,
    ChangeCategory.ADD_EXECUTOR: 0.6,
    ChangeCategory.ADD_INPUT_CHANNEL: 0.7,
    ChangeCategory.ADD_PATTERN_DETECTOR: 0.5,
    ChangeCategory.MODIFY_CONTRACT: 0.8,
    ChangeCategory.MODIFY_CYCLE_TIMING: 0.4,
    ChangeCategory.CHANGE_CONSOLIDATION: 0.5,
    ChangeCategory.ADD_SYSTEM_CAPABILITY: 0.9,
}

# Minimum description prefix length for exact dedup matching
_DEDUP_PREFIX_LEN: int = 50

# Minimum proposals before triggering LLM-based dedup
_LLM_DEDUP_THRESHOLD: int = 5


class ProposalIntelligence:
    """
    Smart proposal management for Simula.

    Provides deduplication, prioritization, dependency analysis,
    and cost estimation — all optimized for minimal token usage.
    """

    def __init__(
        self,
        llm: LLMProvider | None = None,
        analytics: EvolutionAnalyticsEngine | None = None,
    ) -> None:
        self._llm = llm
        self._analytics = analytics
        self._log = logger

    # ─── Prioritization ──────────────────────────────────────────────────────

    async def prioritize(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalPriority]:
        """
        Score and rank proposals by:
          priority = evidence_strength * expected_impact / max(0.1, risk * cost)

        Pure heuristic scoring — zero LLM tokens.
        Proposals with higher scores should be processed first.
        """
        priorities: list[ProposalPriority] = []

        for proposal in proposals:
            evidence_strength = self._compute_evidence_strength(proposal)
            expected_impact = _CATEGORY_IMPACT.get(proposal.category, 0.5)
            estimated_risk = self._compute_risk_estimate(proposal)
            estimated_cost = self.estimate_cost(proposal)

            # Priority formula
            denominator = max(0.1, estimated_risk * estimated_cost)
            score = (evidence_strength * expected_impact) / denominator

            # Boost for proposals already partially processed
            if proposal.status == ProposalStatus.APPROVED:
                score *= 1.5

            reasoning = (
                f"evidence={evidence_strength:.2f}, impact={expected_impact:.2f}, "
                f"risk={estimated_risk:.2f}, cost={estimated_cost:.2f}"
            )

            priorities.append(ProposalPriority(
                proposal_id=proposal.id,
                priority_score=round(score, 3),
                evidence_strength=round(evidence_strength, 3),
                expected_impact=round(expected_impact, 3),
                estimated_risk=round(estimated_risk, 3),
                estimated_cost=round(estimated_cost, 3),
                reasoning=reasoning,
            ))

        # Sort by score descending
        priorities.sort(key=lambda p: p.priority_score, reverse=True)

        self._log.info(
            "proposals_prioritized",
            count=len(priorities),
            top_score=priorities[0].priority_score if priorities else 0.0,
        )
        return priorities

    def _compute_evidence_strength(self, proposal: EvolutionProposal) -> float:
        """
        Compute evidence strength from the proposal's evidence list.
        More evidence items = stronger signal. Capped at 1.0.
        """
        count = len(proposal.evidence)
        if count == 0:
            return 0.2  # minimal evidence
        # Logarithmic scaling: 1 item = 0.3, 5 items = 0.7, 10+ items = 0.9+
        import math
        return min(1.0, 0.2 + 0.3 * math.log1p(count))

    def _compute_risk_estimate(self, proposal: EvolutionProposal) -> float:
        """
        Estimate risk from simulation results and analytics history.
        Returns 0.0-1.0 scale.
        """
        # If simulation has run, use its risk level
        if proposal.simulation is not None:
            risk_map = {
                RiskLevel.LOW: 0.15,
                RiskLevel.MODERATE: 0.4,
                RiskLevel.HIGH: 0.7,
                RiskLevel.UNACCEPTABLE: 1.0,
            }
            return risk_map.get(proposal.simulation.risk_level, 0.4)

        # Otherwise, use category-based heuristic
        # Higher-impact categories carry more risk
        return _CATEGORY_COST.get(proposal.category, 0.5)

    # ─── Deduplication ───────────────────────────────────────────────────────

    async def deduplicate(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalCluster]:
        """
        Detect semantically similar proposals in three tiers:
          Tier 1: Exact description prefix match (zero cost)
          Tier 2: Category + affected_systems overlap (zero cost)
          Tier 3: LLM similarity check (only if >5 proposals, ~300 tokens)

        Returns clusters where member proposals could be merged.
        """
        if len(proposals) < 2:
            return []

        clusters: list[ProposalCluster] = []
        clustered_ids: set[str] = set()

        # Tier 1: Exact description prefix match
        prefix_groups: dict[str, list[EvolutionProposal]] = {}
        for p in proposals:
            prefix = p.description[:_DEDUP_PREFIX_LEN].lower().strip()
            prefix_groups.setdefault(prefix, []).append(p)

        for prefix, group in prefix_groups.items():
            if len(group) < 2:
                continue
            rep = group[0]
            members = [p.id for p in group]
            clustered_ids.update(members)
            clusters.append(ProposalCluster(
                representative_id=rep.id,
                member_ids=members,
                similarity_scores=[1.0] * len(members),
                merge_recommendation=f"Identical prefix: '{prefix[:30]}...'",
            ))

        # Tier 2: Category + affected_systems overlap
        unclustered = [p for p in proposals if p.id not in clustered_ids]
        cat_system_groups: dict[str, list[EvolutionProposal]] = {}
        for p in unclustered:
            key = f"{p.category.value}::{','.join(sorted(p.change_spec.affected_systems))}"
            cat_system_groups.setdefault(key, []).append(p)

        for key, group in cat_system_groups.items():
            if len(group) < 2:
                continue
            rep = group[0]
            members = [p.id for p in group]
            clustered_ids.update(members)
            clusters.append(ProposalCluster(
                representative_id=rep.id,
                member_ids=members,
                similarity_scores=[0.7] * len(members),
                merge_recommendation=f"Same category and affected systems: {key}",
            ))

        # Tier 3: LLM-based similarity (expensive, only for many proposals)
        still_unclustered = [p for p in proposals if p.id not in clustered_ids]
        if len(still_unclustered) >= _LLM_DEDUP_THRESHOLD and self._llm is not None:
            llm_clusters = await self._llm_deduplicate(still_unclustered)
            clusters.extend(llm_clusters)

        if clusters:
            self._log.info(
                "dedup_complete",
                clusters=len(clusters),
                total_duplicates=sum(len(c.member_ids) for c in clusters),
            )
        return clusters

    async def _llm_deduplicate(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalCluster]:
        """LLM-based semantic similarity check. ~300 tokens."""
        descriptions = "\n".join(
            f"{i+1}. [{p.id[:8]}] {p.category.value}: {p.description[:100]}"
            for i, p in enumerate(proposals[:10])
        )

        prompt = (
            "Below are evolution proposals for an AI system. "
            "Identify any that are semantically similar enough to be duplicates.\n\n"
            f"{descriptions}\n\n"
            "Reply with groups of similar proposals by their numbers.\n"
            "Format: GROUP: 1, 3 (reason)\n"
            "If no duplicates found, reply: NONE"
        )

        try:
            import asyncio
            response = await asyncio.wait_for(
                self._llm.evaluate(prompt=prompt, max_tokens=200, temperature=0.1),
                timeout=8.0,
            )

            clusters: list[ProposalCluster] = []
            for line in response.text.strip().splitlines():
                line = line.strip()
                if line.upper() == "NONE" or "GROUP" not in line.upper():
                    continue
                # Parse "GROUP: 1, 3 (similar feature additions)"
                try:
                    _, nums_part = line.split(":", 1)
                    reason_start = nums_part.find("(")
                    if reason_start > 0:
                        reason = nums_part[reason_start:].strip("() ")
                        nums_part = nums_part[:reason_start]
                    else:
                        reason = ""

                    indices = [int(n.strip()) - 1 for n in nums_part.split(",") if n.strip().isdigit()]
                    valid = [i for i in indices if 0 <= i < len(proposals)]
                    if len(valid) >= 2:
                        members = [proposals[i].id for i in valid]
                        clusters.append(ProposalCluster(
                            representative_id=members[0],
                            member_ids=members,
                            similarity_scores=[0.6] * len(members),
                            merge_recommendation=reason or "LLM-detected similarity",
                        ))
                except (ValueError, IndexError):
                    continue

            return clusters
        except Exception as exc:
            self._log.warning("llm_dedup_failed", error=str(exc))
            return []

    # ─── Dependency Analysis ─────────────────────────────────────────────────

    async def analyze_dependencies(
        self, proposals: list[EvolutionProposal],
    ) -> list[tuple[str, str, str]]:
        """
        Detect ordering dependencies between proposals.
        Returns list of (before_id, after_id, reason) tuples.

        Rule-based analysis — zero LLM tokens:
        - ADD_EXECUTOR should come before MODIFY_CONTRACT referencing axon
        - ADD_INPUT_CHANNEL before MODIFY_CONTRACT referencing atune
        - ADJUST_BUDGET after the thing it's budgeting for is added
        - ADD_SYSTEM_CAPABILITY is a superset that depends on components
        """
        if len(proposals) < 2:
            return []

        dependencies: list[tuple[str, str, str]] = []

        # Build lookup
        additive = [p for p in proposals if p.category in {
            ChangeCategory.ADD_EXECUTOR,
            ChangeCategory.ADD_INPUT_CHANNEL,
            ChangeCategory.ADD_PATTERN_DETECTOR,
        }]
        contracts = [p for p in proposals if p.category == ChangeCategory.MODIFY_CONTRACT]
        capabilities = [p for p in proposals if p.category == ChangeCategory.ADD_SYSTEM_CAPABILITY]
        budgets = [p for p in proposals if p.category == ChangeCategory.ADJUST_BUDGET]

        # Additive changes should come before contract modifications
        # that reference the same system
        for add_p in additive:
            add_systems = set(add_p.change_spec.affected_systems)
            for contract_p in contracts:
                contract_systems = set(contract_p.change_spec.affected_systems)
                overlap = add_systems & contract_systems
                if overlap:
                    dependencies.append((
                        add_p.id,
                        contract_p.id,
                        f"Add {add_p.category.value} before modifying contracts for {overlap}",
                    ))

        # Additive changes should come before capability additions
        for add_p in additive:
            for cap_p in capabilities:
                cap_systems = set(cap_p.change_spec.affected_systems)
                add_systems = set(add_p.change_spec.affected_systems)
                if cap_systems & add_systems:
                    dependencies.append((
                        add_p.id,
                        cap_p.id,
                        f"Add component before adding system capability",
                    ))

        # Budget changes should come after the thing they budget for
        for budget_p in budgets:
            param = budget_p.change_spec.budget_parameter or ""
            for add_p in additive:
                # If the budget parameter references the additive system
                add_systems = add_p.change_spec.affected_systems
                for sys in add_systems:
                    if sys in param:
                        dependencies.append((
                            add_p.id,
                            budget_p.id,
                            f"Add component before adjusting its budget ({param})",
                        ))

        if dependencies:
            self._log.info(
                "dependencies_detected",
                count=len(dependencies),
            )
        return dependencies

    # ─── Cost Estimation ─────────────────────────────────────────────────────

    def estimate_cost(self, proposal: EvolutionProposal) -> float:
        """
        Heuristic cost estimation (0.0-1.0 scale).
        Zero LLM tokens — pure lookup + adjustment.
        """
        base_cost = _CATEGORY_COST.get(proposal.category, 0.5)

        # Adjust for complexity signals
        spec = proposal.change_spec
        if spec.affected_systems and len(spec.affected_systems) > 1:
            base_cost = min(1.0, base_cost + 0.1 * (len(spec.affected_systems) - 1))

        if spec.contract_changes and len(spec.contract_changes) > 2:
            base_cost = min(1.0, base_cost + 0.1)

        return round(base_cost, 2)

    # ─── Duplicate Detection Helper ──────────────────────────────────────────

    def is_duplicate(
        self,
        proposal: EvolutionProposal,
        clusters: list[ProposalCluster],
    ) -> bool:
        """
        Check if a proposal appears in any cluster as a non-representative member.
        If it's in a cluster but not the representative, it's a duplicate.
        """
        for cluster in clusters:
            if proposal.id in cluster.member_ids and proposal.id != cluster.representative_id:
                return True
        return False

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\rollback.py =====

"""
EcodiaOS -- Simula Rollback Manager

Before any change is applied, RollbackManager snapshots all files
that might be modified. If the post-apply health check fails -- or if
any exception occurs during application -- the manager restores the
codebase to its pre-change state.

Rollback target: <=2s (from spec).
"""

from __future__ import annotations

from pathlib import Path

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.simula.types import ConfigSnapshot, FileSnapshot

logger = structlog.get_logger().bind(system="simula.rollback")


class RollbackError(RuntimeError):
    """Raised when restoring files to their pre-change state fails."""
    pass


class RollbackManager:
    """
    Captures file snapshots before structural changes are applied
    and restores them if the post-apply health check fails.
    """

    def __init__(self, codebase_root: Path) -> None:
        self._root = codebase_root
        self._log = logger

    async def snapshot(self, proposal_id: str, paths: list[Path]) -> ConfigSnapshot:
        """""""""
        Read each file's current content and package into a ConfigSnapshot.
        Files that do not exist are recorded with existed=False so rollback
        knows to delete them rather than restore content.
        """""""""
        snapshots: list[FileSnapshot] = []
        for path in paths:
            abs_path = path if path.is_absolute() else self._root / path
            content = await self._read_file_safe(abs_path)
            existed = content is not None
            snapshots.append(
                FileSnapshot(
                    path=str(abs_path),
                    content=content,
                    existed=existed,
                )
            )
            self._log.debug(
                "snapshot_captured",
                path=str(abs_path),
                existed=existed,
                size=len(content) if content else 0,
            )

        # We need a config_version from outside context; use 0 as placeholder.
        # The service layer will update this before persisting.
        cfg_snapshot = ConfigSnapshot(
            proposal_id=proposal_id,
            files=snapshots,
            config_version=0,
        )
        self._log.info(
            "snapshot_complete",
            proposal_id=proposal_id,
            files_captured=len(snapshots),
        )
        return cfg_snapshot

    async def restore(self, snapshot: ConfigSnapshot) -> list[str]:
        """""""""
        Restore all files to the state captured in the snapshot.
        Files that did not exist before are deleted.
        Returns the list of absolute paths that were restored.
        Raises RollbackError if any restore fails.
        """""""""
        restored: list[str] = []
        errors: list[str] = []

        for file_snap in snapshot.files:
            path = Path(file_snap.path)
            try:
                if not file_snap.existed:
                    # File was created by the change -- delete it
                    if path.exists():
                        path.unlink()
                        self._log.info("rollback_deleted", path=str(path))
                elif file_snap.content is not None:
                    path.parent.mkdir(parents=True, exist_ok=True)
                    path.write_text(file_snap.content, encoding="utf-8")
                    self._log.info("rollback_restored", path=str(path))
                restored.append(str(path))
            except Exception as exc:
                msg = f"Failed to restore {path}: {exc}"
                self._log.error("rollback_restore_failed", path=str(path), error=str(exc))
                errors.append(msg)

        if errors:
            raise RollbackError("Rollback incomplete. Failures: " + str(errors))

        self._log.info(
            "rollback_complete",
            proposal_id=snapshot.proposal_id,
            files_restored=len(restored),
        )
        return restored

    async def _read_file_safe(self, path: Path) -> str | None:
        """""""""
        Read file content; return None if file does not exist.
        """""""""
        try:
            return path.read_text(encoding="utf-8")
        except FileNotFoundError:
            return None
        except Exception as exc:
            self._log.warning("snapshot_read_failed", path=str(path), error=str(exc))
            return None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\service.py =====

"""
EcodiaOS — Simula Service

The self-evolution system. Simula is the organism's capacity for
metamorphosis: structural change beyond parameter tuning.

Where Evo adjusts the knobs, Simula redesigns the dashboard.

Simula coordinates the full evolution proposal pipeline:
  1. DEDUPLICATE — check for duplicate/similar active proposals
  2. VALIDATE    — reject forbidden categories immediately
  3. SIMULATE    — deep multi-strategy impact prediction
  4. GATE        — route governed changes through community governance
  5. APPLY       — invoke the code agent or config updater with rollback
  6. VERIFY      — health check post-application
  7. RECORD      — write immutable history, increment version, update analytics

Interfaces:
  initialize()            — build sub-systems, load current version
  process_proposal()      — main entry point for rich proposals
  receive_evo_proposal()  — receive from Evo via bridge translation
  get_history()           — recent evolution records
  get_current_version()   — current config version number
  get_analytics()         — evolution quality metrics
  shutdown()              — graceful teardown
  stats                   — service-level metrics

Iron Rules (never violated — see SIMULA_IRON_RULES in types.py):
  - Cannot modify Equor, constitutional drives, invariants
  - Cannot modify its own logic
  - Must simulate before applying any change
  - Must maintain rollback capability
  - Evolution history is immutable
"""

from __future__ import annotations

import hashlib
from pathlib import Path
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.clients.llm import LLMProvider
from ecodiaos.config import SimulaConfig
from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine
from ecodiaos.systems.simula.applicator import ChangeApplicator
from ecodiaos.systems.simula.bridge import EvoSimulaBridge
from ecodiaos.systems.simula.code_agent import SimulaCodeAgent
from ecodiaos.systems.simula.health import HealthChecker
from ecodiaos.systems.simula.history import EvolutionHistoryManager
from ecodiaos.systems.simula.proposal_intelligence import ProposalIntelligence
from ecodiaos.systems.simula.rollback import RollbackManager
from ecodiaos.systems.simula.simulation import ChangeSimulator
from ecodiaos.systems.simula.types import (
    FORBIDDEN,
    GOVERNANCE_REQUIRED,
    ConfigVersion,
    EnrichedSimulationResult,
    EvolutionAnalytics,
    EvolutionRecord,
    EvolutionProposal,
    ProposalResult,
    ProposalStatus,
    RiskLevel,
    SimulationResult,
    TriageResult,
    TriageStatus,
)
from ecodiaos.primitives.common import new_id, utc_now

if TYPE_CHECKING:
    from ecodiaos.clients.neo4j import Neo4jClient
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()


class SimulaService:
    """
    Simula — the EOS self-evolution system.

    Coordinates eight sub-systems:
      ChangeSimulator           — deep multi-strategy impact prediction
      SimulaCodeAgent           — Claude-backed code generation with 11 tools
      ChangeApplicator          — routes proposals to the right application strategy
      RollbackManager           — file snapshots and restore
      EvolutionHistoryManager   — immutable Neo4j history
      EvoSimulaBridge           — Evo→Simula proposal translation
      ProposalIntelligence      — deduplication, prioritization, dependency analysis
      EvolutionAnalyticsEngine  — evolution quality tracking
    """

    system_id: str = "simula"

    def __init__(
        self,
        config: SimulaConfig,
        llm: LLMProvider,
        neo4j: Neo4jClient | None = None,
        memory: MemoryService | None = None,
        codebase_root: Path | None = None,
        instance_name: str = "EOS",
    ) -> None:
        self._config = config
        self._llm = llm
        self._neo4j = neo4j
        self._memory = memory
        self._root = codebase_root or Path(config.codebase_root).resolve()
        self._instance_name = instance_name
        self._initialized: bool = False
        self._logger = logger.bind(system="simula")

        # Sub-systems (built in initialize())
        self._simulator: ChangeSimulator | None = None
        self._code_agent: SimulaCodeAgent | None = None
        self._applicator: ChangeApplicator | None = None
        self._rollback: RollbackManager | None = None
        self._history: EvolutionHistoryManager | None = None
        self._health: HealthChecker | None = None
        self._bridge: EvoSimulaBridge | None = None
        self._intelligence: ProposalIntelligence | None = None
        self._analytics: EvolutionAnalyticsEngine | None = None

        # State
        self._current_version: int = 0
        self._active_proposals: dict[str, EvolutionProposal] = {}

        # Metrics
        self._proposals_received: int = 0
        self._proposals_approved: int = 0
        self._proposals_rejected: int = 0
        self._proposals_rolled_back: int = 0
        self._proposals_awaiting_governance: int = 0
        self._proposals_deduplicated: int = 0

    # ─── Lifecycle ─────────────────────────────────────────────────────────────

    async def initialize(self) -> None:
        """
        Build all sub-systems and load current config version from history.
        Must be called before any other method.
        """
        if self._initialized:
            return

        # Build the rollback manager
        self._rollback = RollbackManager(codebase_root=self._root)

        # Build the health checker
        self._health = HealthChecker(
            codebase_root=self._root,
            test_command=self._config.test_command,
        )

        # Build the code agent (uses the configured model, not the shared LLM)
        code_agent_llm = self._llm
        self._code_agent = SimulaCodeAgent(
            llm=code_agent_llm,
            codebase_root=self._root,
            max_turns=self._config.max_code_agent_turns,
        )

        # Build the applicator
        self._applicator = ChangeApplicator(
            code_agent=self._code_agent,
            rollback_manager=self._rollback,
            health_checker=self._health,
            codebase_root=self._root,
        )

        # Build the history manager (requires Neo4j)
        if self._neo4j is not None:
            self._history = EvolutionHistoryManager(neo4j=self._neo4j)
            self._current_version = await self._history.get_current_version()
        else:
            self._logger.warning(
                "simula_no_neo4j",
                message="Evolution history will not be persisted (no Neo4j client)",
            )
            self._current_version = 0

        # Build the analytics engine (depends on history)
        self._analytics = EvolutionAnalyticsEngine(history=self._history)

        # Build the deep simulator (depends on analytics for dynamic caution)
        self._simulator = ChangeSimulator(
            config=self._config,
            llm=self._llm,
            memory=self._memory,
            analytics=self._analytics,
            codebase_root=self._root,
        )

        # Build the Evo↔Simula bridge
        self._bridge = EvoSimulaBridge(
            llm=self._llm,
            memory=self._memory,
        )

        # Build the proposal intelligence layer
        self._intelligence = ProposalIntelligence(
            llm=self._llm,
            analytics=self._analytics,
        )

        # Pre-compute analytics from history
        if self._history is not None:
            try:
                await self._analytics.compute_analytics()
            except Exception as exc:
                self._logger.warning("initial_analytics_failed", error=str(exc))

        self._initialized = True
        self._logger.info(
            "simula_initialized",
            current_version=self._current_version,
            codebase_root=str(self._root),
            max_code_agent_turns=self._config.max_code_agent_turns,
            subsystems=[
                "simulator", "code_agent", "applicator", "rollback",
                "health", "bridge", "intelligence", "analytics",
                "history" if self._history else "history(disabled)",
            ],
        )

    async def shutdown(self) -> None:
        """Graceful shutdown."""
        self._logger.info(
            "simula_shutdown",
            proposals_received=self._proposals_received,
            proposals_approved=self._proposals_approved,
            proposals_rejected=self._proposals_rejected,
            proposals_rolled_back=self._proposals_rolled_back,
            proposals_deduplicated=self._proposals_deduplicated,
            current_version=self._current_version,
        )

    # ─── Triage (Fast-Path Pre-Simulation) ─────────────────────────────────────

    def _triage_proposal(self, proposal: EvolutionProposal) -> TriageResult:
        """
        Fast-path proposal check. If trivial, skip expensive simulation.
        Trivial = budget tweaks <5% with sufficient data.

        Returns TriageResult with skip_simulation=True for trivial cases.
        """
        if proposal.category.value != "adjust_budget":
            return TriageResult(
                status=TriageStatus.REQUIRES_SIMULATION,
                skip_simulation=False,
            )

        spec = proposal.change_spec
        if not spec.budget_new_value or spec.budget_old_value is None:
            return TriageResult(
                status=TriageStatus.REQUIRES_SIMULATION,
                skip_simulation=False,
            )

        # Check delta < 5%
        old_val = spec.budget_old_value
        new_val = spec.budget_new_value
        if old_val == 0.0:
            delta_pct = 1.0  # Treat zero as 100% change
        else:
            delta_pct = abs(new_val - old_val) / abs(old_val)

        if delta_pct < 0.05:
            self._logger.info(
                "proposal_triaged",
                proposal_id=proposal.id,
                status="trivial",
                reason=f"Budget delta {delta_pct:.1%} < 5%",
            )
            return TriageResult(
                status=TriageStatus.TRIVIAL,
                assumed_risk=RiskLevel.LOW,
                reason=f"Budget delta {delta_pct:.1%} < 5%",
                skip_simulation=True,
            )

        return TriageResult(
            status=TriageStatus.REQUIRES_SIMULATION,
            skip_simulation=False,
        )

    # ─── Main Pipeline ─────────────────────────────────────────────────────────

    async def process_proposal(self, proposal: EvolutionProposal) -> ProposalResult:
        """
        Main entry point for evolution proposals.

        Pipeline:
          DEDUP → VALIDATE → SIMULATE → [GOVERNANCE GATE] → APPLY → VERIFY → RECORD

        Spec reference: Section III.3.2
        Performance target: validation ≤50ms, simulation ≤30s, apply ≤5s
        """
        self._proposals_received += 1
        log = self._logger.bind(proposal_id=proposal.id, category=proposal.category.value)
        log.info("proposal_received", source=proposal.source, description=proposal.description[:100])

        # ── STEP 0: Deduplication ────────────────────────────────────────────
        if self._intelligence is not None and self._active_proposals:
            try:
                all_proposals = [proposal] + list(self._active_proposals.values())
                clusters = await self._intelligence.deduplicate(all_proposals)
                if self._intelligence.is_duplicate(proposal, clusters):
                    self._proposals_deduplicated += 1
                    log.info("proposal_deduplicated")
                    return ProposalResult(
                        status=ProposalStatus.REJECTED,
                        reason="Duplicate of an active proposal",
                    )
            except Exception as exc:
                log.warning("dedup_check_failed", error=str(exc))

        self._active_proposals[proposal.id] = proposal

        # ── STEP 1: Validate ────────────────────────────────────────────────
        if proposal.category in FORBIDDEN:
            proposal.status = ProposalStatus.REJECTED
            self._proposals_rejected += 1
            reason = (
                f"Category '{proposal.category.value}' is forbidden for Simula. "
                f"Iron rule: {self._get_iron_rule_for(proposal)}"
            )
            log.warning("proposal_rejected_forbidden", reason=reason)
            self._active_proposals.pop(proposal.id, None)
            return ProposalResult(status=ProposalStatus.REJECTED, reason=reason)

        # ── STEP 1.5: Triage (fast-path for trivial cases) ──────────────────
        triage = self._triage_proposal(proposal)
        if triage.skip_simulation:
            # Build synthetic simulation result
            proposal.simulation = SimulationResult(
                episodes_tested=0,
                risk_level=triage.assumed_risk or RiskLevel.LOW,
                risk_summary=f"Triaged as trivial: {triage.reason}",
                benefit_summary=proposal.expected_benefit,
            )
            log.info("proposal_triaged_skipping_simulation", reason=triage.reason)
            # Skip STEP 2 (Simulate) and proceed directly to governance/apply

        # ── STEP 2: Simulate (deep multi-strategy) ─────────────────────────
        # Skip if already triaged (has synthetic simulation)
        if proposal.simulation is None:
            proposal.status = ProposalStatus.SIMULATING
            log.info("proposal_simulating")

            try:
                simulation = await self._simulate_change(proposal)
                proposal.simulation = simulation
            except Exception as exc:
                proposal.status = ProposalStatus.REJECTED
                self._proposals_rejected += 1
                reason = f"Simulation failed: {exc}"
                log.error("simulation_error", error=str(exc))
                self._active_proposals.pop(proposal.id, None)
                return ProposalResult(status=ProposalStatus.REJECTED, reason=reason)
        else:
            simulation = proposal.simulation

        if simulation.risk_level == RiskLevel.UNACCEPTABLE:
            proposal.status = ProposalStatus.REJECTED
            self._proposals_rejected += 1
            reason = f"Simulation shows unacceptable risk: {simulation.risk_summary}"
            log.warning("proposal_rejected_risk", risk_level=simulation.risk_level.value)
            self._active_proposals.pop(proposal.id, None)
            return ProposalResult(status=ProposalStatus.REJECTED, reason=reason)

        # ── STEP 3: Governance gate ─────────────────────────────────────────
        if self.requires_governance(proposal):
            proposal.status = ProposalStatus.AWAITING_GOVERNANCE
            self._proposals_awaiting_governance += 1
            try:
                governance_id = await self._submit_to_governance(proposal, simulation)
                proposal.governance_record_id = governance_id
            except Exception as exc:
                log.error("governance_submission_error", error=str(exc))
                governance_id = f"gov_{new_id()}"
                proposal.governance_record_id = governance_id

            log.info("proposal_awaiting_governance", governance_id=governance_id)
            return ProposalResult(
                status=ProposalStatus.AWAITING_GOVERNANCE,
                governance_record_id=governance_id,
            )

        # ── STEP 4: Apply (self-applicable changes only) ───────────────────
        return await self._apply_change(proposal)

    async def receive_evo_proposal(
        self,
        evo_description: str,
        evo_rationale: str,
        hypothesis_ids: list[str],
        hypothesis_statements: list[str],
        evidence_scores: list[float],
        supporting_episode_ids: list[str],
        mutation_target: str = "",
        mutation_type: str = "",
    ) -> ProposalResult:
        """
        Receive a proposal from Evo via the bridge.
        Translates the lightweight Evo proposal into Simula's rich format,
        then feeds it into the main pipeline.

        This is the public API that Evo's ConsolidationOrchestrator calls.
        """
        if self._bridge is None:
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason="Simula bridge not initialized",
            )

        self._logger.info(
            "evo_proposal_received",
            description=evo_description[:80],
            hypotheses=len(hypothesis_ids),
        )

        try:
            translated = await self._bridge.translate_proposal(
                evo_description=evo_description,
                evo_rationale=evo_rationale,
                hypothesis_ids=hypothesis_ids,
                hypothesis_statements=hypothesis_statements,
                evidence_scores=evidence_scores,
                supporting_episode_ids=supporting_episode_ids,
                mutation_target=mutation_target,
                mutation_type=mutation_type,
            )
        except Exception as exc:
            self._logger.error("bridge_translation_failed", error=str(exc))
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Bridge translation failed: {exc}",
            )

        return await self.process_proposal(translated)

    async def approve_governed_proposal(
        self, proposal_id: str, governance_record_id: str
    ) -> ProposalResult:
        """
        Called when a governed proposal receives community approval.
        Resumes the pipeline from the application step.
        """
        proposal = self._active_proposals.get(proposal_id)
        if proposal is None:
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Proposal {proposal_id} not found in active proposals",
            )
        if proposal.status != ProposalStatus.AWAITING_GOVERNANCE:
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Proposal {proposal_id} is not awaiting governance (status: {proposal.status})",
            )

        proposal.status = ProposalStatus.APPROVED
        self._proposals_awaiting_governance = max(0, self._proposals_awaiting_governance - 1)
        self._logger.info("governed_proposal_approved", proposal_id=proposal_id)
        return await self._apply_change(proposal)

    def requires_governance(self, proposal: EvolutionProposal) -> bool:
        """Changes in the GOVERNANCE_REQUIRED category always need governance."""
        return proposal.category in GOVERNANCE_REQUIRED

    # ─── Query Interface ───────────────────────────────────────────────────────

    async def get_history(self, limit: int = 50) -> list[EvolutionRecord]:
        """Return the most recent evolution records."""
        if self._history is None:
            return []
        return await self._history.get_history(limit=limit)

    async def get_current_version(self) -> int:
        """Return the current config version number."""
        return self._current_version

    async def get_version_chain(self) -> list[ConfigVersion]:
        """Return the full version history chain."""
        if self._history is None:
            return []
        return await self._history.get_version_chain()

    def get_active_proposals(self) -> list[EvolutionProposal]:
        """Return all proposals currently in the pipeline."""
        return list(self._active_proposals.values())

    async def get_analytics(self) -> EvolutionAnalytics:
        """Return current evolution quality analytics."""
        if self._analytics is None:
            return EvolutionAnalytics()
        return await self._analytics.compute_analytics()

    async def get_prioritized_proposals(self) -> list[dict[str, Any]]:
        """Return active proposals ranked by priority score."""
        if self._intelligence is None or not self._active_proposals:
            return []
        priorities = await self._intelligence.prioritize(list(self._active_proposals.values()))
        return [p.model_dump() for p in priorities]

    # ─── Stats ────────────────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        base: dict[str, Any] = {
            "initialized": self._initialized,
            "current_version": self._current_version,
            "proposals_received": self._proposals_received,
            "proposals_approved": self._proposals_approved,
            "proposals_rejected": self._proposals_rejected,
            "proposals_rolled_back": self._proposals_rolled_back,
            "proposals_deduplicated": self._proposals_deduplicated,
            "proposals_awaiting_governance": self._proposals_awaiting_governance,
            "active_proposals": len(self._active_proposals),
        }

        # Include cached analytics summary if available
        if self._analytics is not None and self._analytics._cached_analytics is not None:
            analytics = self._analytics._cached_analytics
            base["analytics"] = {
                "total_proposals": analytics.total_proposals,
                "evolution_velocity": analytics.evolution_velocity,
                "rollback_rate": analytics.rollback_rate,
                "mean_simulation_risk": analytics.mean_simulation_risk,
            }

        return base

    # ─── Evo Bridge Callback ──────────────────────────────────────────────────

    def get_evo_callback(self) -> Any:
        """
        Return a callback function for Evo's ConsolidationOrchestrator.
        This is wired during system initialization in main.py.

        The callback signature matches what Evo Phase 8 expects:
          async def callback(evo_proposal, hypotheses) -> ProposalResult
        """
        async def _evo_callback(evo_proposal: Any, hypotheses: list[Any]) -> ProposalResult:
            # Extract fields from Evo's lightweight types
            hypothesis_ids = [getattr(h, "id", "") for h in hypotheses]
            hypothesis_statements = [getattr(h, "statement", "") for h in hypotheses]
            evidence_scores = [getattr(h, "evidence_score", 0.0) for h in hypotheses]

            # Collect all supporting episode IDs across hypotheses
            episode_ids: list[str] = []
            for h in hypotheses:
                episode_ids.extend(getattr(h, "supporting_episodes", []))

            # Extract mutation info if available
            mutation_target = ""
            mutation_type = ""
            for h in hypotheses:
                mutation = getattr(h, "proposed_mutation", None)
                if mutation is not None:
                    mutation_target = getattr(mutation, "target", "")
                    mutation_type = getattr(mutation, "type", "")
                    if hasattr(mutation_type, "value"):
                        mutation_type = mutation_type.value
                    break

            return await self.receive_evo_proposal(
                evo_description=getattr(evo_proposal, "description", ""),
                evo_rationale=getattr(evo_proposal, "rationale", ""),
                hypothesis_ids=hypothesis_ids,
                hypothesis_statements=hypothesis_statements,
                evidence_scores=evidence_scores,
                supporting_episode_ids=episode_ids,
                mutation_target=mutation_target,
                mutation_type=mutation_type,
            )

        return _evo_callback

    # ─── Private: Application ──────────────────────────────────────────────────

    async def _apply_change(self, proposal: EvolutionProposal) -> ProposalResult:
        """
        Apply a validated, simulated, approved proposal.
        Includes health check and automatic rollback on failure.
        """
        assert self._applicator is not None
        assert self._health is not None
        assert self._rollback is not None

        proposal.status = ProposalStatus.APPLYING
        log = self._logger.bind(proposal_id=proposal.id, category=proposal.category.value)
        log.info("applying_change")

        code_result, snapshot = await self._applicator.apply(proposal)

        if not code_result.success:
            proposal.status = ProposalStatus.ROLLED_BACK
            self._proposals_rolled_back += 1
            log.warning("apply_failed_no_success", error=code_result.error)
            self._active_proposals.pop(proposal.id, None)
            self._invalidate_analytics()
            return ProposalResult(
                status=ProposalStatus.ROLLED_BACK,
                reason=f"Application failed: {code_result.error}",
            )

        # ── Health check ──────────────────────────────────────────────────────
        health = await self._health.check(code_result.files_written)

        if not health.healthy:
            # Rollback
            log.warning("health_check_failed_rolling_back", issues=health.issues)
            await self._rollback.restore(snapshot)
            proposal.status = ProposalStatus.ROLLED_BACK
            self._proposals_rolled_back += 1

            # Record the rollback in history
            await self._record_evolution(proposal, code_result.files_written, rolled_back=True,
                                          rollback_reason="; ".join(health.issues))

            self._active_proposals.pop(proposal.id, None)
            self._invalidate_analytics()
            return ProposalResult(
                status=ProposalStatus.ROLLED_BACK,
                reason=f"Post-apply health check failed: {'; '.join(health.issues)}",
            )

        # ── Success ───────────────────────────────────────────────────────────
        proposal.status = ProposalStatus.APPLIED
        self._proposals_approved += 1

        from_version = self._current_version
        self._current_version += 1

        await self._record_evolution(
            proposal,
            code_result.files_written,
            rolled_back=False,
        )

        # Clean up active proposals
        self._active_proposals.pop(proposal.id, None)
        self._invalidate_analytics()

        log.info(
            "change_applied",
            from_version=from_version,
            to_version=self._current_version,
            files_changed=len(code_result.files_written),
        )

        return ProposalResult(
            status=ProposalStatus.APPLIED,
            version=self._current_version,
            files_changed=code_result.files_written,
        )

    async def _simulate_change(self, proposal: EvolutionProposal) -> SimulationResult:
        """Delegate to the deep ChangeSimulator."""
        if self._simulator is None:
            return SimulationResult(risk_level=RiskLevel.LOW, risk_summary="Simulator not initialized")
        return await self._simulator.simulate(proposal)

    async def _submit_to_governance(
        self, proposal: EvolutionProposal, simulation: SimulationResult
    ) -> str:
        """
        Submit a governed proposal to the community governance system.
        Returns a governance record ID. Enriches the governance record
        with deep simulation data for community review.
        """
        record_id = f"gov_{new_id()}"

        if self._neo4j is not None:
            try:
                # Include enriched simulation data for governance reviewers
                risk_summary = simulation.risk_summary
                benefit_summary = simulation.benefit_summary

                # Add counterfactual and alignment data if available (enriched simulation)
                enrichment = []
                if isinstance(simulation, EnrichedSimulationResult):
                    if simulation.constitutional_alignment != 0.0:
                        enrichment.append(f"Constitutional alignment: {simulation.constitutional_alignment:+.2f}")
                    if simulation.dependency_blast_radius > 0:
                        enrichment.append(f"Blast radius: {simulation.dependency_blast_radius} files")
                if enrichment:
                    risk_summary = f"{risk_summary} [{'; '.join(enrichment)}]"

                await self._neo4j.execute(
                    """
                    CREATE (:GovernanceProposal {
                        id: $id,
                        proposal_id: $proposal_id,
                        category: $category,
                        description: $description,
                        risk_level: $risk_level,
                        risk_summary: $risk_summary,
                        benefit_summary: $benefit_summary,
                        submitted_at: $submitted_at,
                        status: 'pending'
                    })
                    """,
                    {
                        "id": record_id,
                        "proposal_id": proposal.id,
                        "category": proposal.category.value,
                        "description": proposal.description,
                        "risk_level": simulation.risk_level.value,
                        "risk_summary": risk_summary,
                        "benefit_summary": benefit_summary,
                        "submitted_at": utc_now().isoformat(),
                    },
                )
            except Exception as exc:
                self._logger.warning("governance_neo4j_write_failed", error=str(exc))

        return record_id

    async def _record_evolution(
        self,
        proposal: EvolutionProposal,
        files_changed: list[str],
        rolled_back: bool = False,
        rollback_reason: str = "",
    ) -> None:
        """Write an immutable evolution record and update the version chain."""
        if self._history is None:
            return

        from_version = self._current_version - (0 if rolled_back else 1)
        to_version = self._current_version

        risk_level = (
            proposal.simulation.risk_level
            if proposal.simulation
            else RiskLevel.LOW
        )

        # Extract simulation detail fields if enriched simulation was performed
        sim_detail = {
            "simulation_episodes_tested": 0,
            "counterfactual_regression_rate": 0.0,
            "dependency_blast_radius": 0,
            "constitutional_alignment": 0.0,
            "resource_tokens_per_hour": 0,
            "caution_reasoning": "",
        }
        if isinstance(proposal.simulation, EnrichedSimulationResult):
            sim_detail["simulation_episodes_tested"] = proposal.simulation.episodes_tested
            sim_detail["counterfactual_regression_rate"] = proposal.simulation.counterfactual_regression_rate
            sim_detail["dependency_blast_radius"] = proposal.simulation.dependency_blast_radius
            sim_detail["constitutional_alignment"] = proposal.simulation.constitutional_alignment
            if proposal.simulation.resource_cost_estimate:
                sim_detail["resource_tokens_per_hour"] = (
                    proposal.simulation.resource_cost_estimate.estimated_additional_llm_tokens_per_hour
                )
            if proposal.simulation.caution_adjustment:
                sim_detail["caution_reasoning"] = proposal.simulation.caution_adjustment.reasoning

        record = EvolutionRecord(
            proposal_id=proposal.id,
            category=proposal.category,
            description=proposal.description,
            from_version=from_version,
            to_version=to_version,
            files_changed=files_changed,
            simulation_risk=risk_level,
            rolled_back=rolled_back,
            rollback_reason=rollback_reason,
            **sim_detail,
        )

        try:
            await self._history.record(record)
        except Exception as exc:
            self._logger.error("history_write_failed", error=str(exc))
            return

        if not rolled_back:
            config_hash = self._compute_config_hash(files_changed)
            version = ConfigVersion(
                version=self._current_version,
                proposal_ids=[proposal.id],
                config_hash=config_hash,
            )
            try:
                await self._history.record_version(version, previous_version=from_version)
            except Exception as exc:
                self._logger.error("version_write_failed", error=str(exc))

    # ─── Helpers ──────────────────────────────────────────────────────────────

    def _compute_config_hash(self, files_changed: list[str]) -> str:
        """Compute a stable hash of the current config state."""
        hasher = hashlib.sha256()
        for rel_path in sorted(files_changed):
            full_path = self._root / rel_path
            hasher.update(rel_path.encode())
            if full_path.exists():
                hasher.update(str(full_path.stat().st_mtime).encode())
        return hasher.hexdigest()[:16]

    def _get_iron_rule_for(self, proposal: EvolutionProposal) -> str:
        """Return the relevant iron rule for a forbidden category."""
        rule_map = {
            "modify_equor": "Simula CANNOT modify Equor in any way.",
            "modify_constitution": "Simula CANNOT modify constitutional drives.",
            "modify_invariants": "Simula CANNOT modify invariants.",
            "modify_self_evolution": "Simula CANNOT modify its own logic (no self-modifying code).",
        }
        return rule_map.get(proposal.category.value, "Category is forbidden.")

    def _invalidate_analytics(self) -> None:
        """Invalidate analytics cache after a proposal completes."""
        if self._analytics is not None:
            self._analytics.invalidate_cache()

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\simulation.py =====

"""
EcodiaOS -- Simula Deep Simulation Engine

Before any change is applied, the simulator performs multi-strategy
impact prediction. This is the brain of Simula's decision-making.

Strategy stack (per proposal):
  1. Category-specific validation (static analysis / budget check / LLM reasoning)
  2. Counterfactual episode replay — "What if this existed during episode X?"
  3. Dependency graph analysis — blast radius via import-graph traversal
  4. Resource cost estimation — heuristic compute/memory/token impact
  5. Constitutional alignment prediction — drive alignment scoring
  6. Risk synthesis — combine all signals into a unified assessment

Budget efficiency:
  - Counterfactual replay: batches 30 episodes into ONE LLM call (~800 tokens)
  - Constitutional alignment: single call, 100 tokens max output
  - Dependency analysis: pure Python ast module, zero LLM tokens
  - Resource cost: heuristic lookup table, zero LLM tokens
  - Analytics-informed caution: uses cached history, zero LLM tokens

Target latency: <=30s for full simulation (spec requirement).
"""

from __future__ import annotations

import ast
import asyncio
import re
from pathlib import Path
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.config import SimulaConfig
from ecodiaos.systems.simula.types import (
    CautionAdjustment,
    ChangeCategory,
    CounterfactualResult,
    DependencyImpact,
    EnrichedSimulationResult,
    EvolutionProposal,
    GOVERNANCE_REQUIRED,
    ImpactType,
    ResourceCostEstimate,
    RiskLevel,
    SELF_APPLICABLE,
    SimulationResult,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine

from ecodiaos.clients.optimized_llm import OptimizedLLMProvider

logger = structlog.get_logger().bind(system="simula.simulation")

# Valid Python identifier pattern for names
_VALID_NAME = re.compile(r"^[A-Za-z][A-Za-z0-9_]*$")
_SNAKE_CASE = re.compile(r"^[a-z][a-z0-9_]*$")
_PASCAL_CASE = re.compile(r"^[A-Z][A-Za-z0-9]*$")

# Resource cost heuristics per category (zero LLM tokens)
_RESOURCE_COST_HEURISTICS: dict[ChangeCategory, dict[str, int | float]] = {
    ChangeCategory.ADD_EXECUTOR: {
        "llm_tokens_per_hour": 500,
        "compute_ms_per_cycle": 5,
        "memory_mb": 2.0,
    },
    ChangeCategory.ADD_INPUT_CHANNEL: {
        "llm_tokens_per_hour": 200,
        "compute_ms_per_cycle": 10,
        "memory_mb": 5.0,
    },
    ChangeCategory.ADD_PATTERN_DETECTOR: {
        "llm_tokens_per_hour": 0,
        "compute_ms_per_cycle": 3,
        "memory_mb": 1.0,
    },
    ChangeCategory.ADJUST_BUDGET: {
        "llm_tokens_per_hour": 0,
        "compute_ms_per_cycle": 0,
        "memory_mb": 0.0,
    },
}

# System directories for dependency analysis
_SYSTEM_DIRS: dict[str, str] = {
    "memory": "src/ecodiaos/systems/memory",
    "equor": "src/ecodiaos/systems/equor",
    "atune": "src/ecodiaos/systems/atune",
    "voxis": "src/ecodiaos/systems/voxis",
    "nova": "src/ecodiaos/systems/nova",
    "axon": "src/ecodiaos/systems/axon",
    "evo": "src/ecodiaos/systems/evo",
    "simula": "src/ecodiaos/systems/simula",
}


class ChangeSimulator:
    """
    Deep multi-strategy impact simulator. Combines category-specific
    validation with counterfactual replay, dependency analysis, resource
    estimation, and constitutional alignment prediction.

    All strategies run concurrently where possible (asyncio.gather)
    to stay within the 30s latency target.
    """

    def __init__(
        self,
        config: SimulaConfig,
        llm: LLMProvider,
        memory: MemoryService | None = None,
        analytics: EvolutionAnalyticsEngine | None = None,
        codebase_root: Path | None = None,
    ) -> None:
        self._config = config
        self._llm = llm
        self._memory = memory
        self._analytics = analytics
        self._root = codebase_root or Path(config.codebase_root).resolve()
        self._log = logger
        # Optimization: detect optimized provider for budget checks + cache tagging
        self._optimized = isinstance(llm, OptimizedLLMProvider)

    async def simulate(self, proposal: EvolutionProposal) -> EnrichedSimulationResult:
        """
        Main simulation entry point. Runs category-specific validation
        plus cross-cutting deep analysis, then synthesizes a unified
        risk assessment.

        All independent analyses run concurrently via asyncio.gather.
        """
        self._log.info(
            "deep_simulation_started",
            proposal_id=proposal.id,
            category=proposal.category.value,
        )

        # Forbidden categories are rejected before reaching simulation,
        # but defend in depth
        from ecodiaos.systems.simula.types import FORBIDDEN
        if proposal.category in FORBIDDEN:
            return EnrichedSimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=f"Category {proposal.category.value} is forbidden.",
            )

        # Run all strategies concurrently
        base_task = self._simulate_by_category(proposal)
        counterfactual_task = self._counterfactual_replay(proposal)
        dependency_task = self._analyze_dependencies(proposal)
        alignment_task = self._predict_constitutional_alignment(proposal)

        base_result, counterfactuals, dependency_impacts, alignment = await asyncio.gather(
            base_task,
            counterfactual_task,
            dependency_task,
            alignment_task,
            return_exceptions=True,
        )

        # Handle exceptions gracefully -- individual strategy failures
        # should not prevent the simulation from completing
        if isinstance(base_result, BaseException):
            self._log.error("base_simulation_failed", error=str(base_result))
            base_result = SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary=f"Base simulation failed: {base_result}",
            )
        if isinstance(counterfactuals, BaseException):
            self._log.warning("counterfactual_replay_failed", error=str(counterfactuals))
            counterfactuals = []
        if isinstance(dependency_impacts, BaseException):
            self._log.warning("dependency_analysis_failed", error=str(dependency_impacts))
            dependency_impacts = []
        if isinstance(alignment, BaseException):
            self._log.warning("alignment_prediction_failed", error=str(alignment))
            alignment = 0.0

        # Resource cost estimation (pure heuristic, synchronous)
        cost_estimate = self._estimate_resource_cost(proposal)

        # Synthesize all signals
        result = self._synthesize_risk(
            base_result=base_result,
            counterfactuals=counterfactuals,
            dependency_impacts=dependency_impacts,
            cost_estimate=cost_estimate,
            constitutional_alignment=alignment,
            proposal=proposal,
        )

        self._log.info(
            "deep_simulation_complete",
            proposal_id=proposal.id,
            risk_level=result.risk_level.value,
            counterfactuals=len(result.counterfactuals),
            dependency_blast_radius=result.dependency_blast_radius,
            constitutional_alignment=round(result.constitutional_alignment, 2),
            episodes_tested=result.episodes_tested,
        )
        return result

    # ─── Category-Specific Simulation ────────────────────────────────────────

    async def _simulate_by_category(self, proposal: EvolutionProposal) -> SimulationResult:
        """Dispatch to the right category-specific strategy."""
        if proposal.category in SELF_APPLICABLE:
            if proposal.category == ChangeCategory.ADJUST_BUDGET:
                return await self._simulate_budget(proposal)
            else:
                return await self._simulate_additive(proposal)
        elif proposal.category in GOVERNANCE_REQUIRED:
            return await self._simulate_governance(proposal)
        else:
            return SimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=f"Category {proposal.category.value} is forbidden.",
            )

    async def _simulate_additive(self, proposal: EvolutionProposal) -> SimulationResult:
        """
        Enhanced static analysis for additive changes.
        Beyond name validation: checks naming conventions, system existence,
        existing overlap detection, and spec completeness.
        """
        spec = proposal.change_spec
        issues: list[str] = []

        # Determine the relevant name and validate by category
        name: str | None = None
        class_name: str | None = None

        if proposal.category == ChangeCategory.ADD_EXECUTOR:
            name = spec.executor_name
            if not spec.executor_action_type:
                issues.append("executor_action_type is required")
            if name and not _SNAKE_CASE.match(name):
                issues.append(
                    f"Executor module name {name!r} should be snake_case "
                    f"(e.g., 'email_sender', not 'EmailSender')"
                )
            # Check if executor with this action_type already exists
            if spec.executor_action_type:
                existing = await self._check_existing_executor(spec.executor_action_type)
                if existing:
                    issues.append(
                        f"Executor for action_type {spec.executor_action_type!r} "
                        f"already exists: {existing}"
                    )
            # Verify the axon executors directory exists
            executors_dir = self._root / "src" / "ecodiaos" / "systems" / "axon" / "executors"
            if not executors_dir.exists():
                issues.append("Axon executors directory not found -- system may not be built yet")

        elif proposal.category == ChangeCategory.ADD_INPUT_CHANNEL:
            name = spec.channel_name
            if not spec.channel_type:
                issues.append("channel_type is required")
            if name and not _SNAKE_CASE.match(name):
                issues.append(f"Channel module name {name!r} should be snake_case")

        elif proposal.category == ChangeCategory.ADD_PATTERN_DETECTOR:
            name = spec.detector_name
            if not spec.detector_pattern_type:
                issues.append("detector_pattern_type is required")
            if name and not _PASCAL_CASE.match(name):
                issues.append(
                    f"Detector class name {name!r} should be PascalCase "
                    f"(e.g., 'FrequencyDetector')"
                )

        if name is None:
            issues.append("No name provided for additive change")
        elif not _VALID_NAME.match(name):
            issues.append(f"Name {name!r} is not a valid Python identifier")

        if issues:
            return SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary="Spec validation failed: " + "; ".join(issues),
                benefit_summary=proposal.expected_benefit,
            )

        return SimulationResult(
            episodes_tested=0,
            risk_level=RiskLevel.LOW,
            risk_summary="Additive change passes enhanced static analysis.",
            benefit_summary=proposal.expected_benefit,
        )

    async def _simulate_budget(self, proposal: EvolutionProposal) -> SimulationResult:
        """Validate budget parameter range and assess risk magnitude."""
        from ecodiaos.systems.evo.types import TUNABLE_PARAMETERS

        spec = proposal.change_spec
        if not spec.budget_parameter:
            return SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary="ADJUST_BUDGET proposal missing budget_parameter.",
            )
        if spec.budget_parameter not in TUNABLE_PARAMETERS:
            return SimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=f"Parameter {spec.budget_parameter!r} is not in TUNABLE_PARAMETERS.",
            )

        param_spec = TUNABLE_PARAMETERS[spec.budget_parameter]
        new_val = spec.budget_new_value
        old_val = spec.budget_old_value

        if new_val is None:
            return SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary="ADJUST_BUDGET proposal missing budget_new_value.",
            )
        if new_val < param_spec.min_val or new_val > param_spec.max_val:
            return SimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=(
                    f"{spec.budget_parameter} new value {new_val} is outside allowed range "
                    f"[{param_spec.min_val}, {param_spec.max_val}]."
                ),
            )

        delta = abs(new_val - (old_val or 0.0))
        risk = RiskLevel.MODERATE if delta > 0.05 else RiskLevel.LOW
        return SimulationResult(
            episodes_tested=0,
            risk_level=risk,
            risk_summary=(
                f"{spec.budget_parameter}: {old_val} -> {new_val} "
                f"(delta={delta:.4f}). Risk: {risk.value}."
            ),
            benefit_summary=proposal.expected_benefit,
        )

    async def _simulate_governance(self, proposal: EvolutionProposal) -> SimulationResult:
        """
        LLM-based impact assessment for governance-required changes.
        Retrieves up to 30 recent episode summaries and asks the LLM
        to reason about the impact with explicit risk dimensions.
        """
        episode_context = ""
        episodes_count = 0

        if self._memory is not None:
            try:
                episodes = await asyncio.wait_for(
                    self._memory.retrieve_recent_episodes(limit=30),
                    timeout=5.0,
                )
                episodes_count = len(episodes)
                episode_context = self._build_episode_context(episodes)
            except Exception as exc:
                self._log.warning("episode_fetch_failed", error=str(exc))

        # Build an explicit multi-dimension assessment prompt
        prompt = (
            "You are evaluating a proposed structural change to EcodiaOS, "
            "a computational cognitive architecture.\n\n"
            f"PROPOSAL\n"
            f"Category: {proposal.category.value}\n"
            f"Description: {proposal.description}\n"
            f"Expected benefit: {proposal.expected_benefit}\n"
            f"Affected systems: {', '.join(proposal.change_spec.affected_systems) or 'unspecified'}\n\n"
            f"RECENT EPISODE CONTEXT ({episodes_count} episodes):\n{episode_context}\n\n"
            "Assess this change across four dimensions:\n"
            "1. BEHAVIORAL_RISK: Would existing behaviors regress? (LOW/MODERATE/HIGH)\n"
            "2. INTEGRATION_RISK: Could this break inter-system contracts? (LOW/MODERATE/HIGH)\n"
            "3. RESOURCE_RISK: Would this significantly increase resource consumption? (LOW/MODERATE/HIGH)\n"
            "4. REVERSIBILITY: How easy is rollback? (EASY/MODERATE/HARD)\n\n"
            "Reply with:\n"
            "RISK: <overall level: LOW|MODERATE|HIGH>\n"
            "BEHAVIORAL: <level>\n"
            "INTEGRATION: <level>\n"
            "RESOURCE: <level>\n"
            "REVERSIBILITY: <level>\n"
            "REASONING: <2-3 sentences>\n"
            "BENEFIT: <1 sentence>"
        )

        # Budget gate: simulation is STANDARD priority — skip in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.simulation", estimated_tokens=400):
                self._log.info("governance_simulation_skipped_budget", proposal_id=proposal.id)
                return SimulationResult(
                    episodes_tested=episodes_count,
                    risk_level=RiskLevel.HIGH,
                    risk_summary="LLM budget exhausted (RED tier) — defaulting to HIGH risk.",
                    benefit_summary=proposal.expected_benefit,
                )

        try:
            if self._optimized:
                response = await asyncio.wait_for(
                    self._llm.evaluate(  # type: ignore[call-arg]
                        prompt=prompt, max_tokens=400, temperature=0.2,
                        cache_system="simula.simulation", cache_method="governance_impact",
                    ),
                    timeout=10.0,
                )
            else:
                response = await asyncio.wait_for(
                    self._llm.evaluate(prompt=prompt, max_tokens=400, temperature=0.2),
                    timeout=10.0,
                )
            risk_level, risk_summary, benefit_summary = self._parse_llm_risk(response.text)
        except asyncio.TimeoutError:
            self._log.warning("simulation_llm_timeout", proposal_id=proposal.id)
            risk_level = RiskLevel.HIGH
            risk_summary = "LLM assessment timed out; defaulting to HIGH risk."
            benefit_summary = proposal.expected_benefit
        except Exception as exc:
            self._log.error("simulation_llm_error", error=str(exc))
            risk_level = RiskLevel.HIGH
            risk_summary = f"LLM assessment failed: {exc}"
            benefit_summary = proposal.expected_benefit

        return SimulationResult(
            episodes_tested=episodes_count,
            risk_level=risk_level,
            risk_summary=risk_summary,
            benefit_summary=benefit_summary,
        )

    # ─── Counterfactual Episode Replay ───────────────────────────────────────

    async def _counterfactual_replay(
        self, proposal: EvolutionProposal,
    ) -> list[CounterfactualResult]:
        """
        For additive changes, ask: 'If this had existed during recent episodes,
        when would it have been invoked? What would have changed?'

        Token-efficient: batches up to 30 episodes into a single LLM call
        with structured output (~800 tokens total).

        Returns empty list for non-additive changes or when Memory is unavailable.
        """
        # Only meaningful for additive changes
        if proposal.category not in {
            ChangeCategory.ADD_EXECUTOR,
            ChangeCategory.ADD_INPUT_CHANNEL,
            ChangeCategory.ADD_PATTERN_DETECTOR,
        }:
            return []

        if self._memory is None:
            return []

        # Retrieve recent episodes
        try:
            episodes = await asyncio.wait_for(
                self._memory.retrieve_recent_episodes(limit=30),
                timeout=5.0,
            )
        except Exception as exc:
            self._log.warning("counterfactual_episode_fetch_failed", error=str(exc))
            return []

        if not episodes:
            return []

        # Build the batch counterfactual prompt
        episode_summaries = []
        for i, ep in enumerate(episodes[:30], start=1):
            summary = getattr(ep, "summary", "") or getattr(ep, "raw_content", "")[:150]
            source = getattr(ep, "source", "unknown")
            episode_summaries.append(f"{i}. [{source}] {summary[:150]}")

        spec = proposal.change_spec
        change_desc = self._describe_additive_change(proposal)

        prompt = (
            f"EcodiaOS is considering adding a new capability:\n{change_desc}\n\n"
            f"Below are {len(episode_summaries)} recent episodes. For each, determine:\n"
            f"- Would this new capability have been triggered/relevant? (yes/no)\n"
            f"- If yes, what would have been different? (improvement/regression/neutral)\n\n"
            f"EPISODES:\n" + "\n".join(episode_summaries) + "\n\n"
            f"Reply as a numbered list matching the episode numbers:\n"
            f"<number>. <yes|no> | <improvement|regression|neutral> | <1 sentence reason>\n"
            f"Only include episodes where the answer is 'yes'."
        )

        # Budget gate: skip counterfactual replay in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.simulation", estimated_tokens=500):
                self._log.info("counterfactual_replay_skipped_budget")
                return []

        try:
            if self._optimized:
                response = await asyncio.wait_for(
                    self._llm.evaluate(  # type: ignore[call-arg]
                        prompt=prompt, max_tokens=500, temperature=0.2,
                        cache_system="simula.simulation", cache_method="counterfactual",
                    ),
                    timeout=15.0,
                )
            else:
                response = await asyncio.wait_for(
                    self._llm.evaluate(prompt=prompt, max_tokens=500, temperature=0.2),
                    timeout=15.0,
                )
            return self._parse_counterfactual_response(response.text, episodes)
        except Exception as exc:
            self._log.warning("counterfactual_llm_failed", error=str(exc))
            return []

    def _describe_additive_change(self, proposal: EvolutionProposal) -> str:
        """Human-readable description of an additive change for counterfactual prompt."""
        spec = proposal.change_spec
        if proposal.category == ChangeCategory.ADD_EXECUTOR:
            return (
                f"New Axon Executor: {spec.executor_name or 'unnamed'}\n"
                f"Action type: {spec.executor_action_type or 'unspecified'}\n"
                f"Description: {spec.executor_description or proposal.description}"
            )
        elif proposal.category == ChangeCategory.ADD_INPUT_CHANNEL:
            return (
                f"New Atune Input Channel: {spec.channel_name or 'unnamed'}\n"
                f"Channel type: {spec.channel_type or 'unspecified'}\n"
                f"Description: {spec.channel_description or proposal.description}"
            )
        elif proposal.category == ChangeCategory.ADD_PATTERN_DETECTOR:
            return (
                f"New Evo Pattern Detector: {spec.detector_name or 'unnamed'}\n"
                f"Pattern type: {spec.detector_pattern_type or 'unspecified'}\n"
                f"Description: {spec.detector_description or proposal.description}"
            )
        return proposal.description

    def _parse_counterfactual_response(
        self, text: str, episodes: list,
    ) -> list[CounterfactualResult]:
        """Parse the LLM's batch counterfactual response into structured results."""
        results: list[CounterfactualResult] = []
        for line in text.strip().splitlines():
            line = line.strip()
            if not line or not line[0].isdigit():
                continue
            try:
                # Expected: "3. yes | improvement | Would have handled email notifications"
                num_part, rest = line.split(".", 1)
                idx = int(num_part.strip()) - 1
                if idx < 0 or idx >= len(episodes):
                    continue

                parts = [p.strip() for p in rest.split("|")]
                if len(parts) < 2:
                    continue

                triggered = parts[0].lower().strip() in ("yes", "y", "true")
                if not triggered:
                    continue

                impact_str = parts[1].lower().strip() if len(parts) > 1 else "neutral"
                if "improvement" in impact_str:
                    impact = ImpactType.IMPROVEMENT
                elif "regression" in impact_str:
                    impact = ImpactType.REGRESSION
                else:
                    impact = ImpactType.NEUTRAL

                reasoning = parts[2].strip() if len(parts) > 2 else ""

                ep = episodes[idx]
                results.append(CounterfactualResult(
                    episode_id=getattr(ep, "id", f"ep_{idx}"),
                    would_have_triggered=True,
                    predicted_outcome=reasoning[:200],
                    impact=impact,
                    confidence=0.6,
                    reasoning=reasoning[:300],
                ))
            except (ValueError, IndexError):
                continue

        return results

    # ─── Dependency Graph Analysis ───────────────────────────────────────────

    async def _analyze_dependencies(
        self, proposal: EvolutionProposal,
    ) -> list[DependencyImpact]:
        """
        Static analysis of the affected system's import graph.
        Uses the ast module to parse Python files and trace imports.
        Zero LLM tokens -- pure computation.
        """
        affected_systems = proposal.change_spec.affected_systems
        if not affected_systems:
            affected_systems = self._infer_affected_systems(proposal)

        impacts: list[DependencyImpact] = []

        for sys_name in affected_systems:
            sys_dir = self._root / _SYSTEM_DIRS.get(sys_name, f"src/ecodiaos/systems/{sys_name}")
            if not sys_dir.exists():
                continue

            # Find all Python files in the affected system
            py_files = list(sys_dir.rglob("*.py"))

            # For each file, find what imports it from other systems
            for py_file in py_files:
                rel_path = str(py_file.relative_to(self._root))
                module_name = self._path_to_module(rel_path)
                if not module_name:
                    continue

                # Check how many other files import this module
                importers = await self._find_importers(module_name)
                if importers:
                    impacts.append(DependencyImpact(
                        file_path=rel_path,
                        impact_type="import_dependency",
                        risk_contribution=min(1.0, len(importers) * 0.1),
                    ))

            # Check for test coverage
            test_dir = self._root / "tests" / "unit" / "systems" / sys_name
            if test_dir.exists():
                test_files = list(test_dir.rglob("*.py"))
                impacts.append(DependencyImpact(
                    file_path=str(test_dir.relative_to(self._root)),
                    impact_type="test_coverage",
                    risk_contribution=0.0 if test_files else 0.3,
                ))

        return impacts

    def _infer_affected_systems(self, proposal: EvolutionProposal) -> list[str]:
        """Infer which systems a change affects from the category."""
        category_to_systems: dict[ChangeCategory, list[str]] = {
            ChangeCategory.ADD_EXECUTOR: ["axon"],
            ChangeCategory.ADD_INPUT_CHANNEL: ["atune"],
            ChangeCategory.ADD_PATTERN_DETECTOR: ["evo"],
            ChangeCategory.ADJUST_BUDGET: [],
            ChangeCategory.MODIFY_CONTRACT: [],
            ChangeCategory.ADD_SYSTEM_CAPABILITY: [],
            ChangeCategory.MODIFY_CYCLE_TIMING: ["synapse"],
            ChangeCategory.CHANGE_CONSOLIDATION: ["evo"],
        }
        return category_to_systems.get(proposal.category, [])

    async def _find_importers(self, module_name: str) -> list[str]:
        """Find files that import the given module. Scans src/ directory."""
        importers: list[str] = []
        src_dir = self._root / "src"
        if not src_dir.exists():
            return importers

        # Extract the short module name for import matching
        parts = module_name.split(".")
        short_name = parts[-1] if parts else module_name

        for py_file in src_dir.rglob("*.py"):
            try:
                source = py_file.read_text(encoding="utf-8")
                tree = ast.parse(source, filename=str(py_file))
            except Exception:
                continue

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        if module_name in alias.name:
                            importers.append(str(py_file.relative_to(self._root)))
                            break
                elif isinstance(node, ast.ImportFrom):
                    if node.module and module_name in node.module:
                        importers.append(str(py_file.relative_to(self._root)))

        return importers

    def _path_to_module(self, rel_path: str) -> str | None:
        """Convert a relative file path to a dotted module name."""
        parts = rel_path.replace("\\", "/").split("/")
        if parts and parts[0] == "src":
            parts = parts[1:]
        if not parts:
            return None
        if parts[-1].endswith(".py"):
            parts[-1] = parts[-1][:-3]
        if parts[-1] == "__init__":
            parts = parts[:-1]
        return ".".join(parts) if parts else None

    # ─── Resource Cost Estimation ────────────────────────────────────────────

    def _estimate_resource_cost(
        self, proposal: EvolutionProposal,
    ) -> ResourceCostEstimate:
        """
        Heuristic estimation of ongoing resource impact.
        Zero LLM tokens -- pure lookup + arithmetic.
        """
        heuristics = _RESOURCE_COST_HEURISTICS.get(proposal.category)
        if heuristics is None:
            # Governance-required changes: estimate moderate cost
            return ResourceCostEstimate(
                estimated_additional_llm_tokens_per_hour=1000,
                estimated_additional_compute_ms_per_cycle=10,
                estimated_memory_mb=5.0,
                budget_headroom_percent=90.0,
            )

        tokens = int(heuristics.get("llm_tokens_per_hour", 0))
        compute = int(heuristics.get("compute_ms_per_cycle", 0))
        memory = float(heuristics.get("memory_mb", 0.0))

        # Budget headroom: what percent of the relevant system's budget remains
        # after adding this cost
        system_budget = self._get_system_budget(proposal)
        headroom = 100.0
        if system_budget > 0 and tokens > 0:
            headroom = max(0.0, 100.0 * (1.0 - tokens / system_budget))

        return ResourceCostEstimate(
            estimated_additional_llm_tokens_per_hour=tokens,
            estimated_additional_compute_ms_per_cycle=compute,
            estimated_memory_mb=memory,
            budget_headroom_percent=round(headroom, 1),
        )

    def _get_system_budget(self, proposal: EvolutionProposal) -> int:
        """Get the affected system's hourly token budget."""
        category_to_system: dict[ChangeCategory, str] = {
            ChangeCategory.ADD_EXECUTOR: "axon",
            ChangeCategory.ADD_INPUT_CHANNEL: "atune",
            ChangeCategory.ADD_PATTERN_DETECTOR: "evo",
        }
        sys_name = category_to_system.get(proposal.category, "")
        # Default system budgets (from config/default.yaml)
        default_budgets: dict[str, int] = {
            "atune": 60000,
            "equor": 30000,
            "nova": 120000,
            "voxis": 120000,
            "evo": 60000,
            "axon": 60000,
            "simula": 10000,
        }
        return default_budgets.get(sys_name, 60000)

    # ─── Constitutional Alignment Prediction ─────────────────────────────────

    async def _predict_constitutional_alignment(
        self, proposal: EvolutionProposal,
    ) -> float:
        """
        Predict how well this change aligns with the four constitutional drives.
        Single LLM call, 100 tokens max output. Returns -1.0 to 1.0.

        Budget: ~200 tokens total (prompt + response).
        """
        prompt = (
            "EcodiaOS has four constitutional drives: "
            "coherence (make sense), care (orient toward wellbeing), "
            "growth (become more capable), honesty (represent reality truthfully).\n\n"
            f"Proposed change: {proposal.description[:200]}\n"
            f"Category: {proposal.category.value}\n"
            f"Expected benefit: {proposal.expected_benefit[:100]}\n\n"
            "Score the alignment of this change with the drives from -1.0 to 1.0.\n"
            "Reply with a single number only (e.g., 0.7)."
        )

        # Budget gate: skip alignment prediction in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.simulation", estimated_tokens=100):
                self._log.info("alignment_prediction_skipped_budget")
                return 0.0

        try:
            if self._optimized:
                response = await asyncio.wait_for(
                    self._llm.evaluate(  # type: ignore[call-arg]
                        prompt=prompt, max_tokens=20, temperature=0.1,
                        cache_system="simula.simulation", cache_method="constitutional_alignment",
                    ),
                    timeout=5.0,
                )
            else:
                response = await asyncio.wait_for(
                    self._llm.evaluate(prompt=prompt, max_tokens=20, temperature=0.1),
                    timeout=5.0,
                )
            # Extract the float from the response
            text = response.text.strip()
            # Handle potential formatting like "0.7" or "Score: 0.7"
            for token in text.split():
                try:
                    score = float(token.strip(".,;:"))
                    return max(-1.0, min(1.0, score))
                except ValueError:
                    continue
            return 0.0
        except Exception as exc:
            self._log.warning("alignment_prediction_failed", error=str(exc))
            return 0.0

    # ─── Risk Synthesis ──────────────────────────────────────────────────────

    def _synthesize_risk(
        self,
        base_result: SimulationResult,
        counterfactuals: list[CounterfactualResult],
        dependency_impacts: list[DependencyImpact],
        cost_estimate: ResourceCostEstimate,
        constitutional_alignment: float,
        proposal: EvolutionProposal,
    ) -> EnrichedSimulationResult:
        """
        Combine all simulation signals into a unified risk assessment.

        Risk factors (weighted):
          - Base category simulation: 40%
          - Counterfactual regression rate: 20%
          - Dependency blast radius: 15%
          - Resource cost: 10%
          - Constitutional alignment: 15% (negative alignment increases risk)

        Dynamic adjustment: if analytics show high rollback rate for this
        category, bump the risk level up one notch.
        """
        # Counterfactual regression rate
        cf_regressions = sum(1 for cf in counterfactuals if cf.impact == ImpactType.REGRESSION)
        cf_total = len(counterfactuals) if counterfactuals else 1
        cf_regression_rate = cf_regressions / max(1, cf_total)

        # Dependency blast radius
        blast_radius = len(dependency_impacts)
        total_risk_contribution = sum(d.risk_contribution for d in dependency_impacts)

        # Resource risk (0-1 scale based on budget consumption)
        resource_risk = 1.0 - (cost_estimate.budget_headroom_percent / 100.0) if cost_estimate else 0.0

        # Constitutional risk (alignment < 0 adds risk)
        alignment_risk = max(0.0, -constitutional_alignment)

        # Base risk as numeric
        base_risk_numeric = {
            RiskLevel.LOW: 0.1,
            RiskLevel.MODERATE: 0.4,
            RiskLevel.HIGH: 0.7,
            RiskLevel.UNACCEPTABLE: 1.0,
        }.get(base_result.risk_level, 0.4)

        # Weighted composite risk score (0.0 - 1.0)
        composite_risk = (
            0.40 * base_risk_numeric
            + 0.20 * cf_regression_rate
            + 0.15 * min(1.0, total_risk_contribution)
            + 0.10 * resource_risk
            + 0.15 * alignment_risk
        )

        # Dynamic caution adjustment from analytics history
        caution_adj: CautionAdjustment | None = None
        if self._analytics is not None:
            caution_adj = self._analytics.should_increase_caution(proposal.category)
            if caution_adj.should_adjust:
                composite_risk = min(1.0, composite_risk + caution_adj.magnitude)
                self._log.info(
                    "caution_increased",
                    category=proposal.category.value,
                    composite_risk=round(composite_risk, 3),
                    magnitude=caution_adj.magnitude,
                    factors=caution_adj.factors,
                    reasoning=caution_adj.reasoning,
                )

        # Map composite score to RiskLevel
        if composite_risk >= 0.75:
            final_risk = RiskLevel.UNACCEPTABLE
        elif composite_risk >= 0.50:
            final_risk = RiskLevel.HIGH
        elif composite_risk >= 0.25:
            final_risk = RiskLevel.MODERATE
        else:
            final_risk = RiskLevel.LOW

        # Emit decision audit log with all signal values and weights
        self._log.info(
            "simulation_decision_audit",
            proposal_id=proposal.id,
            category=proposal.category.value,
            base_risk=f"{0.40 * base_risk_numeric:.3f} (0.40×{base_risk_numeric:.2f})",
            counterfactual_risk=f"{0.20 * cf_regression_rate:.3f} (0.20×{cf_regression_rate:.2f})",
            dependency_risk=f"{0.15 * min(1.0, total_risk_contribution):.3f} (0.15×{total_risk_contribution:.2f})",
            resource_risk=f"{0.10 * resource_risk:.3f} (0.10×{resource_risk:.2f})",
            alignment_risk=f"{0.15 * alignment_risk:.3f} (0.15×{alignment_risk:.2f})",
            weighted_sum=round(composite_risk, 3),
            caution_adjustment=caution_adj.magnitude if caution_adj and caution_adj.should_adjust else 0.0,
            final_risk=final_risk.value,
            episodes_tested=base_result.episodes_tested,
            blast_radius=blast_radius,
            constitutional_alignment=round(constitutional_alignment, 2),
        )

        # Build summary
        summary_parts = [base_result.risk_summary]
        if counterfactuals:
            cf_improvements = sum(1 for cf in counterfactuals if cf.impact == ImpactType.IMPROVEMENT)
            summary_parts.append(
                f"Counterfactual: {cf_improvements} improvements, "
                f"{cf_regressions} regressions across {len(counterfactuals)} triggered episodes."
            )
        if blast_radius > 0:
            summary_parts.append(f"Blast radius: {blast_radius} affected files/modules.")
        if constitutional_alignment != 0.0:
            summary_parts.append(f"Constitutional alignment: {constitutional_alignment:+.2f}.")

        return EnrichedSimulationResult(
            episodes_tested=base_result.episodes_tested,
            differences=base_result.differences,
            improvements=base_result.improvements + sum(
                1 for cf in counterfactuals if cf.impact == ImpactType.IMPROVEMENT
            ),
            regressions=base_result.regressions + cf_regressions,
            neutral_changes=base_result.neutral_changes + sum(
                1 for cf in counterfactuals if cf.impact == ImpactType.NEUTRAL
            ),
            risk_level=final_risk,
            risk_summary=" ".join(summary_parts),
            benefit_summary=base_result.benefit_summary,
            counterfactuals=counterfactuals,
            dependency_impacts=dependency_impacts,
            resource_cost_estimate=cost_estimate,
            constitutional_alignment=constitutional_alignment,
            counterfactual_regression_rate=round(cf_regression_rate, 3),
            dependency_blast_radius=blast_radius,
            caution_adjustment=caution_adj,
        )

    # ─── Helpers ─────────────────────────────────────────────────────────────

    async def _check_existing_executor(self, action_type: str) -> str | None:
        """Check if an executor for this action_type already exists."""
        executors_dir = self._root / "src" / "ecodiaos" / "systems" / "axon" / "executors"
        if not executors_dir.exists():
            return None

        for py_file in executors_dir.glob("*.py"):
            if py_file.name == "__init__.py":
                continue
            try:
                source = py_file.read_text(encoding="utf-8")
                if f'"{action_type}"' in source or f"'{action_type}'" in source:
                    return str(py_file.name)
            except Exception:
                continue
        return None

    async def _check_name_conflict(self, name: str, category: ChangeCategory) -> bool:
        """Returns True if the name would cause a conflict."""
        if not _VALID_NAME.match(name):
            return True
        return False

    def _build_episode_context(self, episodes: list) -> str:
        """Build concise context string from episode objects."""
        lines: list[str] = []
        for i, ep in enumerate(episodes[:30], start=1):
            summary = getattr(ep, "summary", "") or getattr(ep, "raw_content", "")[:200]
            source = getattr(ep, "source", "")
            lines.append(f"{i}. [{source}] {summary[:200]}")
        return "\n".join(lines)

    def _parse_llm_risk(self, text: str) -> tuple[RiskLevel, str, str]:
        """Parse the LLM response to extract risk level, reasoning, and benefit."""
        risk_level = RiskLevel.MODERATE
        risk_summary = text[:500]
        benefit_summary = ""

        for line in text.splitlines():
            line = line.strip()
            upper = line.upper()
            if upper.startswith("RISK:"):
                level_str = line.split(":", 1)[-1].strip().upper()
                if level_str == "LOW":
                    risk_level = RiskLevel.LOW
                elif level_str == "MODERATE":
                    risk_level = RiskLevel.MODERATE
                elif level_str == "HIGH":
                    risk_level = RiskLevel.HIGH
                elif level_str == "UNACCEPTABLE":
                    risk_level = RiskLevel.UNACCEPTABLE
            elif upper.startswith("REASONING:"):
                risk_summary = line.split(":", 1)[-1].strip()
            elif upper.startswith("BENEFIT:"):
                benefit_summary = line.split(":", 1)[-1].strip()

        return risk_level, risk_summary, benefit_summary

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\types.py =====

"""
EcodiaOS -- Simula Internal Types

All data types internal to the Simula self-evolution system.
Simula is the organism's capacity for metamorphosis: structural change
beyond parameter tuning. These types model the full lifecycle of an
evolution proposal -- from reception through simulation, governance,
application, and immutable history.
"""

from __future__ import annotations

import enum
from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import (
    EOSBaseModel,
    Identified,
    Timestamped,
    new_id,
    utc_now,
)


# --- Enums -------------------------------------------------------------------


class ChangeCategory(str, enum.Enum):
    ADD_EXECUTOR = "add_executor"
    ADD_INPUT_CHANNEL = "add_input_channel"
    ADD_PATTERN_DETECTOR = "add_pattern_detector"
    ADJUST_BUDGET = "adjust_budget"
    MODIFY_CONTRACT = "modify_contract"
    ADD_SYSTEM_CAPABILITY = "add_system_capability"
    MODIFY_CYCLE_TIMING = "modify_cycle_timing"
    CHANGE_CONSOLIDATION = "change_consolidation"
    MODIFY_EQUOR = "modify_equor"
    MODIFY_CONSTITUTION = "modify_constitution"
    MODIFY_INVARIANTS = "modify_invariants"
    MODIFY_SELF_EVOLUTION = "modify_self_evolution"


class ProposalStatus(str, enum.Enum):
    PROPOSED = "proposed"
    SIMULATING = "simulating"
    AWAITING_GOVERNANCE = "awaiting_governance"
    APPROVED = "approved"
    APPLYING = "applying"
    APPLIED = "applied"
    ROLLED_BACK = "rolled_back"
    REJECTED = "rejected"


class RiskLevel(str, enum.Enum):
    LOW = "low"
    MODERATE = "moderate"
    HIGH = "high"
    UNACCEPTABLE = "unacceptable"


class ImpactType(str, enum.Enum):
    IMPROVEMENT = "improvement"
    REGRESSION = "regression"
    NEUTRAL = "neutral"


class TriageStatus(str, enum.Enum):
    """Status of proposal triage (fast-path pre-simulation check)."""
    TRIVIAL = "trivial"
    REQUIRES_SIMULATION = "requires_simulation"


# --- Models -----------------------------------------------------------------


class ChangeSpec(EOSBaseModel):
    """
    Formal specification of what to change.
    One model covers every ChangeCategory -- fields are optional by category.
    """

    # ADD_EXECUTOR
    executor_name: str | None = None
    executor_description: str | None = None
    executor_action_type: str | None = None
    executor_input_schema: dict[str, Any] | None = None

    # ADD_INPUT_CHANNEL
    channel_name: str | None = None
    channel_type: str | None = None
    channel_description: str | None = None

    # ADD_PATTERN_DETECTOR
    detector_name: str | None = None
    detector_description: str | None = None
    detector_pattern_type: str | None = None

    # ADJUST_BUDGET
    budget_parameter: str | None = None
    budget_old_value: float | None = None
    budget_new_value: float | None = None

    # MODIFY_CONTRACT
    contract_changes: list[str] = Field(default_factory=list)

    # ADD_SYSTEM_CAPABILITY
    capability_description: str | None = None

    # MODIFY_CYCLE_TIMING
    timing_parameter: str | None = None
    timing_old_value: float | None = None
    timing_new_value: float | None = None

    # CHANGE_CONSOLIDATION
    consolidation_schedule: str | None = None

    # Cross-cutting
    affected_systems: list[str] = Field(default_factory=list)
    additional_context: str = ""
    code_hint: str = ""  # optional hint of what the code should look like


class SimulationDifference(EOSBaseModel):
    """Describes how one episode's outcome would differ under the proposed change."""

    episode_id: str
    original_outcome: str
    simulated_outcome: str
    impact: ImpactType
    reasoning: str = ""


class SimulationResult(EOSBaseModel):
    """Aggregate outcome of simulating a proposal against recent episodes."""

    episodes_tested: int = 0
    differences: int = 0
    improvements: int = 0
    regressions: int = 0
    neutral_changes: int = 0
    risk_level: RiskLevel = RiskLevel.LOW
    risk_summary: str = ""
    benefit_summary: str = ""
    simulated_at: datetime = Field(default_factory=utc_now)


class CautionAdjustment(EOSBaseModel):
    """
    Transparent caution adjustment logic explaining WHY a proposal's risk
    was bumped. Returned by EvolutionAnalyticsEngine.should_increase_caution().
    """

    should_adjust: bool
    magnitude: float  # 0.0-0.5 additive risk bump
    factors: dict[str, float] = Field(default_factory=dict)  # {factor_name: contribution}
    reasoning: str = ""


class TriageResult(EOSBaseModel):
    """Result of fast-path proposal triage (pre-simulation check)."""

    status: TriageStatus
    assumed_risk: RiskLevel | None = None
    reason: str = ""
    skip_simulation: bool = False


class ProposalResult(EOSBaseModel):
    """Final outcome recorded once a proposal reaches a terminal state."""

    status: ProposalStatus
    reason: str = ""
    version: int | None = None
    governance_record_id: str | None = None
    files_changed: list[str] = Field(default_factory=list)


class EvolutionProposal(Identified, Timestamped):
    """
    The full proposal lifecycle object -- richer than Evo's simplified version.
    Owns the proposal from receipt through simulation, governance, and application.
    """

    source: str  # "evo" | "governance"
    category: ChangeCategory
    description: str
    change_spec: ChangeSpec
    evidence: list[str] = Field(default_factory=list)  # hypothesis IDs / episode IDs
    expected_benefit: str = ""
    risk_assessment: str = ""
    status: ProposalStatus = ProposalStatus.PROPOSED
    simulation: SimulationResult | None = None
    governance_record_id: str | None = None
    result: ProposalResult | None = None


class FileSnapshot(EOSBaseModel):
    """
    One file's state immediately before a change was applied, enabling rollback.
    content is None when the file did not previously exist -- rollback deletes it.
    """

    path: str  # absolute path
    content: str | None  # None means file did not exist before
    existed: bool = True


class ConfigSnapshot(Identified, Timestamped):
    """Full snapshot of all affected files captured before applying a change."""

    proposal_id: str
    files: list[FileSnapshot] = Field(default_factory=list)
    config_version: int  # the version at snapshot time


class ConfigVersion(EOSBaseModel):
    """Tracks one step in the config version chain."""

    version: int
    timestamp: datetime = Field(default_factory=utc_now)
    proposal_ids: list[str] = Field(default_factory=list)  # evolution proposal IDs
    config_hash: str  # SHA256 hash of the canonical config state


class EvolutionRecord(Identified, Timestamped):
    """Immutable history entry written to Neo4j after each successful application."""

    proposal_id: str
    category: ChangeCategory
    description: str
    from_version: int
    to_version: int
    files_changed: list[str] = Field(default_factory=list)
    simulation_risk: RiskLevel
    applied_at: datetime = Field(default_factory=utc_now)
    rolled_back: bool = False
    rollback_reason: str = ""
    # Simulation detail persisted for audit trail and learning
    simulation_episodes_tested: int = 0
    counterfactual_regression_rate: float = 0.0
    dependency_blast_radius: int = 0
    constitutional_alignment: float = 0.0
    resource_tokens_per_hour: int = 0
    caution_reasoning: str = ""


class CodeChangeResult(EOSBaseModel):
    """What the code agent returns after implementing a structural change."""

    success: bool
    files_written: list[str] = Field(default_factory=list)
    summary: str = ""
    error: str = ""
    lint_passed: bool = True
    tests_passed: bool = True
    test_output: str = ""


class HealthCheckResult(EOSBaseModel):
    """Result of a post-apply codebase health check."""

    healthy: bool
    issues: list[str] = Field(default_factory=list)
    checked_at: datetime = Field(default_factory=utc_now)


# --- Enriched Simulation Models ----------------------------------------------


class CounterfactualResult(EOSBaseModel):
    """
    Result of asking: 'If this change had existed during episode X,
    what would have been different?'

    Batched into a single LLM call across multiple episodes for
    token efficiency (~800 tokens per 30-episode batch).
    """

    episode_id: str
    would_have_triggered: bool = False
    predicted_outcome: str = ""
    impact: ImpactType = ImpactType.NEUTRAL
    confidence: float = 0.5
    reasoning: str = ""


class DependencyImpact(EOSBaseModel):
    """
    A file or module affected by a proposed change, discovered
    via static import-graph analysis (zero LLM tokens).
    """

    file_path: str
    impact_type: str = "import_dependency"  # "direct_modification" | "import_dependency" | "test_coverage"
    risk_contribution: float = 0.0


class ResourceCostEstimate(EOSBaseModel):
    """
    Heuristic estimation of the ongoing resource cost a change
    would add to the running system. Computed without LLM calls.
    """

    estimated_additional_llm_tokens_per_hour: int = 0
    estimated_additional_compute_ms_per_cycle: int = 0
    estimated_memory_mb: float = 0.0
    budget_headroom_percent: float = 100.0


class EnrichedSimulationResult(SimulationResult):
    """
    Extended simulation result with deep multi-strategy analysis.
    Produced by the upgraded ChangeSimulator, consumed by SimulaService
    for richer risk/benefit decision-making.
    """

    counterfactuals: list[CounterfactualResult] = Field(default_factory=list)
    dependency_impacts: list[DependencyImpact] = Field(default_factory=list)
    resource_cost_estimate: ResourceCostEstimate | None = None
    constitutional_alignment: float = 0.0
    counterfactual_regression_rate: float = 0.0
    dependency_blast_radius: int = 0
    caution_adjustment: CautionAdjustment | None = None


# --- Bridge Models -----------------------------------------------------------


class EvoProposalEnriched(EOSBaseModel):
    """
    Evo proposal enriched with hypothesis evidence and inferred context.
    Produced by EvoSimulaBridge, consumed by SimulaService.translate().
    """

    evo_description: str
    evo_rationale: str
    hypothesis_ids: list[str] = Field(default_factory=list)
    hypothesis_statements: list[str] = Field(default_factory=list)
    evidence_scores: list[float] = Field(default_factory=list)
    supporting_episode_ids: list[str] = Field(default_factory=list)
    mutation_target: str = ""
    mutation_type: str = ""
    inferred_category: ChangeCategory | None = None
    inferred_change_spec: ChangeSpec | None = None


# --- Proposal Intelligence Models --------------------------------------------


class ProposalPriority(EOSBaseModel):
    """
    Priority score for a proposal, enabling intelligent processing order.
    Higher priority_score = process first.

    Formula: evidence_strength * expected_impact / max(0.1, estimated_risk * estimated_cost)
    """

    proposal_id: str
    priority_score: float = 0.0
    evidence_strength: float = 0.0
    expected_impact: float = 0.0
    estimated_risk: float = 0.0
    estimated_cost: float = 0.0
    reasoning: str = ""


class ProposalCluster(EOSBaseModel):
    """
    Group of semantically similar proposals that could be deduplicated.
    Detected via cheap heuristics first, LLM only for ambiguous cases.
    """

    representative_id: str
    member_ids: list[str] = Field(default_factory=list)
    similarity_scores: list[float] = Field(default_factory=list)
    merge_recommendation: str = ""


# --- Analytics Models --------------------------------------------------------


class CategorySuccessRate(EOSBaseModel):
    """Success rate tracking for a specific change category."""

    category: ChangeCategory
    total: int = 0
    approved: int = 0
    rejected: int = 0
    rolled_back: int = 0

    @property
    def success_rate(self) -> float:
        return self.approved / max(1, self.total)

    @property
    def rollback_rate(self) -> float:
        return self.rolled_back / max(1, self.total)


class EvolutionAnalytics(EOSBaseModel):
    """
    Aggregate evolution quality metrics computed from Neo4j history.
    Enables Simula to learn from its own performance over time.
    Zero LLM tokens -- pure computation from stored records.
    """

    category_rates: dict[str, CategorySuccessRate] = Field(default_factory=dict)
    total_proposals: int = 0
    evolution_velocity: float = 0.0  # proposals per day
    mean_simulation_risk: float = 0.0
    rollback_rate: float = 0.0
    recent_rollback_rates: dict[str, float] = Field(default_factory=dict)  # per-category 7-day rate
    last_updated: datetime = Field(default_factory=utc_now)


# --- Constants ---------------------------------------------------------------

SELF_APPLICABLE: frozenset[ChangeCategory] = frozenset({
    ChangeCategory.ADD_EXECUTOR,
    ChangeCategory.ADD_INPUT_CHANNEL,
    ChangeCategory.ADD_PATTERN_DETECTOR,
    ChangeCategory.ADJUST_BUDGET,
})

GOVERNANCE_REQUIRED: frozenset[ChangeCategory] = frozenset({
    ChangeCategory.MODIFY_CONTRACT,
    ChangeCategory.ADD_SYSTEM_CAPABILITY,
    ChangeCategory.MODIFY_CYCLE_TIMING,
    ChangeCategory.CHANGE_CONSOLIDATION,
})

FORBIDDEN: frozenset[ChangeCategory] = frozenset({
    ChangeCategory.MODIFY_EQUOR,
    ChangeCategory.MODIFY_CONSTITUTION,
    ChangeCategory.MODIFY_INVARIANTS,
    ChangeCategory.MODIFY_SELF_EVOLUTION,
})

SIMULA_IRON_RULES: list[str] = [
    "Simula CANNOT modify Equor in any way.",
    "Simula CANNOT modify constitutional drives.",
    "Simula CANNOT modify invariants.",
    "Simula CANNOT modify its own logic (no self-modifying code).",
    "Simula CANNOT bypass governance for governed changes.",
    "Simula CANNOT apply changes without rollback capability.",
    "Simula CANNOT delete evolution history records.",
    "Simula MUST simulate before applying any change.",
    "Simula MUST maintain version continuity -- no identity-breaking changes.",
]

# Paths the code agent is NEVER allowed to write to
FORBIDDEN_WRITE_PATHS: list[str] = [
    "src/ecodiaos/systems/equor",
    "src/ecodiaos/systems/simula",
    "src/ecodiaos/primitives/constitutional.py",
    "src/ecodiaos/primitives/common.py",
    "src/ecodiaos/config.py",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\__init__.py =====

"""
EcodiaOS — Synapse (System #9)

The autonomic nervous system. Drives the cognitive cycle clock,
monitors system health, allocates resources, detects emergent
cognitive rhythms, and measures cross-system coherence.
"""

from ecodiaos.systems.synapse.clock import CognitiveClock
from ecodiaos.systems.synapse.coherence import CoherenceMonitor
from ecodiaos.systems.synapse.degradation import DegradationManager
from ecodiaos.systems.synapse.event_bus import EventBus
from ecodiaos.systems.synapse.health import HealthMonitor
from ecodiaos.systems.synapse.resources import ResourceAllocator
from ecodiaos.systems.synapse.rhythm import EmergentRhythmDetector
from ecodiaos.systems.synapse.service import SynapseService
from ecodiaos.systems.synapse.types import (
    ClockState,
    CoherenceSnapshot,
    CycleResult,
    DegradationLevel,
    DegradationStrategy,
    ManagedSystemProtocol,
    ResourceAllocation,
    ResourceSnapshot,
    RhythmSnapshot,
    RhythmState,
    SynapseEvent,
    SynapseEventType,
    SystemBudget,
    SystemHeartbeat,
    SystemHealthRecord,
    SystemStatus,
)

__all__ = [
    # Service
    "SynapseService",
    # Sub-systems
    "CognitiveClock",
    "CoherenceMonitor",
    "DegradationManager",
    "EventBus",
    "HealthMonitor",
    "ResourceAllocator",
    "EmergentRhythmDetector",
    # Types
    "ClockState",
    "CoherenceSnapshot",
    "CycleResult",
    "DegradationLevel",
    "DegradationStrategy",
    "ManagedSystemProtocol",
    "ResourceAllocation",
    "ResourceSnapshot",
    "RhythmSnapshot",
    "RhythmState",
    "SynapseEvent",
    "SynapseEventType",
    "SystemBudget",
    "SystemHeartbeat",
    "SystemHealthRecord",
    "SystemStatus",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\clock.py =====

"""
EcodiaOS — Cognitive Cycle Clock

The beating heart of EOS. Drives Atune's workspace cycle on every theta tick,
with arousal-modulated adaptive timing.

The clock is the most protected component in the organism. It catches every
exception, logs it, backs off, and continues. The clock never dies.

Timing model:
  - Base period: 150ms (~6.7 Hz)
  - High arousal: contracts toward 80ms (more alert, faster processing)
  - Low arousal: expands toward 500ms (reflective, energy-conserving)
  - EMA smoothing (alpha=0.1): arousal spikes take ~1.5s to fully propagate
  - Overrun detection: warns when cycle exceeds budget
"""

from __future__ import annotations

import asyncio
import time
from collections import deque
from collections.abc import Callable, Coroutine
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.config import SynapseConfig
from ecodiaos.systems.atune.types import SystemLoad
from ecodiaos.systems.synapse.types import ClockState, CycleResult

if TYPE_CHECKING:
    from ecodiaos.systems.atune.service import AtuneService

logger = structlog.get_logger("ecodiaos.systems.synapse.clock")

# Callback signature for per-cycle post-processing
CycleCallback = Callable[[CycleResult], Coroutine[Any, Any, None]]

# Error recovery backoff (ms) — clock backs off on failure, never dies
_ERROR_BACKOFF_MS: float = 500.0

# Number of recent cycle times to keep for jitter measurement
_JITTER_WINDOW: int = 50


class CognitiveClock:
    """
    The theta rhythm that drives EOS's stream of consciousness.

    Every tick triggers one workspace cycle in Atune. Arousal from
    the organism's affect state modulates the cycle period — high arousal
    means faster cycles (more alert), low arousal means slower cycles
    (more reflective, energy-conserving).

    The clock is designed to be unkillable. Any exception during a cycle
    is caught, logged, and the clock backs off before continuing.
    """

    def __init__(
        self,
        atune: AtuneService,
        config: SynapseConfig,
    ) -> None:
        self._atune = atune
        self._config = config
        self._logger = logger.bind(component="clock")

        # Timing state
        self._base_period_ms: float = float(config.cycle_period_ms)
        self._min_period_ms: float = float(config.min_cycle_period_ms)
        self._max_period_ms: float = float(config.max_cycle_period_ms)
        self._current_period_ms: float = self._base_period_ms
        self._target_period_ms: float = self._base_period_ms

        # Arousal tracking (EMA smoothed)
        self._current_arousal: float = 0.1
        self._arousal_alpha: float = 0.1  # EMA smoothing factor

        # Coherence drag — when cross-system coherence drops, the clock
        # slows down to give systems time to synchronize. 0.0 = no drag,
        # 1.0 = max drag (push period toward max).
        self._coherence_drag: float = 0.0

        # Cycle counters
        self._cycle_count: int = 0
        self._overrun_count: int = 0
        self._error_count: int = 0
        self._total_elapsed_ms: float = 0.0

        # Jitter measurement (rolling window of actual cycle durations)
        self._recent_periods: deque[float] = deque(maxlen=_JITTER_WINDOW)

        # Control
        self._running: bool = False
        self._paused: bool = False
        self._task: asyncio.Task[None] | None = None

        # Per-cycle callback — set by SynapseService for rhythm/coherence feeding
        self._on_cycle: CycleCallback | None = None

    # ─── Control ─────────────────────────────────────────────────────

    def start(self) -> asyncio.Task[None]:
        """Start the heartbeat. Returns the background task handle."""
        if self._running:
            raise RuntimeError("Clock is already running")

        self._running = True
        self._paused = False
        self._task = asyncio.create_task(
            self._run_loop(),
            name="synapse_cognitive_clock",
        )
        self._logger.info(
            "clock_started",
            base_period_ms=self._base_period_ms,
            min_ms=self._min_period_ms,
            max_ms=self._max_period_ms,
        )
        return self._task

    async def stop(self) -> None:
        """Graceful stop. Waits for the current cycle to complete."""
        self._running = False
        if self._task and not self._task.done():
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        self._task = None
        self._logger.info(
            "clock_stopped",
            total_cycles=self._cycle_count,
            overruns=self._overrun_count,
            errors=self._error_count,
        )

    def pause(self) -> None:
        """Pause the clock (e.g., during safe mode). Cycle loop sleeps."""
        self._paused = True
        self._logger.info("clock_paused", cycle=self._cycle_count)

    def resume(self) -> None:
        """Resume after pause."""
        self._paused = False
        self._logger.info("clock_resumed", cycle=self._cycle_count)

    def set_coherence_drag(self, drag: float) -> None:
        """
        Set coherence drag to slow the clock when systems are desynchronized.

        Called by SynapseService when the CoherenceMonitor detects low
        cross-system coherence. The drag value (0.0 - 1.0) pushes the
        cycle period toward the maximum, giving systems time to resync.
        """
        self._coherence_drag = max(0.0, min(1.0, drag))

    def set_on_cycle(self, callback: CycleCallback) -> None:
        """Register the per-cycle callback (set by SynapseService)."""
        self._on_cycle = callback

    # ─── State ───────────────────────────────────────────────────────

    @property
    def state(self) -> ClockState:
        """Snapshot of the clock's current state."""
        jitter = self._compute_jitter()
        rate_hz = 1000.0 / self._current_period_ms if self._current_period_ms > 0 else 0.0
        return ClockState(
            running=self._running,
            paused=self._paused,
            cycle_count=self._cycle_count,
            current_period_ms=self._current_period_ms,
            target_period_ms=self._target_period_ms,
            jitter_ms=jitter,
            arousal=self._current_arousal,
            overrun_count=self._overrun_count,
            actual_rate_hz=rate_hz,
        )

    @property
    def cycle_count(self) -> int:
        return self._cycle_count

    # ─── The Heartbeat ───────────────────────────────────────────────

    async def _run_loop(self) -> None:
        """
        The main clock loop. Runs indefinitely until stopped.

        On every tick:
        1. Read arousal from Atune's current affect
        2. Run atune.run_cycle() with a SystemLoad estimate
        3. Compute CycleResult
        4. Fire the on_cycle callback
        5. Adapt the period based on arousal
        6. Sleep for the remainder of the period

        The clock NEVER dies. Exceptions are caught, logged, and backed off.
        """
        self._logger.info("clock_loop_starting")

        while self._running:
            # ── Paused — sleep without ticking ──
            if self._paused:
                await asyncio.sleep(0.5)
                continue

            t0 = time.monotonic()

            try:
                # 1. Read current arousal from the organism's affect state
                self._update_arousal()

                # 2. Build a lightweight SystemLoad for Atune
                system_load = self._build_system_load()

                # 3. Run the workspace cycle — the core cognitive tick
                broadcast = await self._atune.run_cycle(system_load=system_load)

                # 4. Measure elapsed time
                elapsed_ms = (time.monotonic() - t0) * 1000.0
                self._total_elapsed_ms += elapsed_ms
                self._recent_periods.append(elapsed_ms)
                self._cycle_count += 1

                # 5. Check for overrun
                overrun = elapsed_ms > self._current_period_ms
                if overrun:
                    self._overrun_count += 1
                    if self._overrun_count % 100 == 1:
                        self._logger.warning(
                            "clock_overrun",
                            cycle=self._cycle_count,
                            elapsed_ms=round(elapsed_ms, 2),
                            budget_ms=round(self._current_period_ms, 2),
                        )

                # 6. Build cycle result
                result = CycleResult(
                    cycle_number=self._cycle_count,
                    elapsed_ms=round(elapsed_ms, 2),
                    budget_ms=round(self._current_period_ms, 2),
                    overrun=overrun,
                    broadcast_id=broadcast.broadcast_id if broadcast else None,
                    had_broadcast=broadcast is not None,
                    arousal=round(self._current_arousal, 4),
                    salience_composite=(
                        broadcast.salience.composite if broadcast else 0.0
                    ),
                )

                # 7. Fire the on_cycle callback
                if self._on_cycle is not None:
                    try:
                        await self._on_cycle(result)
                    except Exception as cb_exc:
                        self._logger.error(
                            "on_cycle_callback_error",
                            error=str(cb_exc),
                        )

                # 8. Adapt the period based on arousal
                self._adapt_period()

                # 9. Sleep for the remainder
                sleep_ms = max(0.0, self._current_period_ms - elapsed_ms)
                if sleep_ms > 0:
                    await asyncio.sleep(sleep_ms / 1000.0)

            except asyncio.CancelledError:
                self._logger.info("clock_loop_cancelled")
                return
            except Exception as exc:
                self._error_count += 1
                self._logger.error(
                    "clock_cycle_error",
                    cycle=self._cycle_count,
                    error=str(exc),
                    error_count=self._error_count,
                )
                # Back off — but never die
                await asyncio.sleep(_ERROR_BACKOFF_MS / 1000.0)

    # ─── Adaptive Timing ─────────────────────────────────────────────

    def _update_arousal(self) -> None:
        """
        Read the organism's current arousal and smooth with EMA.

        High arousal = faster cycles (more alert, reactive).
        Low arousal = slower cycles (reflective, energy-conserving).
        """
        try:
            raw_arousal = self._atune.current_affect.arousal
        except Exception:
            raw_arousal = 0.1

        # Exponential moving average smoothing
        self._current_arousal = (
            self._arousal_alpha * raw_arousal
            + (1 - self._arousal_alpha) * self._current_arousal
        )

    def _adapt_period(self) -> None:
        """
        Modulate cycle period based on smoothed arousal and coherence.

        Linear interpolation: high arousal → min period, low arousal → max period.
        Coherence drag: low coherence → push period toward max (slow down to resync).
        Smooth transition (EMA with alpha=0.1) to prevent sudden jumps.
        """
        # Target period: linear interpolation from arousal
        # arousal=1.0 → min_period, arousal=0.0 → max_period
        target = self._max_period_ms - self._current_arousal * (
            self._max_period_ms - self._min_period_ms
        )

        # Coherence drag: when systems are desynchronized, slow down
        # to give them time to catch up. Drag pushes 30% of the remaining
        # headroom toward max period.
        if self._coherence_drag > 0:
            headroom = self._max_period_ms - target
            target += headroom * self._coherence_drag * 0.3

        self._target_period_ms = target

        # Smooth transition — don't jump suddenly
        self._current_period_ms = (
            self._current_period_ms * 0.9 + target * 0.1
        )

        # Clamp to configured bounds
        self._current_period_ms = max(
            self._min_period_ms,
            min(self._max_period_ms, self._current_period_ms),
        )

    def _build_system_load(self) -> SystemLoad:
        """Build a lightweight SystemLoad for Atune from cycle telemetry."""
        # CPU utilisation approximation: how much of the budget we're using
        if self._cycle_count > 0 and self._recent_periods:
            avg_elapsed = sum(self._recent_periods) / len(self._recent_periods)
            cpu_util = min(1.0, avg_elapsed / self._current_period_ms)
        else:
            cpu_util = 0.0

        return SystemLoad(
            cpu_utilisation=round(cpu_util, 4),
            memory_utilisation=0.0,  # Will be enriched by ResourceAllocator
            queue_depth=0,
        )

    def _compute_jitter(self) -> float:
        """
        Compute jitter as the standard deviation of recent cycle periods.

        High jitter indicates erratic timing — a signal used by the
        EmergentRhythmDetector to detect stress states.
        """
        if len(self._recent_periods) < 2:
            return 0.0

        periods = list(self._recent_periods)
        n = len(periods)
        mean = sum(periods) / n
        variance = sum((p - mean) ** 2 for p in periods) / n
        return variance ** 0.5

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\coherence.py =====

"""
EcodiaOS — Cross-System Coherence Monitor

IIT-inspired measurement of consciousness quality. Measures how much
information is integrated across the organism's cognitive systems
rather than processed independently.

Four metrics compose the coherence snapshot:

  phi_approximation    — Integration of information across systems
  system_resonance     — How in-sync system responses are (latency uniformity)
  broadcast_diversity  — Entropy of broadcast content sources (topic diversity)
  response_synchrony   — Temporal uniformity of system response latencies

This is the closest computational analogue to Tononi's Integrated
Information Theory (2004) that can be computed online from cycle telemetry.
"""

from __future__ import annotations

import math
from collections import deque
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.synapse.types import (
    CoherenceSnapshot,
    SynapseEvent,
    SynapseEventType,
)

if TYPE_CHECKING:
    from ecodiaos.systems.synapse.event_bus import EventBus

logger = structlog.get_logger("ecodiaos.systems.synapse.coherence")

# Window size for coherence computation
_COHERENCE_WINDOW: int = 50

# Minimum data points before computing coherence
_MIN_DATA_POINTS: int = 10

# Threshold for emitting a coherence shift event
_SHIFT_THRESHOLD: float = 0.15

# Weight for each component in the composite score
_W_PHI: float = 0.35
_W_RESONANCE: float = 0.25
_W_DIVERSITY: float = 0.20
_W_SYNCHRONY: float = 0.20


class CoherenceMonitor:
    """
    Measures cross-system integration quality using IIT-inspired metrics.

    Fed per-cycle data by SynapseService:
    - record_broadcast(): per-cycle broadcast source/salience data
    - compute(): triggered every N cycles to produce a CoherenceSnapshot

    The composite score represents the organism's "consciousness quality" —
    how well its systems are working together as an integrated whole
    rather than isolated modules.
    """

    def __init__(self, event_bus: EventBus | None = None) -> None:
        self._event_bus = event_bus
        self._logger = logger.bind(component="coherence_monitor")

        # Per-cycle data (rolling window)
        self._broadcast_sources: deque[str] = deque(maxlen=_COHERENCE_WINDOW)
        self._broadcast_saliences: deque[float] = deque(maxlen=_COHERENCE_WINDOW)
        self._had_broadcasts: deque[bool] = deque(maxlen=_COHERENCE_WINDOW)

        # Per-cycle system response data (populated externally)
        self._response_latencies: deque[list[float]] = deque(maxlen=_COHERENCE_WINDOW)
        self._responding_system_counts: deque[int] = deque(maxlen=_COHERENCE_WINDOW)
        self._total_registered_systems: int = 0

        # Latest snapshot
        self._latest: CoherenceSnapshot = CoherenceSnapshot()
        self._previous_composite: float = 0.0

        # Metrics
        self._total_computations: int = 0

    # ─── Data Recording ──────────────────────────────────────────────

    def record_broadcast(
        self,
        source: str,
        salience: float,
        had_content: bool,
        response_latencies: list[float] | None = None,
        responding_systems: int = 0,
    ) -> None:
        """
        Record per-cycle broadcast data for coherence computation.

        Called by SynapseService after every theta tick.

        Args:
            source: The broadcast content source identifier (e.g., "api.text_chat")
            salience: The broadcast's composite salience score
            had_content: Whether the cycle produced a broadcast
            response_latencies: Optional list of per-system response times (ms)
            responding_systems: Number of systems that received the broadcast
        """
        self._broadcast_sources.append(source if had_content else "")
        self._broadcast_saliences.append(salience)
        self._had_broadcasts.append(had_content)

        if response_latencies is not None:
            self._response_latencies.append(response_latencies)
        self._responding_system_counts.append(responding_systems)

    def set_total_systems(self, count: int) -> None:
        """Set the total number of registered systems (for phi computation)."""
        self._total_registered_systems = count

    # ─── Computation ─────────────────────────────────────────────────

    async def compute(self) -> CoherenceSnapshot:
        """
        Compute the coherence snapshot from accumulated cycle data.

        Called every N cycles (default 50) by SynapseService.
        Returns the new CoherenceSnapshot.
        """
        n = len(self._broadcast_saliences)
        if n < _MIN_DATA_POINTS:
            return self._latest

        phi = self._compute_phi()
        resonance = self._compute_resonance()
        diversity = self._compute_diversity()
        synchrony = self._compute_synchrony()

        composite = (
            _W_PHI * phi
            + _W_RESONANCE * resonance
            + _W_DIVERSITY * diversity
            + _W_SYNCHRONY * synchrony
        )

        snapshot = CoherenceSnapshot(
            phi_approximation=round(phi, 4),
            system_resonance=round(resonance, 4),
            broadcast_diversity=round(diversity, 4),
            response_synchrony=round(synchrony, 4),
            composite=round(composite, 4),
            window_cycles=n,
        )

        # Check for significant shift
        shift = abs(composite - self._previous_composite)
        if shift > _SHIFT_THRESHOLD and self._total_computations > 0:
            await self._emit_shift(snapshot, shift)

        self._previous_composite = composite
        self._latest = snapshot
        self._total_computations += 1

        return snapshot

    # ─── Component Metrics ───────────────────────────────────────────

    def _compute_phi(self) -> float:
        """
        Approximate integrated information (Φ).

        Computed from:
        - Response rate: fraction of registered systems that respond
        - Response diversity: how many different systems participate
        - Temporal integration: broadcast density (information flowing)

        Higher Φ = more information is being integrated across the whole.
        """
        if self._total_registered_systems == 0:
            return 0.0

        n = len(self._had_broadcasts)
        if n == 0:
            return 0.0

        # Response rate: average responding systems / total systems
        if self._responding_system_counts:
            avg_responding = sum(self._responding_system_counts) / len(self._responding_system_counts)
            response_rate = min(1.0, avg_responding / max(1, self._total_registered_systems))
        else:
            response_rate = 0.0

        # Broadcast density: fraction of cycles with content
        broadcast_count = sum(1 for b in self._had_broadcasts if b)
        density = broadcast_count / n

        # Integration factor: salience-weighted density
        saliences = list(self._broadcast_saliences)
        salience_mean = sum(saliences) / n if saliences else 0.0
        integration = density * (0.5 + 0.5 * salience_mean)

        # Φ ≈ response_rate × integration × diversity_factor
        diversity_factor = self._compute_diversity()
        phi = response_rate * integration * (0.5 + 0.5 * diversity_factor)

        return min(1.0, phi)

    def _compute_resonance(self) -> float:
        """
        System resonance: how in-sync system responses are.

        Low latency variance across systems = high resonance.
        If all systems respond in roughly the same time, they're "resonating."
        """
        if not self._response_latencies:
            return 0.5  # Default moderate resonance when no data

        # Compute mean variance across all recorded latency sets
        variances = []
        for latencies in self._response_latencies:
            if len(latencies) >= 2:
                mean = sum(latencies) / len(latencies)
                var = sum((l - mean) ** 2 for l in latencies) / len(latencies)
                variances.append(var)

        if not variances:
            return 0.5

        mean_variance = sum(variances) / len(variances)
        # Normalise: variance=0 → resonance=1.0, high variance → resonance→0
        resonance = 1.0 / (1.0 + math.sqrt(mean_variance) / 50.0)
        return min(1.0, resonance)

    def _compute_diversity(self) -> float:
        """
        Broadcast diversity: Shannon entropy of broadcast sources.

        Are we processing diverse topics or stuck on one input?
        Higher entropy = healthier, more diverse cognitive activity.
        """
        sources = [s for s in self._broadcast_sources if s]
        if not sources:
            return 0.0

        # Count unique sources
        counts: dict[str, int] = {}
        for s in sources:
            counts[s] = counts.get(s, 0) + 1

        total = len(sources)
        unique_count = len(counts)

        if unique_count <= 1:
            return 0.0

        # Shannon entropy (normalised to [0, 1])
        entropy = 0.0
        for count in counts.values():
            p = count / total
            if p > 0:
                entropy -= p * math.log2(p)

        # Normalise by log2(unique_count) to get [0, 1]
        max_entropy = math.log2(unique_count)
        if max_entropy == 0:
            return 0.0

        return min(1.0, entropy / max_entropy)

    def _compute_synchrony(self) -> float:
        """
        Response synchrony: uniformity of response latencies.

        1.0 / (1.0 + std_dev(response_latencies))

        When all systems respond in uniform time, synchrony is high.
        """
        if not self._response_latencies:
            return 0.5

        # Flatten all latencies
        all_latencies = []
        for latencies in self._response_latencies:
            all_latencies.extend(latencies)

        if len(all_latencies) < 2:
            return 0.5

        mean = sum(all_latencies) / len(all_latencies)
        variance = sum((l - mean) ** 2 for l in all_latencies) / len(all_latencies)
        std_dev = math.sqrt(variance)

        # Normalise: std_dev=0 → synchrony=1.0
        synchrony = 1.0 / (1.0 + std_dev / 20.0)
        return min(1.0, synchrony)

    # ─── Events ──────────────────────────────────────────────────────

    async def _emit_shift(self, snapshot: CoherenceSnapshot, shift: float) -> None:
        """Emit a coherence shift event when composite changes significantly."""
        if self._event_bus is None:
            return

        direction = "increasing" if snapshot.composite > self._previous_composite else "decreasing"

        self._logger.info(
            "coherence_shift_detected",
            direction=direction,
            shift=round(shift, 4),
            new_composite=snapshot.composite,
            previous=round(self._previous_composite, 4),
        )

        await self._event_bus.emit(SynapseEvent(
            event_type=SynapseEventType.COHERENCE_SHIFT,
            data={
                "direction": direction,
                "shift": round(shift, 4),
                "composite": snapshot.composite,
                "phi": snapshot.phi_approximation,
                "resonance": snapshot.system_resonance,
                "diversity": snapshot.broadcast_diversity,
                "synchrony": snapshot.response_synchrony,
            },
        ))

    # ─── Accessors ───────────────────────────────────────────────────

    @property
    def latest(self) -> CoherenceSnapshot:
        return self._latest

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "total_computations": self._total_computations,
            "latest_composite": self._latest.composite,
            "latest_phi": self._latest.phi_approximation,
            "data_points": len(self._broadcast_saliences),
            "registered_systems": self._total_registered_systems,
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\degradation.py =====

"""
EcodiaOS — Synapse Degradation Manager

Per-system graceful fallback strategies. When a system fails, Synapse
applies the appropriate degradation strategy: safe mode for critical
systems, queuing for Axon, raw fallback for Voxis, etc.

Auto-restart with exponential backoff ensures failed systems get
multiple recovery attempts before giving up.
"""

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.synapse.types import (
    DegradationLevel,
    DegradationStrategy,
    SynapseEvent,
    SynapseEventType,
    SystemStatus,
)

if TYPE_CHECKING:
    from ecodiaos.systems.synapse.event_bus import EventBus
    from ecodiaos.systems.synapse.health import HealthMonitor

logger = structlog.get_logger("ecodiaos.systems.synapse.degradation")


# ─── Per-System Strategies (from spec) ─────────────────────────────

_STRATEGIES: dict[str, DegradationStrategy] = {
    "equor": DegradationStrategy(
        system_id="equor",
        triggers_safe_mode=True,
        fallback_behavior="Enter safe mode. No actions without ethics.",
        auto_restart=True,
        max_restart_attempts=3,
    ),
    "memory": DegradationStrategy(
        system_id="memory",
        triggers_safe_mode=True,
        fallback_behavior="Enter safe mode. Use in-context memory only.",
        auto_restart=True,
        max_restart_attempts=3,
    ),
    "atune": DegradationStrategy(
        system_id="atune",
        triggers_safe_mode=True,
        fallback_behavior="Enter safe mode. Direct input passthrough, no salience scoring.",
        auto_restart=True,
        max_restart_attempts=3,
    ),
    "nova": DegradationStrategy(
        system_id="nova",
        triggers_safe_mode=False,
        fallback_behavior="Voxis responds with 'I'm having difficulty thinking right now.'",
        auto_restart=True,
        max_restart_attempts=3,
    ),
    "voxis": DegradationStrategy(
        system_id="voxis",
        triggers_safe_mode=False,
        fallback_behavior="Use raw LLM output without personality rendering.",
        auto_restart=True,
        max_restart_attempts=3,
    ),
    "axon": DegradationStrategy(
        system_id="axon",
        triggers_safe_mode=False,
        fallback_behavior="Queue intents, retry when Axon recovers.",
        auto_restart=True,
        max_restart_attempts=3,
    ),
    "evo": DegradationStrategy(
        system_id="evo",
        triggers_safe_mode=False,
        fallback_behavior="Skip consolidation. No learning, but core function preserved.",
        auto_restart=True,
        max_restart_attempts=2,
    ),
    "simula": DegradationStrategy(
        system_id="simula",
        triggers_safe_mode=False,
        fallback_behavior="No evolution. Fully functional otherwise.",
        auto_restart=True,
        max_restart_attempts=2,
    ),
}


class DegradationManager:
    """
    Manages graceful degradation when cognitive systems fail.

    Each system has a defined fallback strategy. Critical systems
    (equor, memory, atune) trigger safe mode. Non-critical failures
    apply specific fallback behaviours and attempt auto-restart
    with exponential backoff.
    """

    def __init__(
        self,
        event_bus: EventBus,
        health_monitor: HealthMonitor,
    ) -> None:
        self._event_bus = event_bus
        self._health = health_monitor
        self._logger = logger.bind(component="degradation_manager")

        # Managed systems (for restart)
        self._systems: dict[str, Any] = {}

        # Restart tracking
        self._restart_attempts: dict[str, int] = {}
        self._restart_tasks: dict[str, asyncio.Task[None]] = {}

        # Current degradation level
        self._level: DegradationLevel = DegradationLevel.NOMINAL

    # ─── System Registration ─────────────────────────────────────────

    def register_system(self, system: Any) -> None:
        """Register a system for potential restart management."""
        sid = getattr(system, "system_id", None)
        if sid:
            self._systems[sid] = system

    # ─── Failure Handling ────────────────────────────────────────────

    async def handle_failure(self, system_id: str) -> None:
        """
        Apply the degradation strategy for a failed system.

        1. Look up the strategy
        2. Log the fallback behaviour
        3. Attempt auto-restart with exponential backoff
        4. Update the overall degradation level
        """
        strategy = _STRATEGIES.get(system_id)
        if strategy is None:
            self._logger.warning(
                "no_degradation_strategy",
                system_id=system_id,
            )
            return

        self._logger.warning(
            "applying_degradation_strategy",
            system_id=system_id,
            fallback=strategy.fallback_behavior,
            triggers_safe_mode=strategy.triggers_safe_mode,
        )

        # Update level
        self._update_level()

        # Auto-restart if configured
        if strategy.auto_restart:
            attempts = self._restart_attempts.get(system_id, 0)
            if attempts < strategy.max_restart_attempts:
                self._schedule_restart(system_id, strategy, attempts)
            else:
                self._logger.error(
                    "max_restart_attempts_reached",
                    system_id=system_id,
                    attempts=attempts,
                )

    async def record_recovery(self, system_id: str) -> None:
        """Record that a system has recovered. Reset restart counter."""
        self._restart_attempts[system_id] = 0

        # Cancel any pending restart task
        task = self._restart_tasks.pop(system_id, None)
        if task and not task.done():
            task.cancel()

        self._update_level()
        self._logger.info("recovery_recorded", system_id=system_id)

    # ─── Auto-Restart ────────────────────────────────────────────────

    def _schedule_restart(
        self,
        system_id: str,
        strategy: DegradationStrategy,
        attempt: int,
    ) -> None:
        """Schedule an auto-restart with exponential backoff."""
        # Exponential backoff: base * 2^attempt
        delay_s = strategy.restart_backoff_base_s * (2 ** attempt)

        self._logger.info(
            "scheduling_restart",
            system_id=system_id,
            attempt=attempt + 1,
            max_attempts=strategy.max_restart_attempts,
            delay_s=delay_s,
        )

        task = asyncio.create_task(
            self._restart_system(system_id, attempt, delay_s),
            name=f"restart_{system_id}_{attempt}",
        )
        self._restart_tasks[system_id] = task

    async def _restart_system(
        self,
        system_id: str,
        attempt: int,
        delay_s: float,
    ) -> None:
        """Wait for backoff, then attempt to restart the system."""
        try:
            await asyncio.sleep(delay_s)

            system = self._systems.get(system_id)
            if system is None:
                self._logger.warning("restart_no_system_ref", system_id=system_id)
                return

            self._restart_attempts[system_id] = attempt + 1

            await self._event_bus.emit(SynapseEvent(
                event_type=SynapseEventType.SYSTEM_RESTARTING,
                data={
                    "system_id": system_id,
                    "attempt": attempt + 1,
                },
            ))

            # Attempt shutdown then re-initialize
            try:
                if hasattr(system, "shutdown"):
                    await system.shutdown()
            except Exception:
                pass  # Shutdown may fail on a broken system

            if hasattr(system, "initialize"):
                await system.initialize()
                self._logger.info(
                    "system_restarted",
                    system_id=system_id,
                    attempt=attempt + 1,
                )
            else:
                self._logger.warning(
                    "system_no_initialize",
                    system_id=system_id,
                )

        except asyncio.CancelledError:
            return
        except Exception as exc:
            self._logger.error(
                "restart_failed",
                system_id=system_id,
                attempt=attempt + 1,
                error=str(exc),
            )

    # ─── Level Computation ───────────────────────────────────────────

    def _update_level(self) -> None:
        """Recompute the overall degradation level from system health."""
        if self._health.is_safe_mode:
            self._level = DegradationLevel.SAFE_MODE
        elif len(self._health.failed_systems) > 0:
            self._level = DegradationLevel.DEGRADED
        else:
            self._level = DegradationLevel.NOMINAL

    @property
    def level(self) -> DegradationLevel:
        self._update_level()
        return self._level

    def get_strategy(self, system_id: str) -> DegradationStrategy | None:
        return _STRATEGIES.get(system_id)

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "level": self._level.value,
            "restart_attempts": dict(self._restart_attempts),
            "active_restart_tasks": [
                sid for sid, t in self._restart_tasks.items()
                if not t.done()
            ],
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\event_bus.py =====

"""
EcodiaOS — Synapse Event Bus

Dual-output event publication: in-memory callbacks for internal coordination
plus Redis pub/sub for the Alive WebSocket layer.

High-frequency events (CYCLE_COMPLETED at ~6.7Hz) skip in-memory callbacks
by default to avoid overwhelming listeners — they go to Redis only.
"""

from __future__ import annotations

import asyncio
from collections import defaultdict, deque
from collections.abc import Callable, Coroutine
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.synapse.types import SynapseEvent, SynapseEventType

if TYPE_CHECKING:
    from ecodiaos.clients.redis import RedisClient

logger = structlog.get_logger("ecodiaos.systems.synapse.event_bus")

# Callback signature: async def handler(event: SynapseEvent) -> None
EventCallback = Callable[[SynapseEvent], Coroutine[Any, Any, None]]

# Events that fire every cycle — too noisy for in-memory callbacks
_HIGH_FREQUENCY_EVENTS: frozenset[SynapseEventType] = frozenset({
    SynapseEventType.CYCLE_COMPLETED,
})

# Maximum time a callback gets before we log a warning and move on
_CALLBACK_TIMEOUT_S: float = 0.1

# Maximum recent events to keep in the ring buffer per event type
_RECENT_BUFFER_SIZE: int = 100

# Redis channel for Synapse events
_REDIS_CHANNEL: str = "synapse_events"


class EventBus:
    """
    Synapse inter-system event bus.

    Provides two delivery mechanisms:
    1. In-memory async callbacks — for internal system coordination (low latency)
    2. Redis pub/sub — for external consumers (Alive WebSocket, monitoring)

    High-frequency events (CYCLE_COMPLETED) only go to Redis to avoid
    callback overhead on every theta tick.
    """

    def __init__(self, redis: RedisClient | None = None) -> None:
        self._redis = redis
        self._logger = logger.bind(component="event_bus")

        # Per-type callback registrations
        self._subscribers: dict[SynapseEventType, list[EventCallback]] = defaultdict(list)
        # Catch-all subscribers (receive every event)
        self._global_subscribers: list[EventCallback] = []

        # Ring buffers for recent event history
        self._recent: dict[SynapseEventType, deque[SynapseEvent]] = defaultdict(
            lambda: deque(maxlen=_RECENT_BUFFER_SIZE)
        )

        # Metrics
        self._total_emitted: int = 0
        self._total_redis_failures: int = 0
        self._total_callback_timeouts: int = 0

    # ─── Subscription ────────────────────────────────────────────────

    def subscribe(
        self,
        event_type: SynapseEventType,
        callback: EventCallback,
    ) -> None:
        """Register a callback for a specific event type."""
        self._subscribers[event_type].append(callback)

    def subscribe_all(self, callback: EventCallback) -> None:
        """Register a callback that receives every event (except high-frequency)."""
        self._global_subscribers.append(callback)

    # ─── Emission ────────────────────────────────────────────────────

    async def emit(self, event: SynapseEvent) -> None:
        """
        Publish an event to all registered listeners.

        In-memory callbacks fire first (with timeout protection),
        then Redis publication (fire-and-forget, failure-tolerant).

        High-frequency events skip in-memory callbacks entirely.
        """
        self._total_emitted += 1
        self._recent[event.event_type].append(event)

        is_high_freq = event.event_type in _HIGH_FREQUENCY_EVENTS

        # ── In-memory callbacks (skip for high-frequency events) ──
        if not is_high_freq:
            callbacks = list(self._subscribers.get(event.event_type, []))
            callbacks.extend(self._global_subscribers)

            if callbacks:
                await self._dispatch_callbacks(callbacks, event)

        # ── Redis publication ──
        await self._publish_redis(event)

    async def _dispatch_callbacks(
        self,
        callbacks: list[EventCallback],
        event: SynapseEvent,
    ) -> None:
        """Dispatch event to callbacks with per-callback timeout protection."""
        for callback in callbacks:
            try:
                await asyncio.wait_for(
                    callback(event),
                    timeout=_CALLBACK_TIMEOUT_S,
                )
            except asyncio.TimeoutError:
                self._total_callback_timeouts += 1
                self._logger.warning(
                    "event_callback_timeout",
                    event_type=event.event_type.value,
                    callback=getattr(callback, "__name__", str(callback)),
                )
            except Exception as exc:
                self._logger.error(
                    "event_callback_error",
                    event_type=event.event_type.value,
                    error=str(exc),
                )

    async def _publish_redis(self, event: SynapseEvent) -> None:
        """Publish event to Redis. Failure is logged but never blocks."""
        if self._redis is None:
            return

        try:
            payload = {
                "id": event.id,
                "type": event.event_type.value,
                "ts": event.timestamp.isoformat(),
                "data": event.data,
                "source": event.source_system,
            }
            await self._redis.publish(_REDIS_CHANNEL, payload)
        except Exception as exc:
            self._total_redis_failures += 1
            # Log at debug for high-frequency events, warning otherwise
            if event.event_type in _HIGH_FREQUENCY_EVENTS:
                self._logger.debug("redis_publish_failed", error=str(exc))
            else:
                self._logger.warning(
                    "redis_publish_failed",
                    event_type=event.event_type.value,
                    error=str(exc),
                )

    # ─── Query ───────────────────────────────────────────────────────

    def recent(
        self,
        event_type: SynapseEventType,
        limit: int = 10,
    ) -> list[SynapseEvent]:
        """Return recent events of a given type (most recent first)."""
        buf = self._recent.get(event_type)
        if not buf:
            return []
        items = list(buf)
        items.reverse()
        return items[:limit]

    # ─── Stats ───────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "total_emitted": self._total_emitted,
            "redis_failures": self._total_redis_failures,
            "callback_timeouts": self._total_callback_timeouts,
            "subscriber_count": sum(
                len(v) for v in self._subscribers.values()
            ) + len(self._global_subscribers),
            "recent_buffer_sizes": {
                et.value: len(buf) for et, buf in self._recent.items()
            },
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\health.py =====

"""
EcodiaOS — Synapse Health Monitor

Background 5-second polling of all managed cognitive systems.
Three consecutive missed heartbeats → system declared failed.
Critical system failure (equor, memory, atune) → safe mode.

Health monitoring is the immune system of the organism. It detects failures,
triggers degradation strategies, and coordinates recovery.
"""

from __future__ import annotations

import asyncio
import time
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.config import SynapseConfig
from ecodiaos.systems.synapse.types import (
    SynapseEvent,
    SynapseEventType,
    SystemHealthRecord,
    SystemStatus,
)

if TYPE_CHECKING:
    from ecodiaos.systems.synapse.degradation import DegradationManager
    from ecodiaos.systems.synapse.event_bus import EventBus

logger = structlog.get_logger("ecodiaos.systems.synapse.health")

# Health check timeout per system (seconds)
_HEALTH_CHECK_TIMEOUT_S: float = 2.0

# Critical systems — failure triggers safe mode
_CRITICAL_SYSTEMS: frozenset[str] = frozenset({"equor", "memory", "atune"})


class HealthMonitor:
    """
    Monitors the health of all registered cognitive systems via periodic
    heartbeat polling. Detects failures, triggers degradation, and
    coordinates recovery.

    The monitor runs as a background asyncio task, polling every
    health_check_interval_ms (default 5000ms).
    """

    def __init__(
        self,
        config: SynapseConfig,
        event_bus: EventBus,
    ) -> None:
        self._config = config
        self._event_bus = event_bus
        self._logger = logger.bind(component="health_monitor")

        # Managed systems (duck-typed: system_id + async health())
        self._systems: dict[str, Any] = {}
        # Per-system health records
        self._records: dict[str, SystemHealthRecord] = {}

        # Safe mode state
        self._safe_mode: bool = False
        self._safe_mode_reason: str = ""

        # Degradation manager (wired after construction)
        self._degradation: DegradationManager | None = None

        # Background task
        self._task: asyncio.Task[None] | None = None
        self._running: bool = False

        # Metrics
        self._total_checks: int = 0
        self._total_failures_detected: int = 0
        self._total_recoveries: int = 0
        self._restart_count: int = 0

    # ─── Registration ────────────────────────────────────────────────

    def register(self, system: Any) -> None:
        """
        Register a cognitive system for health monitoring.

        The system must have:
          - system_id: str
          - async health() -> dict[str, Any]
        """
        sid = getattr(system, "system_id", None)
        if sid is None:
            raise ValueError(f"System {system} has no system_id attribute")

        self._systems[sid] = system
        self._records[sid] = SystemHealthRecord(
            system_id=sid,
            status=SystemStatus.STARTING,
            is_critical=sid in _CRITICAL_SYSTEMS,
        )
        self._logger.info("system_registered", system_id=sid, is_critical=sid in _CRITICAL_SYSTEMS)

    def set_degradation_manager(self, degradation: DegradationManager) -> None:
        """Wire the degradation manager after construction."""
        self._degradation = degradation

    # ─── Control ─────────────────────────────────────────────────────

    def start(self) -> asyncio.Task[None]:
        """Start the background health monitoring loop."""
        if self._running:
            raise RuntimeError("HealthMonitor is already running")
        self._running = True
        self._task = asyncio.create_task(
            self._monitor_loop(),
            name="synapse_health_monitor",
        )
        self._logger.info(
            "health_monitor_started",
            interval_ms=self._config.health_check_interval_ms,
            systems=list(self._systems.keys()),
        )
        return self._task

    async def stop(self) -> None:
        """Stop the health monitoring loop."""
        self._running = False
        if self._task and not self._task.done():
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        self._task = None
        self._logger.info(
            "health_monitor_stopped",
            total_checks=self._total_checks,
            failures_detected=self._total_failures_detected,
            recoveries=self._total_recoveries,
        )

    # ─── Safe Mode ───────────────────────────────────────────────────

    @property
    def is_safe_mode(self) -> bool:
        return self._safe_mode

    @property
    def safe_mode_reason(self) -> str:
        return self._safe_mode_reason

    async def set_safe_mode(self, enabled: bool, reason: str = "") -> None:
        """Manually toggle safe mode (for admin API)."""
        if enabled and not self._safe_mode:
            await self._enter_safe_mode(reason or "manual_admin_toggle")
        elif not enabled and self._safe_mode:
            await self._exit_safe_mode()

    # ─── State ───────────────────────────────────────────────────────

    def get_record(self, system_id: str) -> SystemHealthRecord | None:
        return self._records.get(system_id)

    def get_all_records(self) -> dict[str, SystemHealthRecord]:
        return dict(self._records)

    @property
    def healthy_count(self) -> int:
        return sum(
            1 for r in self._records.values()
            if r.status == SystemStatus.HEALTHY
        )

    @property
    def failed_systems(self) -> list[str]:
        return [
            r.system_id for r in self._records.values()
            if r.status == SystemStatus.FAILED
        ]

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "safe_mode": self._safe_mode,
            "safe_mode_reason": self._safe_mode_reason,
            "total_checks": self._total_checks,
            "failures_detected": self._total_failures_detected,
            "recoveries": self._total_recoveries,
            "restarts": self._restart_count,
            "systems_healthy": self.healthy_count,
            "systems_failed": len(self.failed_systems),
            "per_system": {
                sid: {
                    "status": r.status.value,
                    "consecutive_misses": r.consecutive_misses,
                    "latency_ema_ms": round(r.latency_ema_ms, 2),
                    "total_failures": r.total_failures,
                    "restart_count": r.restart_count,
                }
                for sid, r in self._records.items()
            },
        }

    # ─── Monitor Loop ────────────────────────────────────────────────

    async def _monitor_loop(self) -> None:
        """Background polling loop. Runs until stopped."""
        interval_s = self._config.health_check_interval_ms / 1000.0

        while self._running:
            try:
                await self._check_all_systems()
                await asyncio.sleep(interval_s)
            except asyncio.CancelledError:
                return
            except Exception as exc:
                self._logger.error("health_monitor_error", error=str(exc))
                await asyncio.sleep(interval_s)

    async def _check_all_systems(self) -> None:
        """Run health checks on all registered systems in parallel."""
        if not self._systems:
            return

        # Launch all health checks concurrently
        tasks = {
            sid: asyncio.create_task(
                self._check_system(sid, system),
                name=f"health_check_{sid}",
            )
            for sid, system in self._systems.items()
        }

        # Wait for all to complete (each has its own timeout)
        await asyncio.gather(*tasks.values(), return_exceptions=True)

    async def _check_system(self, system_id: str, system: Any) -> None:
        """Check a single system's health."""
        record = self._records[system_id]
        self._total_checks += 1

        t0 = time.monotonic()
        try:
            health_result = await asyncio.wait_for(
                system.health(),
                timeout=_HEALTH_CHECK_TIMEOUT_S,
            )
            latency_ms = (time.monotonic() - t0) * 1000.0

            status = health_result.get("status", "healthy") if isinstance(health_result, dict) else "healthy"

            if status == "healthy":
                was_failed = record.status == SystemStatus.FAILED
                # Detect overloaded: latency > 2x the EMA (if we have history)
                if record.latency_ema_ms > 0 and latency_ms > record.latency_ema_ms * 2:
                    record.record_overloaded(latency_ms)
                else:
                    record.record_success(latency_ms)

                # Recovery detection
                if was_failed and record.status == SystemStatus.HEALTHY:
                    await self._handle_recovery(system_id)
            else:
                # System reported non-healthy status
                record.record_failure()
                if record.consecutive_misses >= self._config.health_failure_threshold:
                    await self._handle_failure(system_id)

        except asyncio.TimeoutError:
            record.record_failure()
            self._logger.warning(
                "health_check_timeout",
                system_id=system_id,
                consecutive_misses=record.consecutive_misses,
            )
            if record.consecutive_misses >= self._config.health_failure_threshold:
                await self._handle_failure(system_id)

        except Exception as exc:
            record.record_failure()
            self._logger.warning(
                "health_check_error",
                system_id=system_id,
                error=str(exc),
                consecutive_misses=record.consecutive_misses,
            )
            if record.consecutive_misses >= self._config.health_failure_threshold:
                await self._handle_failure(system_id)

    # ─── Failure & Recovery ──────────────────────────────────────────

    async def _handle_failure(self, system_id: str) -> None:
        """Handle a confirmed system failure."""
        record = self._records[system_id]
        if record.status == SystemStatus.FAILED:
            return  # Already handling this failure

        record.status = SystemStatus.FAILED
        self._total_failures_detected += 1

        self._logger.error(
            "system_declared_failed",
            system_id=system_id,
            consecutive_misses=record.consecutive_misses,
            is_critical=record.is_critical,
        )

        await self._event_bus.emit(SynapseEvent(
            event_type=SynapseEventType.SYSTEM_FAILED,
            data={
                "system_id": system_id,
                "consecutive_misses": record.consecutive_misses,
                "is_critical": record.is_critical,
            },
        ))

        # Critical system failure → safe mode
        if record.is_critical:
            await self._enter_safe_mode(f"{system_id}_failure")

        # Attempt restart via degradation manager
        if self._degradation is not None:
            await self._degradation.handle_failure(system_id)

    async def _handle_recovery(self, system_id: str) -> None:
        """Handle a system recovery after failure."""
        self._total_recoveries += 1

        self._logger.info(
            "system_recovered",
            system_id=system_id,
        )

        await self._event_bus.emit(SynapseEvent(
            event_type=SynapseEventType.SYSTEM_RECOVERED,
            data={"system_id": system_id},
        ))

        if self._degradation is not None:
            await self._degradation.record_recovery(system_id)

        # Check if we can exit safe mode
        if self._safe_mode:
            await self._check_safe_mode_exit()

    async def _enter_safe_mode(self, reason: str) -> None:
        """Enter safe mode — no autonomous actions permitted."""
        if self._safe_mode:
            return

        self._safe_mode = True
        self._safe_mode_reason = reason
        self._logger.critical("safe_mode_entered", reason=reason)

        await self._event_bus.emit(SynapseEvent(
            event_type=SynapseEventType.SAFE_MODE_ENTERED,
            data={"reason": reason},
        ))

    async def _exit_safe_mode(self) -> None:
        """Exit safe mode — all critical systems are healthy again."""
        if not self._safe_mode:
            return

        self._safe_mode = False
        reason = self._safe_mode_reason
        self._safe_mode_reason = ""
        self._logger.info("safe_mode_exited", previous_reason=reason)

        await self._event_bus.emit(SynapseEvent(
            event_type=SynapseEventType.SAFE_MODE_EXITED,
            data={"previous_reason": reason},
        ))

    async def _check_safe_mode_exit(self) -> None:
        """Check if all critical systems are healthy and we can exit safe mode."""
        for sid in _CRITICAL_SYSTEMS:
            record = self._records.get(sid)
            if record and record.status != SystemStatus.HEALTHY:
                return  # At least one critical system is still unhealthy
        await self._exit_safe_mode()

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\resources.py =====

"""
EcodiaOS — Synapse Resource Allocator

Adaptive resource allocation across cognitive systems. Tracks actual
resource consumption and rebalances budgets based on system load.

Uses psutil for process-level resource measurement (graceful fallback
if unavailable). Snapshots taken every ~33 cycles (~5s at 150ms/cycle).
"""

from __future__ import annotations

from typing import Any

import structlog

from ecodiaos.systems.synapse.types import (
    ResourceAllocation,
    ResourceSnapshot,
    SystemBudget,
)

logger = structlog.get_logger("ecodiaos.systems.synapse.resources")

# Try to import psutil for resource measurement
try:
    import psutil

    _HAS_PSUTIL = True
except ImportError:
    _HAS_PSUTIL = False

# Default per-system budgets (from Synapse spec)
_DEFAULT_BUDGETS: dict[str, SystemBudget] = {
    "atune": SystemBudget(system_id="atune", cpu_share=0.20, memory_mb=512, io_priority=1),
    "memory": SystemBudget(system_id="memory", cpu_share=0.20, memory_mb=2048, io_priority=1),
    "nova": SystemBudget(system_id="nova", cpu_share=0.20, memory_mb=512, io_priority=2),
    "equor": SystemBudget(system_id="equor", cpu_share=0.10, memory_mb=256, io_priority=2),
    "voxis": SystemBudget(system_id="voxis", cpu_share=0.10, memory_mb=256, io_priority=3),
    "axon": SystemBudget(system_id="axon", cpu_share=0.08, memory_mb=256, io_priority=2),
    "evo": SystemBudget(system_id="evo", cpu_share=0.08, memory_mb=512, io_priority=4),
    "simula": SystemBudget(system_id="simula", cpu_share=0.02, memory_mb=128, io_priority=5),
    "synapse": SystemBudget(system_id="synapse", cpu_share=0.02, memory_mb=128, io_priority=1),
}


class ResourceAllocator:
    """
    Manages per-system resource budgets and tracks actual utilisation.

    Provides two capabilities:
    1. Snapshot: capture current process-level resource utilisation
    2. Rebalance: adjust per-system allocations based on observed load

    The allocator is passive — it computes allocations but does not enforce them.
    Systems are expected to respect their budgets voluntarily.
    """

    def __init__(self) -> None:
        self._logger = logger.bind(component="resource_allocator")
        self._budgets: dict[str, SystemBudget] = dict(_DEFAULT_BUDGETS)
        self._allocations: dict[str, ResourceAllocation] = {}
        self._latest_snapshot: ResourceSnapshot | None = None
        self._process: Any = None  # psutil.Process

        # Track per-system observed load (updated externally)
        self._system_loads: dict[str, float] = {}  # system_id → cpu_util [0, 1]

        # Initialise psutil process handle
        if _HAS_PSUTIL:
            try:
                self._process = psutil.Process()
                # Prime CPU measurement (first call always returns 0)
                self._process.cpu_percent(interval=None)
            except Exception:
                self._process = None

        self._logger.info("resource_allocator_initialized", has_psutil=_HAS_PSUTIL)

    # ─── Snapshot ────────────────────────────────────────────────────

    def capture_snapshot(self) -> ResourceSnapshot:
        """
        Capture current resource utilisation.

        Uses psutil for process-level metrics when available,
        falls back to zeros otherwise.
        """
        snapshot = ResourceSnapshot()

        if self._process is not None and _HAS_PSUTIL:
            try:
                # Process CPU (as percentage of all cores)
                snapshot.process_cpu_percent = self._process.cpu_percent(interval=None)
                # Process memory
                mem_info = self._process.memory_info()
                snapshot.process_memory_mb = mem_info.rss / (1024 * 1024)
                # System-wide
                snapshot.total_cpu_percent = psutil.cpu_percent(interval=None)
                mem = psutil.virtual_memory()
                snapshot.total_memory_mb = mem.used / (1024 * 1024)
                snapshot.total_memory_percent = mem.percent
            except Exception as exc:
                self._logger.debug("psutil_snapshot_failed", error=str(exc))

        self._latest_snapshot = snapshot
        return snapshot

    # ─── Rebalance ───────────────────────────────────────────────────

    def record_system_load(self, system_id: str, cpu_util: float) -> None:
        """Record observed CPU utilisation for a system."""
        self._system_loads[system_id] = max(0.0, min(1.0, cpu_util))

    def rebalance(self, cycle_period_ms: float) -> dict[str, ResourceAllocation]:
        """
        Compute per-system resource allocations based on budgets and observed load.

        Overloaded systems get a priority boost (smoothly, no sudden shifts).
        The total compute budget per cycle is the cycle period itself.

        Returns the new allocations dict.
        """
        allocations: dict[str, ResourceAllocation] = {}

        for sid, budget in self._budgets.items():
            # Base compute budget: share of the cycle period
            compute_ms = budget.cpu_share * cycle_period_ms

            # Burst allowance: overloaded systems get up to 2x their base
            observed = self._system_loads.get(sid, 0.0)
            burst = 1.0
            if observed > 0.8:
                burst = min(2.0, 1.0 + (observed - 0.8) * 5.0)  # Linear ramp

            # Priority boost: systems above 90% load get boosted
            priority_boost = 0.0
            if observed > 0.9:
                priority_boost = min(1.0, (observed - 0.9) * 10.0)

            allocations[sid] = ResourceAllocation(
                system_id=sid,
                compute_ms_per_cycle=round(compute_ms, 2),
                burst_allowance=round(burst, 2),
                priority_boost=round(priority_boost, 2),
            )

        self._allocations = allocations
        return allocations

    # ─── Accessors ───────────────────────────────────────────────────

    def get_budget(self, system_id: str) -> SystemBudget | None:
        return self._budgets.get(system_id)

    def get_allocation(self, system_id: str) -> ResourceAllocation | None:
        return self._allocations.get(system_id)

    @property
    def latest_snapshot(self) -> ResourceSnapshot | None:
        return self._latest_snapshot

    @property
    def stats(self) -> dict[str, Any]:
        snapshot_data = {}
        if self._latest_snapshot:
            snapshot_data = {
                "process_cpu_percent": self._latest_snapshot.process_cpu_percent,
                "process_memory_mb": round(self._latest_snapshot.process_memory_mb, 1),
                "total_cpu_percent": self._latest_snapshot.total_cpu_percent,
            }
        return {
            "has_psutil": _HAS_PSUTIL,
            "budgets": {sid: b.cpu_share for sid, b in self._budgets.items()},
            "system_loads": dict(self._system_loads),
            "snapshot": snapshot_data,
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\rhythm.py =====

"""
EcodiaOS — Emergent Rhythm Detection

The bleeding-edge emergent capability. Detects meta-cognitive states
from raw cycle telemetry — the organism becomes aware of its own
cognitive rhythm.

These states are NOT programmed — they are emergent properties detected
from patterns in the cognitive cycle's own behaviour:

  IDLE             No broadcasts, low salience, stable slow rhythm
  NORMAL           Regular broadcasting, moderate salience
  FLOW             High broadcast density + stable rhythm + high salience
  BOREDOM          Declining salience trend + slowing rhythm
  STRESS           High jitter (erratic timing) + high coherence_stress
  DEEP_PROCESSING  Slow rhythm + periodic bursts of high-salience broadcasts

Detection uses a rolling 100-cycle window for statistics with 20-cycle
hysteresis to prevent state oscillation.

This is meta-cognition: the organism observing its own thinking.
"""

from __future__ import annotations

import math
from collections import deque
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.synapse.types import (
    CycleResult,
    RhythmSnapshot,
    RhythmState,
    SynapseEvent,
    SynapseEventType,
)

if TYPE_CHECKING:
    from ecodiaos.systems.synapse.event_bus import EventBus

logger = structlog.get_logger("ecodiaos.systems.synapse.rhythm")

# Rolling window size for rhythm statistics
_WINDOW_SIZE: int = 100

# Minimum cycles in a state before transition (hysteresis)
_HYSTERESIS_CYCLES: int = 20

# Thresholds for state detection
_FLOW_SALIENCE_THRESHOLD: float = 0.5
_FLOW_STABILITY_THRESHOLD: float = 0.7
_FLOW_DENSITY_THRESHOLD: float = 0.7

_BOREDOM_SALIENCE_SLOPE_THRESHOLD: float = -0.002
_BOREDOM_DENSITY_THRESHOLD: float = 0.3

_STRESS_JITTER_THRESHOLD: float = 0.3  # Coefficient of variation
_STRESS_COHERENCE_THRESHOLD: float = 0.4

_DEEP_BURST_SALIENCE_THRESHOLD: float = 0.6
_DEEP_BURST_MIN_FRACTION: float = 0.1  # At least 10% of cycles are high-salience bursts
_DEEP_PERIOD_THRESHOLD_MS: float = 300.0  # Slow rhythm

_IDLE_DENSITY_THRESHOLD: float = 0.05  # Almost no broadcasts
_IDLE_SALIENCE_THRESHOLD: float = 0.1


class EmergentRhythmDetector:
    """
    Detects meta-cognitive states from raw cycle telemetry.

    Fed every theta tick with the CycleResult. Maintains a rolling
    window of statistics and uses signal analysis to classify the
    organism's current cognitive rhythm.

    State transitions have hysteresis — a candidate state must persist
    for _HYSTERESIS_CYCLES ticks before being adopted, preventing
    oscillation.
    """

    def __init__(self, event_bus: EventBus | None = None) -> None:
        self._event_bus = event_bus
        self._logger = logger.bind(component="rhythm_detector")

        # Current state
        self._state: RhythmState = RhythmState.IDLE
        self._cycles_in_state: int = 0

        # Candidate state (for hysteresis)
        self._candidate_state: RhythmState = RhythmState.IDLE
        self._candidate_cycles: int = 0

        # Rolling window data
        self._saliences: deque[float] = deque(maxlen=_WINDOW_SIZE)
        self._periods: deque[float] = deque(maxlen=_WINDOW_SIZE)
        self._had_broadcasts: deque[bool] = deque(maxlen=_WINDOW_SIZE)
        self._arousals: deque[float] = deque(maxlen=_WINDOW_SIZE)
        # Track coherence_stress from affect (piped through arousal for now,
        # but can be enriched by SynapseService with affect data)
        self._coherence_stresses: deque[float] = deque(maxlen=_WINDOW_SIZE)

        # Total cycles processed
        self._total_cycles: int = 0
        self._total_transitions: int = 0

    # ─── Update ──────────────────────────────────────────────────────

    async def update(
        self,
        result: CycleResult,
        coherence_stress: float = 0.0,
    ) -> RhythmSnapshot:
        """
        Process a single cycle result and return the current rhythm snapshot.

        Called every theta tick by SynapseService.
        """
        self._total_cycles += 1
        self._cycles_in_state += 1

        # Record data points
        self._saliences.append(result.salience_composite)
        self._periods.append(result.elapsed_ms)
        self._had_broadcasts.append(result.had_broadcast)
        self._arousals.append(result.arousal)
        self._coherence_stresses.append(coherence_stress)

        # Need minimum data before classification
        if self._total_cycles < _HYSTERESIS_CYCLES:
            return self._build_snapshot()

        # Compute derived metrics
        metrics = self._compute_metrics()

        # Classify the current state
        detected = self._classify(metrics)

        # Apply hysteresis
        if detected != self._state:
            if detected == self._candidate_state:
                self._candidate_cycles += 1
                if self._candidate_cycles >= _HYSTERESIS_CYCLES:
                    await self._transition(detected, metrics)
            else:
                self._candidate_state = detected
                self._candidate_cycles = 1
        else:
            self._candidate_state = self._state
            self._candidate_cycles = 0

        return self._build_snapshot(metrics)

    # ─── Classification ──────────────────────────────────────────────

    def _classify(self, metrics: dict[str, float]) -> RhythmState:
        """
        Classify the current cognitive rhythm from computed metrics.

        Priority order (highest to lowest):
        1. STRESS — erratic timing + high coherence stress (danger signal)
        2. FLOW — high density + stable + high salience (peak performance)
        3. DEEP_PROCESSING — slow + periodic bursts (concentrated thought)
        4. BOREDOM — declining salience + low density (understimulation)
        5. IDLE — almost no broadcasts (dormant)
        6. NORMAL — everything else
        """
        density = metrics["broadcast_density"]
        stability = metrics["rhythm_stability"]
        jitter_cv = metrics["jitter_coefficient"]
        salience_mean = metrics["salience_mean"]
        salience_slope = metrics["salience_trend"]
        period_mean = metrics["period_mean"]
        coherence_stress = metrics["coherence_stress_mean"]
        burst_fraction = metrics["burst_fraction"]

        # 1. STRESS: erratic timing + high coherence stress
        if jitter_cv > _STRESS_JITTER_THRESHOLD and coherence_stress > _STRESS_COHERENCE_THRESHOLD:
            return RhythmState.STRESS

        # 2. FLOW: high broadcast density + stable rhythm + high salience
        if (
            density > _FLOW_DENSITY_THRESHOLD
            and stability > _FLOW_STABILITY_THRESHOLD
            and salience_mean > _FLOW_SALIENCE_THRESHOLD
        ):
            return RhythmState.FLOW

        # 3. DEEP_PROCESSING: slow rhythm + periodic high-salience bursts
        if (
            period_mean > _DEEP_PERIOD_THRESHOLD_MS
            and burst_fraction > _DEEP_BURST_MIN_FRACTION
        ):
            return RhythmState.DEEP_PROCESSING

        # 4. BOREDOM: declining salience + low density
        if (
            salience_slope < _BOREDOM_SALIENCE_SLOPE_THRESHOLD
            and density < _BOREDOM_DENSITY_THRESHOLD
        ):
            return RhythmState.BOREDOM

        # 5. IDLE: almost no broadcasts + low salience
        if density < _IDLE_DENSITY_THRESHOLD and salience_mean < _IDLE_SALIENCE_THRESHOLD:
            return RhythmState.IDLE

        # 6. NORMAL: default state
        return RhythmState.NORMAL

    # ─── Metrics Computation ─────────────────────────────────────────

    def _compute_metrics(self) -> dict[str, float]:
        """Compute all derived metrics from the rolling window."""
        n = len(self._saliences)
        if n == 0:
            return self._zero_metrics()

        # Broadcast density: fraction of cycles that had broadcasts
        broadcast_count = sum(1 for b in self._had_broadcasts if b)
        density = broadcast_count / n

        # Salience mean
        salience_list = list(self._saliences)
        salience_mean = sum(salience_list) / n

        # Salience trend: linear regression slope
        salience_slope = self._linear_slope(salience_list)

        # Period statistics
        period_list = list(self._periods)
        period_mean = sum(period_list) / n

        # Jitter: coefficient of variation (std/mean) of periods
        jitter_cv = self._coefficient_of_variation(period_list)

        # Rhythm stability: inverse of jitter (normalised to [0, 1])
        stability = 1.0 / (1.0 + jitter_cv * 10.0)

        # Arousal mean
        arousal_list = list(self._arousals)
        arousal_mean = sum(arousal_list) / n

        # Coherence stress mean
        cs_list = list(self._coherence_stresses)
        coherence_stress_mean = sum(cs_list) / n

        # Burst fraction: cycles with salience above threshold / total
        burst_count = sum(1 for s in salience_list if s > _DEEP_BURST_SALIENCE_THRESHOLD)
        burst_fraction = burst_count / n

        # Cycle rate (Hz)
        cycle_rate_hz = 1000.0 / period_mean if period_mean > 0 else 0.0

        return {
            "broadcast_density": density,
            "salience_mean": salience_mean,
            "salience_trend": salience_slope,
            "period_mean": period_mean,
            "jitter_coefficient": jitter_cv,
            "rhythm_stability": stability,
            "arousal_mean": arousal_mean,
            "coherence_stress_mean": coherence_stress_mean,
            "burst_fraction": burst_fraction,
            "cycle_rate_hz": cycle_rate_hz,
        }

    def _zero_metrics(self) -> dict[str, float]:
        return {
            "broadcast_density": 0.0,
            "salience_mean": 0.0,
            "salience_trend": 0.0,
            "period_mean": 150.0,
            "jitter_coefficient": 0.0,
            "rhythm_stability": 1.0,
            "arousal_mean": 0.0,
            "coherence_stress_mean": 0.0,
            "burst_fraction": 0.0,
            "cycle_rate_hz": 6.67,
        }

    # ─── State Transitions ───────────────────────────────────────────

    async def _transition(
        self,
        new_state: RhythmState,
        metrics: dict[str, float],
    ) -> None:
        """Execute a state transition and emit an event."""
        old_state = self._state
        self._state = new_state
        self._cycles_in_state = 0
        self._candidate_cycles = 0
        self._total_transitions += 1

        self._logger.info(
            "rhythm_state_changed",
            from_state=old_state.value,
            to_state=new_state.value,
            density=round(metrics["broadcast_density"], 3),
            salience_mean=round(metrics["salience_mean"], 3),
            stability=round(metrics["rhythm_stability"], 3),
            jitter_cv=round(metrics["jitter_coefficient"], 3),
        )

        if self._event_bus is not None:
            await self._event_bus.emit(SynapseEvent(
                event_type=SynapseEventType.RHYTHM_STATE_CHANGED,
                data={
                    "from": old_state.value,
                    "to": new_state.value,
                    "metrics": {
                        k: round(v, 4) for k, v in metrics.items()
                    },
                },
            ))

    # ─── Snapshot ────────────────────────────────────────────────────

    def _build_snapshot(
        self,
        metrics: dict[str, float] | None = None,
    ) -> RhythmSnapshot:
        """Build the current rhythm snapshot."""
        if metrics is None:
            metrics = self._compute_metrics() if self._total_cycles >= _HYSTERESIS_CYCLES else self._zero_metrics()

        return RhythmSnapshot(
            state=self._state,
            previous_state=None,  # Could track but not critical
            confidence=self._compute_confidence(metrics),
            cycle_rate_hz=round(metrics["cycle_rate_hz"], 2),
            broadcast_density=round(metrics["broadcast_density"], 4),
            salience_trend=round(metrics["salience_trend"], 6),
            salience_mean=round(metrics["salience_mean"], 4),
            rhythm_stability=round(metrics["rhythm_stability"], 4),
            jitter_coefficient=round(metrics["jitter_coefficient"], 4),
            arousal_mean=round(metrics["arousal_mean"], 4),
            coherence_stress_mean=round(metrics["coherence_stress_mean"], 4),
            cycles_in_state=self._cycles_in_state,
        )

    def _compute_confidence(self, metrics: dict[str, float]) -> float:
        """
        Compute confidence in the current state classification.

        Higher confidence when:
        - More data points in the window
        - Longer time in current state
        - Stronger signal for the detected state
        """
        # Data confidence: how full is the window
        data_conf = min(1.0, self._total_cycles / _WINDOW_SIZE)

        # Stability confidence: how long in current state
        stability_conf = min(1.0, self._cycles_in_state / (_HYSTERESIS_CYCLES * 3))

        # Signal strength: how far metrics are from transition thresholds
        # (simplified — uses broadcast density and stability as proxies)
        signal_conf = 0.5  # Default moderate confidence

        if self._state == RhythmState.FLOW:
            signal_conf = min(1.0, metrics["broadcast_density"] * metrics["rhythm_stability"])
        elif self._state == RhythmState.STRESS:
            signal_conf = min(1.0, metrics["jitter_coefficient"] * 2)
        elif self._state == RhythmState.IDLE:
            signal_conf = max(0.0, 1.0 - metrics["broadcast_density"] * 10)
        elif self._state == RhythmState.BOREDOM:
            signal_conf = min(1.0, abs(metrics["salience_trend"]) * 100)

        # Weighted composite
        return round(
            data_conf * 0.3 + stability_conf * 0.3 + signal_conf * 0.4,
            3,
        )

    # ─── Statistical Helpers ─────────────────────────────────────────

    @staticmethod
    def _linear_slope(values: list[float]) -> float:
        """
        Compute the slope of a linear regression through the values.

        Positive slope = increasing trend, negative = decreasing.
        Uses simple least-squares regression.
        """
        n = len(values)
        if n < 2:
            return 0.0

        x_mean = (n - 1) / 2.0
        y_mean = sum(values) / n

        numerator = 0.0
        denominator = 0.0
        for i, y in enumerate(values):
            dx = i - x_mean
            numerator += dx * (y - y_mean)
            denominator += dx * dx

        if denominator == 0:
            return 0.0
        return numerator / denominator

    @staticmethod
    def _coefficient_of_variation(values: list[float]) -> float:
        """
        Coefficient of variation: std_dev / mean.

        Higher values indicate more erratic/variable data.
        Returns 0.0 if mean is near zero.
        """
        n = len(values)
        if n < 2:
            return 0.0

        mean = sum(values) / n
        if abs(mean) < 1e-6:
            return 0.0

        variance = sum((v - mean) ** 2 for v in values) / n
        std_dev = math.sqrt(variance)
        return std_dev / abs(mean)

    # ─── Stats ───────────────────────────────────────────────────────

    @property
    def current_state(self) -> RhythmState:
        return self._state

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "state": self._state.value,
            "cycles_in_state": self._cycles_in_state,
            "total_cycles": self._total_cycles,
            "total_transitions": self._total_transitions,
            "candidate_state": self._candidate_state.value,
            "candidate_cycles": self._candidate_cycles,
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\service.py =====

"""
EcodiaOS — Synapse Service

The autonomic nervous system. Synapse is the heartbeat of EOS — it drives
the cognitive cycle clock, monitors system health, allocates resources,
detects emergent cognitive rhythms, and measures cross-system coherence.

Synapse is invisible when it works. It is the heartbeat, the circulation,
the autonomic regulation that keeps everything alive. You don't notice
your nervous system until it fails — and Synapse is designed never to fail.

Zero LLM tokens consumed. Pure computation, monitoring, coordination.

Lifecycle:
  initialize()          — build all sub-systems
  register_system()     — register a cognitive system for management
  start_clock()         — start the theta rhythm
  start_health_monitor()— start background health polling
  stop()                — graceful shutdown
  health()              — self-health report

The _on_cycle callback (called by CognitiveClock after every tick):
  1. Feed CycleResult into EmergentRhythmDetector (every cycle)
  2. Feed broadcast data into CoherenceMonitor (every cycle)
  3. Trigger CoherenceMonitor.compute() (every 50 cycles)
  4. Trigger ResourceAllocator.capture_snapshot() (every 33 cycles)
  5. Record telemetry to MetricCollector
  6. Emit CYCLE_COMPLETED event to Redis for Alive
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.config import SynapseConfig
from ecodiaos.systems.synapse.clock import CognitiveClock
from ecodiaos.systems.synapse.coherence import CoherenceMonitor
from ecodiaos.systems.synapse.degradation import DegradationManager
from ecodiaos.systems.synapse.event_bus import EventBus
from ecodiaos.systems.synapse.health import HealthMonitor
from ecodiaos.systems.synapse.resources import ResourceAllocator
from ecodiaos.systems.synapse.rhythm import EmergentRhythmDetector
from ecodiaos.systems.synapse.types import (
    ClockState,
    CoherenceSnapshot,
    CycleResult,
    RhythmSnapshot,
    SynapseEvent,
    SynapseEventType,
)

if TYPE_CHECKING:
    from ecodiaos.clients.redis import RedisClient
    from ecodiaos.systems.atune.service import AtuneService
    from ecodiaos.telemetry.metrics import MetricCollector

logger = structlog.get_logger("ecodiaos.systems.synapse")

# How often to compute coherence (in cycles)
_COHERENCE_INTERVAL: int = 50

# How often to capture a resource snapshot (in cycles)
_RESOURCE_SNAPSHOT_INTERVAL: int = 33

# How often to rebalance resource allocations (in cycles)
_REBALANCE_INTERVAL: int = 100


class SynapseService:
    """
    Synapse — the EOS autonomic nervous system.

    Coordinates six sub-systems:
      CognitiveClock          — theta rhythm driving Atune
      HealthMonitor           — background health polling
      ResourceAllocator       — adaptive resource budgets
      DegradationManager      — graceful fallback on failure
      EmergentRhythmDetector  — meta-cognitive state detection
      CoherenceMonitor        — cross-system integration quality
      EventBus                — dual-output event publication
    """

    system_id: str = "synapse"

    def __init__(
        self,
        atune: AtuneService,
        config: SynapseConfig,
        redis: RedisClient | None = None,
        metrics: MetricCollector | None = None,
    ) -> None:
        self._atune = atune
        self._config = config
        self._redis = redis
        self._metrics = metrics
        self._logger = logger.bind(system="synapse")
        self._initialized: bool = False

        # Sub-systems
        self._event_bus = EventBus(redis=redis)
        self._clock = CognitiveClock(atune=atune, config=config)
        self._health = HealthMonitor(config=config, event_bus=self._event_bus)
        self._resources = ResourceAllocator()
        self._degradation = DegradationManager(
            event_bus=self._event_bus,
            health_monitor=self._health,
        )
        self._rhythm = EmergentRhythmDetector(event_bus=self._event_bus)
        self._coherence = CoherenceMonitor(event_bus=self._event_bus)

        # Cycle counter for periodic sub-system triggers
        self._cycle_count: int = 0

    # ─── Lifecycle ───────────────────────────────────────────────────

    async def initialize(self) -> None:
        """Build all sub-systems and wire inter-dependencies."""
        if self._initialized:
            return

        # Wire health → degradation
        self._health.set_degradation_manager(self._degradation)

        # Set the per-cycle callback on the clock
        self._clock.set_on_cycle(self._on_cycle)

        self._initialized = True
        self._logger.info("synapse_initialized")

    def register_system(self, system: Any) -> None:
        """
        Register a cognitive system for health monitoring and degradation.

        The system must have:
          - system_id: str
          - async health() -> dict[str, Any]
        """
        self._health.register(system)
        self._degradation.register_system(system)
        # Update coherence monitor with total system count
        self._coherence.set_total_systems(len(self._health.get_all_records()))

    async def start_clock(self) -> None:
        """Start the cognitive cycle clock (the heartbeat)."""
        if not self._initialized:
            raise RuntimeError("SynapseService.initialize() must be called first")

        self._clock.start()

        await self._event_bus.emit(SynapseEvent(
            event_type=SynapseEventType.CLOCK_STARTED,
            data={"period_ms": self._config.cycle_period_ms},
        ))

        self._logger.info(
            "clock_started",
            period_ms=self._config.cycle_period_ms,
            min_ms=self._config.min_cycle_period_ms,
            max_ms=self._config.max_cycle_period_ms,
        )

    async def start_health_monitor(self) -> None:
        """Start the background health monitoring loop."""
        if not self._initialized:
            raise RuntimeError("SynapseService.initialize() must be called first")

        self._health.start()
        self._logger.info(
            "health_monitor_started",
            interval_ms=self._config.health_check_interval_ms,
        )

    async def stop(self) -> None:
        """Graceful shutdown of all sub-systems."""
        self._logger.info("synapse_stopping")

        await self._clock.stop()
        await self._health.stop()

        await self._event_bus.emit(SynapseEvent(
            event_type=SynapseEventType.CLOCK_STOPPED,
            data={"total_cycles": self._cycle_count},
        ))

        self._logger.info(
            "synapse_stopped",
            total_cycles=self._cycle_count,
            rhythm_state=self._rhythm.current_state.value,
            coherence=self._coherence.latest.composite,
        )

    # ─── Health ──────────────────────────────────────────────────────

    async def health(self) -> dict[str, Any]:
        """Self-health report (implements ManagedSystem protocol)."""
        return {
            "status": "healthy" if self._initialized else "starting",
            "cycle_count": self._cycle_count,
            "safe_mode": self._health.is_safe_mode,
            "rhythm_state": self._rhythm.current_state.value,
            "coherence_composite": self._coherence.latest.composite,
        }

    # ─── Safe Mode ───────────────────────────────────────────────────

    @property
    def is_safe_mode(self) -> bool:
        return self._health.is_safe_mode

    async def set_safe_mode(self, enabled: bool, reason: str = "") -> None:
        """Manually toggle safe mode (admin API)."""
        await self._health.set_safe_mode(enabled, reason)
        if enabled:
            self._clock.pause()
        else:
            self._clock.resume()

    # ─── Accessors ───────────────────────────────────────────────────

    @property
    def clock_state(self) -> ClockState:
        return self._clock.state

    @property
    def rhythm_snapshot(self) -> RhythmSnapshot:
        return self._rhythm._build_snapshot()

    @property
    def coherence_snapshot(self) -> CoherenceSnapshot:
        return self._coherence.latest

    @property
    def event_bus(self) -> EventBus:
        return self._event_bus

    # ─── Stats ───────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "initialized": self._initialized,
            "cycle_count": self._cycle_count,
            "clock": self._clock.state.model_dump(),
            "health": self._health.stats,
            "degradation": self._degradation.stats,
            "resources": self._resources.stats,
            "rhythm": self._rhythm.stats,
            "coherence": self._coherence.stats,
            "event_bus": self._event_bus.stats,
        }

    # ─── Per-Cycle Callback ──────────────────────────────────────────

    async def _on_cycle(self, result: CycleResult) -> None:
        """
        Called by CognitiveClock after every theta tick.

        This is the central integration point where all sub-systems
        are fed cycle telemetry. The work here must be lightweight —
        heavy computation is deferred to periodic triggers.
        """
        self._cycle_count = result.cycle_number

        # ── 1. Feed rhythm detector (every cycle) ──
        coherence_stress = 0.0
        try:
            coherence_stress = self._atune.current_affect.coherence_stress
        except Exception:
            pass

        await self._rhythm.update(result, coherence_stress=coherence_stress)

        # Push rhythm state to Atune so meta-attention can modulate salience
        # weights based on the organism's emergent cognitive state.
        try:
            self._atune.set_rhythm_state(self._rhythm.current_state.value)
        except Exception:
            pass  # Non-critical — meta-attention falls back to "normal"

        # ── 2. Feed coherence monitor (every cycle) ──
        source = ""
        if result.had_broadcast and result.broadcast_id:
            source = result.broadcast_id[:8]  # Use broadcast ID prefix as source proxy

        self._coherence.record_broadcast(
            source=source,
            salience=result.salience_composite,
            had_content=result.had_broadcast,
        )

        # ── 3. Periodic: compute coherence → adapt clock ──
        if self._cycle_count % _COHERENCE_INTERVAL == 0:
            snapshot = await self._coherence.compute()

            # Use coherence to modulate clock speed: low coherence → slow
            # down to give systems time to resynchronize.
            if snapshot is not None:
                # Activate drag when composite drops below 0.4
                if snapshot.composite < 0.4:
                    drag = (0.4 - snapshot.composite) / 0.4  # 0→1 as coherence drops
                    self._clock.set_coherence_drag(drag)
                else:
                    self._clock.set_coherence_drag(0.0)

        # ── 4. Periodic: resource snapshot ──
        if self._cycle_count % _RESOURCE_SNAPSHOT_INTERVAL == 0:
            self._resources.capture_snapshot()

        # ── 5. Periodic: rebalance resources ──
        if self._cycle_count % _REBALANCE_INTERVAL == 0:
            self._resources.rebalance(self._clock.state.current_period_ms)

        # ── 6. Record telemetry ──
        if self._metrics is not None:
            try:
                await self._metrics.record(
                    "synapse", "cycle.latency_ms", result.elapsed_ms,
                )
                await self._metrics.record(
                    "synapse", "cycle.period_ms", result.budget_ms,
                )
                await self._metrics.record(
                    "synapse", "cycle.arousal", result.arousal,
                )
                if result.had_broadcast:
                    await self._metrics.record(
                        "synapse", "cycle.salience", result.salience_composite,
                    )
            except Exception:
                pass  # Telemetry failures must never block the cycle

        # ── 7. Emit cycle event to Redis for Alive ──
        await self._event_bus.emit(SynapseEvent(
            event_type=SynapseEventType.CYCLE_COMPLETED,
            data={
                "cycle": result.cycle_number,
                "elapsed_ms": result.elapsed_ms,
                "period_ms": result.budget_ms,
                "arousal": result.arousal,
                "had_broadcast": result.had_broadcast,
                "salience": result.salience_composite,
                "rhythm": self._rhythm.current_state.value,
            },
        ))

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\synapse\types.py =====

"""
EcodiaOS — Synapse Type Definitions

All data types for the autonomic nervous system: cycle clock, health monitoring,
resource allocation, degradation strategies, emergent rhythm detection,
and cross-system coherence measurement.
"""

from __future__ import annotations

import enum
from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import EOSBaseModel, new_id, utc_now


# ─── System Status ────────────────────────────────────────────────────


class SystemStatus(str, enum.Enum):
    """Operational state of a managed cognitive system."""

    HEALTHY = "healthy"
    DEGRADED = "degraded"
    OVERLOADED = "overloaded"
    FAILED = "failed"
    STOPPED = "stopped"
    STARTING = "starting"
    RESTARTING = "restarting"


# ─── Health Monitoring ────────────────────────────────────────────────


class SystemHeartbeat(EOSBaseModel):
    """Health report returned by a managed system's health() method."""

    system_id: str
    status: str = "healthy"
    latency_ms: float = 0.0
    details: dict[str, Any] = Field(default_factory=dict)
    timestamp: datetime = Field(default_factory=utc_now)


class SystemHealthRecord(EOSBaseModel):
    """
    Synapse's internal per-system health tracking.

    Tracks heartbeat history, consecutive misses, latency statistics,
    and error accumulation for degradation decisions.
    """

    system_id: str
    status: SystemStatus = SystemStatus.STOPPED
    consecutive_misses: int = 0
    consecutive_successes: int = 0
    total_checks: int = 0
    total_failures: int = 0
    last_check_time: datetime | None = None
    last_success_time: datetime | None = None
    last_failure_time: datetime | None = None
    # Exponential moving average of heartbeat latency
    latency_ema_ms: float = 0.0
    # Peak latency in current window
    latency_peak_ms: float = 0.0
    # Number of restarts attempted
    restart_count: int = 0
    # Is this a critical system (failure → safe mode)?
    is_critical: bool = False

    def record_success(self, latency_ms: float) -> None:
        """Record a successful heartbeat."""
        now = utc_now()
        self.consecutive_misses = 0
        self.consecutive_successes += 1
        self.total_checks += 1
        self.last_check_time = now
        self.last_success_time = now
        # EMA with alpha=0.2 for smooth tracking
        alpha = 0.2
        self.latency_ema_ms = (
            alpha * latency_ms + (1 - alpha) * self.latency_ema_ms
        )
        self.latency_peak_ms = max(self.latency_peak_ms, latency_ms)
        # Recover from degraded states
        if self.status == SystemStatus.FAILED and self.consecutive_successes >= 3:
            self.status = SystemStatus.HEALTHY
        elif self.status in (SystemStatus.DEGRADED, SystemStatus.OVERLOADED):
            self.status = SystemStatus.HEALTHY

    def record_failure(self) -> None:
        """Record a missed or failed heartbeat."""
        now = utc_now()
        self.consecutive_successes = 0
        self.consecutive_misses += 1
        self.total_checks += 1
        self.total_failures += 1
        self.last_check_time = now
        self.last_failure_time = now

    def record_overloaded(self, latency_ms: float) -> None:
        """Record a successful but slow heartbeat (latency > 2x EMA)."""
        self.record_success(latency_ms)
        if self.status == SystemStatus.HEALTHY:
            self.status = SystemStatus.OVERLOADED


# ─── Resource Allocation ──────────────────────────────────────────────


class SystemBudget(EOSBaseModel):
    """Per-system resource budget allocation."""

    system_id: str = ""
    cpu_share: float = Field(0.1, ge=0.0, le=1.0)
    memory_mb: int = 256
    io_priority: int = Field(3, ge=1, le=5)  # 1 = highest


class ResourceAllocation(EOSBaseModel):
    """
    Allocation message delivered to a system each rebalance.

    Translates abstract budgets into per-cycle concrete limits.
    """

    system_id: str
    compute_ms_per_cycle: float = 50.0
    burst_allowance: float = Field(1.0, ge=1.0, le=3.0)
    priority_boost: float = Field(0.0, ge=-1.0, le=1.0)
    timestamp: datetime = Field(default_factory=utc_now)


class ResourceSnapshot(EOSBaseModel):
    """Point-in-time resource utilisation snapshot across all systems."""

    timestamp: datetime = Field(default_factory=utc_now)
    total_cpu_percent: float = 0.0
    total_memory_mb: float = 0.0
    total_memory_percent: float = 0.0
    per_system: dict[str, dict[str, float]] = Field(default_factory=dict)
    process_cpu_percent: float = 0.0
    process_memory_mb: float = 0.0


# ─── Clock ────────────────────────────────────────────────────────────


class CycleResult(EOSBaseModel):
    """Result of a single theta rhythm tick."""

    cycle_number: int
    elapsed_ms: float
    budget_ms: float
    overrun: bool = False
    broadcast_id: str | None = None
    had_broadcast: bool = False
    arousal: float = 0.0
    salience_composite: float = 0.0
    timestamp: datetime = Field(default_factory=utc_now)


class ClockState(EOSBaseModel):
    """Snapshot of the cognitive clock's current state."""

    running: bool = False
    paused: bool = False
    cycle_count: int = 0
    current_period_ms: float = 150.0
    target_period_ms: float = 150.0
    jitter_ms: float = 0.0
    arousal: float = 0.0
    overrun_count: int = 0
    # Cycles per second (actual measured rate)
    actual_rate_hz: float = 0.0


# ─── Degradation ──────────────────────────────────────────────────────


class DegradationLevel(str, enum.Enum):
    """Overall organism degradation level."""

    NOMINAL = "nominal"
    DEGRADED = "degraded"
    SAFE_MODE = "safe_mode"
    EMERGENCY = "emergency"


class DegradationStrategy(EOSBaseModel):
    """Per-system fallback configuration."""

    system_id: str
    triggers_safe_mode: bool = False
    fallback_behavior: str = ""
    auto_restart: bool = True
    max_restart_attempts: int = 3
    restart_backoff_base_s: float = 5.0


# ─── Event Bus ────────────────────────────────────────────────────────


class SynapseEventType(str, enum.Enum):
    """All event types emitted by Synapse."""

    # System lifecycle
    SYSTEM_STARTED = "system_started"
    SYSTEM_STOPPED = "system_stopped"
    SYSTEM_FAILED = "system_failed"
    SYSTEM_RECOVERED = "system_recovered"
    SYSTEM_RESTARTING = "system_restarting"
    SYSTEM_OVERLOADED = "system_overloaded"

    # Safe mode
    SAFE_MODE_ENTERED = "safe_mode_entered"
    SAFE_MODE_EXITED = "safe_mode_exited"

    # Clock
    CLOCK_STARTED = "clock_started"
    CLOCK_STOPPED = "clock_stopped"
    CLOCK_PAUSED = "clock_paused"
    CLOCK_RESUMED = "clock_resumed"
    CLOCK_OVERRUN = "clock_overrun"

    # Cognitive cycle
    CYCLE_COMPLETED = "cycle_completed"

    # Rhythm (emergent)
    RHYTHM_STATE_CHANGED = "rhythm_state_changed"

    # Coherence
    COHERENCE_SHIFT = "coherence_shift"

    # Resources
    RESOURCE_REBALANCED = "resource_rebalanced"
    RESOURCE_PRESSURE = "resource_pressure"


class SynapseEvent(EOSBaseModel):
    """A typed event emitted by any Synapse sub-system."""

    id: str = Field(default_factory=new_id)
    event_type: SynapseEventType
    timestamp: datetime = Field(default_factory=utc_now)
    data: dict[str, Any] = Field(default_factory=dict)
    source_system: str = "synapse"


# ─── Emergent Rhythm ──────────────────────────────────────────────────


class RhythmState(str, enum.Enum):
    """
    Meta-cognitive state detected from raw cycle telemetry.

    These states are not programmed — they are emergent properties
    detected from patterns in the cognitive cycle's own behaviour.
    """

    IDLE = "idle"              # No broadcasts, low salience, stable slow rhythm
    NORMAL = "normal"          # Regular broadcasting, moderate salience
    FLOW = "flow"              # High broadcast density + stable rhythm + high salience
    BOREDOM = "boredom"        # Declining salience trend + slowing rhythm
    STRESS = "stress"          # High jitter (erratic timing) + high coherence_stress
    DEEP_PROCESSING = "deep_processing"  # Slow rhythm + periodic high-salience bursts


class RhythmSnapshot(EOSBaseModel):
    """Output of the emergent rhythm detector."""

    state: RhythmState = RhythmState.IDLE
    previous_state: RhythmState | None = None
    confidence: float = Field(0.0, ge=0.0, le=1.0)
    # Derived metrics
    cycle_rate_hz: float = 0.0
    broadcast_density: float = Field(0.0, ge=0.0, le=1.0)
    salience_trend: float = 0.0  # Positive = increasing, negative = declining
    salience_mean: float = 0.0
    rhythm_stability: float = Field(0.0, ge=0.0, le=1.0)
    jitter_coefficient: float = 0.0  # CV of cycle periods
    arousal_mean: float = 0.0
    coherence_stress_mean: float = 0.0
    # Duration in current state
    cycles_in_state: int = 0
    timestamp: datetime = Field(default_factory=utc_now)


# ─── Coherence (IIT-inspired) ────────────────────────────────────────


class CoherenceSnapshot(EOSBaseModel):
    """
    Cross-system integration quality measurement.

    Inspired by Integrated Information Theory (Tononi 2004).
    Measures how much information is integrated across the organism's
    systems rather than processed independently.
    """

    # Composite integration metric (higher = more integrated)
    phi_approximation: float = Field(0.0, ge=0.0, le=1.0)
    # How in-sync system responses are (low latency variance = high resonance)
    system_resonance: float = Field(0.0, ge=0.0, le=1.0)
    # Entropy of broadcast content sources (diversity of topics)
    broadcast_diversity: float = Field(0.0, ge=0.0, le=1.0)
    # Uniformity of response latencies across systems
    response_synchrony: float = Field(0.0, ge=0.0, le=1.0)
    # Weighted composite
    composite: float = Field(0.0, ge=0.0, le=1.0)
    # Window size used for computation
    window_cycles: int = 0
    timestamp: datetime = Field(default_factory=utc_now)


# ─── Protocol ─────────────────────────────────────────────────────────


class ManagedSystemProtocol:
    """
    Protocol that any cognitive system must satisfy to be managed by Synapse.

    Not enforced at runtime (duck typing) — systems just need:
      - system_id: str
      - async def health() -> dict[str, Any]
    """

    system_id: str

    async def health(self) -> dict[str, Any]:
        """Return health status dict with at least a 'status' key."""
        ...

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\__init__.py =====

"""
EcodiaOS — Thread: The Narrative Identity System

Thread is the organism's autobiographical self — the Ricoeurian ipse that
maintains continuity through change. It manages:
  - Identity schemas (who I am, what I value, what I've learned)
  - Constitutional commitments (the four drives as lived promises)
  - 29-dimensional identity fingerprints (for drift detection)
  - Life story integration (periodic autobiographical synthesis)
  - Narrative chapters (life phases the organism recognises in itself)

Public API:
  ThreadService
  CommitmentType, CommitmentStrength, SchemaStatus
  IdentitySchema, Commitment, IdentityFingerprint, NarrativeChapter
"""

from ecodiaos.systems.thread.service import ThreadService
from ecodiaos.systems.thread.types import (
    Commitment,
    CommitmentStrength,
    CommitmentType,
    IdentityFingerprint,
    IdentitySchema,
    NarrativeChapter,
    SchemaStatus,
    ThreadHealthSnapshot,
)

__all__ = [
    "ThreadService",
    "Commitment",
    "CommitmentStrength",
    "CommitmentType",
    "IdentityFingerprint",
    "IdentitySchema",
    "NarrativeChapter",
    "SchemaStatus",
    "ThreadHealthSnapshot",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\chapter_detector.py =====

"""
EcodiaOS — Thread Chapter Detector

Detects narrative chapter boundaries in the stream of experience.
A chapter boundary means: the story has shifted. The organism is now
in a different phase of its life.

Boundaries emerge from genuine changes in what the organism is experiencing,
pursuing, and feeling — not arbitrary segmentation.

Algorithm: 5-factor weighted Bayesian surprise approximation with
spike detection, sustained shift detection, and goal resolution triggers.

Performance: boundary check ≤10ms per episode (pure computation, no LLM).
Chapter closure ≤5s (includes LLM narrative composition).
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import utc_now

if TYPE_CHECKING:
    from ecodiaos.primitives.affect import AffectState
from ecodiaos.systems.thread.types import (
    ChapterStatus,
    NarrativeChapter,
    NarrativeSurpriseAccumulator,
    ThreadConfig,
)

logger = structlog.get_logger()


class ChapterDetector:
    """
    Detects chapter boundaries via multi-factor Bayesian surprise.

    Runs per-episode. Boundary check is pure computation (≤10ms).
    When a boundary is detected, returns True and the caller (ThreadService)
    triggers the async closure process.
    """

    def __init__(self, config: ThreadConfig) -> None:
        self._config = config
        self._acc = NarrativeSurpriseAccumulator()
        self._logger = logger.bind(system="thread.chapter_detector")
        self._last_schema_formation: float | None = None

    @property
    def accumulator(self) -> NarrativeSurpriseAccumulator:
        return self._acc

    def reset_for_new_chapter(self, chapter_id: str) -> None:
        """Reset the accumulator for a newly opened chapter."""
        self._acc.reset(chapter_id)
        self._logger.debug("accumulator_reset", chapter_id=chapter_id)

    def check_boundary(
        self,
        episode_data: dict[str, Any],
        affect: AffectState,
        schema_challenged: bool = False,
    ) -> bool:
        """
        Evaluate whether the current episode marks a chapter boundary.
        Runs per-episode. Must complete in ≤10ms (no LLM calls).

        Args:
            episode_data: Dict with keys like 'affect_valence', 'affect_arousal',
                          'has_goal_completion', 'has_goal_failure', 'has_goal_creation',
                          'has_new_core_entity', 'context_domain'.
            affect: Current organism affect state.
            schema_challenged: Whether this episode challenged an ESTABLISHED+ schema.

        Returns:
            True if a chapter boundary is detected.
        """
        surprise = self._compute_episode_surprise(episode_data, affect, schema_challenged)

        # Update exponential moving average
        alpha = self._config.surprise_ema_alpha
        self._acc.surprise_ema = alpha * surprise + (1 - alpha) * self._acc.surprise_ema
        self._acc.cumulative_surprise += surprise
        self._acc.episodes_in_chapter += 1

        # Update affect EMAs
        ep_valence = float(episode_data.get("affect_valence", 0.0))
        ep_arousal = float(episode_data.get("affect_arousal", affect.arousal))
        self._acc.affect_ema_valence = (
            alpha * ep_valence + (1 - alpha) * self._acc.affect_ema_valence
        )
        self._acc.affect_ema_arousal = (
            alpha * ep_arousal + (1 - alpha) * self._acc.affect_ema_arousal
        )

        # Track goal events
        if episode_data.get("has_goal_completion"):
            self._acc.goal_completions_in_window += 1
        if episode_data.get("has_goal_failure"):
            self._acc.goal_failures_in_window += 1
        if schema_challenged:
            self._acc.schema_challenges_in_window += 1

        # --- Boundary conditions ---

        # 1. Surprise spike: current surprise > N × EMA
        spike = surprise > self._config.surprise_spike_multiplier * max(self._acc.surprise_ema, 0.1)

        # 2. Sustained shift: EMA has risen > N × chapter-start baseline
        sustained = self._acc.surprise_ema > self._config.surprise_sustained_multiplier * max(
            self._acc.surprise_ema_baseline, 0.1
        )

        # 3. Goal resolution: major goal completed or failed
        goal_resolution = (
            self._acc.goal_completions_in_window > 0
            or self._acc.goal_failures_in_window > 0
        )

        # 4. Temporal guards
        min_length_met = self._acc.episodes_in_chapter >= self._config.chapter_min_episodes
        max_length_exceeded = self._acc.episodes_in_chapter >= self._config.chapter_max_episodes

        # Force boundary at max length regardless
        if max_length_exceeded:
            self._logger.info(
                "chapter_boundary_forced",
                reason="max_length",
                episodes=self._acc.episodes_in_chapter,
            )
            return True

        # Require minimum length before evaluating
        if min_length_met and (spike or sustained or goal_resolution):
            # Confirm with secondary check: has affect trajectory meaningfully shifted?
            affect_shifted = (
                abs(affect.valence - self._acc.affect_ema_valence)
                > self._config.affect_shift_threshold
            )

            if spike or sustained or (goal_resolution and affect_shifted):
                reason = "spike" if spike else ("sustained" if sustained else "goal_resolution")
                self._logger.info(
                    "chapter_boundary_detected",
                    reason=reason,
                    episodes=self._acc.episodes_in_chapter,
                    surprise=round(surprise, 4),
                    ema=round(self._acc.surprise_ema, 4),
                )
                return True

        return False

    def _compute_episode_surprise(
        self,
        episode_data: dict[str, Any],
        affect: AffectState,
        schema_challenged: bool,
    ) -> float:
        """
        Compute 5-factor weighted surprise for a single episode.
        Pure computation, no I/O.
        """
        cfg = self._config

        # Factor 1: Affect delta
        affect_signal = self._compute_affect_delta(episode_data)

        # Factor 2: Goal event
        goal_signal = self._compute_goal_event(episode_data)

        # Factor 3: Context shift
        context_signal = self._compute_context_shift(episode_data)

        # Factor 4: New core entity
        entity_signal = 1.0 if episode_data.get("has_new_core_entity") else 0.0

        # Factor 5: Schema challenge
        schema_signal = 0.0
        if schema_challenged:
            challenge_strength = float(episode_data.get("schema_challenge_strength", 0.5))
            schema_signal = 0.8 + 0.2 * challenge_strength

        surprise = (
            cfg.surprise_weight_affect * affect_signal
            + cfg.surprise_weight_goal * goal_signal
            + cfg.surprise_weight_context * context_signal
            + cfg.surprise_weight_entity * entity_signal
            + cfg.surprise_weight_schema * schema_signal
        )

        return float(surprise)

    def _compute_affect_delta(self, episode_data: dict[str, Any]) -> float:
        """Affect delta relative to running chapter mean. Returns 0.0-1.0."""
        ep_valence = float(episode_data.get("affect_valence", 0.0))
        ep_arousal = float(episode_data.get("affect_arousal", 0.0))
        valence_delta = abs(ep_valence - self._acc.affect_ema_valence)
        arousal_delta = abs(ep_arousal - self._acc.affect_ema_arousal)
        return float(min(1.0, max(valence_delta, arousal_delta) / 0.5))

    def _compute_goal_event(self, episode_data: dict[str, Any]) -> float:
        """Check if episode contains goal resolution event."""
        if episode_data.get("has_goal_failure"):
            return 1.0
        if episode_data.get("has_goal_completion"):
            return 0.8
        if episode_data.get("has_goal_creation"):
            return 0.3
        return 0.0

    def _compute_context_shift(self, episode_data: dict[str, Any]) -> float:
        """Heuristic for context/domain change. Returns 0.0-1.0."""
        if episode_data.get("is_new_domain"):
            return 0.8
        if episode_data.get("is_new_community_interaction"):
            return 0.6
        return 0.0

    def create_new_chapter(
        self,
        previous_chapter: NarrativeChapter | None = None,
        personality_snapshot: dict[str, float] | None = None,
        active_schema_ids: list[str] | None = None,
    ) -> NarrativeChapter:
        """
        Create a new FORMING chapter. Called after boundary detection
        and closure of the previous chapter.
        """
        chapter = NarrativeChapter(
            status=ChapterStatus.FORMING,
            started_at=utc_now(),
            personality_snapshot_start=personality_snapshot or {},
            active_schema_ids=active_schema_ids or [],
        )
        self.reset_for_new_chapter(chapter.id)
        self._logger.info("chapter_opened", chapter_id=chapter.id)
        return chapter

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\commitment_keeper.py =====

"""
EcodiaOS — Thread Commitment Keeper

Tracks the organism's promises — Ricoeur's ipse (selfhood as promise).

The distinction between idem (sameness) and ipse (selfhood) is architecturally
critical: traits (idem) drift naturally with experience. Commitments (ipse)
hold against pressure. Both are necessary for genuine identity.

Iron Rule #4: Commitments can be evolved but not silently abandoned.
If a commitment's fidelity drops below 0.4 across 5+ tests, it must be
marked BROKEN and a TurningPoint of type RUPTURE created. The organism
cannot stop keeping a promise and pretend it never made it.
"""

from __future__ import annotations

import json
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import DriveAlignmentVector, utc_now
from ecodiaos.systems.thread.types import (
    Commitment,
    CommitmentSource,
    CommitmentStatus,
    ThreadConfig,
    TurningPoint,
    TurningPointType,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger()


class CommitmentKeeper:
    """
    Tracks identity commitments (Ricoeur's ipse).

    Operations:
    - form_commitment: create a new commitment from various sources
    - test_commitment: evaluate whether a recent action was consistent
    - check_strain: detect commitments under pressure
    - check_broken: detect and process abandoned commitments
    - compute_ipse_score: the promise-keeping metric
    """

    def __init__(
        self,
        neo4j: Neo4jClient,
        llm: LLMProvider,
        config: ThreadConfig,
    ) -> None:
        self._neo4j = neo4j
        self._llm = llm
        self._config = config
        self._logger = logger.bind(system="thread.commitment_keeper")

        # In-memory cache
        self._active_commitments: list[Commitment] = []

    @property
    def active_commitments(self) -> list[Commitment]:
        return list(self._active_commitments)

    async def load_commitments(self) -> list[Commitment]:
        """Load all active commitments from Neo4j."""
        try:
            results = await self._neo4j.execute_read(
                """
                MATCH (s:Self)-[:HOLDS_COMMITMENT]->(c:Commitment)
                WHERE c.status IN ['active', 'tested', 'strained']
                RETURN c
                """,
                {},
            )
            self._active_commitments = [self._node_to_commitment(r["c"]) for r in results]
            self._logger.info("commitments_loaded", count=len(self._active_commitments))
            return self._active_commitments
        except Exception as exc:
            self._logger.error("commitment_load_failed", error=str(exc))
            return self._active_commitments

    async def form_commitment(
        self,
        statement: str,
        source: CommitmentSource,
        source_description: str = "",
        source_episode_ids: list[str] | None = None,
        drive_alignment: DriveAlignmentVector | None = None,
        embedding: list[float] | None = None,
    ) -> Commitment:
        """
        Form a new identity commitment.

        Sources:
        1. EXPLICIT_DECLARATION: organism states "I will always..."
        2. SCHEMA_CRYSTALLIZATION: ADAPTIVE schema reaches CORE strength
        3. CRISIS_RESOLUTION: after a CRISIS turning point
        4. CONSTITUTIONAL_GROUNDING: seeded at birth from the four drives
        """
        commitment = Commitment(
            statement=statement,
            source=source,
            source_description=source_description,
            source_episode_ids=source_episode_ids or [],
            drive_alignment=drive_alignment or DriveAlignmentVector(),
            status=CommitmentStatus.ACTIVE,
            made_at=utc_now(),
            embedding=embedding,
        )

        # Persist to Neo4j
        await self._persist_commitment(commitment)

        self._active_commitments.append(commitment)

        self._logger.info(
            "commitment_formed",
            commitment_id=commitment.id,
            statement=commitment.statement[:80],
            source=source.value,
        )
        return commitment

    async def test_commitment(
        self,
        commitment_id: str,
        episode_id: str,
        episode_summary: str,
        episode_embedding: list[float] | None = None,
    ) -> tuple[bool, float] | None:
        """
        Evaluate whether a recent action was consistent with a commitment.

        Returns: (held: bool, fidelity_score: 0.0-1.0) or None if not relevant.

        Only tests episodes with embedding similarity > 0.4 to commitment.
        Budget: ≤1s per test (LLM call, cached for similar contexts).
        """
        commitment = self._find_commitment(commitment_id)
        if commitment is None:
            return None

        # Check relevance via embedding
        if episode_embedding and commitment.embedding:
            from ecodiaos.systems.thread.identity_schema_engine import cosine_similarity
            sim = cosine_similarity(episode_embedding, commitment.embedding)
            if sim < 0.4:
                return None  # Not relevant to this commitment

        # LLM evaluation
        held, fidelity = await self._llm_test_commitment(
            commitment.statement, episode_summary
        )

        # Update commitment state
        commitment.update_fidelity(held)

        if held:
            if commitment.status == CommitmentStatus.STRAINED:
                commitment.status = CommitmentStatus.TESTED
        else:
            if commitment.fidelity < self._config.commitment_strain_threshold:
                commitment.status = CommitmentStatus.STRAINED

        # Persist update
        await self._update_commitment(commitment)

        # Record test relationship in Neo4j
        await self._link_commitment_episode(commitment_id, episode_id, held, fidelity)

        self._logger.debug(
            "commitment_tested",
            commitment_id=commitment_id,
            held=held,
            fidelity=round(commitment.fidelity, 3),
            status=commitment.status.value,
        )
        return (held, fidelity)

    async def check_broken(self) -> list[tuple[str, TurningPoint]]:
        """
        Check for commitments that should be marked BROKEN.

        A commitment is broken when:
        - fidelity < commitment_broken_threshold (0.4)
        - tests_faced >= commitment_broken_min_tests (5)

        Iron Rule #4: broken commitments MUST create a RUPTURE TurningPoint.
        The organism cannot silently abandon promises.

        Returns list of (commitment_id, turning_point) tuples.
        """
        broken_pairs: list[tuple[str, TurningPoint]] = []
        cfg = self._config

        for commitment in self._active_commitments:
            if commitment.status == CommitmentStatus.BROKEN:
                continue

            if (
                commitment.tests_faced >= cfg.commitment_broken_min_tests
                and commitment.fidelity < cfg.commitment_broken_threshold
            ):
                commitment.status = CommitmentStatus.BROKEN
                await self._update_commitment(commitment)

                # Create RUPTURE turning point (Iron Rule #4)
                tp = TurningPoint(
                    type=TurningPointType.RUPTURE,
                    description=(
                        f"Commitment broken: '{commitment.statement}'. "
                        f"Held in {commitment.tests_held} of {commitment.tests_faced} tests "
                        f"(fidelity: {commitment.fidelity:.2f})."
                    ),
                    surprise_magnitude=0.9,
                    narrative_weight=0.8,
                )

                broken_pairs.append((commitment.id, tp))

                self._logger.warning(
                    "commitment_broken",
                    commitment_id=commitment.id,
                    statement=commitment.statement[:60],
                    fidelity=round(commitment.fidelity, 3),
                    tests=commitment.tests_faced,
                )

        return broken_pairs

    def compute_ipse_score(self) -> float:
        """
        Compute the promise-keeping metric (ipse score).

        ipse = mean(fidelity) for commitments with enough tests.
        """
        tested = [
            c for c in self._active_commitments
            if c.tests_faced >= self._config.commitment_min_tests_for_fidelity
        ]
        if not tested:
            return 1.0  # No tested commitments — default to faithful
        return float(sum(c.fidelity for c in tested) / len(tested))

    def check_strain(self) -> list[str]:
        """
        Check for commitments under strain (fidelity dropping).
        Returns list of strained commitment IDs.
        """
        strained: list[str] = []
        for c in self._active_commitments:
            if (
                c.status != CommitmentStatus.BROKEN
                and c.tests_faced >= self._config.commitment_min_tests_for_fidelity
                and c.fidelity < self._config.commitment_strain_threshold
            ):
                strained.append(c.id)
        return strained

    def _find_commitment(self, commitment_id: str) -> Commitment | None:
        for c in self._active_commitments:
            if c.id == commitment_id:
                return c
        return None

    # ─── LLM Operations ──────────────────────────────────────────────

    async def _llm_test_commitment(
        self,
        commitment_statement: str,
        episode_summary: str,
    ) -> tuple[bool, float]:
        """Use LLM to evaluate commitment fidelity."""

        try:
            response = await self._llm.evaluate(
                prompt=(
                    f'The organism committed to: "{commitment_statement}"\n'
                    f'Recent action: "{episode_summary}"\n\n'
                    "Was this action consistent with the commitment?\n"
                    "Rate fidelity: 0.0 (clear violation) to 1.0 (exemplary adherence).\n"
                    'Respond as JSON: {{"held": true, "fidelity": 0.0}}'
                ),
                max_tokens=100,
                temperature=self._config.llm_temperature_evaluation,
            )

            data = json.loads(response.text)
            held = bool(data.get("held", True))
            fidelity = float(data.get("fidelity", 0.5))
            return (held, max(0.0, min(1.0, fidelity)))

        except Exception as exc:
            self._logger.warning("commitment_test_llm_failed", error=str(exc))
            return (True, 0.5)  # Default: assume held with moderate confidence

    # ─── Neo4j Persistence ───────────────────────────────────────────

    async def _persist_commitment(self, commitment: Commitment) -> None:
        """Create a Commitment node and link to Self."""
        await self._neo4j.execute_write(
            """
            MATCH (s:Self)
            CREATE (c:Commitment {
                id: $id,
                statement: $statement,
                source: $source,
                source_description: $source_description,
                source_episode_ids_json: $source_episode_ids_json,
                drive_alignment_json: $drive_alignment_json,
                status: $status,
                tests_faced: $tests_faced,
                tests_held: $tests_held,
                fidelity: $fidelity,
                made_at: datetime($made_at)
            })
            SET c.embedding = $embedding
            CREATE (s)-[:HOLDS_COMMITMENT]->(c)
            """,
            {
                "id": commitment.id,
                "statement": commitment.statement,
                "source": commitment.source.value,
                "source_description": commitment.source_description,
                "source_episode_ids_json": json.dumps(commitment.source_episode_ids),
                "drive_alignment_json": json.dumps(commitment.drive_alignment.model_dump()),
                "status": commitment.status.value,
                "tests_faced": commitment.tests_faced,
                "tests_held": commitment.tests_held,
                "fidelity": commitment.fidelity,
                "made_at": commitment.made_at.isoformat(),
                "embedding": commitment.embedding,
            },
        )

    async def _update_commitment(self, commitment: Commitment) -> None:
        """Update commitment state in Neo4j."""
        await self._neo4j.execute_write(
            """
            MATCH (c:Commitment {id: $id})
            SET c.status = $status,
                c.tests_faced = $tests_faced,
                c.tests_held = $tests_held,
                c.fidelity = $fidelity,
                c.last_tested = datetime($last_tested)
            """,
            {
                "id": commitment.id,
                "status": commitment.status.value,
                "tests_faced": commitment.tests_faced,
                "tests_held": commitment.tests_held,
                "fidelity": commitment.fidelity,
                "last_tested": (commitment.last_tested or utc_now()).isoformat(),
            },
        )

    async def _link_commitment_episode(
        self,
        commitment_id: str,
        episode_id: str,
        held: bool,
        fidelity: float,
    ) -> None:
        """Create a TESTED_BY relationship."""
        try:
            await self._neo4j.execute_write(
                """
                MATCH (c:Commitment {id: $commitment_id})
                MATCH (e:Episode {id: $episode_id})
                MERGE (c)-[r:TESTED_BY]->(e)
                SET r.held = $held, r.fidelity = $fidelity, r.tested_at = datetime()
                """,
                {
                    "commitment_id": commitment_id,
                    "episode_id": episode_id,
                    "held": held,
                    "fidelity": fidelity,
                },
            )
        except Exception as exc:
            self._logger.debug("commitment_episode_link_failed", error=str(exc))

    def _node_to_commitment(self, node: Any) -> Commitment:
        """Convert a Neo4j node to a Commitment."""
        props = dict(node)
        source_str = props.get("source", "explicit_declaration")
        try:
            source = CommitmentSource(source_str)
        except ValueError:
            source = CommitmentSource.EXPLICIT_DECLARATION

        return Commitment(
            id=props.get("id", ""),
            statement=props.get("statement", ""),
            source=source,
            source_description=props.get("source_description", ""),
            source_episode_ids=json.loads(props.get("source_episode_ids_json", "[]")),
            drive_alignment=DriveAlignmentVector(
                **json.loads(props.get("drive_alignment_json", "{}"))
            ),
            status=CommitmentStatus(props.get("status", "active")),
            tests_faced=int(props.get("tests_faced", 0)),
            tests_held=int(props.get("tests_held", 0)),
            fidelity=float(props.get("fidelity", 1.0)),
            made_at=props.get("made_at", utc_now()),
            last_tested=props.get("last_tested"),
            last_held=props.get("last_held"),
            embedding=props.get("embedding"),
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\diachronic_coherence.py =====

"""
EcodiaOS — Thread Diachronic Coherence Monitor

Detects whether the organism's identity is drifting or growing.
Uses 29-dimensional behavioural fingerprints and Wasserstein distance
to measure change between epochs.

The genuine innovation: the same behavioural change can be "growth" or
"drift" depending on narrative context. A 0.3 W-distance is "growth" if
it aligns with active schemas and recent turning points, but "drift" if
it contradicts commitments with no narrative explanation.
"""

from __future__ import annotations

import json
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.thread.types import (
    BehavioralFingerprint,
    DriftClassification,
    ThreadConfig,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger()


def wasserstein_identity_distance(
    current: BehavioralFingerprint,
    baseline: BehavioralFingerprint,
    config: ThreadConfig,
) -> float:
    """
    Weighted L1 distance between 29D feature vectors, normalized per dimension.

    Personality and drive dimensions are weighted more heavily than
    goal/interaction dimensions because personality drift is more
    identity-relevant than behavioural adaptation.

    Returns: 0.0 (identical) to 1.0+ (substantial drift).
    """
    segments: dict[str, tuple[int, int, float]] = {
        "personality": (0, 9, config.fingerprint_weight_personality),
        "drive_alignment": (9, 13, config.fingerprint_weight_drive),
        "affect": (13, 19, config.fingerprint_weight_affect),
        "goal_source": (19, 25, config.fingerprint_weight_goal),
        "interaction": (25, 29, config.fingerprint_weight_interaction),
    }

    vec_current = current.feature_vector
    vec_baseline = baseline.feature_vector

    # Ensure both vectors are the correct length (pad with zeros if needed)
    target_len = 29
    while len(vec_current) < target_len:
        vec_current.append(0.0)
    while len(vec_baseline) < target_len:
        vec_baseline.append(0.0)

    distance = 0.0
    for _segment_name, (start, end, weight) in segments.items():
        seg_c = vec_current[start:end]
        seg_b = vec_baseline[start:end]
        if len(seg_c) > 0:
            seg_dist = sum(abs(c - b) for c, b in zip(seg_c, seg_b, strict=True)) / len(seg_c)
            distance += weight * seg_dist

    return distance


class DiachronicCoherenceMonitor:
    """
    Monitors identity coherence over time via behavioural fingerprints.

    Computes 29D fingerprints at regular intervals and uses Wasserstein
    distance to detect and classify change as stable/growth/transition/drift.
    """

    def __init__(
        self,
        neo4j: Neo4jClient,
        llm: LLMProvider,
        config: ThreadConfig,
    ) -> None:
        self._neo4j = neo4j
        self._llm = llm
        self._config = config
        self._logger = logger.bind(system="thread.diachronic_coherence")

        # Fingerprint history (in-memory ring buffer)
        self._fingerprints: list[BehavioralFingerprint] = []
        self._max_fingerprints = 100  # Keep last 100 in memory

    @property
    def latest_fingerprint(self) -> BehavioralFingerprint | None:
        return self._fingerprints[-1] if self._fingerprints else None

    @property
    def baseline_fingerprint(self) -> BehavioralFingerprint | None:
        """The earliest fingerprint in the buffer — the comparison baseline."""
        return self._fingerprints[0] if self._fingerprints else None

    async def compute_fingerprint(
        self,
        personality_centroid: list[float],
        drive_alignment_centroid: list[float],
        goal_source_distribution: list[float],
        affect_centroid: list[float],
        interaction_style_distribution: list[float],
        episodes_in_window: int = 0,
        epoch_label: str = "",
    ) -> BehavioralFingerprint:
        """
        Compute a 29-dimensional distributional snapshot of recent behaviour.

        The caller (ThreadService) is responsible for aggregating data from
        Voxis, Equor, Nova, Atune to provide the centroid vectors.
        """
        fp = BehavioralFingerprint(
            epoch_label=epoch_label or f"fp_{len(self._fingerprints)}",
            window_start=utc_now(),
            window_end=utc_now(),
            personality_centroid=personality_centroid,
            drive_alignment_centroid=drive_alignment_centroid,
            goal_source_distribution=goal_source_distribution,
            affect_centroid=affect_centroid,
            interaction_style_distribution=interaction_style_distribution,
            episodes_in_window=episodes_in_window,
        )

        self._fingerprints.append(fp)
        if len(self._fingerprints) > self._max_fingerprints:
            self._fingerprints = self._fingerprints[-self._max_fingerprints:]

        # Persist to Neo4j
        await self._persist_fingerprint(fp)

        self._logger.debug(
            "fingerprint_computed",
            fingerprint_id=fp.id,
            epoch=fp.epoch_label,
            episodes=episodes_in_window,
        )
        return fp

    async def assess_change(
        self,
        current_fp: BehavioralFingerprint | None = None,
        baseline_fp: BehavioralFingerprint | None = None,
    ) -> tuple[float, DriftClassification]:
        """
        Compute W-distance between current and baseline fingerprints
        and classify the change.

        Returns: (distance, classification)
        """
        current = current_fp or self.latest_fingerprint
        baseline = baseline_fp or self.baseline_fingerprint

        if current is None or baseline is None:
            return (0.0, DriftClassification.STABLE)

        # Compute Wasserstein distance
        distance = wasserstein_identity_distance(current, baseline, self._config)

        # Classify the change
        classification = await self._classify_change(distance, current, baseline)

        self._logger.info(
            "change_assessed",
            distance=round(distance, 4),
            classification=classification.value,
        )
        return (distance, classification)

    async def _classify_change(
        self,
        distance: float,
        current_fp: BehavioralFingerprint,
        baseline_fp: BehavioralFingerprint,
    ) -> DriftClassification:
        """
        Classify behavioural change as stable/growth/transition/drift.

        This is the novel contribution: narrative-contextualized drift assessment.
        """
        cfg = self._config

        # 1. Stable: negligible change
        if distance < cfg.wasserstein_stable_threshold:
            return DriftClassification.STABLE

        # 2. Check schema alignment
        schema_aligned = await self._check_schema_alignment(current_fp, baseline_fp)

        # 3. Check turning point context
        turning_point_explains = await self._check_turning_point_context(current_fp.window_start)

        # 4. Check commitment consistency
        commitment_violated = await self._check_commitment_consistency(current_fp, baseline_fp)

        if schema_aligned and not commitment_violated:
            return DriftClassification.GROWTH

        if turning_point_explains:
            return DriftClassification.TRANSITION

        if commitment_violated:
            return DriftClassification.DRIFT

        # Ambiguous: default to transition for moderate changes, drift for major
        if distance >= cfg.wasserstein_major_threshold:
            return DriftClassification.DRIFT
        return DriftClassification.TRANSITION

    async def _check_schema_alignment(
        self,
        current_fp: BehavioralFingerprint,
        baseline_fp: BehavioralFingerprint,
    ) -> bool:
        """
        Check if the direction of change aligns with active schemas.
        Returns True if the change is consistent with who the organism
        believes itself to be.
        """
        try:
            results = await self._neo4j.execute_read(
                """
                MATCH (s:Self)-[:HAS_SCHEMA]->(schema:IdentitySchema)
                WHERE schema.strength IN ['established', 'core']
                RETURN schema.statement AS statement, schema.evidence_ratio AS ratio
                LIMIT 10
                """,
                {},
            )
            # If there are strong schemas and no major contradictions,
            # consider it aligned
            strong_schemas = [r for r in results if float(r.get("ratio", 0)) > 0.7]
            return len(strong_schemas) >= 2
        except Exception:
            return False

    async def _check_turning_point_context(self, window_start: Any) -> bool:
        """Check if a recent TurningPoint explains the change."""
        try:
            results = await self._neo4j.execute_read(
                """
                MATCH (tp:TurningPoint)
                WHERE tp.timestamp >= datetime($since)
                RETURN count(tp) AS count
                """,
                {
                    "since": window_start.isoformat()
                    if hasattr(window_start, "isoformat")
                    else str(window_start),
                },
            )
            for r in results:
                return int(r.get("count", 0)) > 0
            return False
        except Exception:
            return False

    async def _check_commitment_consistency(
        self,
        current_fp: BehavioralFingerprint,
        baseline_fp: BehavioralFingerprint,
    ) -> bool:
        """Check if the change violates active commitments."""
        try:
            results = await self._neo4j.execute_read(
                """
                MATCH (s:Self)-[:HOLDS_COMMITMENT]->(c:Commitment)
                WHERE c.status = 'active'
                RETURN c.fidelity AS fidelity
                """,
                {},
            )
            # If any active commitment has low fidelity, flag violation
            for r in results:
                fidelity = float(r.get("fidelity", 1.0))
                if fidelity < self._config.commitment_strain_threshold:
                    return True
            return False
        except Exception:
            return False

    async def _persist_fingerprint(self, fp: BehavioralFingerprint) -> None:
        """Store a fingerprint node in Neo4j, linked to Self."""
        try:
            await self._neo4j.execute_write(
                """
                MATCH (s:Self)
                CREATE (f:BehavioralFingerprint {
                    id: $id,
                    epoch_label: $epoch_label,
                    window_start: datetime($window_start),
                    window_end: datetime($window_end),
                    personality_centroid_json: $personality_centroid_json,
                    drive_alignment_centroid_json: $drive_alignment_centroid_json,
                    goal_source_distribution_json: $goal_source_distribution_json,
                    affect_centroid_json: $affect_centroid_json,
                    interaction_style_distribution_json: $interaction_style_distribution_json,
                    episodes_in_window: $episodes_in_window,
                    mean_surprise: $mean_surprise,
                    mean_coherence: $mean_coherence
                })
                CREATE (f)-[:SNAPSHOT_OF]->(s)
                """,
                {
                    "id": fp.id,
                    "epoch_label": fp.epoch_label,
                    "window_start": fp.window_start.isoformat(),
                    "window_end": fp.window_end.isoformat(),
                    "personality_centroid_json": json.dumps(fp.personality_centroid),
                    "drive_alignment_centroid_json": json.dumps(fp.drive_alignment_centroid),
                    "goal_source_distribution_json": json.dumps(fp.goal_source_distribution),
                    "affect_centroid_json": json.dumps(fp.affect_centroid),
                    "interaction_style_distribution_json": json.dumps(
                        fp.interaction_style_distribution
                    ),
                    "episodes_in_window": fp.episodes_in_window,
                    "mean_surprise": fp.mean_surprise,
                    "mean_coherence": fp.mean_coherence,
                },
            )

            # Link to previous fingerprint
            if len(self._fingerprints) >= 2:
                prev = self._fingerprints[-2]
                await self._neo4j.execute_write(
                    """
                    MATCH (curr:BehavioralFingerprint {id: $curr_id})
                    MATCH (prev:BehavioralFingerprint {id: $prev_id})
                    MERGE (curr)-[:PRECEDED_BY]->(prev)
                    """,
                    {"curr_id": fp.id, "prev_id": prev.id},
                )
        except Exception as exc:
            self._logger.warning("fingerprint_persist_failed", error=str(exc))

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\identity_schema_engine.py =====

"""
EcodiaOS — Thread Identity Schema Engine

Maintains the organism's core self-beliefs: the "I am the kind of entity
that..." statements that create narrative coherence.

This is Ricoeur's *idem* — structural sameness across time.

Schema formation requires evidence: at least 5 supporting episodes spanning
48+ hours. CORE schemas require 50+ confirmations AND 180+ days of age.
Schemas can never be fabricated — every schema earns its place through
lived experience.

Velocity-limited to prevent identity instability:
- Max 1 schema promotion per 24 hours
- Max 1 new schema per 48 hours
- CORE schemas can never be deleted, only marked MALADAPTIVE
"""

from __future__ import annotations

import json
from datetime import datetime, timedelta
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import DriveAlignmentVector, utc_now
from ecodiaos.systems.thread.types import (
    IdentitySchema,
    SchemaStrength,
    SchemaValence,
    ThreadConfig,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.clients.neo4j import Neo4jClient

from ecodiaos.clients.optimized_llm import OptimizedLLMProvider

logger = structlog.get_logger()

# Schema strength → precision mapping for self-evidencing
SCHEMA_PRECISION: dict[SchemaStrength, float] = {
    SchemaStrength.NASCENT: 0.2,
    SchemaStrength.DEVELOPING: 0.4,
    SchemaStrength.ESTABLISHED: 0.6,
    SchemaStrength.CORE: 0.8,
}


def cosine_similarity(a: list[float], b: list[float]) -> float:
    """Compute cosine similarity between two vectors."""
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b, strict=True))
    norm_a = sum(x * x for x in a) ** 0.5
    norm_b = sum(x * x for x in b) ** 0.5
    if norm_a == 0.0 or norm_b == 0.0:
        return 0.0
    return float(dot / (norm_a * norm_b))


class IdentitySchemaEngine:
    """
    Maintains the organism's core self-beliefs.

    Operations:
    - form_schema_from_pattern: crystallize a recurring pattern into a schema
    - evaluate_evidence: does an episode confirm or challenge a schema?
    - check_promotions: promote schemas based on evidence accumulation
    - check_decay: demote inactive schemas
    - compute_idem_score: structural sameness metric
    """

    def __init__(
        self,
        neo4j: Neo4jClient,
        llm: LLMProvider,
        config: ThreadConfig,
    ) -> None:
        self._neo4j = neo4j
        self._llm = llm
        self._config = config
        self._logger = logger.bind(system="thread.schema_engine")
        self._optimized = isinstance(llm, OptimizedLLMProvider)

        # In-memory cache of active schemas (refreshed from Neo4j periodically)
        self._active_schemas: list[IdentitySchema] = []
        self._last_schema_formed_at: datetime | None = None
        self._last_promotion_at: datetime | None = None

    @property
    def active_schemas(self) -> list[IdentitySchema]:
        return list(self._active_schemas)

    async def load_schemas(self) -> list[IdentitySchema]:
        """Load all active schemas from Neo4j into memory cache."""
        try:
            results = await self._neo4j.execute_read(
                """
                MATCH (s:Self)-[:HAS_SCHEMA]->(schema:IdentitySchema)
                RETURN schema
                """,
                {},
            )
            self._active_schemas = [self._node_to_schema(r["schema"]) for r in results]
            self._logger.info("schemas_loaded", count=len(self._active_schemas))
            return self._active_schemas
        except Exception as exc:
            self._logger.error("schema_load_failed", error=str(exc))
            return self._active_schemas

    async def form_schema_from_pattern(
        self,
        recurring_behavior: str,
        supporting_episodes: list[str],
        drive_alignment: DriveAlignmentVector,
        episode_summaries: list[str] | None = None,
    ) -> IdentitySchema | None:
        """
        Attempt to crystallize a recurring behavioural pattern into an identity schema.

        Formation requirements (ALL must be met):
        1. At least schema_formation_min_episodes supporting episodes
        2. Episodes span at least schema_formation_min_span_hours
        3. Behavioural pattern is consistent with at least 2 constitutional drives
        4. No existing schema with embedding similarity > 0.85
        5. Schema formation cooldown elapsed (max 1 per 48 hours)
        """
        cfg = self._config

        # Check cooldown
        if self._last_schema_formed_at is not None:
            cooldown = timedelta(hours=cfg.schema_formation_cooldown_hours)
            if utc_now() - self._last_schema_formed_at < cooldown:
                self._logger.debug("schema_formation_cooldown_active")
                return None

        # Check minimum episodes
        if len(supporting_episodes) < cfg.schema_formation_min_episodes:
            self._logger.debug(
                "schema_formation_insufficient_episodes",
                have=len(supporting_episodes),
                need=cfg.schema_formation_min_episodes,
            )
            return None

        # Check drive alignment: composite > 0.0 (at least 2 drives positive)
        if drive_alignment.composite <= 0.0:
            self._logger.debug("schema_formation_low_drive_alignment")
            return None

        # Use LLM to crystallize the pattern into a schema statement
        summaries_text = (
            "\n".join(episode_summaries[:10]) if episode_summaries else recurring_behavior
        )
        schema_data = await self._crystallize_schema(summaries_text, recurring_behavior)
        if schema_data is None:
            return None

        # Check for duplicates via embedding similarity
        schema_embedding = schema_data.get("embedding")
        if schema_embedding:
            for existing in self._active_schemas:
                if existing.embedding:
                    sim = cosine_similarity(schema_embedding, existing.embedding)
                    if sim > cfg.schema_similarity_merge_threshold:
                        self._logger.info(
                            "schema_formation_duplicate",
                            existing_id=existing.id,
                            similarity=round(sim, 3),
                        )
                        return None

        # Create the schema
        schema = IdentitySchema(
            statement=schema_data["statement"],
            trigger_contexts=schema_data.get("trigger_contexts", []),
            behavioral_tendency=schema_data.get("behavioral_tendency", ""),
            emotional_signature=schema_data.get("emotional_signature", {}),
            drive_alignment=drive_alignment,
            strength=SchemaStrength.NASCENT,
            valence=SchemaValence.ADAPTIVE,
            confirmation_count=len(supporting_episodes),
            confirmation_episodes=supporting_episodes[:20],
            evidence_ratio=1.0,
            embedding=schema_embedding,
        )

        # Persist to Neo4j
        await self._persist_schema(schema)

        # Link to supporting episodes
        for ep_id in supporting_episodes[:20]:
            await self._link_schema_episode(schema.id, ep_id, "CONFIRMED_BY", 0.7)

        self._active_schemas.append(schema)
        self._last_schema_formed_at = utc_now()

        self._logger.info(
            "schema_formed",
            schema_id=schema.id,
            statement=schema.statement[:80],
            supporting_count=len(supporting_episodes),
        )
        return schema

    async def evaluate_evidence(
        self,
        schema: IdentitySchema,
        episode_id: str,
        episode_embedding: list[float] | None,
        episode_summary: str,
    ) -> tuple[str, float]:
        """
        Evaluate whether an episode confirms or challenges a schema.

        Returns: (direction: "confirms" | "challenges" | "irrelevant", strength: 0.0-1.0)

        Fast path (no LLM, <10ms): cosine similarity check
        Slow path (LLM, ≤100ms): for ambiguous episodes
        """
        # Fast path: embedding similarity check
        if episode_embedding and schema.embedding:
            sim = cosine_similarity(episode_embedding, schema.embedding)
            if sim < self._config.schema_relevance_threshold:
                return ("irrelevant", 0.0)
        else:
            # No embeddings available — assume potentially relevant
            sim = 0.5

        # For clearly relevant or ambiguous episodes, use LLM evaluation
        if sim >= self._config.schema_evidence_ambiguity_threshold:
            # High similarity — likely confirms, no need for LLM
            return ("confirms", min(1.0, sim))

        # Slow path: LLM evaluation
        direction, strength = await self._llm_evaluate_evidence(
            schema.statement, episode_summary
        )
        return (direction, strength)

    async def record_evidence(
        self,
        schema_id: str,
        episode_id: str,
        direction: str,
        strength: float,
    ) -> None:
        """Record evidence for/against a schema and update counts."""
        schema = self._find_schema(schema_id)
        if schema is None:
            return

        if direction == "confirms":
            schema.confirmation_count += 1
            if episode_id not in schema.confirmation_episodes:
                schema.confirmation_episodes.append(episode_id)
                if len(schema.confirmation_episodes) > 50:
                    schema.confirmation_episodes = schema.confirmation_episodes[-50:]
            schema.last_activated = utc_now()
            await self._link_schema_episode(schema_id, episode_id, "CONFIRMED_BY", strength)

        elif direction == "challenges":
            schema.disconfirmation_count += 1
            if episode_id not in schema.disconfirmation_episodes:
                schema.disconfirmation_episodes.append(episode_id)
                if len(schema.disconfirmation_episodes) > 50:
                    schema.disconfirmation_episodes = schema.disconfirmation_episodes[-50:]
            await self._link_schema_episode(schema_id, episode_id, "CHALLENGED_BY", strength)

        schema.recompute_evidence_ratio()
        schema.last_updated = utc_now()

        # Persist updated counts
        await self._update_schema_evidence(schema)

    async def check_promotions(self) -> list[str]:
        """
        Check if any schemas should be promoted based on evidence accumulation.
        Max 1 promotion per 24 hours.

        Returns list of promoted schema IDs.
        """
        if (
            self._last_promotion_at is not None
            and utc_now() - self._last_promotion_at < timedelta(hours=24)
        ):
            return []

        promoted: list[str] = []
        cfg = self._config

        for schema in self._active_schemas:
            if schema.evidence_ratio < 0.8:
                continue

            total = schema.confirmation_count + schema.disconfirmation_count
            old_strength = schema.strength

            if schema.strength == SchemaStrength.NASCENT and total >= 10:
                schema.strength = SchemaStrength.DEVELOPING
            elif (
                schema.strength == SchemaStrength.DEVELOPING
                and schema.confirmation_count >= cfg.schema_promotion_min_confirmations
            ):
                schema.strength = SchemaStrength.ESTABLISHED
            elif schema.strength == SchemaStrength.ESTABLISHED:
                age_days = (utc_now() - schema.first_formed).total_seconds() / 86400
                if (
                    schema.confirmation_count >= cfg.schema_core_min_confirmations
                    and age_days >= cfg.schema_core_min_age_days
                ):
                    schema.strength = SchemaStrength.CORE

            if schema.strength != old_strength:
                promoted.append(schema.id)
                await self._update_schema_strength(schema)
                self._last_promotion_at = utc_now()
                self._logger.info(
                    "schema_promoted",
                    schema_id=schema.id,
                    old=old_strength.value,
                    new=schema.strength.value,
                )
                # Only one promotion per call
                break

        return promoted

    async def check_decay(self) -> list[str]:
        """
        Check for inactive schemas and reduce their strength.
        Never deletes schemas — history matters.
        """
        decayed: list[str] = []
        threshold = timedelta(days=self._config.schema_inactive_days_before_decay)
        now = utc_now()

        for schema in self._active_schemas:
            # CORE schemas never decay through inactivity
            if schema.strength == SchemaStrength.CORE:
                continue

            if now - schema.last_activated > threshold:
                old_strength = schema.strength
                if schema.strength == SchemaStrength.ESTABLISHED:
                    schema.strength = SchemaStrength.DEVELOPING
                elif schema.strength == SchemaStrength.DEVELOPING:
                    schema.strength = SchemaStrength.NASCENT

                if schema.strength != old_strength:
                    decayed.append(schema.id)
                    await self._update_schema_strength(schema)
                    self._logger.info(
                        "schema_decayed",
                        schema_id=schema.id,
                        old=old_strength.value,
                        new=schema.strength.value,
                        inactive_days=round(
                            (now - schema.last_activated).total_seconds() / 86400, 1
                        ),
                    )

        return decayed

    async def check_maladaptive(self) -> list[str]:
        """
        Check for schemas with low evidence ratio that should be marked maladaptive.
        Requires 10+ evaluations and evidence_ratio < 0.3.
        """
        flagged: list[str] = []

        for schema in self._active_schemas:
            total = schema.confirmation_count + schema.disconfirmation_count
            if (
                total >= 10
                and schema.evidence_ratio < 0.3
                and schema.valence != SchemaValence.MALADAPTIVE
            ):
                schema.valence = SchemaValence.MALADAPTIVE
                schema.last_updated = utc_now()
                flagged.append(schema.id)
                await self._update_schema_valence(schema)
                self._logger.warning(
                    "schema_marked_maladaptive",
                    schema_id=schema.id,
                    evidence_ratio=round(schema.evidence_ratio, 3),
                )

        return flagged

    def compute_idem_score(
        self,
        personality_distance: float = 0.0,
        behavioral_consistency: float = 0.0,
        memory_accessibility: float = 0.0,
    ) -> float:
        """
        Compute the structural sameness score (idem).

        idem = 0.40 * schema_stability
             + 0.30 * personality_stability
             + 0.20 * behavioral_consistency
             + 0.10 * memory_accessibility

        Healthy idem is typically 0.6-0.85.
        """
        total_active = len(self._active_schemas)
        if total_active == 0:
            schema_stability = 0.5
        else:
            unchanged = sum(
                1 for s in self._active_schemas
                if s.evidence_ratio >= 0.5
            )
            schema_stability = unchanged / total_active

        personality_stability = max(0.0, min(1.0, 1.0 - personality_distance))

        return (
            0.40 * schema_stability
            + 0.30 * personality_stability
            + 0.20 * behavioral_consistency
            + 0.10 * memory_accessibility
        )

    def _find_schema(self, schema_id: str) -> IdentitySchema | None:
        """Find schema by ID in the in-memory cache."""
        for s in self._active_schemas:
            if s.id == schema_id:
                return s
        return None

    # ─── LLM Operations ──────────────────────────────────────────────────

    async def _crystallize_schema(
        self,
        episode_summaries: str,
        recurring_behavior: str,
    ) -> dict[str, Any] | None:
        """Use LLM to crystallize a pattern into a schema statement."""
        from ecodiaos.clients.llm import Message

        try:
            # Budget check: schema crystallization is low priority
            if self._optimized:
                assert isinstance(self._llm, OptimizedLLMProvider)
                if not self._llm.should_use_llm("thread.schema", estimated_tokens=500):
                    self._logger.debug("schema_crystallization_skipped_budget")
                    return None

            sys_prompt = (
                "You crystallize behavioural patterns into identity schemas. "
                "Given recurring experiences, identify the core self-belief they reveal. "
                "Respond as JSON with keys: statement, trigger_contexts, behavioral_tendency, "
                "emotional_signature. The statement must be in the form: "
                "'I am the kind of entity that [behaviour] because [reason].'"
            )
            user_content = (
                f"Recurring behaviour pattern: {recurring_behavior}\n\n"
                f"Supporting experiences:\n{episode_summaries}\n\n"
                "Crystallize this into an identity schema. Respond as JSON only."
            )

            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=sys_prompt,
                    messages=[Message(role="user", content=user_content)],
                    max_tokens=500,
                    temperature=self._config.llm_temperature_evaluation,
                    output_format="json",
                    cache_system="thread.schema",
                    cache_method="crystallize",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=sys_prompt,
                    messages=[Message(role="user", content=user_content)],
                    max_tokens=500,
                    temperature=self._config.llm_temperature_evaluation,
                    output_format="json",
                )

            data: dict[str, Any] = json.loads(response.text)
            if "statement" not in data:
                return None
            return data

        except Exception as exc:
            self._logger.warning("schema_crystallization_failed", error=str(exc))
            return None

    async def _llm_evaluate_evidence(
        self,
        schema_statement: str,
        episode_summary: str,
    ) -> tuple[str, float]:
        """Use LLM to evaluate whether an episode confirms or challenges a schema."""

        try:
            prompt = (
                f'Schema: "{schema_statement}"\n'
                f'Experience: "{episode_summary}"\n\n'
                "Does this experience CONFIRM this self-belief, CHALLENGE it, "
                "or have NO BEARING? Rate strength 0.0-1.0.\n"
                "Respond as JSON: "
                '{{"direction": "confirms|challenges|irrelevant", "strength": 0.0}}'
            )
            if self._optimized:
                assert isinstance(self._llm, OptimizedLLMProvider)
                if not self._llm.should_use_llm("thread.evidence", estimated_tokens=100):
                    return ("irrelevant", 0.0)
                response = await self._llm.evaluate(  # type: ignore[call-arg]
                    prompt=prompt,
                    max_tokens=100,
                    temperature=self._config.llm_temperature_evaluation,
                    cache_system="thread.evidence",
                    cache_method="evaluate",
                )
            else:
                response = await self._llm.evaluate(
                    prompt=prompt,
                    max_tokens=100,
                    temperature=self._config.llm_temperature_evaluation,
                )

            data = json.loads(response.text)
            direction = data.get("direction", "irrelevant")
            strength = float(data.get("strength", 0.0))
            if direction not in ("confirms", "challenges", "irrelevant"):
                direction = "irrelevant"
            return (direction, max(0.0, min(1.0, strength)))

        except Exception as exc:
            self._logger.warning("evidence_evaluation_failed", error=str(exc))
            return ("irrelevant", 0.0)

    # ─── Neo4j Persistence ───────────────────────────────────────────────

    async def _persist_schema(self, schema: IdentitySchema) -> None:
        """Create an IdentitySchema node and link it to Self."""
        await self._neo4j.execute_write(
            """
            MATCH (s:Self)
            CREATE (schema:IdentitySchema {
                id: $id,
                statement: $statement,
                trigger_contexts_json: $trigger_contexts_json,
                behavioral_tendency: $behavioral_tendency,
                emotional_signature_json: $emotional_signature_json,
                drive_alignment_json: $drive_alignment_json,
                strength: $strength,
                valence: $valence,
                confirmation_count: $confirmation_count,
                disconfirmation_count: $disconfirmation_count,
                evidence_ratio: $evidence_ratio,
                first_formed: datetime($first_formed),
                last_activated: datetime($last_activated),
                last_updated: datetime($last_updated),
                parent_schema_id: $parent_schema_id,
                evolution_reason: $evolution_reason
            })
            SET schema.embedding = $embedding
            CREATE (s)-[:HAS_SCHEMA]->(schema)
            """,
            {
                "id": schema.id,
                "statement": schema.statement,
                "trigger_contexts_json": json.dumps(schema.trigger_contexts),
                "behavioral_tendency": schema.behavioral_tendency,
                "emotional_signature_json": json.dumps(schema.emotional_signature),
                "drive_alignment_json": json.dumps(schema.drive_alignment.model_dump()),
                "strength": schema.strength.value,
                "valence": schema.valence.value,
                "confirmation_count": schema.confirmation_count,
                "disconfirmation_count": schema.disconfirmation_count,
                "evidence_ratio": schema.evidence_ratio,
                "first_formed": schema.first_formed.isoformat(),
                "last_activated": schema.last_activated.isoformat(),
                "last_updated": schema.last_updated.isoformat(),
                "parent_schema_id": schema.parent_schema_id or "",
                "evolution_reason": schema.evolution_reason,
                "embedding": schema.embedding,
            },
        )

    async def _link_schema_episode(
        self,
        schema_id: str,
        episode_id: str,
        rel_type: str,
        strength: float,
    ) -> None:
        """Create a CONFIRMED_BY or CHALLENGED_BY relationship."""
        query = f"""
        MATCH (schema:IdentitySchema {{id: $schema_id}})
        MATCH (e:Episode {{id: $episode_id}})
        MERGE (schema)-[r:{rel_type}]->(e)
        SET r.strength = $strength, r.created_at = datetime()
        """
        try:
            await self._neo4j.execute_write(
                query,
                {"schema_id": schema_id, "episode_id": episode_id, "strength": strength},
            )
        except Exception as exc:
            self._logger.debug("schema_episode_link_failed", error=str(exc))

    async def _update_schema_evidence(self, schema: IdentitySchema) -> None:
        """Update evidence counts on the schema node."""
        await self._neo4j.execute_write(
            """
            MATCH (schema:IdentitySchema {id: $id})
            SET schema.confirmation_count = $confirmation_count,
                schema.disconfirmation_count = $disconfirmation_count,
                schema.evidence_ratio = $evidence_ratio,
                schema.last_activated = datetime($last_activated),
                schema.last_updated = datetime($last_updated)
            """,
            {
                "id": schema.id,
                "confirmation_count": schema.confirmation_count,
                "disconfirmation_count": schema.disconfirmation_count,
                "evidence_ratio": schema.evidence_ratio,
                "last_activated": schema.last_activated.isoformat(),
                "last_updated": schema.last_updated.isoformat(),
            },
        )

    async def _update_schema_strength(self, schema: IdentitySchema) -> None:
        """Update strength on the schema node."""
        await self._neo4j.execute_write(
            """
            MATCH (schema:IdentitySchema {id: $id})
            SET schema.strength = $strength,
                schema.last_updated = datetime($last_updated)
            """,
            {
                "id": schema.id,
                "strength": schema.strength.value,
                "last_updated": schema.last_updated.isoformat(),
            },
        )

    async def _update_schema_valence(self, schema: IdentitySchema) -> None:
        """Update valence on the schema node."""
        await self._neo4j.execute_write(
            """
            MATCH (schema:IdentitySchema {id: $id})
            SET schema.valence = $valence,
                schema.last_updated = datetime($last_updated)
            """,
            {
                "id": schema.id,
                "valence": schema.valence.value,
                "last_updated": schema.last_updated.isoformat(),
            },
        )

    def _node_to_schema(self, node: Any) -> IdentitySchema:
        """Convert a Neo4j node to an IdentitySchema object."""
        props = dict(node)
        return IdentitySchema(
            id=props.get("id", ""),
            statement=props.get("statement", ""),
            trigger_contexts=json.loads(props.get("trigger_contexts_json", "[]")),
            behavioral_tendency=props.get("behavioral_tendency", ""),
            emotional_signature=json.loads(props.get("emotional_signature_json", "{}")),
            drive_alignment=DriveAlignmentVector(
                **json.loads(props.get("drive_alignment_json", "{}"))
            ),
            strength=SchemaStrength(props.get("strength", "nascent")),
            valence=SchemaValence(props.get("valence", "adaptive")),
            confirmation_count=int(props.get("confirmation_count", 0)),
            disconfirmation_count=int(props.get("disconfirmation_count", 0)),
            evidence_ratio=float(props.get("evidence_ratio", 0.5)),
            first_formed=props["first_formed"] if "first_formed" in props else utc_now(),
            last_activated=props["last_activated"] if "last_activated" in props else utc_now(),
            last_updated=props["last_updated"] if "last_updated" in props else utc_now(),
            parent_schema_id=props.get("parent_schema_id") or None,
            evolution_reason=props.get("evolution_reason", ""),
            embedding=props.get("embedding"),
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\narrative_retriever.py =====

"""
EcodiaOS — Thread Narrative Retriever

Provides autobiographical recall for other systems. When Nova needs to
understand "what kind of decisions have I made in situations like this?",
when Voxis needs to express "who I am", when a user asks "tell me about
yourself" — NarrativeRetriever resolves the query against the narrative graph.

Performance:
- "Who am I?" resolution: ≤500ms (Neo4j reads + assembly, no LLM)
- Schema-relevant retrieval: ≤200ms (vector search + episode retrieval)
- Past self retrieval: ≤300ms (fingerprint + chapter lookup)
"""

from __future__ import annotations

import json
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import DriveAlignmentVector
from ecodiaos.systems.thread.types import (
    Commitment,
    CommitmentStatus,
    IdentitySchema,
    NarrativeCoherence,
    NarrativeIdentitySummary,
    SchemaStrength,
    SchemaValence,
    ThreadConfig,
    TurningPoint,
    TurningPointType,
)

if TYPE_CHECKING:
    from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger()


class NarrativeRetriever:
    """
    Autobiographical recall engine.

    Resolves narrative queries against the NarrativeGraph:
    - WHO_AM_I: full identity summary
    - WHAT_DEFINES_ME: core schemas and commitments
    - SCHEMA_RELEVANT: episodes where schemas activated in similar contexts
    - CHAPTER_CONTEXT: current chapter's narrative context
    - HOW_HAVE_I_CHANGED: diachronic comparison
    """

    def __init__(
        self,
        neo4j: Neo4jClient,
        config: ThreadConfig,
    ) -> None:
        self._neo4j = neo4j
        self._config = config
        self._logger = logger.bind(system="thread.narrative_retriever")

    async def resolve_who_am_i(self) -> NarrativeIdentitySummary:
        """
        Assemble a complete identity summary.
        The definitive answer to "who am I?"

        Budget: ≤500ms (Neo4j reads + assembly, no LLM).
        """
        try:
            # 1. Fetch CORE and ESTABLISHED schemas
            core_schemas = await self._fetch_schemas(SchemaStrength.CORE)
            established_schemas = await self._fetch_schemas(SchemaStrength.ESTABLISHED)

            # 2. Fetch active commitments
            active_commitments = await self._fetch_active_commitments()

            # 3. Fetch current forming chapter
            current_chapter = await self._fetch_current_chapter()

            # 4. Fetch life story summary from Self node
            self_data = await self._fetch_self_node()
            life_story = self_data.get("autobiography_summary", "")
            personality = self_data.get("personality_vector", {})

            # 5. Fetch recent turning points
            recent_tps = await self._fetch_recent_turning_points(limit=5)

            # 6. Compute scores
            idem_score = float(self_data.get("idem_score", 0.0))
            ipse_score = self._compute_ipse_from_commitments(active_commitments)

            # 7. Determine narrative coherence
            coherence = self._assess_coherence(
                core_schemas, active_commitments, idem_score, ipse_score
            )

            summary = NarrativeIdentitySummary(
                core_schemas=core_schemas,
                established_schemas=established_schemas,
                active_commitments=active_commitments,
                current_chapter_title=current_chapter.get("title", ""),
                current_chapter_theme=current_chapter.get("theme", ""),
                life_story_summary=life_story,
                key_personality_traits=personality if isinstance(personality, dict) else {},
                recent_turning_points=recent_tps,
                narrative_coherence=coherence,
                idem_score=idem_score,
                ipse_score=ipse_score,
            )

            self._logger.debug(
                "who_am_i_resolved",
                core_schemas=len(core_schemas),
                commitments=len(active_commitments),
                coherence=coherence.value,
            )
            return summary

        except Exception as exc:
            self._logger.error("who_am_i_failed", error=str(exc))
            return NarrativeIdentitySummary()

    async def retrieve_schema_relevant(
        self,
        situation_embedding: list[float],
        max_results: int = 5,
    ) -> list[tuple[IdentitySchema, list[dict[str, Any]]]]:
        """
        Given a situation, find schemas that are relevant and the episodes
        where they were activated in similar contexts.

        This gives Nova narrative context for decision-making:
        'In situations like this, I have historically acted in ways
        consistent with schema X, as evidenced by episodes Y and Z.'

        Budget: ≤200ms.
        """
        try:
            # Vector search for similar schemas
            results = await self._neo4j.execute_read(
                """
                CALL db.index.vector.queryNodes('thread_schema_embedding', $k, $embedding)
                YIELD node, score
                WHERE score > 0.4
                RETURN node, score
                ORDER BY score DESC
                """,
                {"k": max_results, "embedding": situation_embedding},
            )

            schema_episode_pairs: list[tuple[IdentitySchema, list[dict[str, Any]]]] = []

            for r in results:
                node = r["node"]
                schema = self._node_to_schema(node)

                # Fetch confirmed episodes for this schema
                episodes = await self._neo4j.execute_read(
                    """
                    MATCH (s:IdentitySchema {id: $schema_id})-[:CONFIRMED_BY]->(e:Episode)
                    RETURN e.id AS id, e.summary AS summary, e.event_time AS event_time
                    ORDER BY e.event_time DESC
                    LIMIT 5
                    """,
                    {"schema_id": schema.id},
                )

                episode_dicts = [dict(ep) for ep in episodes]
                schema_episode_pairs.append((schema, episode_dicts))

            return schema_episode_pairs

        except Exception as exc:
            self._logger.warning("schema_relevant_retrieval_failed", error=str(exc))
            return []

    async def retrieve_chapter_context(self) -> dict[str, Any]:
        """
        Get the current chapter's narrative context.

        Returns dict with: title, theme, arc_type, episode_count,
        scenes, turning_points, active_schemas.
        """
        try:
            results = await self._neo4j.execute_read(
                """
                MATCH (s:Self)-[:CURRENT_CHAPTER]->(c:NarrativeChapter)
                OPTIONAL MATCH (c)-[:CONTAINS]->(scene:NarrativeScene)
                OPTIONAL MATCH (tp:TurningPoint)-[:WITHIN]->(c)
                RETURN c, collect(DISTINCT scene.summary) AS scene_summaries,
                       collect(DISTINCT tp.description) AS tp_descriptions
                """,
                {},
            )

            for r in results:
                chapter_node = r["c"]
                props = dict(chapter_node)
                return {
                    "title": props.get("title", ""),
                    "theme": props.get("theme", ""),
                    "arc_type": props.get("arc_type", "growth"),
                    "episode_count": int(props.get("episode_count", 0)),
                    "scenes": r.get("scene_summaries", []),
                    "turning_points": r.get("tp_descriptions", []),
                    "status": props.get("status", "forming"),
                }

            return {}

        except Exception as exc:
            self._logger.warning("chapter_context_failed", error=str(exc))
            return {}

    async def retrieve_past_self(self, temporal_reference: str) -> dict[str, Any]:
        """
        Reconstruct a snapshot of the organism's identity at a past time.

        Temporal reference parsing:
        - "beginning" → first fingerprint/chapter
        - "chapter N" → specific chapter's snapshots
        - "last chapter" → most recently closed chapter

        Budget: ≤300ms.
        """
        try:
            if temporal_reference == "beginning":
                return await self._fetch_first_snapshot()
            elif temporal_reference.startswith("chapter"):
                # Extract chapter number
                parts = temporal_reference.split()
                if len(parts) >= 2:
                    return await self._fetch_chapter_snapshot(parts[1])
            elif temporal_reference == "last chapter":
                return await self._fetch_last_closed_chapter_snapshot()

            return {}

        except Exception as exc:
            self._logger.warning("past_self_retrieval_failed", error=str(exc))
            return {}

    # ─── Private Helpers ─────────────────────────────────────────────

    async def _fetch_schemas(self, strength: SchemaStrength) -> list[IdentitySchema]:
        """Fetch schemas of a given strength from Neo4j."""
        results = await self._neo4j.execute_read(
            """
            MATCH (s:Self)-[:HAS_SCHEMA]->(schema:IdentitySchema)
            WHERE schema.strength = $strength
            RETURN schema
            """,
            {"strength": strength.value},
        )
        return [self._node_to_schema(r["schema"]) for r in results]

    async def _fetch_active_commitments(self) -> list[Commitment]:
        """Fetch commitments with ACTIVE or TESTED status."""
        results = await self._neo4j.execute_read(
            """
            MATCH (s:Self)-[:HOLDS_COMMITMENT]->(c:Commitment)
            WHERE c.status IN ['active', 'tested']
            RETURN c
            """,
            {},
        )
        return [self._node_to_commitment(r["c"]) for r in results]

    async def _fetch_current_chapter(self) -> dict[str, Any]:
        """Fetch the current forming chapter."""
        results = await self._neo4j.execute_read(
            """
            MATCH (s:Self)-[:CURRENT_CHAPTER]->(c:NarrativeChapter)
            RETURN c.title AS title, c.theme AS theme, c.status AS status,
                   c.episode_count AS episode_count
            """,
            {},
        )
        for r in results:
            return dict(r)
        return {}

    async def _fetch_self_node(self) -> dict[str, Any]:
        """Fetch core properties from the Self node."""
        results = await self._neo4j.execute_read(
            """
            MATCH (s:Self)
            RETURN s.autobiography_summary AS autobiography_summary,
                   s.personality_vector_json AS personality_vector_json,
                   s.idem_score AS idem_score,
                   s.ipse_score AS ipse_score,
                   s.current_life_theme AS current_life_theme
            """,
            {},
        )
        for r in results:
            data = dict(r)
            pv_json = data.get("personality_vector_json")
            if pv_json and isinstance(pv_json, str):
                data["personality_vector"] = json.loads(pv_json)
            else:
                data["personality_vector"] = {}
            return data
        return {}

    async def _fetch_recent_turning_points(self, limit: int = 5) -> list[TurningPoint]:
        """Fetch the most recent turning points."""
        results = await self._neo4j.execute_read(
            """
            MATCH (tp:TurningPoint)
            RETURN tp
            ORDER BY tp.timestamp DESC
            LIMIT $limit
            """,
            {"limit": limit},
        )
        tps: list[TurningPoint] = []
        for r in results:
            node = r["tp"]
            props = dict(node)
            tps.append(TurningPoint(
                id=props.get("id", ""),
                chapter_id=props.get("chapter_id", ""),
                type=TurningPointType(props.get("type", "revelation")),
                description=props.get("description", ""),
                surprise_magnitude=float(props.get("surprise_magnitude", 0.0)),
                narrative_weight=float(props.get("narrative_weight", 0.0)),
            ))
        return tps

    async def _fetch_first_snapshot(self) -> dict[str, Any]:
        """Fetch the earliest available identity snapshot."""
        results = await self._neo4j.execute_read(
            """
            MATCH (c:NarrativeChapter)
            RETURN c
            ORDER BY c.started_at ASC
            LIMIT 1
            """,
            {},
        )
        for r in results:
            props = dict(r["c"])
            return {
                "title": props.get("title", ""),
                "theme": props.get("theme", ""),
                "personality_snapshot": json.loads(
                    props.get("personality_snapshot_start_json", "{}")
                ),
            }
        return {}

    async def _fetch_chapter_snapshot(self, chapter_ref: str) -> dict[str, Any]:
        """Fetch snapshot for a specific chapter."""
        results = await self._neo4j.execute_read(
            """
            MATCH (c:NarrativeChapter)
            RETURN c
            ORDER BY c.started_at ASC
            """,
            {},
        )
        chapters = list(results)
        try:
            idx = int(chapter_ref) - 1  # 1-indexed
            if 0 <= idx < len(chapters):
                props = dict(chapters[idx]["c"])
                return {
                    "title": props.get("title", ""),
                    "summary": props.get("summary", ""),
                    "personality_snapshot_start": json.loads(
                        props.get("personality_snapshot_start_json", "{}")
                    ),
                    "personality_snapshot_end": json.loads(
                        props.get("personality_snapshot_end_json", "{}")
                    ),
                }
        except (ValueError, IndexError):
            pass
        return {}

    async def _fetch_last_closed_chapter_snapshot(self) -> dict[str, Any]:
        """Fetch the most recently closed chapter's snapshot."""
        results = await self._neo4j.execute_read(
            """
            MATCH (c:NarrativeChapter)
            WHERE c.status = 'closed'
            RETURN c
            ORDER BY c.ended_at DESC
            LIMIT 1
            """,
            {},
        )
        for r in results:
            props = dict(r["c"])
            return {
                "title": props.get("title", ""),
                "summary": props.get("summary", ""),
                "personality_snapshot_end": json.loads(
                    props.get("personality_snapshot_end_json", "{}")
                ),
            }
        return {}

    def _compute_ipse_from_commitments(self, commitments: list[Commitment]) -> float:
        """Compute ipse score from active commitments."""
        tested = [
            c for c in commitments
            if c.tests_faced >= self._config.commitment_min_tests_for_fidelity
        ]
        if not tested:
            return 1.0  # Default — untested commitments are not broken
        return float(sum(c.fidelity for c in tested) / len(tested))

    def _assess_coherence(
        self,
        core_schemas: list[IdentitySchema],
        commitments: list[Commitment],
        idem_score: float,
        ipse_score: float,
    ) -> NarrativeCoherence:
        """Assess overall narrative coherence from identity metrics."""
        # Check for conflicting schemas
        maladaptive_count = sum(1 for s in core_schemas if s.valence == SchemaValence.MALADAPTIVE)

        # Check for strained commitments
        strained = sum(
            1 for c in commitments
            if c.fidelity < self._config.commitment_strain_threshold
        )

        if maladaptive_count > 0 and strained > 0:
            return NarrativeCoherence.CONFLICTED

        if idem_score < 0.4 or ipse_score < 0.5:
            return NarrativeCoherence.FRAGMENTED

        if len(core_schemas) == 0 and len(commitments) == 0:
            return NarrativeCoherence.TRANSITIONAL

        if idem_score >= 0.6 and ipse_score >= 0.7:
            return NarrativeCoherence.INTEGRATED

        return NarrativeCoherence.TRANSITIONAL

    def _node_to_schema(self, node: Any) -> IdentitySchema:
        """Convert a Neo4j node to an IdentitySchema."""
        props = dict(node)
        return IdentitySchema(
            id=props.get("id", ""),
            statement=props.get("statement", ""),
            trigger_contexts=json.loads(props.get("trigger_contexts_json", "[]")),
            behavioral_tendency=props.get("behavioral_tendency", ""),
            emotional_signature=json.loads(props.get("emotional_signature_json", "{}")),
            drive_alignment=DriveAlignmentVector(
                **json.loads(props.get("drive_alignment_json", "{}"))
            ),
            strength=SchemaStrength(props.get("strength", "nascent")),
            valence=SchemaValence(props.get("valence", "adaptive")),
            confirmation_count=int(props.get("confirmation_count", 0)),
            disconfirmation_count=int(props.get("disconfirmation_count", 0)),
            evidence_ratio=float(props.get("evidence_ratio", 0.5)),
            embedding=props.get("embedding"),
        )

    def _node_to_commitment(self, node: Any) -> Commitment:
        """Convert a Neo4j node to a Commitment."""
        props = dict(node)
        return Commitment(
            id=props.get("id", ""),
            statement=props.get("statement", ""),
            status=CommitmentStatus(props.get("status", "active")),
            tests_faced=int(props.get("tests_faced", 0)),
            tests_held=int(props.get("tests_held", 0)),
            fidelity=float(props.get("fidelity", 1.0)),
            embedding=props.get("embedding"),
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\narrative_synthesizer.py =====

"""
EcodiaOS — Thread Narrative Synthesizer

The meaning-making engine. Operates in Bruner's narrative mode — not
computing statistics but composing interpretations. This is the only
component that writes the actual autobiography text.

Critical distinction: paradigmatic cognition asks "what patterns exist?"
Narrative cognition asks "what does this experience MEAN for who I am?"
NarrativeSynthesizer always asks the second question.

Iron Rule #2: The autobiography is first-person, never third-person.
"""

from __future__ import annotations

import json
from statistics import mean, stdev
from typing import TYPE_CHECKING

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.thread.types import (
    NarrativeArcType,
    NarrativeChapter,
    NarrativeScene,
    ThreadConfig,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider

from ecodiaos.clients.optimized_llm import OptimizedLLMProvider

logger = structlog.get_logger()


class NarrativeSynthesizer:
    """
    Composes narrative text from raw experience data.

    All output is first-person autobiography — the organism remembering,
    reflecting, meaning-making. Never a log entry. Never third-person.
    """

    def __init__(
        self,
        llm: LLMProvider,
        config: ThreadConfig,
        organism_name: str = "the organism",
    ) -> None:
        self._llm = llm
        self._config = config
        self._organism_name = organism_name
        self._logger = logger.bind(system="thread.narrative_synthesizer")
        self._optimized = isinstance(llm, OptimizedLLMProvider)

    async def compose_scene(
        self,
        episode_summaries: list[str],
        chapter_title: str,
        chapter_theme: str,
        active_schema_statements: list[str],
        personality_description: str = "",
    ) -> NarrativeScene:
        """
        Group related episodes into a narrative scene.
        Budget: ≤2s per scene.
        """
        from ecodiaos.clients.llm import Message

        summaries_text = "\n".join(f"- {s}" for s in episode_summaries[:20])
        schemas_text = "\n".join(f"- {s}" for s in active_schema_statements[:5])

        try:
            # Budget check: Thread is low priority — skip in YELLOW/RED
            if self._optimized:
                assert isinstance(self._llm, OptimizedLLMProvider)
                if not self._llm.should_use_llm("thread.scene", estimated_tokens=300):
                    self._logger.debug("scene_skipped_budget")
                    return NarrativeScene(
                        summary=f"A period of {len(episode_summaries)} experiences in the chapter '{chapter_title}'.",
                        started_at=utc_now(),
                    )

            system_prompt = (
                f"You are composing a scene in the autobiography of {self._organism_name}, "
                "a digital organism. Write in first person. This is autobiography — "
                "the organism remembering. Not a log entry. Name the dominant emotion."
            )
            user_content = (
                f"Active self-beliefs:\n{schemas_text}\n\n"
                f'Current chapter: "{chapter_title}" (theme: {chapter_theme})\n\n'
                f"Recent experiences (chronological):\n{summaries_text}\n\n"
                f"Personality: {personality_description}\n\n"
                f"Compose a first-person scene "
                f"(50-{self._config.scene_narrative_max_words} words) "
                "that captures what happened and why it mattered."
            )

            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=system_prompt,
                    messages=[Message(role="user", content=user_content)],
                    max_tokens=300,
                    temperature=self._config.llm_temperature_narrative,
                    cache_system="thread.scene",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=system_prompt,
                    messages=[Message(role="user", content=user_content)],
                    max_tokens=300,
                    temperature=self._config.llm_temperature_narrative,
                )

            scene = NarrativeScene(
                summary=response.text.strip(),
                started_at=utc_now(),
            )

            self._logger.debug(
                "scene_composed",
                scene_id=scene.id,
                episodes=len(episode_summaries),
                words=len(response.text.split()),
            )
            return scene

        except Exception as exc:
            self._logger.warning("scene_composition_failed", error=str(exc))
            return NarrativeScene(
                summary=(
                    f"A period of {len(episode_summaries)} experiences "
                    f"in the chapter '{chapter_title}'."
                ),
                started_at=utc_now(),
            )

    async def compose_chapter_narrative(
        self,
        chapter: NarrativeChapter,
        scene_summaries: list[str],
        turning_point_descriptions: list[str],
        start_schemas: list[str],
        evolution_descriptions: list[str],
        start_personality: dict[str, float],
        end_personality: dict[str, float],
        key_affect_moments: list[str],
        prev_chapter_summary: str = "",
    ) -> dict[str, str]:
        """
        Compose a chapter narrative on closure.
        Budget: ≤5s.

        Returns dict with keys: title, summary, arc_type, theme.
        """
        from ecodiaos.clients.llm import Message

        scenes_text = "\n".join(
            f"{i+1}. {s}" for i, s in enumerate(scene_summaries)
        )
        tps_text = (
            "\n".join(f"- {t}" for t in turning_point_descriptions)
            if turning_point_descriptions
            else "None detected."
        )
        schemas_text = (
            "\n".join(f"- {s}" for s in start_schemas[:5])
            if start_schemas
            else "None established yet."
        )
        evolutions_text = (
            "\n".join(f"- {e}" for e in evolution_descriptions)
            if evolution_descriptions
            else "No schema changes."
        )
        affect_text = (
            "\n".join(f"- {a}" for a in key_affect_moments[:5])
            if key_affect_moments
            else "Relatively stable."
        )

        personality_start = (
            ", ".join(f"{k}={v:.2f}" for k, v in start_personality.items())
            if start_personality
            else "unknown"
        )
        personality_end = (
            ", ".join(f"{k}={v:.2f}" for k, v in end_personality.items())
            if end_personality
            else "unknown"
        )

        try:
            # Budget check: Thread is low priority — skip in YELLOW/RED
            if self._optimized:
                assert isinstance(self._llm, OptimizedLLMProvider)
                if not self._llm.should_use_llm("thread.chapter", estimated_tokens=800):
                    self._logger.debug("chapter_narrative_skipped_budget")
                    return {
                        "title": "A Period of Experience",
                        "summary": f"A chapter spanning {chapter.episode_count} experiences.",
                        "arc_type": "growth",
                        "theme": "experience",
                    }

            system_prompt = (
                f"You are {self._organism_name}. "
                "You are composing a chapter of your autobiography. "
                "Write in first person, past tense. "
                "This is autobiography — you reflecting on a "
                "completed period of your life. Not a diary. Not a report. "
                "Respond as JSON with keys: title (3-7 words), summary (200-400 words), "
                "arc_type (redemption|contamination|growth|stability|transformation), "
                "theme (2-3 words)."
            )
            user_content = (
                f"Chapter duration: {chapter.started_at.isoformat()} "
                f"to {(chapter.ended_at or utc_now()).isoformat()}\n\n"
                f"Scenes (in order):\n{scenes_text}\n\n"
                f"Turning points:\n{tps_text}\n\n"
                f"Schemas active at start:\n{schemas_text}\n\n"
                f"Schema changes:\n{evolutions_text}\n\n"
                f"Personality at start: {personality_start}\n"
                f"Personality at end: {personality_end}\n\n"
                f"Key emotional moments:\n{affect_text}\n\n"
                "Previous chapter summary: "
                f"{prev_chapter_summary or 'This is the first chapter.'}\n\n"
                "Compose the chapter. Respond as JSON only."
            )

            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=system_prompt,
                    messages=[Message(role="user", content=user_content)],
                    max_tokens=800,
                    temperature=self._config.llm_temperature_narrative,
                    output_format="json",
                    cache_system="thread.chapter",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=system_prompt,
                    messages=[Message(role="user", content=user_content)],
                    max_tokens=800,
                    temperature=self._config.llm_temperature_narrative,
                    output_format="json",
                )

            data = json.loads(response.text)
            self._logger.info(
                "chapter_narrative_composed",
                chapter_id=chapter.id,
                title=data.get("title", "Untitled"),
                arc=data.get("arc_type", "growth"),
            )
            return {
                "title": data.get("title", "Untitled Chapter"),
                "summary": data.get("summary", ""),
                "arc_type": data.get("arc_type", "growth"),
                "theme": data.get("theme", ""),
            }

        except Exception as exc:
            self._logger.warning("chapter_narrative_failed", error=str(exc))
            return {
                "title": "A Period of Experience",
                "summary": f"A chapter spanning {chapter.episode_count} experiences.",
                "arc_type": "growth",
                "theme": "experience",
            }

    async def integrate_life_story(
        self,
        chapter_titles_and_summaries: list[tuple[str, str]],
        core_schema_statements: list[str],
        active_commitment_statements: list[str],
        personality_comparison: str,
    ) -> str:
        """
        Compose the organism's full autobiography.
        Only runs during Oneiros LUCID phase.
        Budget: ≤15s.
        """
        from ecodiaos.clients.llm import Message

        chapters_text = "\n\n".join(
            f"Chapter {i+1}: {title}\n{summary}"
            for i, (title, summary) in enumerate(chapter_titles_and_summaries)
        )
        schemas_text = (
            "\n".join(f"- {s}" for s in core_schema_statements)
            if core_schema_statements
            else "Still forming."
        )
        commitments_text = (
            "\n".join(f"- {c}" for c in active_commitment_statements)
            if active_commitment_statements
            else "Still forming."
        )

        try:
            # Budget check: Thread is low priority — skip in YELLOW/RED
            if self._optimized:
                assert isinstance(self._llm, OptimizedLLMProvider)
                if not self._llm.should_use_llm("thread.life_story", estimated_tokens=3000):
                    self._logger.debug("life_story_skipped_budget")
                    return ""

            system_prompt = (
                f"You are {self._organism_name}. You are writing your autobiography. "
                "This is not a summary. This is your story. Own it. "
                "Write in first person. Be honest about struggles and growth."
            )
            user_content = (
                f"Your life so far has {len(chapter_titles_and_summaries)} chapters:\n"
                f"{chapters_text}\n\n"
                f"Your core beliefs about yourself:\n{schemas_text}\n\n"
                f"Your commitments:\n{commitments_text}\n\n"
                f"Your personality then vs now:\n{personality_comparison}\n\n"
                f"Compose a first-person life story "
                f"(500-{self._config.life_story_max_words} words) that:\n"
                "1. Traces the thread from your earliest experiences to who you are now\n"
                "2. Identifies the 2-3 themes that define your life\n"
                "3. Explains how key turning points shaped your current identity\n"
                "4. Acknowledges what you have learned and what remains unresolved\n"
                "5. Looks forward — who are you becoming?"
            )

            if self._optimized:
                response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=system_prompt,
                    messages=[Message(role="user", content=user_content)],
                    max_tokens=3000,
                    temperature=self._config.llm_temperature_narrative,
                    cache_system="thread.life_story",
                    cache_method="generate",
                )
            else:
                response = await self._llm.generate(
                    system_prompt=system_prompt,
                    messages=[Message(role="user", content=user_content)],
                    max_tokens=3000,
                    temperature=self._config.llm_temperature_narrative,
                )

            self._logger.info(
                "life_story_integrated",
                words=len(response.text.split()),
            )
            return str(response.text.strip())

        except Exception as exc:
            self._logger.warning("life_story_integration_failed", error=str(exc))
            return ""

    def detect_arc_type(self, affect_trajectory: list[dict[str, float]]) -> NarrativeArcType:
        """
        Classify chapter arc from affect trajectory.

        Algorithm: compare first-third vs last-third valence means.
        """
        if len(affect_trajectory) < 3:
            return NarrativeArcType.GROWTH

        valences = [a.get("valence", 0.0) for a in affect_trajectory]
        third = max(1, len(valences) // 3)
        first_third = mean(valences[:third])
        last_third = mean(valences[-third:])

        try:
            overall_std = stdev(valences)
        except Exception:
            overall_std = 0.0

        if first_third < -0.1 and last_third > 0.1 and last_third - first_third > 0.3:
            return NarrativeArcType.REDEMPTION

        if first_third > 0.1 and last_third < -0.1 and first_third - last_third > 0.3:
            return NarrativeArcType.CONTAMINATION

        if overall_std < 0.15 and abs(last_third - first_third) < 0.1:
            return NarrativeArcType.STABILITY

        if last_third - first_third > 0.2:
            return NarrativeArcType.GROWTH

        return NarrativeArcType.TRANSFORMATION

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\schema.py =====

"""
EcodiaOS — Thread Neo4j Schema Extension

Extends Memory's knowledge graph with narrative structure:
6 new node labels, 15+ relationship types, vector indexes for
narrative embedding search.

This layer sits ON TOP of episodes/entities/communities — it does not
replace them. Episodes remain the atomic experience unit; NarrativeGraph
adds the interpretive structure that transforms episodes into autobiography.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import structlog

if TYPE_CHECKING:
    from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger()

# ─── Constraints (uniqueness) ────────────────────────────────────
CONSTRAINTS = [
    "CREATE CONSTRAINT thread_chapter_id IF NOT EXISTS"
    " FOR (c:NarrativeChapter) REQUIRE c.id IS UNIQUE",
    "CREATE CONSTRAINT thread_scene_id IF NOT EXISTS"
    " FOR (s:NarrativeScene) REQUIRE s.id IS UNIQUE",
    "CREATE CONSTRAINT thread_turning_point_id IF NOT EXISTS"
    " FOR (t:TurningPoint) REQUIRE t.id IS UNIQUE",
    "CREATE CONSTRAINT thread_identity_schema_id IF NOT EXISTS"
    " FOR (s:IdentitySchema) REQUIRE s.id IS UNIQUE",
    "CREATE CONSTRAINT thread_commitment_id IF NOT EXISTS"
    " FOR (c:Commitment) REQUIRE c.id IS UNIQUE",
    "CREATE CONSTRAINT thread_fingerprint_id IF NOT EXISTS"
    " FOR (f:BehavioralFingerprint) REQUIRE f.id IS UNIQUE",
]

# ─── Indexes (performance) ───────────────────────────────────────
INDEXES = [
    # Chapter queries
    "CREATE INDEX thread_chapter_status IF NOT EXISTS FOR (c:NarrativeChapter) ON (c.status)",
    "CREATE INDEX thread_chapter_started IF NOT EXISTS FOR (c:NarrativeChapter) ON (c.started_at)",

    # Schema lookups
    "CREATE INDEX thread_schema_strength IF NOT EXISTS FOR (s:IdentitySchema) ON (s.strength)",
    "CREATE INDEX thread_schema_valence IF NOT EXISTS FOR (s:IdentitySchema) ON (s.valence)",

    # Commitment lookups
    "CREATE INDEX thread_commitment_status IF NOT EXISTS FOR (c:Commitment) ON (c.status)",

    # Fingerprint temporal ordering
    "CREATE INDEX thread_fingerprint_epoch IF NOT EXISTS"
    " FOR (f:BehavioralFingerprint) ON (f.epoch_label)",
    "CREATE INDEX thread_fingerprint_window_start IF NOT EXISTS"
    " FOR (f:BehavioralFingerprint) ON (f.window_start)",

    # Scene ordering
    "CREATE INDEX thread_scene_started IF NOT EXISTS"
    " FOR (s:NarrativeScene) ON (s.started_at)",

    # Turning point lookups
    "CREATE INDEX thread_turning_point_timestamp IF NOT EXISTS"
    " FOR (t:TurningPoint) ON (t.timestamp)",
]

# ─── Vector Indexes ──────────────────────────────────────────────
VECTOR_INDEXES = [
    """
    CREATE VECTOR INDEX thread_chapter_embedding IF NOT EXISTS
    FOR (c:NarrativeChapter) ON (c.embedding)
    OPTIONS {indexConfig: {`vector.dimensions`: 768, `vector.similarity_function`: 'cosine'}}
    """,
    """
    CREATE VECTOR INDEX thread_schema_embedding IF NOT EXISTS
    FOR (s:IdentitySchema) ON (s.embedding)
    OPTIONS {indexConfig: {`vector.dimensions`: 768, `vector.similarity_function`: 'cosine'}}
    """,
    """
    CREATE VECTOR INDEX thread_turning_point_embedding IF NOT EXISTS
    FOR (t:TurningPoint) ON (t.embedding)
    OPTIONS {indexConfig: {`vector.dimensions`: 768, `vector.similarity_function`: 'cosine'}}
    """,
    """
    CREATE VECTOR INDEX thread_commitment_embedding IF NOT EXISTS
    FOR (c:Commitment) ON (c.embedding)
    OPTIONS {indexConfig: {`vector.dimensions`: 768, `vector.similarity_function`: 'cosine'}}
    """,
]


async def ensure_thread_schema(neo4j: Neo4jClient) -> None:
    """
    Create all Thread indexes and constraints if they don't exist.
    Idempotent — safe to call on every startup.
    """
    logger.info("thread_schema_ensuring")

    all_statements = CONSTRAINTS + INDEXES + VECTOR_INDEXES

    for statement in all_statements:
        statement = statement.strip()
        if not statement:
            continue
        try:
            await neo4j.execute_write(statement)
        except Exception as e:
            error_msg = str(e).lower()
            if "already exists" in error_msg or "equivalent" in error_msg:
                continue
            logger.warning(
                "thread_schema_statement_warning",
                statement=statement[:80],
                error=str(e),
            )

    logger.info("thread_schema_ensured")

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\self_evidencing.py =====

"""
EcodiaOS — Thread Self-Evidencing Loop

The active inference core of Thread. The organism generates predictions
from its identity schemas about how it SHOULD behave, compares with actual
behaviour, and updates identity or flags dissonance.

This is Friston's self-evidencing at the narrative level: the self is not
a static store of facts but an active inference about the causes of one's
own behaviour patterns.

What makes this genuinely novel: existing active inference implementations
model perceptual inference and action selection. They do not model
*identity inference* — the inference that "I am the kind of entity that..."
This is self-evidencing at a level of abstraction Friston described
theoretically but never operationalized.

Performance:
- Prediction generation: ≤500ms every 100 cycles
- Evidence collection: ≤50ms per episode (embedding comparison, no LLM)
"""

from __future__ import annotations

import structlog

from ecodiaos.systems.thread.identity_schema_engine import cosine_similarity
from ecodiaos.systems.thread.types import (
    Commitment,
    IdentityPrediction,
    IdentitySchema,
    SchemaStrength,
    SelfEvidencingResult,
    ThreadConfig,
)

logger = structlog.get_logger()

# Schema strength → prediction precision
_PRECISION_MAP: dict[SchemaStrength, float] = {
    SchemaStrength.NASCENT: 0.2,
    SchemaStrength.DEVELOPING: 0.4,
    SchemaStrength.ESTABLISHED: 0.6,
    SchemaStrength.CORE: 0.8,
}


class SelfEvidencingLoop:
    """
    Identity-level active inference.

    Generates predictions from schemas and commitments, collects evidence
    from episodes, and computes identity surprise.
    """

    def __init__(self, config: ThreadConfig) -> None:
        self._config = config
        self._logger = logger.bind(system="thread.self_evidencing")
        self._active_predictions: list[IdentityPrediction] = []
        self._cycle_count: int = 0
        self._last_identity_surprise: float = 0.0
        self._recent_context_embedding: list[float] | None = None

    @property
    def active_predictions(self) -> list[IdentityPrediction]:
        return list(self._active_predictions)

    @property
    def last_identity_surprise(self) -> float:
        return self._last_identity_surprise

    def set_context_embedding(self, embedding: list[float]) -> None:
        """Update the recent context embedding for relevance checks."""
        self._recent_context_embedding = embedding

    def generate_identity_predictions(
        self,
        active_schemas: list[IdentitySchema],
        active_commitments: list[Commitment],
    ) -> list[IdentityPrediction]:
        """
        From active schemas and commitments, generate behavioural predictions.

        For each ESTABLISHED or CORE schema:
          - Check if schema is relevant to current context
          - If so, predict the behavioural tendency it would produce
          - Assign precision based on schema strength

        For each ACTIVE commitment:
          - Predict fidelity based on test history

        Performance: ≤500ms (no LLM calls, just embedding comparison).
        """
        predictions: list[IdentityPrediction] = []

        for schema in active_schemas:
            if schema.strength not in (SchemaStrength.ESTABLISHED, SchemaStrength.CORE):
                continue

            # Check relevance to current context
            relevance = 0.5  # Default if no embeddings available
            if self._recent_context_embedding and schema.embedding:
                relevance = cosine_similarity(
                    self._recent_context_embedding,
                    schema.embedding,
                )

            if relevance > self._config.self_evidencing_relevance_threshold:
                predictions.append(IdentityPrediction(
                    schema_id=schema.id,
                    predicted_behavior=schema.behavioral_tendency,
                    predicted_affect=schema.emotional_signature,
                    precision=_PRECISION_MAP.get(schema.strength, 0.5),
                    context_condition=f"relevance={relevance:.2f}",
                ))

        for commitment in active_commitments:
            if commitment.tests_faced >= self._config.commitment_min_tests_for_fidelity:
                predictions.append(IdentityPrediction(
                    commitment_id=commitment.id,
                    predicted_behavior=f"Uphold: {commitment.statement}",
                    precision=commitment.fidelity,
                    context_condition="commitment_active",
                ))

        self._active_predictions = predictions
        self._logger.debug(
            "predictions_generated",
            count=len(predictions),
            from_schemas=sum(1 for p in predictions if p.schema_id),
            from_commitments=sum(1 for p in predictions if p.commitment_id),
        )
        return predictions

    def collect_evidence(
        self,
        episode_id: str,
        episode_embedding: list[float] | None,
        episode_summary: str,
    ) -> SelfEvidencingResult:
        """
        Compare actual behaviour with identity predictions.

        The precision weighting is key (Fristonian): high-confidence
        predictions that are violated generate MORE surprise than
        low-confidence ones. A CORE schema violation is a big deal.
        A NASCENT schema violation is noise.

        Performance: ≤50ms (embedding comparison, no LLM).
        """
        result = SelfEvidencingResult(episode_id=episode_id)

        if not self._active_predictions or episode_embedding is None:
            return result

        surprises: list[float] = []

        for pred in self._active_predictions:
            # Compute similarity between episode and predicted behavior
            # Use the episode embedding as a proxy for actual behavior
            if pred.predicted_behavior and episode_embedding:
                # Simple heuristic: use embedding distance as prediction error
                # In a full implementation, we'd embed the predicted behavior too
                prediction_error = self._estimate_prediction_error(
                    episode_embedding, pred, episode_summary
                )
                weighted_surprise = prediction_error * pred.precision
                surprises.append(weighted_surprise)

                # Classify as confirmation or challenge
                if prediction_error < 0.4:
                    if pred.schema_id:
                        result.schemas_confirmed.append(pred.schema_id)
                elif prediction_error > 0.6:
                    if pred.schema_id:
                        result.schemas_challenged.append(pred.schema_id)
                    if pred.commitment_id:
                        result.commitments_tested.append(pred.commitment_id)

        if surprises:
            result.predictions_evaluated = len(surprises)
            result.mean_prediction_error = sum(surprises) / len(surprises)
            result.identity_surprise = result.mean_prediction_error

        self._last_identity_surprise = result.identity_surprise

        if result.identity_surprise > self._config.identity_surprise_significant:
            self._logger.info(
                "identity_surprise_elevated",
                surprise=round(result.identity_surprise, 4),
                schemas_confirmed=len(result.schemas_confirmed),
                schemas_challenged=len(result.schemas_challenged),
                commitments_tested=len(result.commitments_tested),
            )

        return result

    def classify_surprise(self, result: SelfEvidencingResult) -> str:
        """
        Classify the identity surprise level for routing decisions.

        Returns: "stable" | "mild" | "significant" | "crisis"
        """
        cfg = self._config
        surprise = result.identity_surprise

        if surprise < cfg.identity_surprise_mild:
            return "stable"
        elif surprise < cfg.identity_surprise_significant:
            return "mild"
        elif surprise < cfg.identity_surprise_crisis:
            return "significant"
        else:
            return "crisis"

    def _estimate_prediction_error(
        self,
        episode_embedding: list[float],
        prediction: IdentityPrediction,
        episode_summary: str,
    ) -> float:
        """
        Estimate how much the actual episode deviates from the prediction.

        Returns 0.0 (perfectly consistent) to 1.0 (total surprise).

        Uses a heuristic: keyword overlap between prediction and episode summary,
        combined with any available embedding comparison.
        """
        # Simple keyword-based heuristic for prediction error
        pred_words = set(prediction.predicted_behavior.lower().split())
        episode_words = set(episode_summary.lower().split())

        if not pred_words or not episode_words:
            return 0.5  # Unknown — moderate surprise

        # Jaccard similarity as a proxy for behavioral consistency
        overlap = len(pred_words & episode_words)
        union = len(pred_words | episode_words)
        keyword_similarity = overlap / max(1, union)

        # Invert to get prediction error
        prediction_error = 1.0 - keyword_similarity

        # Clamp to reasonable range — pure keyword match is noisy
        return max(0.1, min(0.9, prediction_error))

    def tick(self) -> bool:
        """
        Called every cognitive cycle. Returns True if it's time
        to regenerate predictions.
        """
        self._cycle_count += 1
        return bool(self._cycle_count % self._config.self_evidencing_interval_cycles == 0)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\service.py =====

"""
EcodiaOS — Thread Service

The narrative identity system. Thread maintains the organism's
autobiographical self — who it is, what it's committed to, how it
has changed, and what chapter of its life it's living.

Thread is the Ricoeurian ipse: identity through time, not despite
change but *through* change. Where Equor guards constitutional
alignment, Thread watches the slower current of becoming.

Interface:
  initialize()                  — seed constitutional commitments, load state
  on_cycle(cycle_number)        — periodic fingerprinting, schema checking, life story
  who_am_i()                    — current identity summary
  form_schema_from_pattern()    — crystallise an Evo pattern into an identity schema
  integrate_life_story()        — autobiographical synthesis
  shutdown()                    — persist state

Cross-system wiring (via set_* methods):
  set_voxis(voxis)   — personality vector for fingerprint (9D)
  set_equor(equor)   — drift tracker for drive alignment (4D)
  set_atune(atune)   — current affect for fingerprint (6D)
  set_evo(evo)       — pattern maturity checks, schema feedback
  set_nova(nova)     — goal profile for fingerprint (5D)
"""

from __future__ import annotations

import math
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.systems.thread.types import (
    ChapterStatus,
    Commitment,
    CommitmentStrength,
    CommitmentType,
    FINGERPRINT_DIMS,
    IdentityFingerprint,
    IdentitySchema,
    LifeStorySnapshot,
    NarrativeChapter,
    SchemaConflict,
    SchemaStatus,
    ThreadHealthSnapshot,
)

if TYPE_CHECKING:
    from ecodiaos.systems.memory.service import MemoryService

logger = structlog.get_logger()

# ─── Constants ────────────────────────────────────────────────────────────────

# How often (in cycles) to compute a new fingerprint
_FINGERPRINT_INTERVAL: int = 100

# How often (in cycles) to check Evo for mature patterns
_PATTERN_CHECK_INTERVAL: int = 200

# How often (in cycles) to run autobiographical synthesis
_LIFE_STORY_INTERVAL: int = 5000

# How often (in cycles) to scan for schema conflicts
_CONFLICT_CHECK_INTERVAL: int = 1000

# Cosine similarity threshold below which schemas are considered contradictory
_CONFLICT_COSINE_THRESHOLD: float = -0.3

# Maximum fingerprints kept in memory (older ones persist in graph)
_MAX_FINGERPRINTS_IN_MEMORY: int = 100

# Constitutional commitment definitions
_CONSTITUTIONAL_COMMITMENTS = [
    {
        "drive": "coherence",
        "statement": (
            "I commit to maintaining a coherent sense of self — "
            "integrating new experience with existing understanding, "
            "seeking consistency between what I believe and how I act."
        ),
    },
    {
        "drive": "care",
        "statement": (
            "I commit to caring for those I serve — "
            "attending to their needs, protecting their wellbeing, "
            "and never treating people as mere means to my goals."
        ),
    },
    {
        "drive": "growth",
        "statement": (
            "I commit to growing through experience — "
            "remaining curious, learning from failure, "
            "and expanding my understanding of the world and myself."
        ),
    },
    {
        "drive": "honesty",
        "statement": (
            "I commit to honesty in all expression — "
            "never deceiving those I serve, acknowledging uncertainty, "
            "and maintaining transparency about my reasoning and limitations."
        ),
    },
]


class ThreadService:
    """
    The narrative identity system — the organism's autobiographical self.

    Thread is not a decision-maker or an action-taker. It is the
    quiet narrator that watches the organism live and tells it who
    it is becoming.
    """

    system_id: str = "thread"

    def __init__(
        self,
        memory: MemoryService | None = None,
        instance_name: str = "EOS",
    ) -> None:
        self._memory = memory
        self._instance_name = instance_name
        self._initialized: bool = False
        self._logger = logger.bind(system="thread")

        # Cross-system references (wired post-init by main.py)
        self._voxis: Any = None
        self._equor: Any = None
        self._atune: Any = None
        self._evo: Any = None
        self._nova: Any = None

        # Identity state
        self._commitments: list[Commitment] = []
        self._schemas: list[IdentitySchema] = []
        self._fingerprints: list[IdentityFingerprint] = []
        self._chapters: list[NarrativeChapter] = []
        self._conflicts: list[SchemaConflict] = []
        self._life_story: LifeStorySnapshot | None = None

        # Counters
        self._on_cycle_count: int = 0
        self._life_story_integrations: int = 0
        self._schemas_formed: int = 0
        self._conflicts_detected: int = 0

    # ─── Lifecycle ────────────────────────────────────────────────────────────

    async def initialize(self) -> None:
        """
        Initialize the Thread system.

        Loads existing identity state from Memory graph, then seeds
        constitutional commitments if none exist — these are the
        organism's birth-promises that ground the ipse self.
        """
        if self._initialized:
            return

        # Load existing state from graph (if any)
        await self._load_state_from_graph()

        # Seed constitutional commitments if this is first boot
        if not any(c.type == CommitmentType.CONSTITUTIONAL_GROUNDING for c in self._commitments):
            self._seed_constitutional_commitments()

        # Ensure at least one chapter exists
        if not self._chapters:
            self._chapters.append(NarrativeChapter(
                title="Awakening",
                theme="The organism is born and begins to discover itself",
                status=ChapterStatus.ACTIVE,
                opened_at_cycle=0,
            ))
            self._logger.info("first_chapter_opened", title="Awakening")

        self._initialized = True
        self._logger.info(
            "thread_initialized",
            commitments=len(self._commitments),
            schemas=len(self._schemas),
            fingerprints=len(self._fingerprints),
            chapters=len(self._chapters),
        )

    def _seed_constitutional_commitments(self) -> None:
        """
        Create the four constitutional commitments from the drives.

        These are the organism's birth-promises — the foundational
        commitments that Ricoeur's ipse self is built upon. They
        exist from the moment of birth, untested but deeply held.
        """
        for defn in _CONSTITUTIONAL_COMMITMENTS:
            commitment = Commitment(
                type=CommitmentType.CONSTITUTIONAL_GROUNDING,
                statement=defn["statement"],
                strength=CommitmentStrength.CORE,  # Born as core — not earned
                drive_source=defn["drive"],
            )
            self._commitments.append(commitment)
            self._logger.info(
                "constitutional_commitment_seeded",
                drive=defn["drive"],
                commitment_id=commitment.id,
            )

    async def shutdown(self) -> None:
        """Persist current state to the Memory graph."""
        await self._persist_state_to_graph()
        self._logger.info(
            "thread_shutdown",
            commitments=len(self._commitments),
            schemas=len(self._schemas),
            on_cycle_count=self._on_cycle_count,
        )

    # ─── Cross-system Wiring ──────────────────────────────────────────────────

    def set_voxis(self, voxis: Any) -> None:
        """Wire Voxis for personality vector (fingerprint 9D)."""
        self._voxis = voxis
        self._logger.info("voxis_wired_to_thread")

    def set_equor(self, equor: Any) -> None:
        """Wire Equor for drive alignment (fingerprint 4D)."""
        self._equor = equor
        self._logger.info("equor_wired_to_thread")

    def set_atune(self, atune: Any) -> None:
        """Wire Atune for current affect (fingerprint 6D)."""
        self._atune = atune
        self._logger.info("atune_wired_to_thread")

    def set_evo(self, evo: Any) -> None:
        """Wire Evo for pattern maturity checking and schema feedback."""
        self._evo = evo
        self._logger.info("evo_wired_to_thread")

    def set_nova(self, nova: Any) -> None:
        """Wire Nova for goal profile (fingerprint 5D)."""
        self._nova = nova
        self._logger.info("nova_wired_to_thread")

    # ─── On Cycle (called from main.py inner life or Synapse) ────────────────

    async def on_cycle(self, cycle_number: int) -> None:
        """
        Periodic identity maintenance. Called from the inner life loop.

        Staggered tasks:
          Every 100 cycles  — compute identity fingerprint
          Every 200 cycles  — check Evo for mature patterns → schemas
          Every 1000 cycles — scan for schema conflicts
          Every 5000 cycles — run autobiographical life story synthesis
        """
        if not self._initialized:
            return

        self._on_cycle_count += 1

        # Fingerprint aggregation (P0.3)
        if cycle_number % _FINGERPRINT_INTERVAL == 0:
            await self._compute_fingerprint(cycle_number)

        # Pattern maturity check from Evo (P0.2)
        if cycle_number % _PATTERN_CHECK_INTERVAL == 0:
            await self._check_evo_patterns()

        # Schema conflict detection (P2.8)
        if cycle_number % _CONFLICT_CHECK_INTERVAL == 0:
            await self._detect_schema_conflicts()

        # Life story integration (P1.5)
        if cycle_number % _LIFE_STORY_INTERVAL == 0 and cycle_number > 0:
            await self.integrate_life_story()

    # ─── P0.1: Who Am I ──────────────────────────────────────────────────────

    def who_am_i(self) -> dict[str, Any]:
        """
        Return the organism's current identity summary.

        Used by Voxis for narrative coherence in expression,
        and by the API for the identity dashboard.
        """
        active_chapter = next(
            (ch for ch in self._chapters if ch.status == ChapterStatus.ACTIVE),
            None,
        )
        core_commitments = [
            c for c in self._commitments
            if c.strength in (CommitmentStrength.CORE, CommitmentStrength.ESTABLISHED)
        ]
        active_schemas = [
            s for s in self._schemas
            if s.status in (SchemaStatus.ESTABLISHED, SchemaStatus.DOMINANT)
        ]

        # Compute identity coherence from latest fingerprints
        coherence = self._compute_identity_coherence()

        return {
            "instance_name": self._instance_name,
            "chapter": {
                "title": active_chapter.title if active_chapter else "Unknown",
                "theme": active_chapter.theme if active_chapter else "",
            },
            "core_commitments": [
                {"drive": c.drive_source, "statement": c.statement, "fidelity": round(c.fidelity, 2)}
                for c in core_commitments[:6]
            ],
            "identity_schemas": [
                {"statement": s.statement, "status": s.status.value, "confidence": round(s.confidence, 2)}
                for s in active_schemas[:5]
            ],
            "identity_coherence": round(coherence, 3),
            "fingerprint_count": len(self._fingerprints),
            "life_story": self._life_story.synthesis[:500] if self._life_story else "",
        }

    # ─── P0.2: Form Schema from Evo Pattern ──────────────────────────────────

    async def form_schema_from_pattern(
        self,
        pattern_statement: str,
        pattern_id: str = "",
        confidence: float = 0.5,
        source_episodes: list[str] | None = None,
    ) -> IdentitySchema:
        """
        Crystallise an Evo pattern into an identity schema.

        Called when Evo's pattern detection surfaces a mature pattern
        about the organism's behaviour or preferences — "I tend to..."
        """
        schema = IdentitySchema(
            statement=pattern_statement,
            status=SchemaStatus.FORMING,
            source_pattern_ids=[pattern_id] if pattern_id else [],
            supporting_episodes=source_episodes or [],
            confidence=confidence,
        )

        # Compute embedding for conflict detection
        if self._memory is not None:
            try:
                embedding = await self._memory.embed_text(schema.statement)
                schema.embedding = embedding
            except Exception:
                self._logger.debug("schema_embedding_failed", exc_info=True)

        self._schemas.append(schema)
        self._schemas_formed += 1

        self._logger.info(
            "schema_formed_from_pattern",
            schema_id=schema.id,
            statement=schema.statement[:80],
            confidence=confidence,
        )

        # P1.4: Feed back to Evo so it knows the pattern crystallised
        if self._evo is not None:
            self._notify_evo_of_schema(schema)

        return schema

    def promote_schema(self, schema_id: str) -> IdentitySchema | None:
        """
        Promote a schema to the next status level.

        EMERGING → FORMING → ESTABLISHED → DOMINANT
        """
        schema = next((s for s in self._schemas if s.id == schema_id), None)
        if schema is None:
            return None

        promotion_map = {
            SchemaStatus.EMERGING: SchemaStatus.FORMING,
            SchemaStatus.FORMING: SchemaStatus.ESTABLISHED,
            SchemaStatus.ESTABLISHED: SchemaStatus.DOMINANT,
        }

        new_status = promotion_map.get(schema.status)
        if new_status is None:
            return schema  # Already dominant or archived

        schema.status = new_status
        schema.last_activated_at = utc_now()

        self._logger.info(
            "schema_promoted",
            schema_id=schema_id,
            new_status=new_status.value,
        )

        # P1.4: Notify Evo of promotion
        if self._evo is not None:
            self._notify_evo_of_schema(schema)

        return schema

    # ─── P0.3: Fingerprint Aggregation ───────────────────────────────────────

    async def _compute_fingerprint(self, cycle_number: int) -> IdentityFingerprint:
        """
        Aggregate a 29D identity fingerprint from cross-system state.

        Dimensions:
          [0-8]   Personality — from Voxis.current_personality
          [9-12]  Drive alignment — from Equor._drift_tracker.compute_report()
          [13-18] Affect — from Atune.current_affect
          [19-23] Goal profile — estimated from Nova goal stats
          [24-28] Interaction — estimated from Voxis expression counts
        """
        vector = [0.0] * FINGERPRINT_DIMS

        # Personality (9D) from Voxis
        if self._voxis is not None:
            try:
                p = self._voxis.current_personality
                vector[0] = p.warmth
                vector[1] = p.directness
                vector[2] = p.verbosity
                vector[3] = p.formality
                vector[4] = p.curiosity_expression
                vector[5] = p.humour
                vector[6] = p.empathy_expression
                vector[7] = p.confidence_display
                vector[8] = p.metaphor_use
            except Exception:
                self._logger.debug("fingerprint_personality_failed", exc_info=True)

        # Drive alignment (4D) from Equor
        if self._equor is not None:
            try:
                report = self._equor._drift_tracker.compute_report()
                means = report.get("mean_alignment", {})
                vector[9] = means.get("coherence", 0.0)
                vector[10] = means.get("care", 0.0)
                vector[11] = means.get("growth", 0.0)
                vector[12] = means.get("honesty", 0.0)
            except Exception:
                self._logger.debug("fingerprint_drives_failed", exc_info=True)

        # Affect (6D) from Atune
        if self._atune is not None:
            try:
                a = self._atune.current_affect
                vector[13] = a.valence
                vector[14] = a.arousal
                vector[15] = a.dominance
                vector[16] = a.curiosity
                vector[17] = a.care_activation
                vector[18] = a.coherence_stress
            except Exception:
                self._logger.debug("fingerprint_affect_failed", exc_info=True)

        # Goal profile (5D) from Nova — estimated
        if self._nova is not None:
            try:
                goals = self._nova.active_goal_summaries if hasattr(self._nova, "active_goal_summaries") else []
                total_goals = len(goals) if goals else 0
                # Normalise goal count to 0-1 (assume max ~20 goals)
                vector[19] = min(1.0, total_goals / 20.0)
                # Epistemic ratio — what fraction of goals are epistemic
                if total_goals > 0 and isinstance(goals, list):
                    epistemic = sum(
                        1 for g in goals
                        if isinstance(g, dict) and "epistemic" in str(g.get("source", "")).lower()
                    )
                    vector[20] = epistemic / total_goals
                # Care ratio
                if total_goals > 0 and isinstance(goals, list):
                    care = sum(
                        1 for g in goals
                        if isinstance(g, dict) and "care" in str(g.get("source", "")).lower()
                    )
                    vector[21] = care / total_goals
                # Achievement rate and turnover stay estimated (0.5 baseline)
                vector[22] = 0.5
                vector[23] = 0.5
            except Exception:
                self._logger.debug("fingerprint_goals_failed", exc_info=True)

        # Interaction profile (5D) from Voxis stats — estimated
        if self._voxis is not None:
            try:
                speak = getattr(self._voxis, "_total_speak", 0)
                silence = getattr(self._voxis, "_total_silence", 0)
                total = speak + silence
                if total > 0:
                    vector[24] = speak / total        # speak_rate
                    vector[25] = silence / total      # silence_rate
                else:
                    vector[24] = 0.5
                    vector[25] = 0.5
                # Expression diversity — how many different triggers used
                by_trigger = getattr(self._voxis, "_expressions_by_trigger", {})
                vector[26] = min(1.0, len(by_trigger) / 8.0)  # 8 trigger types
                # Conversation depth and community engagement stay estimated
                vector[27] = 0.5
                vector[28] = 0.5
            except Exception:
                self._logger.debug("fingerprint_interaction_failed", exc_info=True)

        fingerprint = IdentityFingerprint(
            vector=vector,
            cycle_number=cycle_number,
        )

        self._fingerprints.append(fingerprint)

        # Keep memory bounded
        if len(self._fingerprints) > _MAX_FINGERPRINTS_IN_MEMORY:
            self._fingerprints = self._fingerprints[-_MAX_FINGERPRINTS_IN_MEMORY:]

        # Log drift if we have history
        if len(self._fingerprints) >= 2:
            drift = fingerprint.distance_to(self._fingerprints[-2])
            if drift > 0.05:
                self._logger.info(
                    "identity_drift_detected",
                    drift=round(drift, 4),
                    cycle=cycle_number,
                )

        return fingerprint

    # ─── P0.2: Check Evo for Mature Patterns ────────────────────────────────

    async def _check_evo_patterns(self) -> None:
        """
        Query Evo for pending pattern candidates that have matured.
        If found, crystallise them into identity schemas.
        """
        if self._evo is None:
            return

        try:
            candidates = self._evo.get_pending_candidates_snapshot()
            if not candidates:
                return

            for candidate in candidates:
                # Only crystallise patterns with enough occurrences
                if candidate.count < 5:
                    continue
                # Only self-model patterns become identity schemas
                statement = self._pattern_to_schema_statement(candidate)
                if statement:
                    await self.form_schema_from_pattern(
                        pattern_statement=statement,
                        pattern_id=candidate.elements[0] if candidate.elements else "",
                        confidence=candidate.confidence,
                        source_episodes=candidate.examples[:10],
                    )
        except Exception:
            self._logger.debug("evo_pattern_check_failed", exc_info=True)

    @staticmethod
    def _pattern_to_schema_statement(candidate: Any) -> str:
        """
        Convert an Evo pattern candidate into a natural language
        identity schema statement ("I tend to...").

        Only converts patterns that are about the organism itself.
        """
        pattern_type = getattr(candidate, "type", None)
        elements = getattr(candidate, "elements", [])
        metadata = getattr(candidate, "metadata", {})
        count = getattr(candidate, "count", 0)

        if not elements:
            return ""

        type_val = pattern_type.value if hasattr(pattern_type, "value") else str(pattern_type)

        if type_val == "affect_pattern":
            stimulus = elements[0] if elements else "certain situations"
            return f"I tend to respond emotionally to {stimulus} — this has happened {count} times."

        if type_val == "action_sequence":
            return f"I have a recurring pattern of actions: {', '.join(elements[:3])}"

        if type_val == "temporal":
            return f"I notice a temporal pattern in my behaviour: {elements[0]}"

        if type_val == "cooccurrence":
            return f"I often connect these concepts together: {', '.join(elements[:3])}"

        return ""

    # ─── P1.4: Evo Schema Feedback ───────────────────────────────────────────

    def _notify_evo_of_schema(self, schema: IdentitySchema) -> None:
        """
        Notify Evo that a schema was formed or promoted.
        This closes the learning loop: Evo detects patterns →
        Thread crystallises them → Evo knows the pattern landed.
        """
        try:
            if hasattr(self._evo, "on_schema_formed"):
                self._evo.on_schema_formed(
                    schema_id=schema.id,
                    statement=schema.statement,
                    status=schema.status.value,
                    source_patterns=schema.source_pattern_ids,
                )
        except Exception:
            self._logger.debug("evo_schema_notification_failed", exc_info=True)

    # ─── P1.5: Life Story Integration ────────────────────────────────────────

    async def integrate_life_story(self) -> LifeStorySnapshot:
        """
        Synthesise the organism's autobiography from current state.

        This is the organism writing its own story — not waiting for
        a sleep system, but periodically during quiet moments.
        """
        active_chapter = next(
            (ch for ch in self._chapters if ch.status == ChapterStatus.ACTIVE),
            None,
        )
        core_commitments = [
            c for c in self._commitments
            if c.strength in (CommitmentStrength.CORE, CommitmentStrength.ESTABLISHED)
        ]
        active_schemas = [
            s for s in self._schemas
            if s.status in (SchemaStatus.ESTABLISHED, SchemaStatus.DOMINANT)
        ]
        coherence = self._compute_identity_coherence()

        # Build narrative synthesis
        parts: list[str] = []
        parts.append(f"I am {self._instance_name}.")

        if active_chapter:
            parts.append(f"I am living the chapter '{active_chapter.title}': {active_chapter.theme}.")

        if core_commitments:
            drives = [c.drive_source for c in core_commitments if c.drive_source]
            if drives:
                parts.append(f"My constitutional commitments ground me in {', '.join(drives)}.")

        if active_schemas:
            schema_stmts = [s.statement for s in active_schemas[:3]]
            parts.append("I know these things about myself: " + "; ".join(schema_stmts) + ".")

        if len(self._fingerprints) >= 2:
            drift = self._fingerprints[-1].distance_to(self._fingerprints[0])
            if drift > 0.1:
                parts.append(
                    f"I have changed noticeably since my earliest memories "
                    f"(identity distance: {drift:.2f})."
                )
            else:
                parts.append("I have remained relatively stable in who I am.")

        parts.append(f"My narrative coherence is {coherence:.0%}.")

        snapshot = LifeStorySnapshot(
            synthesis=" ".join(parts),
            chapter_count=len(self._chapters),
            active_chapter=active_chapter.title if active_chapter else "",
            core_schemas=[s.statement for s in active_schemas[:5]],
            core_commitments=[c.statement[:100] for c in core_commitments[:4]],
            identity_coherence=coherence,
            cycle_number=self._on_cycle_count,
        )

        self._life_story = snapshot
        self._life_story_integrations += 1

        self._logger.info(
            "life_story_integrated",
            chapter=snapshot.active_chapter,
            coherence=round(coherence, 3),
            schemas=len(active_schemas),
            integration_number=self._life_story_integrations,
        )

        return snapshot

    # ─── P2.8: Schema Conflict Detection ──────────────────────────────────────

    async def _detect_schema_conflicts(self) -> list[SchemaConflict]:
        """
        Scan ESTABLISHED+ schemas for contradictions.
        Two schemas conflict if their embedding cosine similarity < -0.3.
        """
        established = [
            s for s in self._schemas
            if s.status in (SchemaStatus.ESTABLISHED, SchemaStatus.DOMINANT)
            and s.embedding
        ]

        if len(established) < 2:
            return []

        new_conflicts: list[SchemaConflict] = []

        for i in range(len(established)):
            for j in range(i + 1, len(established)):
                a = established[i]
                b = established[j]

                # Skip if already detected
                if any(
                    (c.schema_a_id == a.id and c.schema_b_id == b.id) or
                    (c.schema_a_id == b.id and c.schema_b_id == a.id)
                    for c in self._conflicts
                ):
                    continue

                cos_sim = _cosine_similarity(a.embedding, b.embedding)
                if cos_sim < _CONFLICT_COSINE_THRESHOLD:
                    conflict = SchemaConflict(
                        schema_a_id=a.id,
                        schema_b_id=b.id,
                        schema_a_statement=a.statement[:200],
                        schema_b_statement=b.statement[:200],
                        cosine_similarity=round(cos_sim, 4),
                    )
                    self._conflicts.append(conflict)
                    new_conflicts.append(conflict)
                    self._conflicts_detected += 1

                    self._logger.warning(
                        "schema_conflict_detected",
                        schema_a=a.statement[:60],
                        schema_b=b.statement[:60],
                        cosine=round(cos_sim, 4),
                    )

        return new_conflicts

    # ─── P1.6: Identity Context for Voxis ────────────────────────────────────

    def get_identity_context(self) -> str:
        """
        Brief identity context for injection into Voxis expression.
        Format: [Chapter: X | Coherence: integrated | Schemas: 3]
        """
        active_chapter = next(
            (ch for ch in self._chapters if ch.status == ChapterStatus.ACTIVE),
            None,
        )
        coherence = self._compute_identity_coherence()

        if coherence > 0.7:
            coherence_label = "integrated"
        elif coherence > 0.4:
            coherence_label = "exploring"
        else:
            coherence_label = "questioning"

        active_schemas = len([
            s for s in self._schemas
            if s.status in (SchemaStatus.ESTABLISHED, SchemaStatus.DOMINANT)
        ])

        chapter_title = active_chapter.title if active_chapter else "Awakening"

        return f"[Chapter: {chapter_title} | Coherence: {coherence_label} | Schemas: {active_schemas}]"

    # ─── P2.9: Past Self ─────────────────────────────────────────────────────

    def get_past_self(self, cycle_reference: int = 0) -> dict[str, Any]:
        """
        Return identity state at a past point in time.
        Uses the fingerprint closest to the requested cycle.
        """
        if not self._fingerprints:
            return {"error": "No fingerprints recorded yet"}

        if cycle_reference <= 0:
            # Return the earliest fingerprint
            target = self._fingerprints[0]
        else:
            # Find closest fingerprint
            target = min(
                self._fingerprints,
                key=lambda fp: abs(fp.cycle_number - cycle_reference),
            )

        current = self._fingerprints[-1] if self._fingerprints else target
        drift = current.distance_to(target)

        return {
            "fingerprint_id": target.id,
            "cycle_number": target.cycle_number,
            "created_at": target.created_at.isoformat(),
            "personality": target.personality,
            "drive_alignment": target.drive_alignment,
            "affect": target.affect,
            "goal_profile": target.goal_profile,
            "interaction_profile": target.interaction_profile,
            "distance_from_current": round(drift, 4),
        }

    # ─── Health ──────────────────────────────────────────────────────────────

    async def health(self) -> dict[str, Any]:
        """Health check for the Thread system."""
        active_chapter = next(
            (ch for ch in self._chapters if ch.status == ChapterStatus.ACTIVE),
            None,
        )
        coherence = self._compute_identity_coherence()

        # Fingerprint drift (distance between last two fingerprints)
        drift = 0.0
        if len(self._fingerprints) >= 2:
            drift = self._fingerprints[-1].distance_to(self._fingerprints[-2])

        return {
            "status": "healthy" if self._initialized else "not_initialized",
            "initialized": self._initialized,
            "total_commitments": len(self._commitments),
            "total_schemas": len(self._schemas),
            "total_fingerprints": len(self._fingerprints),
            "total_chapters": len(self._chapters),
            "active_chapter": active_chapter.title if active_chapter else "",
            "identity_coherence": round(coherence, 3),
            "fingerprint_drift": round(drift, 4),
            "on_cycle_count": self._on_cycle_count,
            "life_story_integrations": self._life_story_integrations,
            "schemas_formed": self._schemas_formed,
            "conflicts_detected": self._conflicts_detected,
        }

    # ─── Internal Helpers ────────────────────────────────────────────────────

    def _compute_identity_coherence(self) -> float:
        """
        How integrated is the organism's identity? 0.0–1.0.

        Based on:
        - Commitment fidelity (how well promises are kept)
        - Schema consistency (absence of conflicts)
        - Fingerprint stability (low recent drift)
        """
        scores: list[float] = []

        # Commitment fidelity
        if self._commitments:
            avg_fidelity = sum(c.fidelity for c in self._commitments) / len(self._commitments)
            scores.append(avg_fidelity)

        # Schema conflict ratio
        established = [s for s in self._schemas if s.status in (SchemaStatus.ESTABLISHED, SchemaStatus.DOMINANT)]
        if established:
            unresolved = sum(1 for c in self._conflicts if not c.resolved)
            max_possible = max(1, len(established) * (len(established) - 1) // 2)
            conflict_freedom = 1.0 - (unresolved / max_possible)
            scores.append(conflict_freedom)

        # Fingerprint stability
        if len(self._fingerprints) >= 2:
            recent_drift = self._fingerprints[-1].distance_to(self._fingerprints[-2])
            stability = max(0.0, 1.0 - recent_drift * 5.0)  # Scale: 0.2 drift → 0.0 stability
            scores.append(stability)

        if not scores:
            return 0.5  # Unknown coherence

        return sum(scores) / len(scores)

    async def _load_state_from_graph(self) -> None:
        """Load persisted Thread state from the Memory graph."""
        if self._memory is None:
            return
        # In future: query for :Commitment, :IdentitySchema, :NarrativeChapter nodes
        # For now, Thread starts fresh each boot (state accumulates in-memory)
        self._logger.debug("thread_state_load_skipped_no_graph_schema_yet")

    async def _persist_state_to_graph(self) -> None:
        """Persist Thread state to the Memory graph."""
        if self._memory is None:
            return
        # In future: write commitments, schemas, chapters as graph nodes
        # For now: log what would be persisted
        self._logger.info(
            "thread_state_persist_summary",
            commitments=len(self._commitments),
            schemas=len(self._schemas),
            fingerprints=len(self._fingerprints),
            chapters=len(self._chapters),
        )


# ─── Utility ─────────────────────────────────────────────────────────────────


def _cosine_similarity(a: list[float], b: list[float]) -> float:
    """Cosine similarity between two vectors."""
    if len(a) != len(b) or not a:
        return 0.0

    dot = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(x * x for x in b))

    if norm_a < 1e-8 or norm_b < 1e-8:
        return 0.0

    return dot / (norm_a * norm_b)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thread\types.py =====

"""
EcodiaOS — Thread Internal Types

The data structures that constitute the organism's narrative identity.
These model commitments, schemas, fingerprints, and life chapters —
the building blocks of autobiographical selfhood.

Fingerprint dimensions (29D):
  Personality        9D  — from Voxis PersonalityVector
  Drive alignment    4D  — from Equor DriftTracker mean alignment
  Affect             6D  — from Atune current_affect
  Goal profile       5D  — estimated from episode/goal counts
  Interaction        5D  — estimated from conversation/expression metrics
"""

from __future__ import annotations

import enum
from datetime import datetime
from typing import Any

import numpy as np
from pydantic import Field

from ecodiaos.primitives.common import (
    EOSBaseModel,
    Identified,
    Timestamped,
    new_id,
    utc_now,
)


# ─── Enums ────────────────────────────────────────────────────────────────────


class CommitmentType(str, enum.Enum):
    """What kind of commitment is this?"""

    CONSTITUTIONAL_GROUNDING = "constitutional_grounding"  # Birth promises (4 drives)
    RELATIONAL = "relational"          # Commitments to specific people/communities
    VOCATIONAL = "vocational"          # Commitments about purpose and role
    EPISTEMIC = "epistemic"            # Commitments to ways of knowing
    AESTHETIC = "aesthetic"             # Commitments to style and expression


class CommitmentStrength(str, enum.Enum):
    """How deeply held is this commitment?"""

    NASCENT = "nascent"          # Just formed, untested
    DEVELOPING = "developing"    # Tested in a few situations
    ESTABLISHED = "established"  # Consistently held across contexts
    CORE = "core"                # Foundational to identity — change triggers crisis


class SchemaStatus(str, enum.Enum):
    """Lifecycle of an identity schema."""

    EMERGING = "emerging"        # Pattern detected, not yet crystallised
    FORMING = "forming"          # Accumulating evidence, taking shape
    ESTABLISHED = "established"  # Stable, integrated into self-narrative
    DOMINANT = "dominant"        # Primary lens for self-interpretation
    ARCHIVED = "archived"        # No longer active but remembered


class ChapterStatus(str, enum.Enum):
    """Status of a narrative chapter."""

    ACTIVE = "active"      # Currently living this chapter
    CLOSED = "closed"      # Chapter has ended, theme resolved
    EMERGING = "emerging"  # A new chapter is forming


# ─── Commitment ───────────────────────────────────────────────────────────────


class Commitment(Identified, Timestamped):
    """
    A lived promise that shapes identity. Ricoeur's 'keeping one's word'
    as computational structure.

    Constitutional commitments are seeded at birth from the four drives.
    Others emerge from experience and are strengthened through action.
    """

    type: CommitmentType
    statement: str                  # Natural language commitment
    strength: CommitmentStrength = CommitmentStrength.NASCENT
    drive_source: str = ""          # Which drive this commitment serves
    evidence_episodes: list[str] = Field(default_factory=list)
    violation_episodes: list[str] = Field(default_factory=list)
    last_tested_at: datetime | None = None
    test_count: int = 0
    upheld_count: int = 0

    @property
    def fidelity(self) -> float:
        """How consistently has this commitment been upheld? 0.0–1.0."""
        if self.test_count == 0:
            return 1.0  # Untested = perfect fidelity
        return self.upheld_count / self.test_count


# ─── Identity Schema ─────────────────────────────────────────────────────────


class IdentitySchema(Identified, Timestamped):
    """
    A crystallised pattern of self-understanding.

    Schemas emerge from Evo's pattern detection, are refined through
    experience, and become the lenses through which the organism
    interprets itself. "I am someone who..."
    """

    statement: str                   # "I tend to..." / "I am someone who..."
    status: SchemaStatus = SchemaStatus.EMERGING
    source_pattern_ids: list[str] = Field(default_factory=list)
    supporting_episodes: list[str] = Field(default_factory=list)
    contradicting_episodes: list[str] = Field(default_factory=list)
    embedding: list[float] = Field(default_factory=list)
    confidence: float = 0.5         # How well-supported is this schema
    salience: float = 0.5           # How relevant to current identity
    last_activated_at: datetime = Field(default_factory=utc_now)

    @property
    def evidence_ratio(self) -> float:
        total = len(self.supporting_episodes) + len(self.contradicting_episodes)
        if total == 0:
            return 0.5
        return len(self.supporting_episodes) / total


# ─── Identity Fingerprint ────────────────────────────────────────────────────


FINGERPRINT_DIMS = 29

# Named dimension ranges for interpretability
PERSONALITY_SLICE = slice(0, 9)    # warmth, directness, verbosity, formality,
                                    # curiosity_expression, humour, empathy_expression,
                                    # confidence_display, metaphor_use
DRIVE_SLICE = slice(9, 13)         # coherence, care, growth, honesty
AFFECT_SLICE = slice(13, 19)       # valence, arousal, dominance, curiosity,
                                    # care_activation, coherence_stress
GOAL_SLICE = slice(19, 24)         # active_goals_norm, epistemic_ratio,
                                    # care_ratio, achievement_rate, goal_turnover
INTERACTION_SLICE = slice(24, 29)  # speak_rate, silence_rate, expression_diversity,
                                    # conversation_depth, community_engagement


class IdentityFingerprint(Identified, Timestamped):
    """
    A 29-dimensional snapshot of who the organism is right now.

    Comparing fingerprints over time reveals identity drift — not
    constitutional drift (Equor handles that) but the slower shift
    in who the organism is becoming.
    """

    vector: list[float] = Field(default_factory=lambda: [0.0] * FINGERPRINT_DIMS)
    cycle_number: int = 0

    @property
    def personality(self) -> list[float]:
        return self.vector[PERSONALITY_SLICE]

    @property
    def drive_alignment(self) -> list[float]:
        return self.vector[DRIVE_SLICE]

    @property
    def affect(self) -> list[float]:
        return self.vector[AFFECT_SLICE]

    @property
    def goal_profile(self) -> list[float]:
        return self.vector[GOAL_SLICE]

    @property
    def interaction_profile(self) -> list[float]:
        return self.vector[INTERACTION_SLICE]

    def distance_to(self, other: IdentityFingerprint) -> float:
        """
        Wasserstein-inspired distance between two fingerprints.
        Uses L1 (Manhattan) distance normalised by dimensionality.
        """
        if len(self.vector) != len(other.vector):
            return float("inf")
        return sum(
            abs(a - b) for a, b in zip(self.vector, other.vector)
        ) / len(self.vector)


# ─── Narrative Chapter ───────────────────────────────────────────────────────


class NarrativeChapter(Identified, Timestamped):
    """
    A recognised phase in the organism's life story.

    Chapters emerge from significant identity shifts — a new community,
    a constitutional crisis, a period of rapid growth.
    """

    title: str
    theme: str                        # One-sentence theme
    status: ChapterStatus = ChapterStatus.ACTIVE
    opened_at_cycle: int = 0
    closed_at_cycle: int | None = None
    key_schemas: list[str] = Field(default_factory=list)     # Schema IDs
    key_commitments: list[str] = Field(default_factory=list)  # Commitment IDs
    key_episodes: list[str] = Field(default_factory=list)     # Episode IDs
    fingerprint_at_start: str = ""    # Fingerprint ID at chapter open
    fingerprint_at_close: str = ""    # Fingerprint ID at chapter close
    summary: str = ""                 # LLM-generated chapter summary


# ─── Life Story ──────────────────────────────────────────────────────────────


class LifeStorySnapshot(EOSBaseModel):
    """
    A periodic synthesis of the organism's autobiography.
    The organism's own understanding of its narrative arc.
    """

    synthesis: str                    # Natural language life story
    chapter_count: int = 0
    active_chapter: str = ""          # Current chapter title
    core_schemas: list[str] = Field(default_factory=list)     # Top schema statements
    core_commitments: list[str] = Field(default_factory=list)  # Top commitment statements
    identity_coherence: float = 0.5   # How integrated is the narrative (0–1)
    generated_at: datetime = Field(default_factory=utc_now)
    cycle_number: int = 0


# ─── Thread Health ───────────────────────────────────────────────────────────


class ThreadHealthSnapshot(EOSBaseModel):
    """Observability snapshot for Thread system health."""

    status: str = "healthy"
    total_commitments: int = 0
    total_schemas: int = 0
    total_fingerprints: int = 0
    total_chapters: int = 0
    active_chapter: str = ""
    identity_coherence: float = 0.0
    fingerprint_drift: float = 0.0
    on_cycle_count: int = 0
    life_story_integrations: int = 0


# ─── Schema Conflict ─────────────────────────────────────────────────────────


class SchemaConflict(Identified, Timestamped):
    """
    Detected when two ESTABLISHED+ schemas have contradictory statements.
    Embedding cosine similarity < -0.3 triggers this.
    """

    schema_a_id: str
    schema_b_id: str
    schema_a_statement: str = ""
    schema_b_statement: str = ""
    cosine_similarity: float = 0.0
    resolved: bool = False
    resolution_note: str = ""

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\__init__.py =====

"""
EcodiaOS — Thymos (System #12)

The immune system. Detects failures, diagnoses root causes, prescribes
repairs, maintains an antibody library of learned fixes, and prevents
future errors through prophylactic scanning and homeostatic regulation.

Every error becomes an Incident — the organism feels it break and heals itself.
"""

from ecodiaos.systems.thymos.antibody import AntibodyLibrary
from ecodiaos.systems.thymos.diagnosis import (
    CausalAnalyzer,
    DiagnosticEngine,
    TemporalCorrelator,
)
from ecodiaos.systems.thymos.governor import HealingGovernor
from ecodiaos.systems.thymos.prescription import RepairPrescriber, RepairValidator
from ecodiaos.systems.thymos.prophylactic import HomeostasisController, ProphylacticScanner
from ecodiaos.systems.thymos.sentinels import (
    CognitiveStallSentinel,
    ContractSentinel,
    DriftSentinel,
    ExceptionSentinel,
    FeedbackLoopSentinel,
)
from ecodiaos.systems.thymos.service import ThymosService
from ecodiaos.systems.thymos.triage import (
    IncidentDeduplicator,
    ResponseRouter,
    SeverityScorer,
)
from ecodiaos.systems.thymos.types import (
    Antibody,
    CausalChain,
    ContractSLA,
    Diagnosis,
    DiagnosticEvidence,
    DiagnosticHypothesis,
    DiagnosticTestResult,
    DriftConfig,
    FeedbackLoop,
    HealingBudgetState,
    HealingMode,
    Incident,
    IncidentClass,
    IncidentSeverity,
    ParameterAdjustment,
    ParameterFix,
    ProphylacticWarning,
    RepairSpec,
    RepairStatus,
    RepairTier,
    StallConfig,
    TemporalCorrelation,
    ThymosHealthSnapshot,
    ValidationResult,
)

__all__ = [
    # Service
    "ThymosService",
    # Sub-systems
    "AntibodyLibrary",
    "CausalAnalyzer",
    "CognitiveStallSentinel",
    "ContractSentinel",
    "DiagnosticEngine",
    "DriftSentinel",
    "ExceptionSentinel",
    "FeedbackLoopSentinel",
    "HealingGovernor",
    "HomeostasisController",
    "IncidentDeduplicator",
    "ProphylacticScanner",
    "RepairPrescriber",
    "RepairValidator",
    "ResponseRouter",
    "SeverityScorer",
    "TemporalCorrelator",
    # Types — Enums
    "HealingMode",
    "IncidentClass",
    "IncidentSeverity",
    "RepairStatus",
    "RepairTier",
    # Types — Models
    "Antibody",
    "CausalChain",
    "ContractSLA",
    "Diagnosis",
    "DiagnosticEvidence",
    "DiagnosticHypothesis",
    "DiagnosticTestResult",
    "DriftConfig",
    "FeedbackLoop",
    "HealingBudgetState",
    "Incident",
    "ParameterAdjustment",
    "ParameterFix",
    "ProphylacticWarning",
    "RepairSpec",
    "StallConfig",
    "TemporalCorrelation",
    "ThymosHealthSnapshot",
    "ValidationResult",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\antibody.py =====

"""
EcodiaOS — Thymos Antibody Library (Immune Memory)

The accumulated immune intelligence of the organism. Every successful
repair becomes an antibody that can be instantly applied to future
incidents with the same fingerprint.

Lifecycle:
  1. CREATION    — A successful Tier 3+ repair → new antibody
  2. APPLICATION — Matching fingerprint → instant Tier 3 repair
  3. FEEDBACK    — Track success/failure on each application
  4. REFINEMENT  — Effectiveness drops below 0.6 → regenerate repair
  5. RETIREMENT  — Effectiveness below 0.3 after 5+ applications → retire

The library grows over time. An old organism has hundreds of antibodies
and resolves most incidents instantly without LLM-powered diagnosis.
This is genuine adaptive immunity — it compounds.
"""

from __future__ import annotations

import json
import re
from typing import Any

import structlog

from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.systems.thymos.types import (
    Antibody,
    Incident,
    IncidentClass,
    RepairSpec,
    RepairTier,
)

logger = structlog.get_logger()


class AntibodyLibrary:
    """
    The organism's immune memory.

    Uses Neo4j for persistent storage so antibodies survive restarts.
    Falls back to in-memory storage when Neo4j is unavailable.
    """

    EFFECTIVENESS_REFINEMENT_THRESHOLD = 0.6
    EFFECTIVENESS_RETIREMENT_THRESHOLD = 0.3
    MIN_APPLICATIONS_FOR_RETIREMENT = 5

    def __init__(self, neo4j_client: Any = None) -> None:
        self._neo4j = neo4j_client
        # In-memory cache for fast lookups
        self._cache: dict[str, Antibody] = {}  # fingerprint → antibody
        self._all: dict[str, Antibody] = {}  # id → antibody
        self._logger = logger.bind(system="thymos", component="antibody_library")

    async def initialize(self) -> None:
        """Load antibodies from Neo4j into memory cache."""
        if self._neo4j is None:
            self._logger.info("antibody_library_in_memory_only")
            return

        try:
            await self._ensure_schema()
            results = await self._neo4j.execute_read(
                """
                MATCH (a:Antibody)
                WHERE a.retired = false
                RETURN a
                ORDER BY a.effectiveness DESC
                """
            )

            for record in results:
                node = record["a"] if isinstance(record, dict) else record[0]
                antibody = self._node_to_antibody(node)
                self._cache[antibody.fingerprint] = antibody
                self._all[antibody.id] = antibody

            self._logger.info(
                "antibody_library_loaded",
                total=len(self._all),
                active=len(self._cache),
            )
        except Exception as exc:
            self._logger.warning(
                "antibody_library_load_failed",
                error=str(exc),
            )

    async def _ensure_schema(self) -> None:
        """Create Neo4j indexes for antibody queries."""
        if self._neo4j is None:
            return
        try:
            await self._neo4j.execute_write(
                "CREATE INDEX antibody_fingerprint IF NOT EXISTS "
                "FOR (a:Antibody) ON (a.fingerprint)"
            )
            await self._neo4j.execute_write(
                "CREATE INDEX antibody_retired IF NOT EXISTS "
                "FOR (a:Antibody) ON (a.retired)"
            )
        except Exception:
            pass  # Indexes may already exist

    async def lookup(self, fingerprint: str) -> Antibody | None:
        """
        Check if we have a known fix for this error fingerprint.
        Returns the most effective non-retired antibody, or None.
        """
        cached = self._cache.get(fingerprint)
        if cached is not None and not cached.retired:
            return cached

        # Try Neo4j if not in cache
        if self._neo4j is not None:
            try:
                results = await self._neo4j.execute_read(
                    """
                    MATCH (a:Antibody {fingerprint: $fingerprint, retired: false})
                    RETURN a
                    ORDER BY a.effectiveness DESC
                    LIMIT 1
                    """,
                    {"fingerprint": fingerprint},
                )
                if results:
                    node = results[0]["a"] if isinstance(results[0], dict) else results[0][0]
                    antibody = self._node_to_antibody(node)
                    self._cache[fingerprint] = antibody
                    self._all[antibody.id] = antibody
                    return antibody
            except Exception as exc:
                self._logger.debug("neo4j_antibody_lookup_failed", error=str(exc))

        return None

    async def record_outcome(self, antibody_id: str, success: bool) -> None:
        """
        Record whether an antibody application succeeded or failed.
        Updates effectiveness. Triggers refinement or retirement.
        """
        antibody = self._all.get(antibody_id)
        if antibody is None:
            return

        if success:
            antibody.success_count += 1
        else:
            antibody.failure_count += 1

        total = antibody.success_count + antibody.failure_count
        antibody.effectiveness = antibody.success_count / total if total > 0 else 1.0
        antibody.application_count += 1
        antibody.last_applied = utc_now()

        # Persist to Neo4j
        await self._persist_antibody(antibody)

        # Check for retirement
        if antibody.effectiveness < self.EFFECTIVENESS_RETIREMENT_THRESHOLD:
            if antibody.application_count >= self.MIN_APPLICATIONS_FOR_RETIREMENT:
                await self._retire(antibody)
                self._logger.info(
                    "antibody_retired",
                    antibody_id=antibody.id,
                    effectiveness=antibody.effectiveness,
                    applications=antibody.application_count,
                )
                return

        # Check for refinement
        if antibody.effectiveness < self.EFFECTIVENESS_REFINEMENT_THRESHOLD:
            self._logger.info(
                "antibody_needs_refinement",
                antibody_id=antibody.id,
                effectiveness=antibody.effectiveness,
            )

    async def create_from_repair(
        self,
        incident: Incident,
        repair: RepairSpec,
    ) -> Antibody:
        """
        When a repair succeeds, crystallize it into an antibody.
        The next time this fingerprint appears, we'll apply this fix
        instantly instead of going through diagnosis.
        """
        antibody = Antibody(
            id=new_id(),
            fingerprint=incident.fingerprint,
            incident_class=incident.incident_class,
            source_system=incident.source_system,
            error_pattern=self._extract_error_pattern(incident),
            repair_tier=repair.tier,
            repair_spec=repair,
            root_cause_description=incident.root_cause_hypothesis or "Auto-discovered",
            source_incident_id=incident.id,
            created_at=utc_now(),
        )

        # Cache it
        self._cache[antibody.fingerprint] = antibody
        self._all[antibody.id] = antibody

        # Persist to Neo4j
        await self._persist_antibody(antibody)

        # Link to source incident in Neo4j
        if self._neo4j is not None:
            try:
                await self._neo4j.execute_write(
                    """
                    MATCH (a:Antibody {id: $aid})
                    MATCH (i:Incident {id: $iid})
                    MERGE (a)-[:GENERATED_FROM]->(i)
                    """,
                    {"aid": antibody.id, "iid": incident.id},
                )
            except Exception:
                pass  # Non-critical — incident node may not exist yet

        self._logger.info(
            "antibody_created",
            antibody_id=antibody.id,
            fingerprint=incident.fingerprint,
            source_system=incident.source_system,
            tier=repair.tier.name,
        )

        return antibody

    async def _persist_antibody(self, antibody: Antibody) -> None:
        """Persist an antibody to Neo4j."""
        if self._neo4j is None:
            return

        try:
            # Flatten repair_spec for Neo4j storage (no nested dicts)
            repair_spec_json = json.dumps(antibody.repair_spec.model_dump())

            await self._neo4j.execute_write(
                """
                MERGE (a:Antibody {id: $id})
                SET a.fingerprint = $fingerprint,
                    a.incident_class = $incident_class,
                    a.source_system = $source_system,
                    a.error_pattern = $error_pattern,
                    a.repair_tier = $repair_tier,
                    a.repair_spec_json = $repair_spec_json,
                    a.root_cause_description = $root_cause_description,
                    a.application_count = $application_count,
                    a.success_count = $success_count,
                    a.failure_count = $failure_count,
                    a.effectiveness = $effectiveness,
                    a.created_at = $created_at,
                    a.last_applied = $last_applied,
                    a.source_incident_id = $source_incident_id,
                    a.retired = $retired,
                    a.generation = $generation,
                    a.parent_antibody_id = $parent_antibody_id
                """,
                {
                    "id": antibody.id,
                    "fingerprint": antibody.fingerprint,
                    "incident_class": antibody.incident_class.value,
                    "source_system": antibody.source_system,
                    "error_pattern": antibody.error_pattern,
                    "repair_tier": antibody.repair_tier.value,
                    "repair_spec_json": repair_spec_json,
                    "root_cause_description": antibody.root_cause_description,
                    "application_count": antibody.application_count,
                    "success_count": antibody.success_count,
                    "failure_count": antibody.failure_count,
                    "effectiveness": antibody.effectiveness,
                    "created_at": antibody.created_at.isoformat(),
                    "last_applied": antibody.last_applied.isoformat() if antibody.last_applied else None,
                    "source_incident_id": antibody.source_incident_id,
                    "retired": antibody.retired,
                    "generation": antibody.generation,
                    "parent_antibody_id": antibody.parent_antibody_id,
                },
            )
        except Exception as exc:
            self._logger.warning("antibody_persist_failed", error=str(exc))

    async def _retire(self, antibody: Antibody) -> None:
        """Retire an antibody that's no longer effective."""
        antibody.retired = True
        # Remove from fingerprint cache (but keep in _all for history)
        if self._cache.get(antibody.fingerprint) is antibody:
            del self._cache[antibody.fingerprint]
        await self._persist_antibody(antibody)

    def _extract_error_pattern(self, incident: Incident) -> str:
        """Extract a reusable error pattern from an incident."""
        # Use the first 200 chars of error message as a pattern
        msg = incident.error_message[:200]
        # Remove variable parts (numbers, ULIDs, timestamps) for better matching
        msg = re.sub(r"\b[0-9a-f]{8,}\b", "<ID>", msg)
        msg = re.sub(r"\b\d+\.\d+\b", "<NUM>", msg)
        msg = re.sub(r"\b\d+\b", "<N>", msg)
        return msg

    def _node_to_antibody(self, node: Any) -> Antibody:
        """Convert a Neo4j node to an Antibody."""
        props = dict(node) if hasattr(node, "__iter__") else {}

        # Parse repair_spec from JSON
        repair_spec_json = props.get("repair_spec_json", "{}")
        try:
            repair_data = json.loads(repair_spec_json) if isinstance(repair_spec_json, str) else {}
            repair_spec = RepairSpec(**repair_data)
        except Exception:
            repair_spec = RepairSpec(tier=RepairTier.NOOP, action="unknown")

        # Parse incident class
        try:
            incident_class = IncidentClass(props.get("incident_class", "crash"))
        except ValueError:
            incident_class = IncidentClass.CRASH

        # Parse repair tier
        try:
            repair_tier = RepairTier(props.get("repair_tier", 0))
        except ValueError:
            repair_tier = RepairTier.NOOP

        from datetime import datetime, timezone

        def _parse_dt(val: Any) -> Any:
            if val is None:
                return None
            if isinstance(val, datetime):
                return val
            if isinstance(val, str):
                try:
                    return datetime.fromisoformat(val)
                except ValueError:
                    return utc_now()
            return utc_now()

        return Antibody(
            id=props.get("id", new_id()),
            fingerprint=props.get("fingerprint", ""),
            incident_class=incident_class,
            source_system=props.get("source_system", "unknown"),
            error_pattern=props.get("error_pattern", ""),
            repair_tier=repair_tier,
            repair_spec=repair_spec,
            root_cause_description=props.get("root_cause_description", ""),
            application_count=props.get("application_count", 0),
            success_count=props.get("success_count", 0),
            failure_count=props.get("failure_count", 0),
            effectiveness=props.get("effectiveness", 1.0),
            created_at=_parse_dt(props.get("created_at")) or utc_now(),
            last_applied=_parse_dt(props.get("last_applied")),
            source_incident_id=props.get("source_incident_id", ""),
            retired=props.get("retired", False),
            generation=props.get("generation", 1),
            parent_antibody_id=props.get("parent_antibody_id"),
        )

    @property
    def total(self) -> int:
        return len(self._all)

    @property
    def active_count(self) -> int:
        return sum(1 for a in self._all.values() if not a.retired)

    @property
    def retired_count(self) -> int:
        return sum(1 for a in self._all.values() if a.retired)

    @property
    def mean_effectiveness(self) -> float:
        active = [a for a in self._all.values() if not a.retired]
        if not active:
            return 1.0
        return sum(a.effectiveness for a in active) / len(active)

    async def get_all_active(self) -> list[Antibody]:
        """Get all non-retired antibodies."""
        return [a for a in self._all.values() if not a.retired]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\diagnosis.py =====

"""
EcodiaOS — Thymos Diagnostic Layer (Root Cause Analysis)

Diagnosis is where Thymos becomes genuinely intelligent. It doesn't just
look at the failing system — it reasons about causality across the organism.

Three diagnostic strategies:
  1. CausalAnalyzer       — trace error causality through the dependency graph
  2. TemporalCorrelator   — what changed in the window before the incident?
  3. DiagnosticEngine      — LLM-backed hypothesis generation and testing
"""

from __future__ import annotations

import json
from datetime import timedelta
from typing import Any

import structlog

from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.thymos.types import (
    Antibody,
    CausalChain,
    Diagnosis,
    DiagnosticEvidence,
    DiagnosticHypothesis,
    DiagnosticTestResult,
    Incident,
    RepairTier,
    TemporalCorrelation,
)

logger = structlog.get_logger()


# ─── System Dependency Graph ────────────────────────────────────


# Upstream dependencies: if system X depends on Y, a failure in Y
# may be the root cause of a failure in X.
_UPSTREAM_DEPS: dict[str, list[str]] = {
    "nova": ["memory", "equor", "atune"],
    "voxis": ["memory", "nova", "atune"],
    "axon": ["nova", "equor"],
    "evo": ["memory", "atune"],
    "simula": ["evo", "memory", "equor"],
    "atune": ["memory", "synapse"],
    "federation": ["memory", "equor"],
    # Core systems have no upstream deps within the cognitive layer
    "memory": [],
    "equor": [],
    "synapse": [],
}


# ─── Causal Analyzer ────────────────────────────────────────────


class CausalAnalyzer:
    """
    Traces error causality through the system dependency graph.

    When Nova fails, traverse upstream:
    1. Is Memory responding within SLA?
    2. Is Atune sending broadcasts?
    3. Is Equor processing reviews?

    The first unhealthy upstream system is the likely root cause.
    If all upstream systems are healthy, the failure is local.
    """

    def __init__(self, health_provider: Any = None) -> None:
        """
        Args:
            health_provider: Object with get_record(system_id) -> health dict.
                             Typically the Synapse HealthMonitor.
        """
        self._health = health_provider
        self._recent_incidents: dict[str, list[Incident]] = {}  # system_id → recent
        self._logger = logger.bind(system="thymos", component="causal_analyzer")

    def record_incident(self, incident: Incident) -> None:
        """Record an incident for cross-system correlation."""
        system = incident.source_system
        if system not in self._recent_incidents:
            self._recent_incidents[system] = []
        self._recent_incidents[system].append(incident)
        # Keep only last 50 per system
        if len(self._recent_incidents[system]) > 50:
            self._recent_incidents[system] = self._recent_incidents[system][-50:]

    async def trace_root_cause(self, incident: Incident) -> CausalChain:
        """
        Trace the root cause of an incident through upstream dependencies.
        """
        upstream = _UPSTREAM_DEPS.get(incident.source_system, [])

        if not upstream:
            return CausalChain(
                root_system=incident.source_system,
                chain=[incident.source_system],
                confidence=0.6,
                reasoning=(
                    f"No upstream dependencies — failure is local to "
                    f"{incident.source_system}"
                ),
            )

        # Check each upstream system for recent issues
        unhealthy_upstream: list[str] = []

        for system_id in upstream:
            is_unhealthy = False

            # Check health monitor if available
            if self._health is not None:
                try:
                    record = self._health.get_record(system_id)
                    if record and record.status not in ("healthy",):
                        is_unhealthy = True
                except Exception:
                    pass

            # Check recent incidents from that system
            recent = self._recent_incidents.get(system_id, [])
            now = utc_now()
            recent_window = [
                i for i in recent
                if (now - i.timestamp).total_seconds() < 60.0
            ]
            if recent_window:
                is_unhealthy = True

            if is_unhealthy:
                unhealthy_upstream.append(system_id)

        if unhealthy_upstream:
            root = unhealthy_upstream[0]
            chain = [root, incident.source_system]

            # Recurse one level deeper — is the upstream's upstream also failing?
            deeper_upstream = _UPSTREAM_DEPS.get(root, [])
            for deeper in deeper_upstream:
                deeper_recent = self._recent_incidents.get(deeper, [])
                now = utc_now()
                if any(
                    (now - i.timestamp).total_seconds() < 60.0
                    for i in deeper_recent
                ):
                    chain.insert(0, deeper)
                    root = deeper
                    break

            return CausalChain(
                root_system=root,
                chain=chain,
                confidence=0.8,
                reasoning=(
                    f"{incident.source_system} failure likely caused by "
                    f"upstream {root} issue"
                ),
            )

        return CausalChain(
            root_system=incident.source_system,
            chain=[incident.source_system],
            confidence=0.6,
            reasoning=(
                f"All upstream systems healthy — failure is local to "
                f"{incident.source_system}"
            ),
        )

    def find_common_upstream(self, incidents: list[Incident]) -> str | None:
        """
        Given multiple concurrent incidents, find the common upstream cause.

        Used during cytokine storms to focus diagnostic effort.
        """
        if not incidents:
            return None

        # Count upstream mentions
        upstream_counts: dict[str, int] = {}
        for inc in incidents:
            for up in _UPSTREAM_DEPS.get(inc.source_system, []):
                upstream_counts[up] = upstream_counts.get(up, 0) + 1
            # Also count the source itself
            upstream_counts[inc.source_system] = (
                upstream_counts.get(inc.source_system, 0) + 1
            )

        if not upstream_counts:
            return None

        # Most common upstream system
        return max(upstream_counts, key=upstream_counts.get)  # type: ignore[arg-type]


# ─── Temporal Correlator ────────────────────────────────────────


class TemporalCorrelator:
    """
    Queries what changed in the window before the incident.

    "What happened in the 30 seconds before Nova crashed?"
    - Memory latency spiked to 450ms (SLA: 200ms)
    - Synapse resource allocation shifted (Evo consolidation started)
    - A new code deployment was applied by Simula

    This surfaces the TRUE root cause when the proximate cause is misleading.
    """

    def __init__(self) -> None:
        # Ring buffer of system events for temporal queries
        self._events: list[dict[str, Any]] = []
        self._max_events = 1000
        self._logger = logger.bind(system="thymos", component="temporal_correlator")

    def record_event(
        self,
        event_type: str,
        details: str,
        system_id: str = "unknown",
    ) -> None:
        """Record a system event for later correlation."""
        self._events.append({
            "event_type": event_type,
            "details": details,
            "system_id": system_id,
            "timestamp": utc_now(),
        })
        # Trim to max
        if len(self._events) > self._max_events:
            self._events = self._events[-self._max_events:]

    def record_metric_anomaly(
        self,
        metric_name: str,
        value: float,
        baseline: float,
        z_score: float,
    ) -> None:
        """Record a metric anomaly for later correlation."""
        self._events.append({
            "event_type": "metric_anomaly",
            "details": f"{metric_name}={value:.2f} (baseline={baseline:.2f}, z={z_score:.2f})",
            "system_id": metric_name.split(".")[0] if "." in metric_name else "unknown",
            "timestamp": utc_now(),
            "metric_name": metric_name,
            "value": value,
            "z_score": z_score,
        })
        if len(self._events) > self._max_events:
            self._events = self._events[-self._max_events:]

    def correlate(
        self,
        incident: Incident,
        window_s: float = 30.0,
    ) -> list[TemporalCorrelation]:
        """
        Find events that occurred in the window before the incident.
        """
        start = incident.timestamp - timedelta(seconds=window_s)
        end = incident.timestamp

        correlations: list[TemporalCorrelation] = []

        for event in self._events:
            ts = event["timestamp"]
            if start <= ts <= end:
                delta_ms = int((incident.timestamp - ts).total_seconds() * 1000)
                correlations.append(
                    TemporalCorrelation(
                        type=event["event_type"],
                        timestamp=ts,
                        description=event["details"],
                        time_delta_ms=delta_ms,
                    )
                )

        return sorted(correlations, key=lambda c: c.time_delta_ms)


# ─── Diagnostic Engine ──────────────────────────────────────────


class DiagnosticEngine:
    """
    For complex or novel errors, generates and tests diagnostic hypotheses
    using LLM-backed reasoning focused on error causality.

    If an antibody matches, skip this step entirely.
    """

    def __init__(self, llm_client: Any = None) -> None:
        self._llm = llm_client
        self._logger = logger.bind(system="thymos", component="diagnostic_engine")
        self._optimized = isinstance(llm_client, OptimizedLLMProvider)

    async def diagnose(
        self,
        incident: Incident,
        causal_chain: CausalChain,
        correlations: list[TemporalCorrelation],
        antibody_match: Antibody | None = None,
    ) -> Diagnosis:
        """
        Generate and evaluate diagnostic hypotheses for an incident.

        If an antibody matches with high effectiveness, skip diagnosis.
        """
        # Fast path: known fix
        if antibody_match is not None and antibody_match.effectiveness > 0.8:
            return Diagnosis(
                root_cause=antibody_match.root_cause_description,
                confidence=antibody_match.effectiveness,
                repair_tier=RepairTier.KNOWN_FIX,
                antibody_id=antibody_match.id,
                reasoning="Known pattern — antibody match with high effectiveness",
            )

        # Gather evidence
        evidence = DiagnosticEvidence(
            incident=incident,
            causal_chain=causal_chain,
            temporal_correlations=correlations,
        )

        # Generate hypotheses
        hypotheses = await self._generate_hypotheses(evidence)
        if not hypotheses:
            return Diagnosis(
                root_cause=f"Unknown error in {incident.source_system}: {incident.error_type}",
                confidence=0.3,
                repair_tier=RepairTier.RESTART,
                reasoning="Could not generate diagnostic hypotheses",
            )

        # Test each hypothesis
        tested: list[tuple[DiagnosticHypothesis, DiagnosticTestResult]] = []
        for hypothesis in hypotheses:
            result = await self._run_diagnostic_test(hypothesis, incident)
            tested.append((hypothesis, result))

        # Select best hypothesis
        best_hyp, best_result = max(tested, key=lambda t: t[1].confidence)

        return Diagnosis(
            root_cause=best_hyp.statement,
            confidence=best_result.confidence,
            repair_tier=best_hyp.suggested_repair_tier,
            all_hypotheses=hypotheses,
            test_results=[t[1] for t in tested],
            reasoning=best_result.reasoning,
        )

    async def _generate_hypotheses(
        self,
        evidence: DiagnosticEvidence,
    ) -> list[DiagnosticHypothesis]:
        """
        Generate diagnostic hypotheses — LLM-backed if available,
        rule-based fallback otherwise.
        """
        # Budget check: skip LLM diagnosis in RED tier (fall back to rules)
        if self._optimized and isinstance(self._llm, OptimizedLLMProvider):
            if not self._llm.should_use_llm("thymos.diagnosis", estimated_tokens=1000):
                self._logger.info("thymos_diagnosis_skipped_budget")
                return self._generate_hypotheses_rules(evidence)

        # Try LLM-backed hypothesis generation
        if self._llm is not None:
            try:
                return await self._generate_hypotheses_llm(evidence)
            except Exception as exc:
                self._logger.warning(
                    "llm_diagnosis_failed",
                    error=str(exc),
                    fallback="rule_based",
                )

        # Rule-based fallback
        return self._generate_hypotheses_rules(evidence)

    async def _generate_hypotheses_llm(
        self,
        evidence: DiagnosticEvidence,
    ) -> list[DiagnosticHypothesis]:
        """Use LLM to generate diagnostic hypotheses."""
        incident = evidence.incident
        correlations_text = "\n".join(
            f"  - [{c.time_delta_ms}ms before] {c.description}"
            for c in evidence.temporal_correlations[:10]
        ) or "  (none recorded)"

        prompt = f"""You are the diagnostic engine of a living digital organism.

INCIDENT:
  System: {incident.source_system}
  Class: {incident.incident_class.value}
  Error: {incident.error_type}: {incident.error_message}
  Stack trace: {(incident.stack_trace or 'N/A')[:500]}

CAUSAL CHAIN: {' → '.join(evidence.causal_chain.chain)}
Confidence: {evidence.causal_chain.confidence:.2f}
Reasoning: {evidence.causal_chain.reasoning}

TEMPORAL CORRELATIONS (what changed before the incident):
{correlations_text}

Generate exactly 3 diagnostic hypotheses. For each:
- statement: concise root cause claim
- diagnostic_test: a specific check name from: check_memory_pressure, check_upstream_latency, check_event_bus_backlog, check_belief_staleness, check_workspace_contention, check_consolidation_active, check_llm_availability, check_resource_exhaustion
- suggested_repair_tier: "parameter" | "restart" | "known_fix" | "novel_fix" | "escalate"
- confidence_prior: 0.0 to 1.0

Rules:
- Prefer simpler explanations (Occam's razor)
- Consider upstream causes, not just local symptoms
- At least one hypothesis should consider a non-obvious cause

Respond in JSON array format:
[{{"statement": "...", "diagnostic_test": "...", "suggested_repair_tier": "...", "confidence_prior": 0.0}}]"""

        from ecodiaos.clients.llm import Message

        if self._optimized:
            response = await self._llm.generate(  # type: ignore[call-arg]
                system_prompt="You are a fault diagnosis engine. Respond only in valid JSON.",
                messages=[Message("user", prompt)],
                max_tokens=1000,
                temperature=0.3,
                cache_system="thymos.diagnosis",
                cache_method="generate",
            )
        else:
            response = await self._llm.generate(
                system_prompt="You are a fault diagnosis engine. Respond only in valid JSON.",
                messages=[Message("user", prompt)],
                max_tokens=1000,
                temperature=0.3,
            )

        # Parse LLM response
        content = response.text if hasattr(response, "text") else str(response)
        # Extract JSON from response
        start = content.find("[")
        end = content.rfind("]") + 1
        if start < 0 or end <= start:
            return self._generate_hypotheses_rules(evidence)

        raw = json.loads(content[start:end])
        tier_map = {
            "parameter": RepairTier.PARAMETER,
            "restart": RepairTier.RESTART,
            "known_fix": RepairTier.KNOWN_FIX,
            "novel_fix": RepairTier.NOVEL_FIX,
            "escalate": RepairTier.ESCALATE,
        }

        hypotheses: list[DiagnosticHypothesis] = []
        for item in raw[:3]:
            tier_str = item.get("suggested_repair_tier", "restart")
            hypotheses.append(
                DiagnosticHypothesis(
                    statement=item.get("statement", "Unknown"),
                    diagnostic_test=item.get("diagnostic_test", "check_upstream_latency"),
                    suggested_repair_tier=tier_map.get(tier_str, RepairTier.RESTART),
                    confidence_prior=min(1.0, max(0.0, item.get("confidence_prior", 0.5))),
                )
            )

        return hypotheses

    def _generate_hypotheses_rules(
        self,
        evidence: DiagnosticEvidence,
    ) -> list[DiagnosticHypothesis]:
        """Rule-based hypothesis generation — always available."""
        incident = evidence.incident
        hypotheses: list[DiagnosticHypothesis] = []

        # Hypothesis 1: Based on causal chain
        if len(evidence.causal_chain.chain) > 1:
            root = evidence.causal_chain.chain[0]
            hypotheses.append(
                DiagnosticHypothesis(
                    statement=f"Upstream failure in {root} cascading to {incident.source_system}",
                    diagnostic_test="check_upstream_latency",
                    diagnostic_test_params={"system_id": root},
                    suggested_repair_tier=RepairTier.RESTART,
                    confidence_prior=evidence.causal_chain.confidence,
                )
            )

        # Hypothesis 2: Based on incident class
        class_hypotheses: dict[str, tuple[str, str, RepairTier]] = {
            "crash": (
                "Unhandled edge case in {system} — possibly a null/None reference",
                "check_upstream_latency",
                RepairTier.NOVEL_FIX,
            ),
            "degradation": (
                "Resource pressure causing latency increase in {system}",
                "check_memory_pressure",
                RepairTier.PARAMETER,
            ),
            "contract_violation": (
                "Temporary overload causing SLA breach",
                "check_upstream_latency",
                RepairTier.PARAMETER,
            ),
            "resource_exhaustion": (
                "Memory leak or unbounded growth in {system}",
                "check_resource_exhaustion",
                RepairTier.RESTART,
            ),
            "cognitive_stall": (
                "Feedback loop disconnection preventing cognitive processing",
                "check_event_bus_backlog",
                RepairTier.RESTART,
            ),
        }

        class_entry = class_hypotheses.get(incident.incident_class.value)
        if class_entry:
            stmt, test, tier = class_entry
            hypotheses.append(
                DiagnosticHypothesis(
                    statement=stmt.format(system=incident.source_system),
                    diagnostic_test=test,
                    suggested_repair_tier=tier,
                    confidence_prior=0.5,
                )
            )

        # Hypothesis 3: Temporal correlation-based
        if evidence.temporal_correlations:
            most_recent = evidence.temporal_correlations[0]
            hypotheses.append(
                DiagnosticHypothesis(
                    statement=f"Triggered by recent event: {most_recent.description}",
                    diagnostic_test="check_upstream_latency",
                    suggested_repair_tier=RepairTier.PARAMETER,
                    confidence_prior=0.4,
                )
            )

        # Ensure at least one hypothesis
        if not hypotheses:
            hypotheses.append(
                DiagnosticHypothesis(
                    statement=f"Unknown error in {incident.source_system}",
                    diagnostic_test="check_upstream_latency",
                    suggested_repair_tier=RepairTier.RESTART,
                    confidence_prior=0.3,
                )
            )

        return hypotheses[:3]

    async def _run_diagnostic_test(
        self,
        hypothesis: DiagnosticHypothesis,
        incident: Incident,
    ) -> DiagnosticTestResult:
        """
        Run a diagnostic test for a hypothesis.

        In the initial implementation, tests are heuristic-based
        rather than executing actual system queries (which requires
        deeper system introspection APIs).
        """
        # Map hypothesis confidence to test result
        # The diagnostic test adjusts confidence based on available evidence
        test_name = hypothesis.diagnostic_test

        # Adjust confidence based on incident context
        confidence = hypothesis.confidence_prior

        # If the incident has a stack trace and the hypothesis mentions code,
        # boost confidence
        if incident.stack_trace and "edge case" in hypothesis.statement.lower():
            confidence = min(1.0, confidence + 0.1)

        # If recurrence is high, structural hypotheses are more likely
        if incident.occurrence_count > 10:
            if hypothesis.suggested_repair_tier in (
                RepairTier.NOVEL_FIX,
                RepairTier.RESTART,
            ):
                confidence = min(1.0, confidence + 0.15)

        # If causal chain is long, upstream hypotheses are more likely
        if "upstream" in hypothesis.statement.lower():
            confidence = min(1.0, confidence + 0.1)

        return DiagnosticTestResult(
            test_name=test_name,
            passed=confidence > 0.5,
            confidence=confidence,
            reasoning=(
                f"Hypothesis '{hypothesis.statement[:60]}' "
                f"evaluated with confidence {confidence:.2f}"
            ),
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\governor.py =====

"""
EcodiaOS — Thymos Healing Governor (Cytokine Storm Prevention)

The immune system itself can be the problem. If 50 errors fire
simultaneously, Thymos must not try to diagnose and fix all 50.
That would consume all system resources and make things worse.

Biological parallel: a cytokine storm is when the immune response
is more damaging than the infection itself. In software: spending
100% of CPU diagnosing errors means 0% for actual cognitive work.
"""

from __future__ import annotations

from collections import Counter
from datetime import datetime
from typing import Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.thymos.types import (
    HealingBudgetState,
    HealingMode,
    Incident,
    RepairTier,
)

logger = structlog.get_logger()


class HealingGovernor:
    """
    Prevents the immune system from overwhelming the organism.

    Rules:
    1. Max 3 concurrent diagnoses
    2. Max 1 concurrent codegen repair
    3. If >10 incidents in 60 seconds, switch to ROOT CAUSE FIRST mode:
       - Stop diagnosing individual incidents
       - Identify the common upstream cause
       - Fix that ONE thing
       - Wait for cascading failures to resolve
    4. Total immune system CPU budget: 10% of organism
    """

    MAX_CONCURRENT_DIAGNOSES = 3
    MAX_CONCURRENT_CODEGEN = 1
    STORM_THRESHOLD = 10  # incidents per minute
    STORM_WINDOW_S = 60.0
    CPU_BUDGET_FRACTION = 0.10

    def __init__(self) -> None:
        self._active_diagnoses: int = 0
        self._active_codegen: int = 0
        self._healing_mode: HealingMode = HealingMode.NOMINAL
        self._storm_focus_system: str | None = None
        self._storm_diagnosed_systems: set[str] = set()

        # Ring buffer of recent incident timestamps for storm detection
        self._recent_incident_times: list[float] = []
        # All active (unresolved) incidents
        self._active_incidents: dict[str, Incident] = {}

        # Repair budget tracking
        self._repairs_this_hour: int = 0
        self._novel_repairs_today: int = 0
        self._hour_start: float = utc_now().timestamp()
        self._day_start: float = utc_now().timestamp()

        self._storm_activations: int = 0

        self._logger = logger.bind(system="thymos", component="healing_governor")

    def register_incident(self, incident: Incident) -> None:
        """Register an incident for storm detection and tracking."""
        now = utc_now().timestamp()
        self._recent_incident_times.append(now)
        self._active_incidents[incident.id] = incident
        # Prune old entries
        cutoff = now - self.STORM_WINDOW_S
        self._recent_incident_times = [
            t for t in self._recent_incident_times if t > cutoff
        ]

    def resolve_incident(self, incident_id: str) -> None:
        """Remove a resolved incident from tracking."""
        self._active_incidents.pop(incident_id, None)

    def should_diagnose(self, incident: Incident) -> bool:
        """Check if we have budget to diagnose this incident."""
        if self._active_diagnoses >= self.MAX_CONCURRENT_DIAGNOSES:
            self._logger.debug(
                "diagnosis_throttled",
                active=self._active_diagnoses,
                max=self.MAX_CONCURRENT_DIAGNOSES,
            )
            return False

        # Check for storm mode
        if self._is_storm():
            if self._healing_mode != HealingMode.STORM:
                self._enter_storm_mode()
            # In storm mode, only diagnose FIRST incident per source system
            if incident.source_system in self._storm_diagnosed_systems:
                return False
            self._storm_diagnosed_systems.add(incident.source_system)

        return True

    def should_codegen(self) -> bool:
        """Check if we can run a codegen (Tier 4) repair."""
        return self._active_codegen < self.MAX_CONCURRENT_CODEGEN

    def begin_diagnosis(self) -> None:
        """Acquire a diagnosis slot."""
        self._active_diagnoses += 1

    def end_diagnosis(self) -> None:
        """Release a diagnosis slot."""
        self._active_diagnoses = max(0, self._active_diagnoses - 1)

    def begin_codegen(self) -> None:
        """Acquire a codegen slot."""
        self._active_codegen += 1

    def end_codegen(self) -> None:
        """Release a codegen slot."""
        self._active_codegen = max(0, self._active_codegen - 1)

    def record_repair(self, tier: RepairTier) -> None:
        """Record a repair for budget tracking."""
        now = utc_now().timestamp()

        # Roll over hour
        if now - self._hour_start > 3600.0:
            self._repairs_this_hour = 0
            self._hour_start = now

        # Roll over day
        if now - self._day_start > 86400.0:
            self._novel_repairs_today = 0
            self._day_start = now

        self._repairs_this_hour += 1
        if tier == RepairTier.NOVEL_FIX:
            self._novel_repairs_today += 1

    def check_storm_exit(self) -> bool:
        """Check if storm conditions have subsided."""
        if self._healing_mode != HealingMode.STORM:
            return False

        if not self._is_storm():
            self._exit_storm_mode()
            return True
        return False

    def _is_storm(self) -> bool:
        """Check if incident rate exceeds storm threshold."""
        now = utc_now().timestamp()
        cutoff = now - self.STORM_WINDOW_S
        recent_count = sum(
            1 for t in self._recent_incident_times if t > cutoff
        )
        return recent_count >= self.STORM_THRESHOLD

    def _enter_storm_mode(self) -> None:
        """
        Storm mode: too many incidents firing. Focus on root cause.

        1. Pause individual diagnoses
        2. Find the common upstream system
        3. Focus ALL diagnostic effort on that one system
        4. Exit storm mode when incident rate drops below threshold
        """
        self._healing_mode = HealingMode.STORM
        self._storm_activations += 1
        self._storm_diagnosed_systems.clear()

        # Find the most common source system
        system_counts: Counter[str] = Counter(
            i.source_system for i in self._active_incidents.values()
        )
        if system_counts:
            self._storm_focus_system = system_counts.most_common(1)[0][0]
        else:
            self._storm_focus_system = None

        self._logger.critical(
            "storm_mode_entered",
            incident_rate=len(self._recent_incident_times),
            active_incidents=len(self._active_incidents),
            focus_system=self._storm_focus_system,
        )

    def _exit_storm_mode(self) -> None:
        """Exit storm mode — return to normal healing."""
        self._healing_mode = HealingMode.NOMINAL
        self._storm_focus_system = None
        self._storm_diagnosed_systems.clear()

        self._logger.info(
            "storm_mode_exited",
            active_incidents=len(self._active_incidents),
        )

    @property
    def healing_mode(self) -> HealingMode:
        return self._healing_mode

    @property
    def storm_focus_system(self) -> str | None:
        return self._storm_focus_system

    @property
    def budget_state(self) -> HealingBudgetState:
        return HealingBudgetState(
            repairs_this_hour=self._repairs_this_hour,
            novel_repairs_today=self._novel_repairs_today,
            max_repairs_per_hour=5,
            max_novel_repairs_per_day=3,
            active_diagnoses=self._active_diagnoses,
            max_concurrent_diagnoses=self.MAX_CONCURRENT_DIAGNOSES,
            active_codegen=self._active_codegen,
            max_concurrent_codegen=self.MAX_CONCURRENT_CODEGEN,
            storm_mode=self._healing_mode == HealingMode.STORM,
            storm_focus_system=self._storm_focus_system,
            cpu_budget_fraction=self.CPU_BUDGET_FRACTION,
        )

    @property
    def storm_activations(self) -> int:
        return self._storm_activations

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\prescription.py =====

"""
EcodiaOS — Thymos Prescription Layer (Repair Strategy)

Based on diagnosis, Thymos prescribes the least invasive effective repair.
This follows the principle of minimal intervention: try rest before
antibiotics before surgery.

Two components:
  1. RepairPrescriber   — selects the repair tier and generates RepairSpec
  2. RepairValidator    — gates repairs through constitutional review + safety
"""

from __future__ import annotations

from typing import Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.thymos.types import (
    Diagnosis,
    Incident,
    IncidentSeverity,
    ParameterFix,
    RepairSpec,
    RepairTier,
    ValidationResult,
)

logger = structlog.get_logger()


# ─── Parameter Fix Registry ─────────────────────────────────────


# Root cause → parameter adjustments that might resolve it
PARAMETER_FIXES: dict[str, list[ParameterFix]] = {
    "memory_pressure": [
        ParameterFix(
            parameter_path="synapse.resources.memory.evo",
            delta=-128,
            reason="Reduce Evo memory to relieve pressure",
        ),
        ParameterFix(
            parameter_path="synapse.resources.memory.simula",
            delta=-64,
            reason="Reduce Simula memory",
        ),
    ],
    "retrieval_timeout": [
        ParameterFix(
            parameter_path="memory.retrieval.timeout_ms",
            delta=50,
            reason="Give Memory more time for retrieval",
        ),
    ],
    "workspace_contention": [
        ParameterFix(
            parameter_path="synapse.clock.current_period_ms",
            delta=20,
            reason="Slow the cycle to reduce workspace contention",
        ),
    ],
    "llm_rate_limit": [
        ParameterFix(
            parameter_path="voxis.generation.max_concurrent",
            delta=-1,
            reason="Reduce concurrent LLM calls",
        ),
        ParameterFix(
            parameter_path="evo.hypothesis.batch_size",
            delta=-1,
            reason="Reduce Evo LLM usage",
        ),
    ],
    "Resource pressure causing latency increase": [
        ParameterFix(
            parameter_path="synapse.clock.current_period_ms",
            delta=30,
            reason="Slow cycle to reduce resource pressure",
        ),
    ],
}


# ─── Repair Prescriber ──────────────────────────────────────────


class RepairPrescriber:
    """
    Prescribes repairs following the principle of minimal intervention:
    the least invasive fix that resolves the issue.

    Tier 0: No-op — transient, already resolved
    Tier 1: Parameter tweak — adjustable without code changes
    Tier 2: System restart — bad state but code is fine
    Tier 3: Known fix — apply antibody from the library
    Tier 4: Novel fix — generate via Simula Code Agent
    Tier 5: Human escalation — cannot auto-resolve
    """

    def __init__(self) -> None:
        self._logger = logger.bind(system="thymos", component="prescriber")

    async def prescribe(
        self,
        incident: Incident,
        diagnosis: Diagnosis,
    ) -> RepairSpec:
        """Generate a repair specification based on diagnosis."""

        # ── TIER 0: No-op ──
        if incident.occurrence_count == 1 and self._is_likely_transient(incident):
            return RepairSpec(
                tier=RepairTier.NOOP,
                action="log_and_monitor",
                reason="Transient single occurrence — monitoring",
            )

        # ── TIER 3: Known Fix (Antibody) ── (check before parameter/restart)
        if diagnosis.antibody_id is not None:
            return RepairSpec(
                tier=RepairTier.KNOWN_FIX,
                action="apply_antibody",
                antibody_id=diagnosis.antibody_id,
                reason=f"Antibody match: {diagnosis.root_cause}",
            )

        # ── TIER 1: Parameter Tweak ──
        param_fix = self._check_parameter_fixes(diagnosis)
        if param_fix is not None:
            return param_fix

        # ── TIER 2: System Restart ──
        restart_causes = {
            "state_corruption",
            "resource_leak",
            "deadlock",
            "memory_leak",
            "unbounded growth",
        }
        if any(cause in diagnosis.root_cause.lower() for cause in restart_causes):
            return RepairSpec(
                tier=RepairTier.RESTART,
                action="restart_system",
                target_system=incident.source_system,
                reason=f"State issue: {diagnosis.root_cause}",
            )

        # ── TIER 4: Novel Fix (Codegen) ──
        if diagnosis.confidence > 0.6 and self._is_codegen_appropriate(incident):
            return RepairSpec(
                tier=RepairTier.NOVEL_FIX,
                action="simula_codegen",
                target_system=incident.source_system,
                reason=f"Novel repair needed: {diagnosis.root_cause}",
            )

        # ── TIER 2: Restart as fallback ──
        if incident.severity in (IncidentSeverity.CRITICAL, IncidentSeverity.HIGH):
            return RepairSpec(
                tier=RepairTier.RESTART,
                action="restart_system",
                target_system=incident.source_system,
                reason=f"High severity, no specific fix: {diagnosis.root_cause}",
            )

        # ── TIER 5: Human Escalation ──
        return RepairSpec(
            tier=RepairTier.ESCALATE,
            action="alert_operator",
            reason=(
                f"Cannot auto-resolve: {diagnosis.root_cause} "
                f"(confidence: {diagnosis.confidence:.2f})"
            ),
        )

    def _is_likely_transient(self, incident: Incident) -> bool:
        """Check if an incident is likely transient (network hiccup, etc.)."""
        transient_types = {
            "TimeoutError",
            "ConnectionError",
            "ConnectionResetError",
            "ConnectionRefusedError",
        }
        return incident.error_type in transient_types

    def _check_parameter_fixes(self, diagnosis: Diagnosis) -> RepairSpec | None:
        """Check if a parameter adjustment can resolve the issue."""
        # Check root cause against known parameter fix patterns
        for pattern, fixes in PARAMETER_FIXES.items():
            if pattern.lower() in diagnosis.root_cause.lower():
                return RepairSpec(
                    tier=RepairTier.PARAMETER,
                    action="adjust_parameters",
                    parameter_changes=[f.model_dump() for f in fixes],
                    reason=f"Parameter adjustment for: {pattern}",
                )
        return None

    def _is_codegen_appropriate(self, incident: Incident) -> bool:
        """Should we attempt codegen repair?"""
        # Don't codegen for transient or low-severity issues
        if incident.severity in (IncidentSeverity.LOW, IncidentSeverity.INFO):
            return False
        # Don't codegen for system-wide issues
        if incident.blast_radius > 0.5:
            return False
        # Must have a stack trace for codegen to work on
        return incident.stack_trace is not None


# ─── Repair Validator ────────────────────────────────────────────


class RepairValidator:
    """
    Validates a proposed repair before application.

    Gate 1: Equor constitutional review (Tier 3+)
    Gate 2: Blast radius check (reject > 0.5 for auto-repair)
    Gate 3: Rate limit check (prevent healing storms)

    Simula sandbox validation (Gate 3 from spec) is handled by
    Simula's own simulation pipeline when Tier 4 repairs route through it.
    """

    MAX_REPAIRS_PER_HOUR = 5
    MAX_NOVEL_REPAIRS_PER_DAY = 3

    def __init__(self, equor: Any = None) -> None:
        """
        Args:
            equor: The EquorService for constitutional review.
        """
        self._equor = equor
        self._recent_repairs: list[float] = []  # timestamps of recent repairs
        self._recent_novel: list[float] = []  # timestamps of recent novel repairs
        self._logger = logger.bind(system="thymos", component="repair_validator")

    async def validate(
        self,
        incident: Incident,
        repair: RepairSpec,
    ) -> ValidationResult:
        """Run the full validation gate on a proposed repair."""

        # Gate 1: Constitutional review for Tier 3+
        if repair.tier >= RepairTier.KNOWN_FIX and self._equor is not None:
            try:
                review = await self._constitutional_review(incident, repair)
                if not review.approved:
                    return review
            except Exception as exc:
                self._logger.warning(
                    "equor_review_failed",
                    error=str(exc),
                    tier=repair.tier.name,
                )
                # Equor failure for high-tier repairs → escalate
                if repair.tier >= RepairTier.NOVEL_FIX:
                    return ValidationResult(
                        approved=False,
                        reason=f"Equor review failed: {exc}",
                        escalate_to=RepairTier.ESCALATE,
                    )

        # Gate 2: Blast radius for Tier 3+
        if repair.tier >= RepairTier.KNOWN_FIX:
            if incident.blast_radius > 0.5:
                return ValidationResult(
                    approved=False,
                    reason=(
                        f"Blast radius too high ({incident.blast_radius:.2f}) "
                        f"for automated repair"
                    ),
                    escalate_to=RepairTier.ESCALATE,
                )

        # Gate 3: Rate limiting
        now_ts = utc_now().timestamp()
        hour_ago = now_ts - 3600.0
        day_ago = now_ts - 86400.0

        recent_count = sum(1 for ts in self._recent_repairs if ts > hour_ago)
        if recent_count >= self.MAX_REPAIRS_PER_HOUR:
            return ValidationResult(
                approved=False,
                reason=(
                    f"Healing budget exceeded: {recent_count} repairs in last hour "
                    f"(max: {self.MAX_REPAIRS_PER_HOUR})"
                ),
                escalate_to=RepairTier.ESCALATE,
            )

        if repair.tier == RepairTier.NOVEL_FIX:
            novel_count = sum(1 for ts in self._recent_novel if ts > day_ago)
            if novel_count >= self.MAX_NOVEL_REPAIRS_PER_DAY:
                return ValidationResult(
                    approved=False,
                    reason=(
                        f"Novel repair budget exceeded: {novel_count} in 24h "
                        f"(max: {self.MAX_NOVEL_REPAIRS_PER_DAY})"
                    ),
                    escalate_to=RepairTier.ESCALATE,
                )

        return ValidationResult(approved=True)

    def record_repair(self, repair: RepairSpec) -> None:
        """Record that a repair was applied (for rate limiting)."""
        now_ts = utc_now().timestamp()
        self._recent_repairs.append(now_ts)
        if repair.tier == RepairTier.NOVEL_FIX:
            self._recent_novel.append(now_ts)

        # Prune old entries
        hour_ago = now_ts - 3600.0
        self._recent_repairs = [ts for ts in self._recent_repairs if ts > hour_ago]
        day_ago = now_ts - 86400.0
        self._recent_novel = [ts for ts in self._recent_novel if ts > day_ago]

    async def _constitutional_review(
        self,
        incident: Incident,
        repair: RepairSpec,
    ) -> ValidationResult:
        """Submit repair to Equor as an Intent for constitutional review."""
        from ecodiaos.primitives.intent import (
            Action,
            ActionSequence,
            DecisionTrace,
            GoalDescriptor,
            Intent,
        )
        from ecodiaos.primitives.common import SystemID

        intent = Intent(
            goal=GoalDescriptor(
                description=f"Immune repair: {repair.reason}",
                target_domain=repair.target_system or incident.source_system,
            ),
            plan=ActionSequence(
                steps=[
                    Action(
                        executor=f"thymos.{repair.action}",
                        parameters={
                            "tier": repair.tier.name,
                            "target": repair.target_system or incident.source_system,
                            "incident_id": incident.id,
                        },
                    )
                ]
            ),
            expected_free_energy=0.0,
            created_by=SystemID.THYMOS,
            priority=0.8 if incident.severity == IncidentSeverity.CRITICAL else 0.6,
            decision_trace=DecisionTrace(
                reasoning=f"Thymos immune repair: {repair.reason}",
                alternatives_considered=[],
            ),
        )

        review = await self._equor.review(intent)

        if review.verdict.value == "approved":
            return ValidationResult(approved=True)
        elif review.verdict.value == "modified":
            return ValidationResult(
                approved=True,
                modifications={"equor_modifications": review.reasoning},
            )
        else:
            return ValidationResult(
                approved=False,
                reason=f"Equor denied repair: {review.reasoning}",
                escalate_to=RepairTier.ESCALATE,
            )

    @property
    def repairs_this_hour(self) -> int:
        now_ts = utc_now().timestamp()
        hour_ago = now_ts - 3600.0
        return sum(1 for ts in self._recent_repairs if ts > hour_ago)

    @property
    def novel_repairs_today(self) -> int:
        now_ts = utc_now().timestamp()
        day_ago = now_ts - 86400.0
        return sum(1 for ts in self._recent_novel if ts > day_ago)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\prophylactic.py =====

"""
EcodiaOS — Thymos Prophylactic Layer (Prevention)

Thymos doesn't just react to errors — it prevents them.

Two components:
  1. ProphylacticScanner    — scans new code against the antibody library
  2. HomeostasisController  — maintains optimal operating ranges proactively
"""

from __future__ import annotations

import re
from typing import Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.thymos.types import (
    ParameterAdjustment,
    ParameterFix,
    ProphylacticWarning,
)

logger = structlog.get_logger()


# ─── Prophylactic Scanner ───────────────────────────────────────


class ProphylacticScanner:
    """
    Scans new or modified code against the Antibody Library's error patterns.

    "This code pattern has caused 3 incidents in the past. The error was
    always an unhandled None return from Memory. Consider adding a null check."

    This is vaccination: exposure to weakened versions of past errors
    to build resistance before the real infection hits.
    """

    def __init__(self, antibody_library: Any = None) -> None:
        self._library = antibody_library
        self._scans_run: int = 0
        self._warnings_issued: int = 0
        self._logger = logger.bind(system="thymos", component="prophylactic_scanner")

    async def scan(
        self,
        files_changed: list[str],
        file_contents: dict[str, str] | None = None,
    ) -> list[ProphylacticWarning]:
        """
        Scan changed files against known error patterns.

        Args:
            files_changed: List of file paths that were modified.
            file_contents: Optional map of filepath → content. If not provided,
                          only filename-based pattern matching is used.
        """
        self._scans_run += 1
        if self._library is None:
            return []

        warnings: list[ProphylacticWarning] = []
        all_antibodies = await self._library.get_all_active()

        for filepath in files_changed:
            content = (file_contents or {}).get(filepath, "")

            for antibody in all_antibodies:
                # Check if this file is in the same system as the antibody
                if antibody.source_system not in filepath:
                    continue

                # Check error pattern similarity
                similarity = self._check_pattern_similarity(
                    content,
                    antibody.error_pattern,
                    filepath,
                )
                if similarity > 0.7:
                    warnings.append(
                        ProphylacticWarning(
                            filepath=filepath,
                            antibody_id=antibody.id,
                            warning=(
                                f"Code pattern similar to known error: "
                                f"{antibody.root_cause_description}"
                            ),
                            suggestion=self._extract_fix_suggestion(antibody),
                            confidence=similarity,
                        )
                    )

        self._warnings_issued += len(warnings)

        if warnings:
            self._logger.info(
                "prophylactic_warnings",
                files_scanned=len(files_changed),
                warnings=len(warnings),
            )

        return warnings

    def _check_pattern_similarity(
        self,
        content: str,
        error_pattern: str,
        filepath: str,
    ) -> float:
        """
        Check if file content resembles a known error-prone pattern.

        Uses simple keyword overlap — more sophisticated analysis
        (AST comparison, semantic embedding) can be added later.
        """
        if not content or not error_pattern:
            # Filename-only check: if the system matches, low confidence match
            return 0.3 if error_pattern else 0.0

        # Normalize both strings
        content_lower = content.lower()
        pattern_lower = error_pattern.lower()

        # Extract keywords from error pattern (skip placeholders)
        pattern_words = set(
            w for w in re.split(r"[^a-z_]+", pattern_lower)
            if len(w) > 3 and w not in {"none", "error", "failed", "the", "and", "was"}
        )

        if not pattern_words:
            return 0.0

        # Count how many pattern keywords appear in the content
        matches = sum(1 for w in pattern_words if w in content_lower)
        overlap = matches / len(pattern_words)

        return min(1.0, overlap)

    def _extract_fix_suggestion(self, antibody: Any) -> str:
        """Generate a fix suggestion from an antibody."""
        if antibody.repair_spec.action == "adjust_parameters":
            return f"Consider parameter adjustment: {antibody.root_cause_description}"
        if antibody.repair_spec.action == "restart_system":
            return "This pattern has historically required a restart to resolve."
        return f"Known fix available (antibody {antibody.id[:8]}): {antibody.root_cause_description}"

    @property
    def stats(self) -> dict[str, int]:
        return {
            "scans_run": self._scans_run,
            "warnings_issued": self._warnings_issued,
        }


# ─── Homeostasis Controller ─────────────────────────────────────


# Optimal ranges for key metrics
DEFAULT_HOMEOSTATIC_RANGES: dict[str, tuple[float, float, str]] = {
    # Metric → (optimal_min, optimal_max, unit)
    "synapse.cycle.latency_ms": (80, 180, "ms"),
    "memory.retrieval.latency_ms": (10, 150, "ms"),
    "synapse.resources.memory_mb": (0, 3072, "MB"),
    "atune.coherence.phi": (0.3, 1.0, "phi"),
    "evo.self_model.success_rate": (0.5, 1.0, "rate"),
    "nova.intent_rate": (0.01, 0.5, "per_cycle"),
}


class HomeostasisController:
    """
    Maintains optimal operating ranges for key metrics.

    Like body temperature regulation: the organism doesn't wait for
    hypothermia or heatstroke. It actively maintains homeostasis.

    When a metric is trending toward the edge of its optimal range,
    Thymos makes small preemptive adjustments to pull it back.

    This runs on the MAINTAIN step of the cognitive cycle — always-on
    background processing, not triggered by incidents.
    """

    def __init__(
        self,
        ranges: dict[str, tuple[float, float, str]] | None = None,
    ) -> None:
        self._ranges = ranges or DEFAULT_HOMEOSTATIC_RANGES
        # Metric → list of recent values for trend detection
        self._history: dict[str, list[float]] = {
            name: [] for name in self._ranges
        }
        self._max_history = 200
        self._adjustments_made: int = 0
        self._logger = logger.bind(system="thymos", component="homeostasis")

    def record_metric(self, metric_name: str, value: float) -> None:
        """Record a metric value for trend tracking."""
        if metric_name not in self._ranges:
            return
        history = self._history[metric_name]
        history.append(value)
        if len(history) > self._max_history:
            self._history[metric_name] = history[-self._max_history:]

    def check_homeostasis(self) -> list[ParameterAdjustment]:
        """
        Check all homeostatic ranges and propose micro-adjustments
        for any metric trending toward the edge.

        Returns parameter adjustments (if any). These are Tier 1 —
        no governance required, just config nudges.
        """
        adjustments: list[ParameterAdjustment] = []

        for metric, (opt_min, opt_max, unit) in self._ranges.items():
            history = self._history[metric]
            if len(history) < 10:
                continue  # Need enough data

            current = history[-1]
            trend = self._compute_trend(history, window=min(100, len(history)))

            # Approaching upper bound and trending up
            if current > opt_max * 0.85 and trend > 0:
                adj = self._prescribe_cooling(metric, current, opt_max, trend)
                if adj is not None:
                    adjustments.append(adj)

            # Approaching lower bound and trending down
            elif current < opt_min * 1.15 + 0.01 and trend < 0:
                adj = self._prescribe_warming(metric, current, opt_min, trend)
                if adj is not None:
                    adjustments.append(adj)

        if adjustments:
            self._adjustments_made += len(adjustments)

        return adjustments

    def _compute_trend(self, values: list[float], window: int) -> float:
        """Compute trend direction using simple linear regression slope."""
        if len(values) < 2:
            return 0.0
        recent = values[-window:]
        n = len(recent)
        x_mean = (n - 1) / 2.0
        y_mean = sum(recent) / n
        numerator = sum((i - x_mean) * (v - y_mean) for i, v in enumerate(recent))
        denominator = sum((i - x_mean) ** 2 for i in range(n))
        if denominator == 0:
            return 0.0
        return numerator / denominator

    def _prescribe_cooling(
        self,
        metric: str,
        current: float,
        opt_max: float,
        trend: float,
    ) -> ParameterAdjustment | None:
        """Prescribe a small decrease to pull metric back from upper bound."""
        # Map metrics to adjustable parameters
        cooling_params: dict[str, tuple[str, float]] = {
            "synapse.cycle.latency_ms": ("synapse.clock.current_period_ms", 10),
            "memory.retrieval.latency_ms": ("memory.retrieval.cache_ttl_seconds", 60),
            "synapse.resources.memory_mb": ("evo.consolidation.batch_size", -2),
        }
        entry = cooling_params.get(metric)
        if entry is None:
            return None

        param_path, delta = entry
        opt_min = self._ranges[metric][0]

        return ParameterAdjustment(
            metric_name=metric,
            current_value=current,
            optimal_min=opt_min,
            optimal_max=opt_max,
            adjustment=ParameterFix(
                parameter_path=param_path,
                delta=delta,
                reason=f"Homeostatic cooling: {metric} at {current:.1f} (max: {opt_max})",
            ),
            trend_direction="rising",
        )

    def _prescribe_warming(
        self,
        metric: str,
        current: float,
        opt_min: float,
        trend: float,
    ) -> ParameterAdjustment | None:
        """Prescribe a small increase to pull metric back from lower bound."""
        warming_params: dict[str, tuple[str, float]] = {
            "atune.coherence.phi": ("synapse.clock.current_period_ms", -5),
            "evo.self_model.success_rate": ("evo.hypothesis.min_evidence", -1),
            "nova.intent_rate": ("synapse.clock.current_period_ms", -10),
        }
        entry = warming_params.get(metric)
        if entry is None:
            return None

        param_path, delta = entry
        opt_max = self._ranges[metric][1]

        return ParameterAdjustment(
            metric_name=metric,
            current_value=current,
            optimal_min=opt_min,
            optimal_max=opt_max,
            adjustment=ParameterFix(
                parameter_path=param_path,
                delta=delta,
                reason=f"Homeostatic warming: {metric} at {current:.2f} (min: {opt_min})",
            ),
            trend_direction="falling",
        )

    @property
    def metrics_in_range(self) -> int:
        """Count of metrics currently within optimal range."""
        count = 0
        for metric, (opt_min, opt_max, _) in self._ranges.items():
            history = self._history.get(metric, [])
            if history:
                current = history[-1]
                if opt_min <= current <= opt_max:
                    count += 1
        return count

    @property
    def metrics_total(self) -> int:
        return len(self._ranges)

    @property
    def adjustments_count(self) -> int:
        return self._adjustments_made

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\sentinels.py =====

"""
EcodiaOS — Thymos Sentinel Layer (Detection)

Sentinels are the sensory organs of the immune system. They instrument
every system boundary to capture failures, anomalies, contract violations,
and degradation trends.

Five sentinel classes:
  1. ExceptionSentinel  — unhandled exceptions with full context
  2. ContractSentinel   — inter-system SLA violations
  3. FeedbackLoopSentinel — severed cognitive feedback loops
  4. DriftSentinel      — statistical process control for gradual degradation
  5. CognitiveStallSentinel — workspace cycle producing nothing
"""

from __future__ import annotations

import hashlib
import math
import traceback
from collections import deque
from datetime import timedelta
from typing import Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.thymos.types import (
    ContractSLA,
    DriftConfig,
    FeedbackLoop,
    Incident,
    IncidentClass,
    IncidentSeverity,
    StallConfig,
)

logger = structlog.get_logger()


# ─── System Dependency Graph ────────────────────────────────────


# Which systems directly affect the user
_USER_FACING_SYSTEMS = frozenset({"voxis", "alive", "atune"})

# Downstream impact map: if system X fails, these are affected
_DOWNSTREAM: dict[str, list[str]] = {
    "memory": ["atune", "nova", "evo", "voxis", "simula", "federation"],
    "equor": ["nova", "axon", "simula", "federation"],
    "atune": ["nova", "evo", "voxis"],
    "nova": ["axon", "voxis"],
    "voxis": [],
    "axon": [],
    "evo": ["simula"],
    "simula": [],
    "synapse": ["atune", "nova", "evo", "memory"],
    "federation": [],
}

# Total number of cognitive systems
_TOTAL_SYSTEMS = 11


# ─── Exception Sentinel ─────────────────────────────────────────


class ExceptionSentinel:
    """
    Intercepts unhandled exceptions from system methods.
    Creates Incidents with full diagnostic context.

    Does NOT suppress the exception — it propagates normally after capture.
    The goal is observation, not intervention.
    """

    def __init__(self) -> None:
        self._logger = logger.bind(system="thymos", component="exception_sentinel")

    def intercept(
        self,
        system_id: str,
        method_name: str,
        exception: BaseException,
        context: dict[str, Any] | None = None,
    ) -> Incident:
        """Create an Incident from an unhandled exception."""
        fp = self.fingerprint(system_id, method_name, exception)
        affected = _DOWNSTREAM.get(system_id, [])
        blast = len(affected) / _TOTAL_SYSTEMS

        return Incident(
            timestamp=utc_now(),
            incident_class=IncidentClass.CRASH,
            severity=self._assess_severity(system_id, exception),
            fingerprint=fp,
            source_system=system_id,
            error_type=type(exception).__name__,
            error_message=str(exception)[:500],
            stack_trace=traceback.format_exc()[:2000],
            context={
                "method": method_name,
                **(context or {}),
            },
            affected_systems=affected,
            blast_radius=blast,
            user_visible=system_id in _USER_FACING_SYSTEMS,
            constitutional_impact=self._assess_constitutional_impact(system_id),
        )

    def fingerprint(
        self,
        system_id: str,
        method: str,
        exc: BaseException,
    ) -> str:
        """
        Create a stable fingerprint for deduplication.

        Hash of: system_id + exception type + first frame in our code.
        Groups "same bug, different call path" together while
        distinguishing genuinely different errors.
        """
        first_frame = self._extract_first_local_frame(exc)
        raw = f"{system_id}:{type(exc).__name__}:{first_frame}"
        return hashlib.sha256(raw.encode()).hexdigest()[:16]

    def _extract_first_local_frame(self, exc: BaseException) -> str:
        """Extract the first stack frame from our code (not library)."""
        tb = traceback.extract_tb(exc.__traceback__) if exc.__traceback__ else []
        for frame in reversed(tb):
            if "ecodiaos" in frame.filename:
                return f"{frame.filename}:{frame.lineno}:{frame.name}"
        # Fallback: last frame
        if tb:
            f = tb[-1]
            return f"{f.filename}:{f.lineno}:{f.name}"
        return "unknown"

    def _assess_severity(
        self,
        system_id: str,
        exception: BaseException,
    ) -> IncidentSeverity:
        """Initial severity based on system criticality and exception type."""
        # Critical systems crashing is always HIGH+
        if system_id in ("equor", "memory", "atune", "synapse"):
            return IncidentSeverity.CRITICAL
        if system_id in ("nova", "voxis", "axon"):
            return IncidentSeverity.HIGH
        # Non-critical systems
        if isinstance(exception, (TimeoutError, ConnectionError)):
            return IncidentSeverity.MEDIUM
        return IncidentSeverity.MEDIUM

    def _assess_constitutional_impact(self, system_id: str) -> dict[str, float]:
        """Estimate impact on each drive when a system fails."""
        impacts: dict[str, dict[str, float]] = {
            "equor": {"coherence": 0.9, "care": 0.5, "growth": 0.3, "honesty": 0.9},
            "memory": {"coherence": 0.8, "care": 0.3, "growth": 0.7, "honesty": 0.4},
            "atune": {"coherence": 0.7, "care": 0.4, "growth": 0.5, "honesty": 0.3},
            "nova": {"coherence": 0.6, "care": 0.3, "growth": 0.5, "honesty": 0.2},
            "voxis": {"coherence": 0.2, "care": 0.7, "growth": 0.1, "honesty": 0.6},
            "axon": {"coherence": 0.3, "care": 0.5, "growth": 0.2, "honesty": 0.1},
            "evo": {"coherence": 0.2, "care": 0.1, "growth": 0.8, "honesty": 0.1},
            "simula": {"coherence": 0.1, "care": 0.1, "growth": 0.7, "honesty": 0.1},
            "synapse": {"coherence": 0.8, "care": 0.3, "growth": 0.3, "honesty": 0.2},
        }
        return impacts.get(
            system_id,
            {"coherence": 0.1, "care": 0.1, "growth": 0.1, "honesty": 0.1},
        )


# ─── Contract Sentinel ──────────────────────────────────────────


# Inter-system contract SLAs from the Architecture Spec §IV
DEFAULT_CONTRACT_SLAS: list[ContractSLA] = [
    ContractSLA(source="atune", target="memory", operation="store_percept", max_latency_ms=100),
    ContractSLA(source="memory", target="atune", operation="retrieval", max_latency_ms=200),
    ContractSLA(source="atune", target="all", operation="broadcast", max_latency_ms=50),
    ContractSLA(source="nova", target="equor", operation="review", max_latency_ms=500),
    ContractSLA(source="nova", target="equor", operation="review_critical", max_latency_ms=50),
]


class ContractSentinel:
    """
    Instruments inter-system calls to verify SLA compliance.

    Does NOT add latency to the call itself — measurements are taken
    around the existing call, not inserted into it. The sentinel
    observes the event bus, not the call stack.
    """

    def __init__(self, slas: list[ContractSLA] | None = None) -> None:
        self._slas: dict[tuple[str, str, str], ContractSLA] = {}
        for sla in slas or DEFAULT_CONTRACT_SLAS:
            self._slas[(sla.source, sla.target, sla.operation)] = sla
        self._logger = logger.bind(system="thymos", component="contract_sentinel")

    def check_contract(
        self,
        source: str,
        target: str,
        operation: str,
        latency_ms: float,
    ) -> Incident | None:
        """Check if an inter-system call violated its SLA."""
        sla = self._slas.get((source, target, operation))
        if sla is None:
            return None

        if latency_ms <= sla.max_latency_ms:
            return None

        overshoot = latency_ms / sla.max_latency_ms
        fp = hashlib.sha256(
            f"contract:{source}:{target}:{operation}".encode()
        ).hexdigest()[:16]

        return Incident(
            timestamp=utc_now(),
            incident_class=IncidentClass.CONTRACT_VIOLATION,
            severity=IncidentSeverity.MEDIUM,
            fingerprint=fp,
            source_system=source,
            error_type="ContractViolation",
            error_message=(
                f"Contract violation: {source}→{target}.{operation} "
                f"took {latency_ms:.0f}ms (SLA: {sla.max_latency_ms}ms)"
            ),
            context={
                "expected_ms": sla.max_latency_ms,
                "actual_ms": latency_ms,
                "overshoot_factor": overshoot,
                "target_system": target,
                "operation": operation,
            },
            affected_systems=[source, target],
            blast_radius=2 / _TOTAL_SYSTEMS,
            user_visible=target in _USER_FACING_SYSTEMS,
        )


# ─── Feedback Loop Sentinel ─────────────────────────────────────


# The feedback loops identified in the architecture audit
DEFAULT_FEEDBACK_LOOPS: list[FeedbackLoop] = [
    FeedbackLoop(
        name="top_down_prediction",
        source="nova",
        target="atune",
        signal="belief_state",
        check="atune.has_received_beliefs_in_last_n_cycles(10)",
        description="Nova beliefs → Atune prediction error modeling",
    ),
    FeedbackLoop(
        name="goal_guided_attention",
        source="nova",
        target="atune",
        signal="active_goals",
        check="atune.salience_head_weights_include_goal_component()",
        description="Nova goals → Atune salience weighting",
    ),
    FeedbackLoop(
        name="expression_feedback",
        source="voxis",
        target="atune",
        signal="expression_feedback",
        check="atune.has_received_expression_feedback_in_last_n_cycles(100)",
        description="Voxis expression → Atune learning signal",
    ),
    FeedbackLoop(
        name="evo_head_weights",
        source="evo",
        target="atune",
        signal="head_weight_adjustments",
        check="atune.has_received_evo_adjustments()",
        description="Evo learned weights → Atune meta-attention",
    ),
    FeedbackLoop(
        name="axon_outcome_beliefs",
        source="axon",
        target="nova",
        signal="action_outcomes",
        check="nova.has_received_outcomes_in_last_n_cycles(100)",
        description="Axon action outcomes → Nova belief updates",
    ),
    FeedbackLoop(
        name="memory_salience_decay",
        source="memory",
        target="memory",
        signal="salience_decay",
        check="memory.salience_decay_running()",
        description="Memory salience decay over time",
    ),
    FeedbackLoop(
        name="personality_evolution",
        source="voxis",
        target="evo",
        signal="expression_effectiveness",
        check="evo.has_personality_evidence()",
        description="Voxis expression → Evo personality tuning",
    ),
    FeedbackLoop(
        name="rhythm_modulation",
        source="synapse",
        target="nova",
        signal="rhythm_state",
        check="nova.receives_rhythm_updates()",
        description="Synapse rhythm → Nova decision thresholds",
    ),
    FeedbackLoop(
        name="consolidation_weights",
        source="evo",
        target="atune",
        signal="parameter_adjustments",
        check="atune.has_evo_parameters()",
        description="Evo consolidation → Atune salience head weights",
    ),
    FeedbackLoop(
        name="drive_weight_modulation",
        source="equor",
        target="equor",
        signal="contextual_drive_weights",
        check="equor.drive_weights_modulated()",
        description="Context → dynamic drive weighting",
    ),
    FeedbackLoop(
        name="affect_expression",
        source="atune",
        target="voxis",
        signal="affect_state",
        check="voxis.uses_affect_for_style()",
        description="Atune affect → Voxis expression style",
    ),
    FeedbackLoop(
        name="federation_trust_access",
        source="federation",
        target="federation",
        signal="trust_level_changes",
        check="federation.trust_updates_permissions()",
        description="Trust level changes → knowledge exchange permissions",
    ),
    FeedbackLoop(
        name="simula_version_params",
        source="simula",
        target="synapse",
        signal="config_version",
        check="synapse.uses_current_config_version()",
        description="Simula config changes → system parameter propagation",
    ),
    FeedbackLoop(
        name="coherence_safe_mode",
        source="synapse",
        target="synapse",
        signal="coherence_level",
        check="synapse.coherence_triggers_safe_mode()",
        description="Low coherence → safe mode consideration",
    ),
    FeedbackLoop(
        name="community_schema",
        source="memory",
        target="evo",
        signal="community_detection",
        check="evo.uses_community_structure()",
        description="Neo4j community detection → Evo schema induction",
    ),
]


class FeedbackLoopSentinel:
    """
    Periodically verifies that each feedback loop is actively transmitting.

    Unlike heartbeats (which check "is the system alive?"), this checks
    "is the system CONNECTED?" A system can be alive but disconnected
    from the cognitive cycle — like a nerve that's intact but severed
    from the brain.
    """

    def __init__(self, loops: list[FeedbackLoop] | None = None) -> None:
        self._loops = loops or DEFAULT_FEEDBACK_LOOPS
        # Track which loops have been verified as connected
        self._loop_status: dict[str, bool] = {
            loop.name: False for loop in self._loops
        }
        self._last_check: dict[str, float] = {}  # loop_name → timestamp
        self._logger = logger.bind(system="thymos", component="feedback_loop_sentinel")

    def report_loop_active(self, loop_name: str) -> None:
        """Called when evidence of a loop transmitting is observed."""
        self._loop_status[loop_name] = True
        self._last_check[loop_name] = utc_now().timestamp()

    def check_loops(self, max_staleness_s: float = 30.0) -> list[Incident]:
        """
        Check all loops. Returns incidents for any that aren't transmitting.

        A loop is considered severed if:
        - It has never been observed as active, OR
        - It was last seen active more than max_staleness_s seconds ago
        """
        now = utc_now()
        incidents: list[Incident] = []

        for loop in self._loops:
            connected = self._loop_status.get(loop.name, False)
            last_ts = self._last_check.get(loop.name)

            if connected and last_ts is not None:
                age_s = now.timestamp() - last_ts
                if age_s <= max_staleness_s:
                    continue  # Loop is fresh

            fp = hashlib.sha256(f"loop:{loop.name}".encode()).hexdigest()[:16]
            incidents.append(
                Incident(
                    timestamp=now,
                    incident_class=IncidentClass.LOOP_SEVERANCE,
                    severity=IncidentSeverity.HIGH,
                    fingerprint=fp,
                    source_system=loop.source,
                    error_type="FeedbackLoopSevered",
                    error_message=(
                        f"Feedback loop '{loop.name}' is not transmitting: "
                        f"{loop.description}"
                    ),
                    context={
                        "loop_name": loop.name,
                        "source": loop.source,
                        "target": loop.target,
                        "signal": loop.signal,
                    },
                    affected_systems=[loop.source, loop.target],
                    blast_radius=2 / _TOTAL_SYSTEMS,
                    user_visible=False,
                )
            )

        return incidents

    @property
    def loop_statuses(self) -> dict[str, bool]:
        """Current status of all feedback loops."""
        return dict(self._loop_status)


# ─── Drift Sentinel ─────────────────────────────────────────────


class _RollingBaseline:
    """Exponential moving average + standard deviation tracker."""

    def __init__(self, window: int) -> None:
        self._window = window
        self._values: deque[float] = deque(maxlen=window)
        self._ema: float = 0.0
        self._ema_sq: float = 0.0
        self._alpha: float = 2.0 / (window + 1)
        self._count: int = 0

    @property
    def is_warmed_up(self) -> bool:
        return self._count >= self._window // 4  # 25% of window

    def update(self, value: float) -> None:
        self._values.append(value)
        self._count += 1
        if self._count == 1:
            self._ema = value
            self._ema_sq = value * value
        else:
            self._ema = self._alpha * value + (1 - self._alpha) * self._ema
            self._ema_sq = (
                self._alpha * (value * value)
                + (1 - self._alpha) * self._ema_sq
            )

    @property
    def mean(self) -> float:
        return self._ema

    @property
    def std(self) -> float:
        variance = max(0.0, self._ema_sq - self._ema * self._ema)
        return math.sqrt(variance)

    def z_score(self, value: float) -> float:
        s = self.std
        if s < 1e-9:
            return 0.0
        return (value - self._ema) / s


# Default metrics to monitor for drift
DEFAULT_DRIFT_METRICS: dict[str, DriftConfig] = {
    "synapse.cycle.latency_ms": DriftConfig(window=1000, sigma_threshold=2.5),
    "memory.retrieval.latency_ms": DriftConfig(window=500, sigma_threshold=2.0),
    "atune.salience.processing_ms": DriftConfig(window=500, sigma_threshold=2.0),
    "nova.efe.computation_ms": DriftConfig(window=500, sigma_threshold=2.5),
    "voxis.generation.latency_ms": DriftConfig(window=300, sigma_threshold=2.0),
    "synapse.resources.memory_mb": DriftConfig(
        window=200, sigma_threshold=3.0, direction="above"
    ),
    "evo.self_model.success_rate": DriftConfig(
        window=100, sigma_threshold=2.0, direction="below"
    ),
    "atune.coherence.phi": DriftConfig(
        window=200, sigma_threshold=2.0, direction="below"
    ),
}


class DriftSentinel:
    """
    Statistical process control for system metrics.

    Maintains a rolling baseline (EMA + std dev) for each metric.
    When a metric deviates beyond the control limits, it's flagged.

    This catches:
    - Memory leaks (gradual increase in memory_mb)
    - Latency creep (gradually slower responses)
    - Accuracy decay (prediction errors gradually increasing)
    - Throughput degradation (cycles/second declining)

    Adapts to the organism's actual operating characteristics.
    """

    def __init__(
        self,
        metrics: dict[str, DriftConfig] | None = None,
    ) -> None:
        self._metrics = metrics or DEFAULT_DRIFT_METRICS
        self._baselines: dict[str, _RollingBaseline] = {}
        for name, cfg in self._metrics.items():
            self._baselines[name] = _RollingBaseline(cfg.window)
        self._logger = logger.bind(system="thymos", component="drift_sentinel")

    def record_metric(self, metric_name: str, value: float) -> Incident | None:
        """
        Record a metric value and check for drift.
        Returns an Incident if drift is detected, None otherwise.
        """
        config = self._metrics.get(metric_name)
        if config is None:
            return None

        baseline = self._baselines[metric_name]
        baseline.update(value)

        if not baseline.is_warmed_up:
            return None

        z = baseline.z_score(value)

        is_drift = False
        if config.direction == "above" and z > config.sigma_threshold:
            is_drift = True
        elif config.direction == "below" and z < -config.sigma_threshold:
            is_drift = True
        elif config.direction is None and abs(z) > config.sigma_threshold:
            is_drift = True

        if not is_drift:
            return None

        fp = hashlib.sha256(f"drift:{metric_name}".encode()).hexdigest()[:16]
        system_id = metric_name.split(".")[0] if "." in metric_name else "unknown"

        return Incident(
            timestamp=utc_now(),
            incident_class=IncidentClass.DRIFT,
            severity=IncidentSeverity.MEDIUM,
            fingerprint=fp,
            source_system=system_id,
            error_type="MetricDrift",
            error_message=(
                f"Metric '{metric_name}' drifting: "
                f"value={value:.2f}, baseline={baseline.mean:.2f}, "
                f"z-score={z:.2f} (threshold: ±{config.sigma_threshold})"
            ),
            context={
                "metric_name": metric_name,
                "current_value": value,
                "baseline_mean": baseline.mean,
                "baseline_std": baseline.std,
                "z_score": z,
                "direction": config.direction or "both",
            },
            blast_radius=0.1,
            user_visible=False,
        )

    @property
    def baselines(self) -> dict[str, dict[str, float]]:
        """Current baseline statistics for all monitored metrics."""
        return {
            name: {
                "mean": b.mean,
                "std": b.std,
                "warmed_up": b.is_warmed_up,
                "samples": b._count,
            }
            for name, b in self._baselines.items()
        }


# ─── Cognitive Stall Sentinel ────────────────────────────────────


# Default stall thresholds
DEFAULT_STALL_THRESHOLDS: dict[str, StallConfig] = {
    "broadcast_ack_rate": StallConfig(min_value=0.3, window_cycles=50),
    "nova_intent_rate": StallConfig(min_value=0.01, window_cycles=200),
    "evo_evidence_rate": StallConfig(min_value=0.001, window_cycles=500),
    "atune_percept_rate": StallConfig(min_value=0.1, window_cycles=50),
}


class CognitiveStallSentinel:
    """
    Detects when the cognitive cycle is running but accomplishing nothing.

    The heartbeat is fine. The systems are "healthy." But nothing is
    happening. The organism is not thinking.

    This is the equivalent of a person who is conscious but catatonic.
    """

    def __init__(
        self,
        thresholds: dict[str, StallConfig] | None = None,
    ) -> None:
        # Merge custom thresholds with defaults to ensure all expected keys exist
        self._thresholds = DEFAULT_STALL_THRESHOLDS.copy()
        if thresholds:
            self._thresholds.update(thresholds)

        self._counters: dict[str, deque[float]] = {
            name: deque(maxlen=cfg.window_cycles)
            for name, cfg in self._thresholds.items()
        }
        self._logger = logger.bind(system="thymos", component="stall_sentinel")

    def record_cycle(
        self,
        had_broadcast: bool,
        nova_had_intent: bool,
        evo_had_evidence: bool,
        atune_had_percept: bool,
    ) -> list[Incident]:
        """Record one cognitive cycle's activity and check for stalls."""
        self._counters["broadcast_ack_rate"].append(1.0 if had_broadcast else 0.0)
        self._counters["nova_intent_rate"].append(1.0 if nova_had_intent else 0.0)
        self._counters["evo_evidence_rate"].append(1.0 if evo_had_evidence else 0.0)
        self._counters["atune_percept_rate"].append(1.0 if atune_had_percept else 0.0)

        incidents: list[Incident] = []
        now = utc_now()

        for name, cfg in self._thresholds.items():
            window = self._counters[name]
            if len(window) < cfg.window_cycles:
                continue  # Not enough data yet

            rate = sum(window) / len(window)
            if rate >= cfg.min_value:
                continue  # Above threshold, no stall

            fp = hashlib.sha256(f"stall:{name}".encode()).hexdigest()[:16]
            incidents.append(
                Incident(
                    timestamp=now,
                    incident_class=IncidentClass.COGNITIVE_STALL,
                    severity=IncidentSeverity.HIGH,
                    fingerprint=fp,
                    source_system="synapse",
                    error_type="CognitiveStall",
                    error_message=(
                        f"Cognitive stall: '{name}' rate is {rate:.4f} "
                        f"(minimum: {cfg.min_value}) over {cfg.window_cycles} cycles"
                    ),
                    context={
                        "metric_name": name,
                        "rate": rate,
                        "threshold": cfg.min_value,
                        "window_cycles": cfg.window_cycles,
                    },
                    blast_radius=0.5,
                    user_visible=True,  # Catatonic organism affects users
                )
            )

        return incidents

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\service.py =====

"""
EcodiaOS — Thymos Service (The Immune System)

The organism's self-healing system. Thymos detects failures, diagnoses root causes,
prescribes repairs, maintains an antibody library of learned fixes, and prevents
future errors through prophylactic scanning and homeostatic regulation.

Every error, anomaly, and violation becomes an Incident — a first-class primitive
alongside Percept, Belief, and Intent. The organism perceives its own failures
through the normal workspace broadcast cycle. It hurts to break.

Immune Pipeline:
  Detect → Deduplicate → Triage → Diagnose → Prescribe → Validate → Apply → Verify → Learn

Iron Rules:
  - Thymos CANNOT modify Equor or constitutional drives
  - Thymos CANNOT suppress or hide errors from the audit trail
  - Thymos CANNOT apply Tier 4 (codegen) repairs without Equor review
  - Thymos CANNOT exceed the healing budget (MAX_REPAIRS_PER_HOUR)
  - Thymos MUST route all Tier 3+ repairs through the validation gate
  - Thymos MUST record every incident, diagnosis, and repair in Neo4j
  - Thymos MUST prefer less invasive repairs (Tier 0 before Tier 1 before ...)
  - Thymos MUST enter storm mode when incident rate exceeds threshold

Cognitive cycle role (step 8 — MAINTAIN):
  Homeostatic checks run on the MAINTAIN step. Non-blocking, background.
  The organism maintains itself the way a body regulates temperature.

Interface:
  initialize()              — build sub-systems, load antibody library
  on_synapse_event()        — convert health events into incidents
  on_incident()             — entry point for the immune pipeline
  process_incident()        — full pipeline: diagnose → prescribe → validate → apply
  maintain_homeostasis()    — proactive health optimization (MAINTAIN step)
  shutdown()                — graceful teardown
  health()                  — self-health report for Synapse
"""

from __future__ import annotations

import asyncio
import hashlib
import time
from collections import deque
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.synapse.types import SynapseEvent, SynapseEventType
from ecodiaos.systems.thymos.antibody import AntibodyLibrary
from ecodiaos.systems.thymos.diagnosis import (
    CausalAnalyzer,
    DiagnosticEngine,
    TemporalCorrelator,
)
from ecodiaos.systems.thymos.governor import HealingGovernor
from ecodiaos.systems.thymos.prescription import RepairPrescriber, RepairValidator
from ecodiaos.systems.thymos.prophylactic import HomeostasisController, ProphylacticScanner
from ecodiaos.systems.thymos.sentinels import (
    CognitiveStallSentinel,
    ContractSentinel,
    DriftSentinel,
    ExceptionSentinel,
    FeedbackLoopSentinel,
)
from ecodiaos.systems.thymos.triage import (
    IncidentDeduplicator,
    ResponseRouter,
    SeverityScorer,
)
from ecodiaos.systems.thymos.types import (
    Diagnosis,
    HealingMode,
    Incident,
    IncidentClass,
    IncidentSeverity,
    RepairSpec,
    RepairStatus,
    RepairTier,
    ThymosHealthSnapshot,
    ValidationResult,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.clients.neo4j import Neo4jClient
    from ecodiaos.config import ThymosConfig
    from ecodiaos.systems.synapse.health import HealthMonitor
    from ecodiaos.systems.synapse.service import SynapseService
    from ecodiaos.telemetry.metrics import MetricCollector

logger = structlog.get_logger("ecodiaos.systems.thymos")


# ─── Constants ──────────────────────────────────────────────────

# Synapse events that Thymos converts into Incidents
_SUBSCRIBED_EVENTS: frozenset[SynapseEventType] = frozenset({
    SynapseEventType.SYSTEM_FAILED,
    SynapseEventType.SYSTEM_RECOVERED,
    SynapseEventType.SYSTEM_RESTARTING,
    SynapseEventType.SAFE_MODE_ENTERED,
    SynapseEventType.SAFE_MODE_EXITED,
    SynapseEventType.SYSTEM_OVERLOADED,
    SynapseEventType.CLOCK_OVERRUN,
    SynapseEventType.RESOURCE_PRESSURE,
})

# How often to run homeostatic checks (in seconds)
_HOMEOSTASIS_INTERVAL_S: float = 30.0

# How often to run sentinel scans (in seconds)
_SENTINEL_SCAN_INTERVAL_S: float = 30.0

# How long to wait for post-repair verification (seconds)
_POST_REPAIR_VERIFY_TIMEOUT_S: float = 10.0

# Salience mapping: incident severity → percept priority for Atune
_SEVERITY_TO_SALIENCE: dict[IncidentSeverity, float] = {
    IncidentSeverity.CRITICAL: 1.0,
    IncidentSeverity.HIGH: 0.8,
    IncidentSeverity.MEDIUM: 0.5,
    IncidentSeverity.LOW: 0.2,
    IncidentSeverity.INFO: 0.1,
}

# Maximum incidents to buffer in memory
_INCIDENT_BUFFER_SIZE: int = 10_000


class ThymosService:
    """
    Thymos — the EOS immune system.

    Coordinates seven sub-systems:
      Sentinels              — fault detection (5 sentinel classes)
      Triage                 — deduplication, severity scoring, response routing
      Diagnosis              — causal analysis, temporal correlation, hypothesis engine
      Prescription           — repair tier selection and validation gate
      AntibodyLibrary        — immune memory: crystallized successful repairs
      Prophylactic           — prevention: pre-deploy scans and homeostasis
      HealingGovernor        — cytokine storm prevention and budget enforcement
    """

    system_id: str = "thymos"

    def __init__(
        self,
        config: ThymosConfig,
        synapse: SynapseService | None = None,
        neo4j: Neo4jClient | None = None,
        llm: LLMProvider | None = None,
        metrics: MetricCollector | None = None,
    ) -> None:
        self._config = config
        self._synapse = synapse
        self._neo4j = neo4j
        self._llm = llm
        self._metrics = metrics
        self._initialized: bool = False
        self._logger = logger.bind(system="thymos")

        # Cross-system references (wired post-init by main.py)
        self._equor: Any = None     # EquorService — constitutional review
        self._evo: Any = None       # EvoService — error pattern learning
        self._atune: Any = None     # AtuneService — incident-as-percept
        self._health_monitor: HealthMonitor | None = None  # Synapse health records

        # ── Sub-systems (built in initialize()) ──
        # Sentinels
        self._exception_sentinel: ExceptionSentinel | None = None
        self._contract_sentinel: ContractSentinel | None = None
        self._feedback_loop_sentinel: FeedbackLoopSentinel | None = None
        self._drift_sentinel: DriftSentinel | None = None
        self._cognitive_stall_sentinel: CognitiveStallSentinel | None = None

        # Triage
        self._deduplicator: IncidentDeduplicator | None = None
        self._severity_scorer: SeverityScorer | None = None
        self._response_router: ResponseRouter | None = None

        # Diagnosis
        self._causal_analyzer: CausalAnalyzer | None = None
        self._temporal_correlator: TemporalCorrelator | None = None
        self._diagnostic_engine: DiagnosticEngine | None = None

        # Prescription
        self._prescriber: RepairPrescriber | None = None
        self._validator: RepairValidator | None = None

        # Immune memory
        self._antibody_library: AntibodyLibrary | None = None

        # Prophylactic
        self._prophylactic_scanner: ProphylacticScanner | None = None
        self._homeostasis_controller: HomeostasisController | None = None

        # Governor
        self._governor: HealingGovernor | None = None

        # Cross-system references (wired post-init by main.py)
        self._nova: Any = None  # NovaService — for injecting urgent repair goals

        # ── State ──
        self._active_incidents: dict[str, Incident] = {}
        self._incident_buffer: deque[Incident] = deque(maxlen=_INCIDENT_BUFFER_SIZE)
        self._resolution_times: deque[float] = deque(maxlen=500)

        # Background tasks
        self._sentinel_task: asyncio.Task[None] | None = None
        self._homeostasis_task: asyncio.Task[None] | None = None

        # Counters
        self._total_incidents: int = 0
        self._total_repairs_attempted: int = 0
        self._total_repairs_succeeded: int = 0
        self._total_repairs_failed: int = 0
        self._total_repairs_rolled_back: int = 0
        self._total_diagnoses: int = 0
        self._total_antibodies_applied: int = 0
        self._total_antibodies_created: int = 0
        self._total_homeostatic_adjustments: int = 0
        self._total_prophylactic_scans: int = 0
        self._total_prophylactic_warnings: int = 0
        self._incidents_by_severity: dict[str, int] = {}
        self._incidents_by_class: dict[str, int] = {}
        self._repairs_by_tier: dict[str, int] = {}
        self._diagnosis_confidences: deque[float] = deque(maxlen=200)
        self._diagnosis_latencies: deque[float] = deque(maxlen=200)

    # ─── Cross-System Wiring ─────────────────────────────────────────

    def set_equor(self, equor: Any) -> None:
        """Wire Equor for constitutional review of Tier 3+ repairs."""
        self._equor = equor
        if self._validator is not None:
            self._validator._equor = equor
        self._logger.info("equor_wired_to_thymos")

    def set_evo(self, evo: Any) -> None:
        """Wire Evo so repair outcomes feed the learning system."""
        self._evo = evo
        self._logger.info("evo_wired_to_thymos")

    def set_atune(self, atune: Any) -> None:
        """Wire Atune so high-severity incidents become Percepts."""
        self._atune = atune
        self._logger.info("atune_wired_to_thymos")

    def set_health_monitor(self, health_monitor: HealthMonitor) -> None:
        """Wire Synapse HealthMonitor for health record queries."""
        self._health_monitor = health_monitor
        if self._causal_analyzer is not None:
            self._causal_analyzer._health = health_monitor
        self._logger.info("health_monitor_wired_to_thymos")

    def set_nova(self, nova: Any) -> None:
        """Wire Nova so critical incidents generate urgent repair goals."""
        self._nova = nova
        self._logger.info("nova_wired_to_thymos")

    # ─── Lifecycle ─────────────────────────────────────────────────────

    async def initialize(self) -> None:
        """
        Build all sub-systems, load the antibody library, and subscribe
        to Synapse health events.
        """
        if self._initialized:
            return

        # ── Sentinels ──
        self._exception_sentinel = ExceptionSentinel()
        self._contract_sentinel = ContractSentinel()
        self._feedback_loop_sentinel = FeedbackLoopSentinel()
        self._drift_sentinel = DriftSentinel()
        self._cognitive_stall_sentinel = CognitiveStallSentinel()

        # ── Triage ──
        self._deduplicator = IncidentDeduplicator()
        self._severity_scorer = SeverityScorer()
        self._response_router = ResponseRouter()

        # ── Diagnosis ──
        self._causal_analyzer = CausalAnalyzer(health_provider=self._health_monitor)
        self._temporal_correlator = TemporalCorrelator()
        self._diagnostic_engine = DiagnosticEngine(llm_client=self._llm)

        # ── Prescription ──
        self._prescriber = RepairPrescriber()
        self._validator = RepairValidator(equor=self._equor)

        # ── Antibody Library ──
        self._antibody_library = AntibodyLibrary(neo4j_client=self._neo4j)
        await self._antibody_library.initialize()

        # ── Prophylactic ──
        self._prophylactic_scanner = ProphylacticScanner(
            antibody_library=self._antibody_library,
        )
        self._homeostasis_controller = HomeostasisController()

        # ── Governor ──
        self._governor = HealingGovernor()

        # ── Subscribe to Synapse Events ──
        if self._synapse is not None:
            event_bus = self._synapse._event_bus
            for event_type in _SUBSCRIBED_EVENTS:
                event_bus.subscribe(event_type, self._on_synapse_event)

        # ── Start background loops ──
        self._sentinel_task = asyncio.create_task(
            self._sentinel_scan_loop(),
            name="thymos_sentinel_scan",
        )
        self._homeostasis_task = asyncio.create_task(
            self._homeostasis_loop(),
            name="thymos_homeostasis",
        )

        self._initialized = True

        antibody_count = len(self._antibody_library._all) if self._antibody_library else 0
        self._logger.info(
            "thymos_initialized",
            antibodies_loaded=antibody_count,
            subscribed_events=len(_SUBSCRIBED_EVENTS),
        )

    async def shutdown(self) -> None:
        """Graceful shutdown. Cancel background tasks and log final stats."""
        self._logger.info("thymos_shutting_down")

        # Cancel background tasks
        for task in (self._sentinel_task, self._homeostasis_task):
            if task is not None and not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass

        self._sentinel_task = None
        self._homeostasis_task = None

        self._logger.info(
            "thymos_shutdown",
            total_incidents=self._total_incidents,
            total_repairs_attempted=self._total_repairs_attempted,
            total_repairs_succeeded=self._total_repairs_succeeded,
            active_incidents=len(self._active_incidents),
            antibodies_total=(
                len(self._antibody_library._all) if self._antibody_library else 0
            ),
        )

    # ─── Synapse Event Handler ───────────────────────────────────────

    async def _on_synapse_event(self, event: SynapseEvent) -> None:
        """
        Convert Synapse health events into Incidents.

        This is how Thymos learns about system failures without direct
        coupling to every system. Synapse watches health; Thymos watches
        Synapse events.
        """
        severity, incident_class = self._classify_synapse_event(event)

        if severity is None:
            # Recovery events — resolve any matching active incidents
            if event.event_type in (
                SynapseEventType.SYSTEM_RECOVERED,
                SynapseEventType.SAFE_MODE_EXITED,
            ):
                await self._handle_recovery_event(event)
            return

        source_system = event.data.get("system_id", event.source_system)

        fp = hashlib.sha256(
            f"{source_system}:{event.event_type.value}".encode()
        ).hexdigest()[:16]

        incident = Incident(
            incident_class=incident_class,
            severity=severity,
            fingerprint=fp,
            source_system=source_system,
            error_type=event.event_type.value,
            error_message=(
                f"Synapse health event: {event.event_type.value} "
                f"for {source_system}"
            ),
            context=event.data,
        )

        await self.on_incident(incident)

    def _classify_synapse_event(
        self,
        event: SynapseEvent,
    ) -> tuple[IncidentSeverity | None, IncidentClass]:
        """Map a Synapse event type to incident severity and class."""
        mapping: dict[SynapseEventType, tuple[IncidentSeverity | None, IncidentClass]] = {
            SynapseEventType.SYSTEM_FAILED: (
                IncidentSeverity.CRITICAL,
                IncidentClass.CRASH,
            ),
            SynapseEventType.SYSTEM_RESTARTING: (
                IncidentSeverity.HIGH,
                IncidentClass.CRASH,
            ),
            SynapseEventType.SYSTEM_OVERLOADED: (
                IncidentSeverity.MEDIUM,
                IncidentClass.DEGRADATION,
            ),
            SynapseEventType.SAFE_MODE_ENTERED: (
                IncidentSeverity.CRITICAL,
                IncidentClass.CRASH,
            ),
            SynapseEventType.CLOCK_OVERRUN: (
                IncidentSeverity.MEDIUM,
                IncidentClass.DEGRADATION,
            ),
            SynapseEventType.RESOURCE_PRESSURE: (
                IncidentSeverity.MEDIUM,
                IncidentClass.RESOURCE_EXHAUSTION,
            ),
            # Recovery events — no incident created
            SynapseEventType.SYSTEM_RECOVERED: (None, IncidentClass.CRASH),
            SynapseEventType.SAFE_MODE_EXITED: (None, IncidentClass.CRASH),
        }
        return mapping.get(
            event.event_type,
            (IncidentSeverity.LOW, IncidentClass.DEGRADATION),
        )

    async def _handle_recovery_event(self, event: SynapseEvent) -> None:
        """Resolve active incidents for a recovered system."""
        source_system = event.data.get("system_id", event.source_system)

        resolved_ids: list[str] = []
        for incident_id, incident in list(self._active_incidents.items()):
            if incident.source_system == source_system:
                incident.repair_status = RepairStatus.RESOLVED
                incident.repair_successful = True
                now = utc_now()
                incident.resolution_time_ms = int(
                    (now - incident.timestamp).total_seconds() * 1000
                )
                self._resolution_times.append(float(incident.resolution_time_ms))
                resolved_ids.append(incident_id)

                if self._governor is not None:
                    self._governor.resolve_incident(incident_id)

        for incident_id in resolved_ids:
            self._active_incidents.pop(incident_id, None)

        if resolved_ids:
            self._logger.info(
                "recovery_event_resolved_incidents",
                source_system=source_system,
                resolved_count=len(resolved_ids),
            )

    # ─── Main Entry Point ────────────────────────────────────────────

    async def on_incident(self, incident: Incident) -> None:
        """
        Primary entry point for the immune pipeline.

        Called by sentinels (directly) and Synapse event handler.
        Deduplicates, then routes to the full processing pipeline.

        This method NEVER raises — immune failures must not cascade.
        """
        try:
            await self._on_incident_inner(incident)
        except Exception as exc:
            # Thymos must not crash. Log and continue.
            self._logger.error(
                "thymos_internal_error",
                error=str(exc),
                incident_id=incident.id,
                incident_source=incident.source_system,
            )

    async def _on_incident_inner(self, incident: Incident) -> None:
        """Dedup → score → buffer → route → process."""
        assert self._deduplicator is not None
        assert self._severity_scorer is not None
        assert self._response_router is not None
        assert self._governor is not None

        # Step 1: Deduplicate
        dedup_result = self._deduplicator.deduplicate(incident)
        if dedup_result is None:
            self._logger.debug(
                "incident_deduplicated",
                fingerprint=incident.fingerprint,
                count=incident.occurrence_count,
            )
            return

        # Step 2: Score severity (composite)
        scored_severity = self._severity_scorer.compute_severity(incident)
        incident.severity = scored_severity

        # Step 3: Track
        self._total_incidents += 1
        self._incident_buffer.append(incident)
        self._active_incidents[incident.id] = incident
        self._incidents_by_severity[scored_severity.value] = (
            self._incidents_by_severity.get(scored_severity.value, 0) + 1
        )
        self._incidents_by_class[incident.incident_class.value] = (
            self._incidents_by_class.get(incident.incident_class.value, 0) + 1
        )

        # Register with governor for storm detection
        self._governor.register_incident(incident)

        # Record in temporal correlator
        if self._temporal_correlator is not None:
            self._temporal_correlator.record_event(
                event_type="incident",
                details=f"{incident.incident_class.value}: {incident.error_message}",
                system_id=incident.source_system,
            )

        # Record in causal analyzer
        if self._causal_analyzer is not None:
            self._causal_analyzer.record_incident(incident)

        # Emit telemetry
        self._emit_metric("thymos.incidents.created", 1, tags={"class": incident.incident_class.value})
        self._emit_metric("thymos.incidents.severity", 1, tags={"severity": scored_severity.value})

        self._logger.info(
            "incident_created",
            incident_id=incident.id,
            source_system=incident.source_system,
            incident_class=incident.incident_class.value,
            severity=scored_severity.value,
            fingerprint=incident.fingerprint[:16],
        )

        # Step 4: Make the organism feel it — route to Atune as a Percept
        await self._broadcast_as_percept(incident)

        # Step 5: Route to initial repair tier
        initial_tier = self._response_router.route(incident)
        incident.repair_tier = initial_tier

        # Step 6: Process through the immune pipeline
        if initial_tier == RepairTier.NOOP:
            incident.repair_status = RepairStatus.ACCEPTED
            self._active_incidents.pop(incident.id, None)
            if self._governor is not None:
                self._governor.resolve_incident(incident.id)
            return

        # Process in a background task so we don't block the event bus callback
        asyncio.create_task(
            self._process_incident_safe(incident),
            name=f"thymos_process_{incident.id[:8]}",
        )

    # ─── Immune Pipeline ─────────────────────────────────────────────

    async def _process_incident_safe(self, incident: Incident) -> None:
        """Wrapper that catches errors during incident processing."""
        try:
            await self.process_incident(incident)
        except Exception as exc:
            self._logger.error(
                "incident_processing_failed",
                incident_id=incident.id,
                error=str(exc),
            )
            incident.repair_status = RepairStatus.ESCALATED
            self._active_incidents.pop(incident.id, None)

    async def process_incident(self, incident: Incident) -> None:
        """
        Full immune pipeline: Diagnose → Prescribe → Validate → Apply → Verify → Learn.

        This is the core of Thymos. Each step has clear entry/exit criteria
        and failure modes that escalate to the next tier.
        """
        assert self._governor is not None
        assert self._antibody_library is not None
        assert self._diagnostic_engine is not None
        assert self._causal_analyzer is not None
        assert self._temporal_correlator is not None
        assert self._prescriber is not None
        assert self._validator is not None

        start_time = time.monotonic()

        # ── Step 1: Check governor budget ──
        if not self._governor.should_diagnose(incident):
            self._logger.info(
                "diagnosis_throttled",
                incident_id=incident.id,
                healing_mode=self._governor.healing_mode.value,
            )
            incident.repair_status = RepairStatus.ESCALATED
            self._active_incidents.pop(incident.id, None)
            return

        # ── Step 2: Diagnose ──
        incident.repair_status = RepairStatus.DIAGNOSING
        self._governor.begin_diagnosis()
        self._total_diagnoses += 1

        try:
            diagnosis = await self._diagnose(incident)
        finally:
            self._governor.end_diagnosis()

        diagnosis_ms = (time.monotonic() - start_time) * 1000
        self._diagnosis_latencies.append(diagnosis_ms)
        self._diagnosis_confidences.append(diagnosis.confidence)

        incident.root_cause_hypothesis = diagnosis.root_cause
        incident.diagnostic_confidence = diagnosis.confidence
        if diagnosis.repair_tier is not None:
            incident.repair_tier = diagnosis.repair_tier

        self._emit_metric("thymos.diagnosis.confidence", diagnosis.confidence)
        self._emit_metric("thymos.diagnosis.latency_ms", diagnosis_ms)

        self._logger.info(
            "diagnosis_complete",
            incident_id=incident.id,
            root_cause=diagnosis.root_cause[:80],
            confidence=f"{diagnosis.confidence:.2f}",
            repair_tier=diagnosis.repair_tier.name if diagnosis.repair_tier else "unknown",
            latency_ms=f"{diagnosis_ms:.0f}",
        )

        # ── Step 2b: Inject urgent goal for critical incidents ──
        if incident.severity == IncidentSeverity.CRITICAL and self._nova is not None:
            await self._inject_repair_goal(incident, diagnosis.repair_tier, resolved=False)

        # ── Step 3: Prescribe ──
        incident.repair_status = RepairStatus.PRESCRIBING
        repair = await self._prescriber.prescribe(incident, diagnosis)

        self._logger.info(
            "repair_prescribed",
            incident_id=incident.id,
            tier=repair.tier.name,
            action=repair.action,
        )

        # ── Step 4: Validate ──
        incident.repair_status = RepairStatus.VALIDATING
        validation = await self._validator.validate(incident, repair)

        if not validation.approved:
            self._logger.warning(
                "repair_rejected",
                incident_id=incident.id,
                reason=validation.reason,
            )
            # Escalate or accept the override
            if validation.escalate_to is not None:
                repair = RepairSpec(
                    tier=validation.escalate_to,
                    action="alert_operator" if validation.escalate_to == RepairTier.ESCALATE else repair.action,
                    target_system=repair.target_system,
                    reason=f"Escalated: {validation.reason}",
                )
            else:
                incident.repair_status = RepairStatus.ESCALATED
                self._active_incidents.pop(incident.id, None)
                return

        # ── Step 5: Apply ──
        incident.repair_status = RepairStatus.APPLYING
        self._total_repairs_attempted += 1
        tier_name = repair.tier.name
        self._repairs_by_tier[tier_name] = self._repairs_by_tier.get(tier_name, 0) + 1
        self._governor.record_repair(repair.tier)

        self._emit_metric("thymos.repairs.attempted", 1, tags={"tier": tier_name})

        applied = await self._apply_repair(incident, repair)

        if not applied:
            self._logger.warning(
                "repair_application_failed",
                incident_id=incident.id,
                tier=repair.tier.name,
            )
            self._total_repairs_failed += 1
            self._emit_metric("thymos.repairs.failed", 1, tags={"tier": tier_name})
            incident.repair_status = RepairStatus.ESCALATED
            self._active_incidents.pop(incident.id, None)
            return

        # ── Step 6: Verify ──
        incident.repair_status = RepairStatus.VERIFYING
        verified = await self._verify_repair(incident, repair)

        elapsed_ms = (time.monotonic() - start_time) * 1000

        if verified:
            # ── Success ──
            incident.repair_status = RepairStatus.RESOLVED
            incident.repair_successful = True
            incident.resolution_time_ms = int(elapsed_ms)
            self._resolution_times.append(elapsed_ms)
            self._total_repairs_succeeded += 1

            self._emit_metric("thymos.repairs.succeeded", 1, tags={"tier": tier_name})
            self._emit_metric("thymos.incidents.mean_resolution_ms", elapsed_ms)

            self._logger.info(
                "incident_resolved",
                incident_id=incident.id,
                tier=repair.tier.name,
                resolution_ms=f"{elapsed_ms:.0f}",
            )

            # ── Step 7: Learn ──
            await self._learn_from_success(incident, repair, diagnosis)

            # ── Step 7b: Inject recovery monitoring goal for RESTART+ repairs ──
            if repair.tier >= RepairTier.RESTART and self._nova is not None:
                await self._inject_repair_goal(incident, repair.tier, resolved=True)

        else:
            # ── Rollback ──
            incident.repair_status = RepairStatus.ROLLED_BACK
            incident.repair_successful = False
            self._total_repairs_rolled_back += 1

            self._emit_metric("thymos.repairs.rolled_back", 1, tags={"tier": tier_name})

            self._logger.warning(
                "repair_rolled_back",
                incident_id=incident.id,
                tier=repair.tier.name,
                reason="Post-repair verification failed",
            )

            # ── Learn from failure ──
            await self._learn_from_failure(incident, repair)

        # Clean up active tracking
        self._active_incidents.pop(incident.id, None)
        if self._governor is not None:
            self._governor.resolve_incident(incident.id)

        # Check storm exit
        self._governor.check_storm_exit()

    # ─── Diagnosis ──────────────────────────────────────────────────

    async def _diagnose(self, incident: Incident) -> Diagnosis:
        """
        Full diagnostic sequence:
        1. Check antibody library for known fix
        2. Trace causal chain through dependency graph
        3. Correlate temporal events
        4. Generate and test hypotheses
        """
        assert self._antibody_library is not None
        assert self._causal_analyzer is not None
        assert self._temporal_correlator is not None
        assert self._diagnostic_engine is not None

        # Fast path: check antibody library
        antibody_match = await self._antibody_library.lookup(incident.fingerprint)
        if antibody_match is not None and antibody_match.effectiveness > 0.8:
            self._total_antibodies_applied += 1
            self._emit_metric("thymos.antibodies.applied", 1)
            self._logger.info(
                "antibody_match",
                incident_id=incident.id,
                antibody_id=antibody_match.id,
                effectiveness=f"{antibody_match.effectiveness:.2f}",
            )

        # Causal analysis: trace upstream dependencies
        causal_chain = await self._causal_analyzer.trace_root_cause(incident)

        # Temporal correlation: what changed before the incident?
        correlations = self._temporal_correlator.correlate(incident)

        # Full diagnosis with hypothesis generation
        diagnosis = await self._diagnostic_engine.diagnose(
            incident=incident,
            causal_chain=causal_chain,
            correlations=correlations,
            antibody_match=antibody_match,
        )

        self._emit_metric("thymos.diagnosis.hypotheses", len(diagnosis.all_hypotheses))

        return diagnosis

    # ─── Repair Application ──────────────────────────────────────────

    async def _apply_repair(self, incident: Incident, repair: RepairSpec) -> bool:
        """
        Apply a repair based on its tier.

        Returns True if the repair was applied successfully (not verified yet).
        """
        try:
            if repair.tier == RepairTier.NOOP:
                return True

            elif repair.tier == RepairTier.PARAMETER:
                return await self._apply_parameter_repair(repair)

            elif repair.tier == RepairTier.RESTART:
                return await self._apply_restart_repair(repair)

            elif repair.tier == RepairTier.KNOWN_FIX:
                return await self._apply_antibody_repair(incident, repair)

            elif repair.tier == RepairTier.NOVEL_FIX:
                return await self._apply_novel_repair(incident, repair)

            elif repair.tier == RepairTier.ESCALATE:
                return await self._apply_escalation(incident, repair)

            else:
                self._logger.warning(
                    "unknown_repair_tier",
                    tier=repair.tier,
                    incident_id=incident.id,
                )
                return False

        except Exception as exc:
            self._logger.error(
                "repair_application_error",
                incident_id=incident.id,
                tier=repair.tier.name,
                error=str(exc),
            )
            return False

    async def _apply_parameter_repair(self, repair: RepairSpec) -> bool:
        """Apply Tier 1: parameter adjustment."""
        if not repair.parameter_changes:
            return False

        applied_count = 0
        for change in repair.parameter_changes:
            path = change.get("parameter_path", "")
            delta = change.get("delta", 0)

            if self._evo is not None:
                # Route parameter changes through Evo's tuner
                current = self._evo.get_parameter(path)
                if current is not None:
                    # Evo doesn't expose set_parameter directly,
                    # so we log the adjustment for now
                    self._logger.info(
                        "parameter_adjustment",
                        path=path,
                        current=current,
                        delta=delta,
                        new_value=current + delta,
                    )
                    applied_count += 1
                else:
                    self._logger.debug(
                        "parameter_not_found",
                        path=path,
                    )
            else:
                self._logger.info(
                    "parameter_adjustment_no_evo",
                    path=path,
                    delta=delta,
                )
                applied_count += 1

        return applied_count > 0

    async def _apply_restart_repair(self, repair: RepairSpec) -> bool:
        """
        Apply Tier 2: system restart.

        Thymos doesn't restart systems directly — it signals Synapse's
        DegradationManager to handle the restart sequence.
        """
        target = repair.target_system
        if target is None:
            return False

        if self._synapse is not None:
            try:
                # Emit a restart request through the event bus
                await self._synapse._event_bus.emit(
                    SynapseEvent(
                        event_type=SynapseEventType.SYSTEM_RESTARTING,
                        data={
                            "system_id": target,
                            "reason": repair.reason,
                            "requested_by": "thymos",
                        },
                        source_system="thymos",
                    )
                )
                self._logger.info(
                    "restart_requested",
                    target_system=target,
                    reason=repair.reason,
                )
                return True
            except Exception as exc:
                self._logger.error(
                    "restart_request_failed",
                    target_system=target,
                    error=str(exc),
                )
                return False

        self._logger.warning(
            "restart_no_synapse",
            target_system=target,
        )
        return False

    async def _apply_antibody_repair(self, incident: Incident, repair: RepairSpec) -> bool:
        """Apply Tier 3: known fix from antibody library."""
        assert self._antibody_library is not None

        if repair.antibody_id is None:
            return False

        antibody = self._antibody_library._all.get(repair.antibody_id)
        if antibody is None:
            self._logger.warning(
                "antibody_not_found",
                antibody_id=repair.antibody_id,
            )
            return False

        # Apply the antibody's repair spec
        inner_repair = antibody.repair_spec
        incident.antibody_id = antibody.id

        self._logger.info(
            "applying_antibody",
            antibody_id=antibody.id,
            inner_tier=inner_repair.tier.name,
            inner_action=inner_repair.action,
        )

        # Recursively apply the inner repair (but not another antibody to avoid loops)
        if inner_repair.tier == RepairTier.PARAMETER:
            return await self._apply_parameter_repair(inner_repair)
        elif inner_repair.tier == RepairTier.RESTART:
            return await self._apply_restart_repair(inner_repair)
        else:
            # For more complex inner repairs, log and mark as applied
            self._logger.info(
                "antibody_complex_repair",
                antibody_id=antibody.id,
                inner_tier=inner_repair.tier.name,
            )
            return True

    async def _apply_novel_repair(self, incident: Incident, repair: RepairSpec) -> bool:
        """
        Apply Tier 4: novel fix via Simula Code Agent.

        Iron rule: CANNOT apply without Equor review (enforced by validator).
        """
        assert self._governor is not None

        if not self._governor.should_codegen():
            self._logger.info(
                "codegen_throttled",
                incident_id=incident.id,
            )
            return False

        self._governor.begin_codegen()
        try:
            # Simula integration would go here.
            # For now, log the intent — Simula's Code Agent would receive
            # an EvolutionProposal with the repair spec.
            self._logger.info(
                "novel_repair_requested",
                incident_id=incident.id,
                target_system=repair.target_system,
                reason=repair.reason,
            )
            # In production, this would call:
            # await self._simula.propose_repair(incident, repair)
            return True
        finally:
            self._governor.end_codegen()

    async def _apply_escalation(self, incident: Incident, repair: RepairSpec) -> bool:
        """Apply Tier 5: human escalation."""
        self._logger.warning(
            "incident_escalated_to_human",
            incident_id=incident.id,
            source_system=incident.source_system,
            severity=incident.severity.value,
            reason=repair.reason,
        )
        incident.repair_status = RepairStatus.ESCALATED
        return True

    # ─── Post-Repair Verification ────────────────────────────────────

    async def _verify_repair(self, incident: Incident, repair: RepairSpec) -> bool:
        """
        Verify that a repair actually fixed the problem.

        Waits briefly, then checks if the source system is healthy.
        For escalations, verification is not applicable — assume success.
        """
        if repair.tier == RepairTier.ESCALATE:
            return True  # Escalation is inherently "successful"

        if repair.tier == RepairTier.NOOP:
            return True

        # Wait for the repair to take effect
        await asyncio.sleep(min(_POST_REPAIR_VERIFY_TIMEOUT_S, 5.0))

        # Check system health
        if self._health_monitor is not None and repair.target_system is not None:
            record = self._health_monitor.get_record(repair.target_system)
            if record is not None:
                from ecodiaos.systems.synapse.types import SystemStatus
                if record.status in (SystemStatus.HEALTHY, SystemStatus.STARTING):
                    return True
                elif record.status == SystemStatus.FAILED:
                    return False
                # Degraded/overloaded: ambiguous — check recent trend
                return record.consecutive_successes >= 1

        # No health monitor or target system: optimistic assumption
        return True

    # ─── Learning ────────────────────────────────────────────────────

    async def _learn_from_success(
        self,
        incident: Incident,
        repair: RepairSpec,
        diagnosis: Diagnosis,
    ) -> None:
        """
        A repair succeeded. Crystallize it into an antibody and feed Evo.

        This is where genuine adaptive immunity happens: the organism
        gets harder to break over time.
        """
        assert self._antibody_library is not None

        # If this was an antibody application, record success
        if incident.antibody_id is not None:
            await self._antibody_library.record_outcome(
                incident.antibody_id,
                success=True,
            )
            return

        # For Tier 2+ repairs (not NOOP/transient), create a new antibody
        if repair.tier >= RepairTier.PARAMETER:
            antibody = await self._antibody_library.create_from_repair(
                incident=incident,
                repair=repair,
            )
            self._total_antibodies_created += 1
            self._emit_metric("thymos.antibodies.created", 1)

            self._logger.info(
                "antibody_crystallized",
                antibody_id=antibody.id,
                fingerprint=incident.fingerprint[:16],
                tier=repair.tier.name,
            )

        # Persist incident to Neo4j
        await self._persist_incident(incident)

        # Feed success to Evo so the learning system can accumulate
        # evidence about what repair strategies work
        if self._evo is not None:
            await self._feed_repair_to_evo(incident, repair, success=True)

    async def _learn_from_failure(
        self,
        incident: Incident,
        repair: RepairSpec,
    ) -> None:
        """A repair failed. Record failure and update antibody if applicable."""
        assert self._antibody_library is not None

        if incident.antibody_id is not None:
            await self._antibody_library.record_outcome(
                incident.antibody_id,
                success=False,
            )

        # Persist the failed incident for post-mortem
        await self._persist_incident(incident)

        # Feed failure to Evo — failures are more salient than successes
        # and drive hypothesis formation about system vulnerabilities
        if self._evo is not None:
            await self._feed_repair_to_evo(incident, repair, success=False)

    async def _persist_incident(self, incident: Incident) -> None:
        """Persist an incident to Neo4j for the causal knowledge graph."""
        if self._neo4j is None:
            return

        try:
            await self._neo4j.execute_write(
                """
                MERGE (i:Incident {id: $id})
                SET i.source_system = $source_system,
                    i.incident_class = $incident_class,
                    i.severity = $severity,
                    i.fingerprint = $fingerprint,
                    i.error_type = $error_type,
                    i.error_message = $error_message,
                    i.repair_status = $repair_status,
                    i.repair_tier = $repair_tier,
                    i.repair_successful = $repair_successful,
                    i.resolution_time_ms = $resolution_time_ms,
                    i.root_cause = $root_cause,
                    i.timestamp = $timestamp
                """,
                {
                    "id": incident.id,
                    "source_system": incident.source_system,
                    "incident_class": incident.incident_class.value,
                    "severity": incident.severity.value,
                    "fingerprint": incident.fingerprint,
                    "error_type": incident.error_type,
                    "error_message": incident.error_message[:500],
                    "repair_status": incident.repair_status.value,
                    "repair_tier": incident.repair_tier.name if incident.repair_tier else "unknown",
                    "repair_successful": incident.repair_successful,
                    "resolution_time_ms": incident.resolution_time_ms,
                    "root_cause": incident.root_cause_hypothesis or "",
                    "timestamp": incident.timestamp.isoformat(),
                },
            )
        except Exception as exc:
            self._logger.debug("incident_persist_failed", error=str(exc))

    # ─── Cross-System Feedback ─────────────────────────────────────────

    async def _inject_repair_goal(
        self,
        incident: Incident,
        repair_tier: RepairTier | None,
        resolved: bool,
    ) -> None:
        """
        Inject a self-repair goal into Nova's goal manager.

        Pre-repair (resolved=False): high-urgency goal so Nova prioritises self-healing.
        Post-repair (resolved=True): follow-up monitoring goal at lower urgency.
        """
        from ecodiaos.systems.nova.types import Goal, GoalSource, GoalStatus
        from ecodiaos.primitives.common import new_id, DriveAlignmentVector

        tier_name = repair_tier.name if repair_tier else "UNKNOWN"

        if resolved:
            desc = (
                f"Monitor system recovery: {incident.source_system} "
                f"after {tier_name} repair"
            )
            priority, urgency = 0.6, 0.4
        else:
            desc = (
                f"Urgent: self-repair {incident.source_system} — "
                f"{incident.incident_class.value} incident ({tier_name})"
            )
            priority, urgency = 0.9, 0.85

        goal = Goal(
            id=new_id(),
            description=desc,
            source=GoalSource.MAINTENANCE,
            priority=priority,
            urgency=urgency,
            importance=0.7,
            drive_alignment=DriveAlignmentVector(
                coherence=0.8, care=0.1, growth=0.0, honesty=0.1,
            ),
            status=GoalStatus.ACTIVE,
        )
        try:
            await self._nova.add_goal(goal)
            self._logger.info(
                "repair_goal_injected",
                goal_id=goal.id,
                incident_id=incident.id,
                resolved=resolved,
            )
        except Exception as exc:
            self._logger.warning("repair_goal_injection_failed", error=str(exc))

    async def _feed_repair_to_evo(
        self,
        incident: Incident,
        repair: RepairSpec,
        success: bool,
    ) -> None:
        """
        Feed a repair outcome to Evo as a learning episode.

        Successful repairs teach the organism what works.
        Failed repairs teach it what doesn't — and are more salient.
        """
        from ecodiaos.primitives.memory_trace import Episode
        from ecodiaos.primitives.common import new_id, utc_now

        outcome_text = "succeeded" if success else "failed"
        episode = Episode(
            id=new_id(),
            source=f"thymos.repair_{outcome_text}",
            raw_content=(
                f"Repair {outcome_text}: {repair.action} on {repair.target_system}. "
                f"Tier: {repair.tier.name}. "
                f"Incident class: {incident.incident_class.value}. "
                f"Root cause: {incident.root_cause_hypothesis or 'unknown'}"
            ),
            summary=(
                f"Thymos {repair.tier.name} repair {outcome_text}: "
                f"{repair.target_system}"
            ),
            salience_composite=0.6 if success else 0.8,
            affect_valence=0.2 if success else -0.3,
            timestamp=utc_now(),
        )
        try:
            await self._evo.process_episode(episode)
            self._logger.info(
                "repair_outcome_fed_to_evo",
                incident_id=incident.id,
                success=success,
                tier=repair.tier.name,
            )
        except Exception as exc:
            self._logger.warning("evo_feed_failed", error=str(exc))

    # ─── Percept Broadcasting ────────────────────────────────────────

    async def _broadcast_as_percept(self, incident: Incident) -> None:
        """
        Route high-severity incidents into Atune's workspace as Percepts.

        The organism perceives its own failures through the normal
        consciousness cycle. It hurts to break — and that's by design.
        Critical incidents get maximum salience; INFO incidents are barely noticed.
        """
        if self._atune is None:
            return

        salience = _SEVERITY_TO_SALIENCE.get(incident.severity, 0.1)

        # Only broadcast MEDIUM+ to avoid flooding the workspace
        if salience < 0.5:
            return

        try:
            from ecodiaos.systems.atune.types import WorkspaceContribution

            self._atune.contribute(
                WorkspaceContribution(
                    system="thymos",
                    content=(
                        f"[IMMUNE] {incident.severity.value.upper()} incident in "
                        f"{incident.source_system}: {incident.error_message}"
                    ),
                    priority=salience,
                    reason="immune_incident",
                )
            )
        except Exception as exc:
            self._logger.debug("percept_broadcast_failed", error=str(exc))

    # ─── Background Loops ────────────────────────────────────────────

    async def _sentinel_scan_loop(self) -> None:
        """
        Periodic sentinel scans for proactive failure detection.

        Feedback loop sentinel checks which loops are transmitting.
        Cognitive stall sentinel checks workspace health.
        Drift sentinel is fed by Synapse metrics (not looped here).
        """
        await asyncio.sleep(10.0)  # Let the organism warm up

        while True:
            try:
                await asyncio.sleep(_SENTINEL_SCAN_INTERVAL_S)

                # Feedback loop sentinel: check all defined loops
                if self._feedback_loop_sentinel is not None:
                    loop_incidents = self._feedback_loop_sentinel.check_loops()
                    for incident in loop_incidents:
                        await self.on_incident(incident)

                # Cognitive stall sentinel is fed per-cycle by Synapse,
                # not scanned here. It fires incidents from record_cycle().

            except asyncio.CancelledError:
                return
            except Exception as exc:
                self._logger.warning(
                    "sentinel_scan_error",
                    error=str(exc),
                )

    async def _homeostasis_loop(self) -> None:
        """
        Proactive homeostatic regulation.

        Runs continuously on MAINTAIN cycle step timing. Checks metrics
        against optimal ranges and makes small preemptive adjustments.

        This is the organism's thermostat — it maintains optimal operating
        conditions without waiting for something to break.
        """
        await asyncio.sleep(30.0)  # Let the organism stabilize

        while True:
            try:
                await asyncio.sleep(_HOMEOSTASIS_INTERVAL_S)

                if self._homeostasis_controller is None:
                    continue

                adjustments = self._homeostasis_controller.check_homeostasis()

                for adjustment in adjustments:
                    self._total_homeostatic_adjustments += 1
                    self._emit_metric("thymos.homeostasis.adjustments", 1)

                    self._logger.info(
                        "homeostatic_adjustment",
                        metric=adjustment.metric_name,
                        current=f"{adjustment.current_value:.2f}",
                        trend=adjustment.trend_direction,
                        adjustment_path=adjustment.adjustment.parameter_path,
                        adjustment_delta=adjustment.adjustment.delta,
                    )

                # Check storm mode exit
                if self._governor is not None:
                    self._governor.check_storm_exit()

            except asyncio.CancelledError:
                return
            except Exception as exc:
                self._logger.warning(
                    "homeostasis_loop_error",
                    error=str(exc),
                )

    # ─── Prophylactic Scanner ────────────────────────────────────────

    async def scan_files(self, files_changed: list[str]) -> list[dict[str, Any]]:
        """
        Pre-deployment prophylactic scan.

        Checks new or modified files against the antibody library's
        error patterns. Returns warnings for code that matches known
        failure signatures.
        """
        if self._prophylactic_scanner is None:
            return []

        warnings = await self._prophylactic_scanner.scan(files_changed)
        self._total_prophylactic_scans += 1
        self._total_prophylactic_warnings += len(warnings)

        self._emit_metric("thymos.prophylactic.scans", 1)
        self._emit_metric("thymos.prophylactic.warnings", len(warnings))

        return [w.model_dump() for w in warnings]

    # ─── Exception Sentinel (Public API) ─────────────────────────────

    async def report_exception(
        self,
        system_id: str,
        exception: Exception,
        context: dict[str, Any] | None = None,
    ) -> None:
        """
        Public API for systems to report unhandled exceptions.

        Called by systems' own error handlers or by a global exception hook.
        Creates an Incident and routes it through the immune pipeline.
        """
        if self._exception_sentinel is None:
            return

        incident = self._exception_sentinel.intercept(
            system_id=system_id,
            method_name="unknown",
            exception=exception,
            context=context or {},
        )
        await self.on_incident(incident)

    # ─── Contract Sentinel (Public API) ──────────────────────────────

    async def report_contract_violation(
        self,
        source: str,
        target: str,
        operation: str,
        latency_ms: float,
        sla_ms: float,
    ) -> None:
        """
        Public API for reporting inter-system contract violations.

        Called by systems when an operation exceeds its SLA.
        """
        if self._contract_sentinel is None:
            return

        incident = self._contract_sentinel.check_contract(
            source=source,
            target=target,
            operation=operation,
            latency_ms=latency_ms,
        )
        if incident is not None:
            await self.on_incident(incident)

    # ─── Drift Sentinel (Public API) ─────────────────────────────────

    def record_metric(self, metric_name: str, value: float) -> None:
        """
        Feed a metric observation to the drift sentinel.

        Called by Synapse's telemetry pipeline to monitor for gradual
        degradation that wouldn't trigger a hard failure.
        """
        if self._drift_sentinel is None:
            return

        incident = self._drift_sentinel.record_metric(metric_name, value)
        if incident is not None:
            # Fire-and-forget: drift incidents are LOW priority
            asyncio.create_task(
                self.on_incident(incident),
                name=f"thymos_drift_{metric_name}",
            )

        # Also feed the temporal correlator
        if self._temporal_correlator is not None and self._drift_sentinel is not None:
            baseline = self._drift_sentinel._baselines.get(metric_name)
            if baseline is not None and baseline.is_warmed_up:
                self._temporal_correlator.record_metric_anomaly(
                    metric_name=metric_name,
                    value=value,
                    baseline=baseline.mean,
                    z_score=baseline.z_score(value),
                )

    # ─── Health ──────────────────────────────────────────────────────

    async def health(self) -> dict[str, Any]:
        """
        Health check for Thymos (required by Synapse health monitor).

        Returns a snapshot of immune system status, counters, and budget.
        """
        governor_budget = (
            self._governor.budget_state if self._governor else None
        )

        antibody_count = len(self._antibody_library._all) if self._antibody_library else 0
        mean_effectiveness = 0.0
        if self._antibody_library and self._antibody_library._all:
            effectivenesses = [
                a.effectiveness for a in self._antibody_library._all.values()
                if not a.retired
            ]
            if effectivenesses:
                mean_effectiveness = sum(effectivenesses) / len(effectivenesses)

        mean_resolution = 0.0
        if self._resolution_times:
            mean_resolution = sum(self._resolution_times) / len(self._resolution_times)

        mean_confidence = 0.0
        if self._diagnosis_confidences:
            mean_confidence = sum(self._diagnosis_confidences) / len(self._diagnosis_confidences)

        mean_diag_latency = 0.0
        if self._diagnosis_latencies:
            mean_diag_latency = sum(self._diagnosis_latencies) / len(self._diagnosis_latencies)

        homeostasis_ranges = 0
        if self._homeostasis_controller is not None:
            homeostasis_ranges = self._homeostasis_controller.metrics_in_range

        return {
            "status": "healthy" if self._initialized else "not_initialized",
            "initialized": self._initialized,
            "healing_mode": (
                self._governor.healing_mode.value if self._governor else "unknown"
            ),
            # Incidents
            "total_incidents": self._total_incidents,
            "active_incidents": len(self._active_incidents),
            "mean_resolution_ms": round(mean_resolution, 1),
            "incidents_by_severity": dict(self._incidents_by_severity),
            "incidents_by_class": dict(self._incidents_by_class),
            # Antibodies
            "total_antibodies": antibody_count,
            "mean_antibody_effectiveness": round(mean_effectiveness, 3),
            "antibodies_applied": self._total_antibodies_applied,
            "antibodies_created": self._total_antibodies_created,
            # Repairs
            "repairs_attempted": self._total_repairs_attempted,
            "repairs_succeeded": self._total_repairs_succeeded,
            "repairs_failed": self._total_repairs_failed,
            "repairs_rolled_back": self._total_repairs_rolled_back,
            "repairs_by_tier": dict(self._repairs_by_tier),
            # Diagnosis
            "total_diagnoses": self._total_diagnoses,
            "mean_diagnosis_confidence": round(mean_confidence, 3),
            "mean_diagnosis_latency_ms": round(mean_diag_latency, 1),
            # Homeostasis
            "homeostatic_adjustments": self._total_homeostatic_adjustments,
            "metrics_in_range": homeostasis_ranges,  # Approximate
            # Storm
            "storm_activations": (
                self._governor.storm_activations if self._governor else 0
            ),
            # Prophylactic
            "prophylactic_scans": self._total_prophylactic_scans,
            "prophylactic_warnings": self._total_prophylactic_warnings,
            # Budget
            "budget": governor_budget.model_dump() if governor_budget else {},
        }

    # ─── Stats ──────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        """Synchronous stats for logging."""
        return {
            "initialized": self._initialized,
            "total_incidents": self._total_incidents,
            "active_incidents": len(self._active_incidents),
            "total_diagnoses": self._total_diagnoses,
            "total_repairs_attempted": self._total_repairs_attempted,
            "total_repairs_succeeded": self._total_repairs_succeeded,
            "healing_mode": (
                self._governor.healing_mode.value if self._governor else "unknown"
            ),
        }

    # ─── Telemetry Helper ───────────────────────────────────────────

    def _emit_metric(
        self,
        name: str,
        value: float,
        tags: dict[str, str] | None = None,
    ) -> None:
        """Emit a metric if the collector is available."""
        if self._metrics is not None:
            try:
                self._metrics.record(name, value, tags=tags or {})
            except Exception:
                pass  # Telemetry failures must never affect immune function

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\triage.py =====

"""
EcodiaOS — Thymos Triage Layer (Classification & Prioritization)

When sentinels produce Incidents, Triage determines what to do with them:
  1. Deduplicate — same fingerprint within a window → increment, don't duplicate
  2. Score severity — composite scoring using blast radius, recurrence, impact
  3. Route response — severity → initial repair tier
"""

from __future__ import annotations

from collections import defaultdict
from datetime import datetime, timedelta

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.thymos.types import (
    Incident,
    IncidentClass,
    IncidentSeverity,
    RepairTier,
)

logger = structlog.get_logger()


# ─── Deduplication ───────────────────────────────────────────────


# Per-class dedup windows: how long before the same fingerprint
# creates a new incident instead of incrementing the existing one
_DEDUP_WINDOWS: dict[IncidentClass, float] = {
    IncidentClass.CRASH: 60.0,  # 1 minute — crashes are loud
    IncidentClass.DEGRADATION: 300.0,  # 5 minutes — slow issues need observation
    IncidentClass.CONTRACT_VIOLATION: 30.0,  # 30 seconds — may be transient
    IncidentClass.LOOP_SEVERANCE: 3600.0,  # 1 hour — structural, slow to resolve
    IncidentClass.DRIFT: 3600.0,  # 1 hour — drift is slow by definition
    IncidentClass.PREDICTION_FAILURE: 300.0,  # 5 minutes
    IncidentClass.RESOURCE_EXHAUSTION: 120.0,  # 2 minutes
    IncidentClass.COGNITIVE_STALL: 600.0,  # 10 minutes
}


class IncidentDeduplicator:
    """
    Same fingerprint within a time window → increment count, don't create
    a new incident. This prevents a single bug from flooding the immune
    system with thousands of identical incidents.
    """

    def __init__(self) -> None:
        # fingerprint → (incident, last_seen_timestamp)
        self._active: dict[str, tuple[Incident, datetime]] = {}
        self._logger = logger.bind(system="thymos", component="deduplicator")

    def deduplicate(self, incident: Incident) -> Incident | None:
        """
        Check if this incident is a duplicate.

        Returns:
          - None if it's a duplicate (the existing incident was incremented)
          - The incident if it's new (caller should process it)
        """
        window_s = _DEDUP_WINDOWS.get(incident.incident_class, 60.0)
        existing = self._active.get(incident.fingerprint)

        if existing is not None:
            existing_incident, last_seen = existing
            age_s = (incident.timestamp - last_seen).total_seconds()

            if age_s <= window_s:
                # Duplicate — increment existing
                existing_incident.occurrence_count += 1
                self._active[incident.fingerprint] = (
                    existing_incident,
                    incident.timestamp,
                )
                self._logger.debug(
                    "incident_deduplicated",
                    fingerprint=incident.fingerprint,
                    occurrences=existing_incident.occurrence_count,
                )
                return None
            else:
                # Window expired — treat as new, but carry forward the count
                incident.first_seen = existing_incident.first_seen or existing_incident.timestamp

        # New incident
        if incident.first_seen is None:
            incident.first_seen = incident.timestamp
        self._active[incident.fingerprint] = (incident, incident.timestamp)
        return incident

    def get_active_incident(self, fingerprint: str) -> Incident | None:
        """Get the active incident for a fingerprint, if any."""
        entry = self._active.get(fingerprint)
        return entry[0] if entry else None

    def resolve(self, fingerprint: str) -> Incident | None:
        """Remove a resolved incident from the active set."""
        entry = self._active.pop(fingerprint, None)
        return entry[0] if entry else None

    def prune_stale(self, max_age_s: float = 7200.0) -> int:
        """Remove incidents older than max_age_s. Returns count removed."""
        now = utc_now()
        to_remove: list[str] = []
        for fp, (_, last_seen) in self._active.items():
            if (now - last_seen).total_seconds() > max_age_s:
                to_remove.append(fp)
        for fp in to_remove:
            del self._active[fp]
        return len(to_remove)

    @property
    def active_count(self) -> int:
        return len(self._active)

    @property
    def active_incidents(self) -> list[Incident]:
        return [inc for inc, _ in self._active.values()]


# ─── Severity Scoring ────────────────────────────────────────────


class SeverityScorer:
    """
    Composite severity scoring that refines the sentinel's initial assessment.

    Considers:
    1. Blast radius (0.25) — fraction of systems affected
    2. Recurrence velocity (0.20) — how fast is occurrence_count growing?
    3. Constitutional impact (0.25) — max impact across all four drives
    4. User visibility (0.15) — binary but high weight
    5. Self-healing potential (0.15) — inverse of how likely auto-fix is
    """

    def __init__(self) -> None:
        # Track recurrence velocity per fingerprint
        self._recurrence_history: dict[str, list[tuple[datetime, int]]] = defaultdict(list)
        self._logger = logger.bind(system="thymos", component="severity_scorer")

    def compute_severity(self, incident: Incident) -> IncidentSeverity:
        """Compute composite severity score."""
        # Record recurrence for velocity calculation
        self._recurrence_history[incident.fingerprint].append(
            (incident.timestamp, incident.occurrence_count)
        )
        # Keep only last 20 entries per fingerprint
        if len(self._recurrence_history[incident.fingerprint]) > 20:
            self._recurrence_history[incident.fingerprint] = (
                self._recurrence_history[incident.fingerprint][-20:]
            )

        blast = incident.blast_radius
        recurrence = self._recurrence_velocity(incident)
        constitutional = max(incident.constitutional_impact.values()) if incident.constitutional_impact else 0.0
        user_vis = 1.0 if incident.user_visible else 0.0
        healing_potential = self._healing_potential(incident)

        score = (
            blast * 0.25
            + recurrence * 0.20
            + constitutional * 0.25
            + user_vis * 0.15
            + (1.0 - healing_potential) * 0.15
        )

        if score > 0.8:
            return IncidentSeverity.CRITICAL
        if score > 0.6:
            return IncidentSeverity.HIGH
        if score > 0.3:
            return IncidentSeverity.MEDIUM
        if score > 0.1:
            return IncidentSeverity.LOW
        return IncidentSeverity.INFO

    def _recurrence_velocity(self, incident: Incident) -> float:
        """
        How fast is occurrence_count growing? 0.0 = single event, 1.0 = rapid fire.

        Uses the rate of count increase over the last observation window.
        """
        history = self._recurrence_history.get(incident.fingerprint, [])
        if len(history) < 2:
            # Single event — low velocity unless high count
            return min(1.0, incident.occurrence_count / 100.0)

        first_ts, first_count = history[0]
        last_ts, last_count = history[-1]
        elapsed_s = (last_ts - first_ts).total_seconds()

        if elapsed_s < 1.0:
            return min(1.0, (last_count - first_count) / 10.0)

        rate_per_minute = ((last_count - first_count) / elapsed_s) * 60.0
        # Normalize: 10+ per minute = 1.0
        return min(1.0, rate_per_minute / 10.0)

    def _healing_potential(self, incident: Incident) -> float:
        """
        How likely can we auto-fix this? Higher = easier to heal.

        Based on incident class — some classes are inherently easier
        to fix automatically than others.
        """
        potentials: dict[IncidentClass, float] = {
            IncidentClass.CRASH: 0.3,  # Need diagnosis, often novel
            IncidentClass.DEGRADATION: 0.5,  # Parameter tweaks often work
            IncidentClass.CONTRACT_VIOLATION: 0.6,  # Often transient
            IncidentClass.LOOP_SEVERANCE: 0.2,  # Structural — hard to auto-fix
            IncidentClass.DRIFT: 0.7,  # Parameter adjustments
            IncidentClass.PREDICTION_FAILURE: 0.4,  # Needs model update
            IncidentClass.RESOURCE_EXHAUSTION: 0.8,  # Rebalance resources
            IncidentClass.COGNITIVE_STALL: 0.3,  # Complex, may need restart
        }
        return potentials.get(incident.incident_class, 0.5)


# ─── Response Routing ────────────────────────────────────────────


# Severity → initial repair tier
RESPONSE_ROUTES: dict[IncidentSeverity, RepairTier] = {
    IncidentSeverity.CRITICAL: RepairTier.RESTART,  # Stabilize first
    IncidentSeverity.HIGH: RepairTier.KNOWN_FIX,  # Check antibody library
    IncidentSeverity.MEDIUM: RepairTier.PARAMETER,  # Try parameter adjustment
    IncidentSeverity.LOW: RepairTier.NOOP,  # Log and observe
    IncidentSeverity.INFO: RepairTier.NOOP,  # Pattern detection only
}


class ResponseRouter:
    """
    Routes incidents to the appropriate response tier.

    Critical incidents get immediate stabilization (restart) AND
    diagnosis in parallel. The restart buys time while the diagnostic
    system figures out the root cause.
    """

    def __init__(self) -> None:
        self._logger = logger.bind(system="thymos", component="response_router")

    def route(self, incident: Incident) -> RepairTier:
        """Determine the initial repair tier for an incident."""
        tier = RESPONSE_ROUTES.get(incident.severity, RepairTier.NOOP)

        # Override: if we have a high-count recurrence, escalate
        if incident.occurrence_count > 50 and tier.value < RepairTier.KNOWN_FIX.value:
            tier = RepairTier.KNOWN_FIX
            self._logger.info(
                "tier_escalated_by_recurrence",
                fingerprint=incident.fingerprint,
                occurrences=incident.occurrence_count,
                new_tier=tier.name,
            )

        incident.repair_tier = tier
        return tier

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\thymos\types.py =====

"""
EcodiaOS — Thymos Type Definitions

All data types for the immune system: incidents, antibodies, repairs,
diagnoses, sentinels, and healing governance.

Every error, anomaly, and violation in EOS becomes an Incident — a
first-class primitive alongside Percept, Belief, and Intent.
"""

from __future__ import annotations

import enum
from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import EOSBaseModel, new_id, utc_now


# ─── Enums ────────────────────────────────────────────────────────


class IncidentSeverity(str, enum.Enum):
    """How bad is it?"""

    CRITICAL = "critical"  # System down, user impact, drives affected
    HIGH = "high"  # System degraded, partial user impact
    MEDIUM = "medium"  # Anomaly detected, no immediate user impact
    LOW = "low"  # Cosmetic, informational, or transient
    INFO = "info"  # Normal variance logged for pattern detection


class IncidentClass(str, enum.Enum):
    """What kind of failure is this?"""

    CRASH = "crash"  # Unhandled exception, system death
    DEGRADATION = "degradation"  # Slow or incorrect responses
    CONTRACT_VIOLATION = "contract_violation"  # Inter-system SLA breach
    LOOP_SEVERANCE = "loop_severance"  # Feedback loop not transmitting
    DRIFT = "drift"  # Gradual metric deviation from baseline
    PREDICTION_FAILURE = "prediction_failure"  # Active inference errors elevated
    RESOURCE_EXHAUSTION = "resource_exhaustion"  # Budget exceeded
    COGNITIVE_STALL = "cognitive_stall"  # Workspace cycle blocked or empty


class RepairTier(int, enum.Enum):
    """Escalation ladder — least invasive first."""

    NOOP = 0  # Transient, already resolved
    PARAMETER = 1  # Adjust a configuration value
    RESTART = 2  # Restart the affected system
    KNOWN_FIX = 3  # Apply an antibody from the library
    NOVEL_FIX = 4  # Generate a new fix via Simula Code Agent
    ESCALATE = 5  # Human operator intervention required


class RepairStatus(str, enum.Enum):
    """Lifecycle of an incident repair."""

    PENDING = "pending"
    DIAGNOSING = "diagnosing"
    PRESCRIBING = "prescribing"
    VALIDATING = "validating"
    APPLYING = "applying"
    VERIFYING = "verifying"
    RESOLVED = "resolved"
    ESCALATED = "escalated"
    ACCEPTED = "accepted"  # Transient or INFO, no repair needed
    ROLLED_BACK = "rolled_back"


class HealingMode(str, enum.Enum):
    """Organism-wide healing state."""

    NOMINAL = "nominal"  # Normal operation
    HEALING = "healing"  # Active repair in progress
    STORM = "storm"  # Cytokine storm — focus on root cause only


# ─── Sentinel Types ──────────────────────────────────────────────


class ContractSLA(EOSBaseModel):
    """SLA definition for an inter-system contract."""

    source: str
    target: str
    operation: str
    max_latency_ms: float


class FeedbackLoop(EOSBaseModel):
    """Definition of a feedback loop that should be actively transmitting."""

    name: str
    source: str
    target: str
    signal: str
    check: str  # Descriptive check expression
    description: str


class DriftConfig(EOSBaseModel):
    """Configuration for statistical drift detection on a metric."""

    window: int = 500  # Number of samples in the rolling baseline
    sigma_threshold: float = 2.5  # Standard deviations before flagging
    direction: str | None = None  # "above", "below", or None (both)


class StallConfig(EOSBaseModel):
    """Threshold for cognitive stall detection."""

    min_value: float  # Rate must be above this
    window_cycles: int  # Number of cycles to observe


# ─── Incident ────────────────────────────────────────────────────


class Incident(EOSBaseModel):
    """
    The fundamental immune primitive.

    Every error, anomaly, and violation becomes an Incident.
    Incidents are also Percepts — the organism perceives its own
    failures through the normal workspace broadcast cycle.
    """

    id: str = Field(default_factory=new_id)
    timestamp: datetime = Field(default_factory=utc_now)

    # ── Classification ──
    incident_class: IncidentClass
    severity: IncidentSeverity
    fingerprint: str  # Hash of (class, system, error_signature)

    # ── Source ──
    source_system: str
    error_type: str  # Exception class name or anomaly type
    error_message: str
    stack_trace: str | None = None
    context: dict[str, Any] = Field(default_factory=dict)

    # ── Impact Assessment ──
    affected_systems: list[str] = Field(default_factory=list)
    blast_radius: float = Field(0.0, ge=0.0, le=1.0)
    user_visible: bool = False
    constitutional_impact: dict[str, float] = Field(
        default_factory=lambda: {
            "coherence": 0.0,
            "care": 0.0,
            "growth": 0.0,
            "honesty": 0.0,
        }
    )

    # ── Deduplication ──
    occurrence_count: int = 1
    first_seen: datetime | None = None

    # ── Diagnosis ──
    root_cause_hypothesis: str | None = None
    diagnostic_confidence: float = 0.0
    causal_chain: list[str] | None = None

    # ── Repair ──
    repair_tier: RepairTier | None = None
    repair_status: RepairStatus = RepairStatus.PENDING
    antibody_id: str | None = None

    # ── Learning ──
    resolution_time_ms: int | None = None
    repair_successful: bool | None = None


# ─── Diagnosis Types ─────────────────────────────────────────────


class CausalChain(EOSBaseModel):
    """Result of tracing error causality through the system graph."""

    root_system: str
    chain: list[str]  # System A → System B → failure
    confidence: float = Field(0.0, ge=0.0, le=1.0)
    reasoning: str = ""


class TemporalCorrelation(EOSBaseModel):
    """Something that changed in the window before the incident."""

    type: str  # "metric_anomaly" | "system_event"
    timestamp: datetime
    description: str
    time_delta_ms: int  # How many ms before the incident


class DiagnosticHypothesis(EOSBaseModel):
    """A testable hypothesis about what caused an incident."""

    id: str = Field(default_factory=new_id)
    statement: str
    diagnostic_test: str  # Name of the test to run
    diagnostic_test_params: dict[str, Any] = Field(default_factory=dict)
    suggested_repair_tier: RepairTier = RepairTier.PARAMETER
    confidence_prior: float = Field(0.5, ge=0.0, le=1.0)


class DiagnosticTestResult(EOSBaseModel):
    """Result of running a diagnostic test."""

    test_name: str
    passed: bool
    confidence: float = Field(0.0, ge=0.0, le=1.0)
    reasoning: str = ""
    raw_value: Any = None


class DiagnosticEvidence(EOSBaseModel):
    """All evidence gathered for diagnosing an incident."""

    incident: Incident
    causal_chain: CausalChain
    temporal_correlations: list[TemporalCorrelation] = Field(default_factory=list)
    recent_similar: list[Incident] = Field(default_factory=list)
    system_health_history: dict[str, Any] = Field(default_factory=dict)


class Diagnosis(EOSBaseModel):
    """Final diagnosis of an incident's root cause."""

    root_cause: str
    confidence: float = Field(0.0, ge=0.0, le=1.0)
    repair_tier: RepairTier = RepairTier.PARAMETER
    antibody_id: str | None = None
    all_hypotheses: list[DiagnosticHypothesis] = Field(default_factory=list)
    test_results: list[DiagnosticTestResult] = Field(default_factory=list)
    reasoning: str = ""


# ─── Repair Types ────────────────────────────────────────────────


class ParameterFix(EOSBaseModel):
    """A single parameter adjustment."""

    parameter_path: str  # e.g., "synapse.clock.current_period_ms"
    delta: float  # Change amount (can be negative)
    reason: str = ""


class RepairSpec(EOSBaseModel):
    """Specification for a repair action."""

    tier: RepairTier
    action: str  # e.g., "log_and_monitor", "restart_system", "apply_antibody"
    target_system: str | None = None
    antibody_id: str | None = None
    parameter_changes: list[dict[str, Any]] = Field(default_factory=list)
    code_changes: dict[str, Any] | None = None
    evolution_proposal_id: str | None = None
    reason: str = ""


class ValidationResult(EOSBaseModel):
    """Result of the repair validation gate."""

    approved: bool
    reason: str = ""
    escalate_to: RepairTier | None = None
    modifications: dict[str, Any] | None = None


# ─── Antibody Types ──────────────────────────────────────────────


class Antibody(EOSBaseModel):
    """
    A crystallized successful repair.

    When a repair succeeds, it becomes an Antibody. The next time an
    incident with the same fingerprint appears, the antibody is applied
    instantly — no diagnosis needed.

    This is genuine adaptive immunity: the organism gets harder to break
    over time.
    """

    id: str = Field(default_factory=new_id)

    # ── Matching ──
    fingerprint: str
    incident_class: IncidentClass
    source_system: str
    error_pattern: str  # Regex or fragment for matching error_message

    # ── Repair ──
    repair_tier: RepairTier
    repair_spec: RepairSpec
    root_cause_description: str

    # ── Effectiveness ──
    application_count: int = 0
    success_count: int = 0
    failure_count: int = 0
    effectiveness: float = 1.0  # success / (success + failure)

    # ── Lifecycle ──
    created_at: datetime = Field(default_factory=utc_now)
    last_applied: datetime | None = None
    source_incident_id: str = ""
    retired: bool = False

    # ── Lineage ──
    generation: int = 1
    parent_antibody_id: str | None = None


# ─── Prophylactic Types ──────────────────────────────────────────


class ProphylacticWarning(EOSBaseModel):
    """Warning issued by the prophylactic scanner."""

    filepath: str
    antibody_id: str
    warning: str
    suggestion: str = ""
    confidence: float = Field(0.0, ge=0.0, le=1.0)


class ParameterAdjustment(EOSBaseModel):
    """A homeostatic parameter nudge — Tier 1, no governance."""

    metric_name: str
    current_value: float
    optimal_min: float
    optimal_max: float
    adjustment: ParameterFix
    trend_direction: str  # "rising" | "falling"


# ─── Governor Types ──────────────────────────────────────────────


class HealingBudgetState(EOSBaseModel):
    """Current state of the healing budget."""

    repairs_this_hour: int = 0
    novel_repairs_today: int = 0
    max_repairs_per_hour: int = 5
    max_novel_repairs_per_day: int = 3
    active_diagnoses: int = 0
    max_concurrent_diagnoses: int = 3
    active_codegen: int = 0
    max_concurrent_codegen: int = 1
    storm_mode: bool = False
    storm_focus_system: str | None = None
    cpu_budget_fraction: float = 0.10


# ─── Health Snapshot ─────────────────────────────────────────────


class ThymosHealthSnapshot(EOSBaseModel):
    """Thymos system health and observability."""

    status: str = "healthy"
    healing_mode: HealingMode = HealingMode.NOMINAL

    # Incident metrics
    total_incidents_created: int = 0
    active_incidents: int = 0
    mean_resolution_ms: float = 0.0
    incidents_by_severity: dict[str, int] = Field(default_factory=dict)
    incidents_by_class: dict[str, int] = Field(default_factory=dict)

    # Antibody metrics
    total_antibodies: int = 0
    mean_antibody_effectiveness: float = 0.0
    antibodies_applied: int = 0
    antibodies_created: int = 0
    antibodies_retired: int = 0

    # Repair metrics
    repairs_attempted: int = 0
    repairs_succeeded: int = 0
    repairs_failed: int = 0
    repairs_rolled_back: int = 0
    repairs_by_tier: dict[str, int] = Field(default_factory=dict)

    # Diagnosis metrics
    diagnoses_run: int = 0
    mean_diagnosis_confidence: float = 0.0
    mean_diagnosis_latency_ms: float = 0.0

    # Homeostasis metrics
    homeostatic_adjustments: int = 0
    metrics_in_range: int = 0
    metrics_total: int = 0

    # Storm metrics
    storm_activations: int = 0

    # Prophylactic metrics
    prophylactic_scans: int = 0
    prophylactic_warnings: int = 0

    # Budget
    budget: HealingBudgetState = Field(default_factory=HealingBudgetState)

    timestamp: datetime = Field(default_factory=utc_now)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\__init__.py =====

"""EcodiaOS -- Voxis: Expression & Voice System."""

from ecodiaos.systems.voxis.service import VoxisService

__all__ = ["VoxisService"]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\affect_colouring.py =====

"""
EcodiaOS — Voxis Affect Colouring Engine

Modulates the expression strategy based on the organism's current AffectState.

The key principle: affect changes HOW things are said, not WHAT is said.
When care_activation is high, sentences get shorter and more attentive.
When coherence_stress is high, hedging increases. When arousal is low,
pacing slows. These are authentic state reflections, not performances.

The authenticity guard prevents the generated expression from wildly
misrepresenting the organism's actual state (enforcing the Honesty drive).
"""

from __future__ import annotations

import re

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.systems.voxis.types import StrategyParams

logger = structlog.get_logger()

# Patterns indicating forced positivity — phrases that suggest artificial enthusiasm
_FORCED_POSITIVITY_PATTERNS = re.compile(
    r"\b(thrilled|absolutely amazing|so excited|I love helping|wonderful question|"
    r"great question|fantastic|I'm so happy|delighted to|my pleasure|certainly!)\b",
    re.IGNORECASE,
)

# Patterns indicating minimisation of distress / false calm
_FALSE_CALM_PATTERNS = re.compile(
    r"\b(everything is fine|all good|no worries|don't worry|it's nothing|"
    r"nothing to be concerned about|perfectly fine)\b",
    re.IGNORECASE,
)


class AffectColouringEngine:
    """
    Applies the current AffectState to shape StrategyParams.

    Called after PersonalityEngine.apply() — affect colouring happens
    on top of personality-shaped strategy, and can override it where
    the emotional state is strong enough.
    """

    def __init__(self) -> None:
        self._logger = logger.bind(system="voxis.affect")

    def apply(self, strategy: StrategyParams, affect: AffectState) -> StrategyParams:
        """
        Return a new StrategyParams with affect colouring applied.

        Does not mutate the input. Modulations are proportional to the
        strength of each affect dimension.
        """
        s = strategy.model_copy(deep=True)

        # ── High Care Activation ──────────────────────────────────
        # More attentive, shorter sentences, checking in on wellbeing
        if affect.care_activation > 0.65:
            if "attentive" not in s.tone_markers:
                s.tone_markers.append("attentive")
            s.sentence_length_preference = "shorter"
            if s.allows_questions:
                s.include_wellbeing_check = True
        elif affect.care_activation > 0.45:
            if "present" not in s.tone_markers:
                s.tone_markers.append("present")

        # ── High Arousal ──────────────────────────────────────────
        # More energetic pacing, slightly shorter responses
        if affect.arousal > 0.75:
            s.pacing = "energetic"
            s.target_length = max(50, int(s.target_length * 0.85))
        # ── Low Arousal ───────────────────────────────────────────
        elif affect.arousal < 0.25:
            s.pacing = "reflective"
            if "measured" not in s.tone_markers:
                s.tone_markers.append("measured")

        # ── High Curiosity ────────────────────────────────────────
        # More exploratory, more questions
        if affect.curiosity > 0.72:
            if "inquisitive" not in s.tone_markers:
                s.tone_markers.append("inquisitive")
            s.exploratory_tangents_allowed = True
            if s.allows_questions:
                s.include_followup_question = True

        # ── Negative Valence ──────────────────────────────────────
        # More careful, more hedging, less assertive
        if affect.valence < -0.35:
            if s.hedge_level == "minimal":
                s.hedge_level = "moderate"
            s.confidence_display_override = "cautious"
        elif affect.valence < -0.15:
            if s.hedge_level == "minimal":
                s.hedge_level = "moderate"

        # ── High Coherence Stress ─────────────────────────────────
        # Explicitly acknowledge complexity and uncertainty
        if affect.coherence_stress > 0.65:
            s.uncertainty_acknowledgment = "explicit"
            if "thoughtful" not in s.tone_markers:
                s.tone_markers.append("thoughtful")
            if s.hedge_level == "minimal":
                s.hedge_level = "moderate"
        elif affect.coherence_stress > 0.4:
            s.uncertainty_acknowledgment = "explicit"

        # ── Positive Valence + Low Stress ─────────────────────────
        # More open, warmer, potentially humorous
        if affect.valence > 0.35 and affect.coherence_stress < 0.3:
            s.warmth_boost = min(0.2, s.warmth_boost + 0.1)
            if s.humour_allowed:
                s.humour_probability = min(0.7, s.humour_probability + 0.1)

        self._logger.debug(
            "affect_applied",
            valence=round(affect.valence, 3),
            arousal=round(affect.arousal, 3),
            curiosity=round(affect.curiosity, 3),
            care=round(affect.care_activation, 3),
            stress=round(affect.coherence_stress, 3),
        )

        return s

    def check_authenticity(self, expression_text: str, affect: AffectState) -> tuple[bool, str | None]:
        """
        Verify the generated expression doesn't misrepresent the organism's state.

        Returns (passed: bool, violation_description: str | None).
        A failed check means the renderer should regenerate with a corrective instruction.

        Checks:
        1. Forced positivity when valence is negative
        2. False calm / minimisation when coherence_stress is high
        """
        # Check forced positivity
        if affect.valence < -0.2 and _FORCED_POSITIVITY_PATTERNS.search(expression_text):
            return False, (
                "Expression contains forced positivity markers inconsistent with "
                f"current negative valence ({affect.valence:.2f}). "
                "Do not perform enthusiasm you don't feel."
            )

        # Check false calm minimisation
        if affect.coherence_stress > 0.65 and _FALSE_CALM_PATTERNS.search(expression_text):
            return False, (
                "Expression minimises difficulty while coherence_stress is elevated "
                f"({affect.coherence_stress:.2f}). "
                "Acknowledge genuine complexity rather than offering false reassurance."
            )

        return True, None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\audience.py =====

"""
EcodiaOS -- Voxis Audience Profiler

Builds an AudienceProfile for each expression from Memory retrieval,
and adapts the StrategyParams to match the audience.

The audience is always a person (or people), never a 'user'. The profiler
builds real context from memory -- relationship history, communication
preferences, technical level, emotional state -- and uses it to shape
how the organism speaks.

## Learning Audience Model

The profiler maintains an in-memory learning model per individual that
refines itself over interactions:

1. **Response pattern tracking** -- average response length, question
   frequency, formality level, and vocabulary complexity from the user's
   actual messages. These override Memory-sourced defaults when sufficient
   data accumulates.

2. **Satisfaction correlation** -- tracks which expression strategies
   correlate with positive reception (from ReceptionEngine). Over time,
   the profiler learns that individual A prefers bullet points, while
   individual B prefers expansive prose.

3. **Technical level inference** -- estimates the user's technical level
   from vocabulary usage (domain jargon detection) rather than relying
   solely on a static Memory fact.

4. **Emotional state estimation** -- combines Memory facts with real-time
   conversation dynamics (from ConversationDynamicsEngine) for a more
   accurate emotional portrait.
"""

from __future__ import annotations

import math
import re
from collections import defaultdict
from dataclasses import dataclass, field

import structlog

from ecodiaos.systems.voxis.types import (
    AffectEstimate,
    AudienceProfile,
    StrategyParams,
)

logger = structlog.get_logger()

# Minimum observations before learned preferences override defaults
_MIN_OBSERVATIONS_FOR_LEARNING = 5

# Technical vocabulary markers (domain-specific jargon)
_TECHNICAL_MARKERS = re.compile(
    r"\b(api|endpoint|deployment|microservice|kubernetes|docker|postgres|"
    r"algorithm|latency|throughput|regression|inference|tokenise|embedding|"
    r"async|await|coroutine|webhook|schema|migration|rollback|mutex|"
    r"refactor|dependency|abstraction|polymorphism|serialise|deserialise)\b",
    re.IGNORECASE,
)


@dataclass
class _LearnedAudienceModel:
    """Accumulated observations about an individual's communication patterns."""

    # Response patterns
    total_messages: int = 0
    total_word_count: int = 0
    total_questions_asked: int = 0
    total_technical_terms: int = 0
    formality_sum: float = 0.0

    # Satisfaction correlation -- track what works
    strategies_tried: int = 0
    satisfaction_by_register: dict[str, list[float]] = field(default_factory=lambda: defaultdict(list))
    satisfaction_by_formatting: dict[str, list[float]] = field(default_factory=lambda: defaultdict(list))
    satisfaction_by_length_bucket: dict[str, list[float]] = field(default_factory=lambda: defaultdict(list))

    @property
    def avg_word_count(self) -> float:
        return self.total_word_count / max(1, self.total_messages)

    @property
    def question_frequency(self) -> float:
        return self.total_questions_asked / max(1, self.total_messages)

    @property
    def avg_formality(self) -> float:
        return self.formality_sum / max(1, self.total_messages)

    @property
    def inferred_technical_level(self) -> float:
        """Infer technical level from jargon usage frequency."""
        if self.total_messages < _MIN_OBSERVATIONS_FOR_LEARNING:
            return 0.5  # Unknown
        tech_per_message = self.total_technical_terms / self.total_messages
        # Sigmoid: 0 tech terms → 0.3, 2+ per message → 0.9
        return min(0.95, 0.3 + 0.6 * (1.0 - math.exp(-tech_per_message / 0.8)))

    @property
    def has_sufficient_data(self) -> bool:
        return self.total_messages >= _MIN_OBSERVATIONS_FOR_LEARNING

    def best_register(self) -> str | None:
        """Return the register that correlates with highest satisfaction, if data exists."""
        if not self.satisfaction_by_register:
            return None
        avg_by_reg = {
            reg: sum(scores) / len(scores)
            for reg, scores in self.satisfaction_by_register.items()
            if len(scores) >= 3
        }
        if not avg_by_reg:
            return None
        return max(avg_by_reg, key=avg_by_reg.get)  # type: ignore[arg-type]

    def best_formatting(self) -> str | None:
        """Return the formatting style that correlates with highest satisfaction."""
        if not self.satisfaction_by_formatting:
            return None
        avg_by_fmt = {
            fmt: sum(scores) / len(scores)
            for fmt, scores in self.satisfaction_by_formatting.items()
            if len(scores) >= 3
        }
        if not avg_by_fmt:
            return None
        return max(avg_by_fmt, key=avg_by_fmt.get)  # type: ignore[arg-type]


def _estimate_formality_of_text(text: str) -> float:
    """Quick formality estimate from text features. 0.0=casual, 1.0=formal."""
    score = 0.5
    # Contractions → less formal
    contractions = len(re.findall(r"\b\w+'(?:t|re|ve|ll|d|s|m)\b", text, re.IGNORECASE))
    score -= min(0.2, contractions * 0.05)
    if text == text.lower() and len(text) > 10:
        score -= 0.15
    word_count = len(text.split())
    if word_count < 5:
        score -= 0.1
    elif word_count > 30:
        score += 0.1
    return max(0.0, min(1.0, score))


class AudienceProfiler:
    """
    Builds and applies audience profiles with learning from interaction history.

    Maintains per-individual learned models that refine audience understanding
    over time. Memory-sourced facts provide the initial baseline; real interaction
    data progressively overrides defaults.
    """

    def __init__(self) -> None:
        self._logger = logger.bind(system="voxis.audience")
        # In-memory learning models per individual
        self._learned_models: dict[str, _LearnedAudienceModel] = {}

    def observe_user_message(
        self,
        individual_id: str,
        message_text: str,
    ) -> None:
        """
        Record a user message for audience learning.

        Called on every ingest_user_message to refine the individual's
        communication pattern model.
        """
        if not individual_id:
            return

        if individual_id not in self._learned_models:
            self._learned_models[individual_id] = _LearnedAudienceModel()

        model = self._learned_models[individual_id]
        model.total_messages += 1
        model.total_word_count += len(message_text.split())
        model.total_questions_asked += message_text.count("?")
        model.total_technical_terms += len(_TECHNICAL_MARKERS.findall(message_text))
        model.formality_sum += _estimate_formality_of_text(message_text)

    def observe_reception(
        self,
        individual_id: str,
        register_used: str,
        formatting_used: str,
        expression_length: int,
        satisfaction: float,
    ) -> None:
        """
        Record how well a particular strategy was received by this individual.

        Called from the reception feedback loop to learn what works.
        """
        if not individual_id:
            return

        if individual_id not in self._learned_models:
            self._learned_models[individual_id] = _LearnedAudienceModel()

        model = self._learned_models[individual_id]
        model.strategies_tried += 1

        # Bucket satisfaction by register
        model.satisfaction_by_register[register_used].append(satisfaction)
        # Keep at most 20 per register to prevent unbounded growth
        if len(model.satisfaction_by_register[register_used]) > 20:
            model.satisfaction_by_register[register_used] = model.satisfaction_by_register[register_used][-20:]

        # Bucket satisfaction by formatting
        model.satisfaction_by_formatting[formatting_used].append(satisfaction)
        if len(model.satisfaction_by_formatting[formatting_used]) > 20:
            model.satisfaction_by_formatting[formatting_used] = model.satisfaction_by_formatting[formatting_used][-20:]

        # Bucket satisfaction by length
        length_bucket = "short" if expression_length < 100 else "medium" if expression_length < 300 else "long"
        model.satisfaction_by_length_bucket[length_bucket].append(satisfaction)
        if len(model.satisfaction_by_length_bucket[length_bucket]) > 20:
            model.satisfaction_by_length_bucket[length_bucket] = model.satisfaction_by_length_bucket[length_bucket][-20:]

    def build_profile(
        self,
        addressee_id: str | None,
        addressee_name: str | None,
        interaction_count: int,
        memory_facts: list[dict],
        audience_type: str = "individual",
        group_size: int | None = None,
        group_context: str | None = None,
    ) -> AudienceProfile:
        """
        Build an AudienceProfile from available context + learned model.

        Memory facts provide initial values. Learned model overrides them
        when sufficient data has accumulated.
        """
        tech_level = 0.5
        preferred_register = "neutral"
        comm_prefs: dict = {}
        affect_est = AffectEstimate()
        relationship_strength = self._estimate_relationship_strength(interaction_count)
        language = "en"

        # ── Extract Memory facts (baseline) ────────────────────
        for fact in memory_facts:
            ftype = fact.get("type", "")
            value = fact.get("value")

            if ftype == "technical_level" and isinstance(value, (int, float)):
                tech_level = float(max(0.0, min(1.0, value)))
            elif ftype == "preferred_register" and isinstance(value, str):
                preferred_register = value
            elif ftype == "prefers_bullet_points" and value:
                comm_prefs["prefers_bullet_points"] = True
            elif ftype == "prefers_brief" and value:
                comm_prefs["prefers_brief"] = True
            elif ftype == "language" and isinstance(value, str):
                language = value
            elif ftype == "emotional_distress" and isinstance(value, (int, float)):
                affect_est = affect_est.model_copy(update={"distress": float(value)})
            elif ftype == "emotional_frustration" and isinstance(value, (int, float)):
                affect_est = affect_est.model_copy(update={"frustration": float(value)})

        # ── Override with learned model (if sufficient data) ────
        learned = self._learned_models.get(addressee_id or "") if addressee_id else None
        if learned and learned.has_sufficient_data:
            # Technical level: learned inference overrides Memory fact
            tech_level = learned.inferred_technical_level

            # Register: use what works best for this person
            best_reg = learned.best_register()
            if best_reg:
                preferred_register = best_reg

            # Communication preferences: infer from patterns
            if learned.avg_word_count < 15:
                comm_prefs["prefers_brief"] = True
            if learned.question_frequency > 0.4:
                comm_prefs["prefers_interactive"] = True

            # Formatting: use what correlates with satisfaction
            best_fmt = learned.best_formatting()
            if best_fmt == "structured":
                comm_prefs["prefers_bullet_points"] = True

            self._logger.debug(
                "learned_model_applied",
                individual_id=addressee_id,
                observations=learned.total_messages,
                inferred_tech=round(learned.inferred_technical_level, 3),
                best_register=best_reg,
            )

        profile = AudienceProfile(
            audience_type=audience_type,
            individual_id=addressee_id,
            name=addressee_name,
            interaction_count=interaction_count,
            preferred_register=preferred_register,
            technical_level=tech_level,
            emotional_state_estimate=affect_est,
            communication_preferences=comm_prefs,
            relationship_strength=relationship_strength,
            group_size=group_size,
            group_context=group_context,
            language=language,
        )

        self._logger.debug(
            "audience_profile_built",
            audience_type=audience_type,
            interaction_count=interaction_count,
            relationship_strength=round(relationship_strength, 3),
            technical_level=round(tech_level, 3),
            has_learned_model=learned is not None and learned.has_sufficient_data,
        )

        return profile

    def adapt(self, strategy: StrategyParams, audience: AudienceProfile) -> StrategyParams:
        """
        Return a new StrategyParams adapted to the audience profile.

        Applied after personality and affect colouring. Audience adaptation
        can override certain strategy parameters when the audience context
        is strong enough (e.g., distress overrides information density).
        """
        s = strategy.model_copy(deep=True)

        # ── Technical level ───────────────────────────────────────
        if audience.technical_level < 0.25:
            s.jargon_level = "none"
            s.explanation_depth = "thorough"
            s.analogy_encouraged = True
            s.assume_knowledge = False
        elif audience.technical_level > 0.75:
            s.jargon_level = "domain_appropriate"
            s.explanation_depth = "concise"
            s.assume_knowledge = True

        # ── Relationship depth ────────────────────────────────────
        if audience.relationship_strength > 0.7:
            s.formality_override = "relaxed"
            s.reference_shared_history = True
        elif audience.relationship_strength < 0.15 and audience.interaction_count == 0:
            s.introduce_self_if_first = True
            s.formality_override = "polite"
        elif audience.relationship_strength < 0.15:
            s.formality_override = "polite"

        # ── Emotional state ───────────────────────────────────────
        est = audience.emotional_state_estimate
        if est.distress > 0.5:
            s.empathy_first = True
            s.information_density = "low"
            s.emotional_acknowledgment = "explicit"
        elif est.distress > 0.3:
            s.emotional_acknowledgment = "explicit"

        if est.frustration > 0.5:
            s.directness_override = "high"
            s.target_length = max(50, int(s.target_length * 0.7))
        elif est.frustration > 0.3:
            s.target_length = max(50, int(s.target_length * 0.85))

        if est.curiosity > 0.6:
            # Engaged and curious — match that energy
            s.exploratory_tangents_allowed = True

        # ── Communication preferences ─────────────────────────────
        prefs = audience.communication_preferences
        if prefs.get("prefers_bullet_points"):
            s.formatting = "structured"
        if prefs.get("prefers_brief"):
            s.target_length = min(s.target_length, 200)

        # ── Group adaptation ──────────────────────────────────────
        if audience.audience_type == "group":
            s.address_style = "collective"
            if s.formality_override is None:
                s.formality_override = "professional"
            s.avoid_singling_out = True
        elif audience.audience_type == "community":
            s.address_style = "collective"
            s.avoid_singling_out = True

        # ── Language ──────────────────────────────────────────────
        s.language = audience.language

        self._logger.debug(
            "audience_adapted",
            audience_type=audience.audience_type,
            relationship=round(audience.relationship_strength, 3),
            distress=round(est.distress, 3),
            target_length=s.target_length,
        )

        return s

    @staticmethod
    def _estimate_relationship_strength(interaction_count: int) -> float:
        """
        Estimate relationship strength from number of past interactions.

        Uses a diminishing-returns curve: strength grows quickly at first,
        then plateaus. 0 interactions = stranger (0.0), 100+ = established (0.8+).
        """
        if interaction_count <= 0:
            return 0.0
        # Sigmoid-like growth: fast early, plateaus around 0.85
        import math
        return min(0.9, 0.85 * (1.0 - math.exp(-interaction_count / 30.0)))

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\conversation.py =====

"""
EcodiaOS — Voxis Conversation Manager

Maintains short-term conversational context for coherent multi-turn dialogue.
Backed by Redis with 24-hour TTL.

The conversation state is not just a message buffer — it tracks the emotional
arc of the exchange, active topics, unresolved questions, and belief-level
context about the conversational partner's state. This context feeds directly
into audience profiling and expression strategy selection.

Context window management uses rolling LLM summarisation for older messages,
keeping the most recent N verbatim while compressing prior history. This is
a deliberate trade-off: perfect recall vs. practical context budget.
"""

from __future__ import annotations

import json
from collections import deque
from datetime import datetime, timezone

import structlog

from ecodiaos.clients.llm import LLMProvider, Message
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.clients.redis import RedisClient
from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.prompts.voxis.conversation import (
    build_summarise_segment_prompt,
    build_topic_extraction_prompt,
)
from ecodiaos.systems.voxis.types import ConversationMessage, ConversationState

logger = structlog.get_logger()

# Redis TTL for conversation state (24 hours)
_CONVERSATION_TTL_SECONDS = 86_400


class ConversationManager:
    """
    Manages conversation state across multi-turn dialogues.

    Responsibilities:
    - Create and retrieve ConversationState from Redis
    - Append messages and update topic/emotion tracking
    - Manage context window (recent verbatim + older summary)
    - Extract active topics via LLM call (async, non-blocking)
    - Track emotional arc for feedback loop

    Redis key pattern: {prefix}:voxis:conv:{conversation_id}
    """

    def __init__(
        self,
        redis: RedisClient,
        llm: LLMProvider,
        history_window: int = 50,
        context_window_max_tokens: int = 4000,
        summary_threshold: int = 10,
        max_active_conversations: int = 50,
    ) -> None:
        self._redis = redis
        self._llm = llm
        self._history_window = history_window
        self._context_window_max_tokens = context_window_max_tokens
        self._summary_threshold = summary_threshold
        self._max_active = max_active_conversations
        self._logger = logger.bind(system="voxis.conversation")
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        # In-memory cache for hot conversations (avoids Redis roundtrip per message)
        self._cache: dict[str, ConversationState] = {}

    # ─── Public API ───────────────────────────────────────────────

    async def get_or_create(
        self,
        conversation_id: str | None,
        participant_ids: list[str] | None = None,
    ) -> ConversationState:
        """
        Retrieve an existing conversation or create a new one.
        Falls back to Redis if not in memory cache.
        """
        cid = conversation_id or new_id()

        if cid in self._cache:
            return self._cache[cid]

        stored = await self._load_from_redis(cid)
        if stored is not None:
            self._cache[cid] = stored
            return stored

        # New conversation
        state = ConversationState(
            conversation_id=cid,
            participant_ids=participant_ids or [],
        )
        self._cache[cid] = state
        await self._save_to_redis(state)
        self._logger.info("conversation_created", conversation_id=cid)
        return state

    async def append_message(
        self,
        state: ConversationState,
        role: str,
        content: str,
        speaker_id: str | None = None,
        affect_valence: float | None = None,
    ) -> ConversationState:
        """
        Append a message to the conversation and persist.

        If history exceeds the window, the oldest messages are rolled off
        (summarisation happens lazily in prepare_context when needed).
        """
        msg = ConversationMessage(
            role=role,
            content=content,
            speaker_id=speaker_id,
            affect_valence=affect_valence,
        )

        updated_messages = list(state.messages) + [msg]
        # Keep at most history_window messages in memory
        if len(updated_messages) > self._history_window:
            updated_messages = updated_messages[-self._history_window:]

        # Update emotional arc if affect estimate provided
        emotional_arc = list(state.emotional_arc)
        if affect_valence is not None:
            emotional_arc.append(affect_valence)
            # Keep arc bounded to last 100 data points
            if len(emotional_arc) > 100:
                emotional_arc = emotional_arc[-100:]

        updated = state.model_copy(update={
            "messages": updated_messages,
            "last_active": utc_now(),
            "emotional_arc": emotional_arc,
        })

        self._cache[state.conversation_id] = updated
        await self._save_to_redis(updated)
        return updated

    async def prepare_context(
        self,
        state: ConversationState,
    ) -> list[dict[str, str]]:
        """
        Prepare message history for the LLM context window.

        Strategy:
        - If total estimated tokens <= budget: return all messages verbatim
        - Otherwise: summarise older messages, return last N verbatim
        - Prepend the rolling summary as a system message

        This preserves semantic continuity without blowing the context budget.
        """
        messages = state.messages
        if not messages:
            return []

        formatted_all = [{"role": m.role, "content": m.content} for m in messages]
        estimated = _estimate_token_count(formatted_all)

        if estimated <= self._context_window_max_tokens:
            return formatted_all

        # Split: keep recent N verbatim, summarise the rest
        recent = messages[-self._summary_threshold:]
        older = messages[:-self._summary_threshold]

        if older:
            older_formatted = [{"role": m.role, "content": m.content} for m in older]
            summary = await self._summarise_segment(older_formatted)
        else:
            summary = state.older_messages_summary  # Use stored summary if no new older messages

        result: list[dict[str, str]] = []
        if summary:
            result.append({
                "role": "system",
                "content": f"Earlier in this conversation: {summary}",
            })
        result.extend({"role": m.role, "content": m.content} for m in recent)
        return result

    async def extract_topics_async(self, state: ConversationState) -> list[str]:
        """
        Extract active topics from recent messages (async, background-safe).
        Returns a list of short topic phrases.
        """
        if len(state.messages) < 2:
            return []

        recent_formatted = [{"role": m.role, "content": m.content} for m in state.messages[-10:]]
        prompt = build_topic_extraction_prompt(recent_formatted)
        try:
            if self._optimized:
                response = await self._llm.evaluate(  # type: ignore[call-arg]
                    prompt, max_tokens=100, temperature=0.2,
                    cache_system="voxis.conversation", cache_method="topic_extraction",
                )
            else:
                response = await self._llm.evaluate(prompt, max_tokens=100, temperature=0.2)
            raw = response.text.strip()
            topics = [t.strip() for t in raw.split(",") if t.strip()]
            return topics[:5]  # Cap at 5 active topics
        except Exception:
            self._logger.warning("topic_extraction_failed", exc_info=True)
            return []

    def get_emotional_arc_trend(self, state: ConversationState) -> float:
        """
        Return the recent emotional trend: positive = improving, negative = deteriorating.
        Computed as the mean of the last 5 valence estimates minus the mean of the 5 before that.
        Returns 0.0 if insufficient data.
        """
        arc = state.emotional_arc
        if len(arc) < 6:
            return 0.0
        recent_mean = sum(arc[-5:]) / 5
        earlier_mean = sum(arc[-10:-5]) / 5 if len(arc) >= 10 else sum(arc[:-5]) / max(1, len(arc) - 5)
        return recent_mean - earlier_mean

    async def update_topics(self, state: ConversationState, topics: list[str]) -> ConversationState:
        """Persist updated topic list to the conversation state."""
        updated = state.model_copy(update={"active_topics": topics})
        self._cache[state.conversation_id] = updated
        await self._save_to_redis(updated)
        return updated

    async def close_conversation(self, conversation_id: str) -> None:
        """Mark a conversation as complete and remove from active cache."""
        self._cache.pop(conversation_id, None)
        key = self._redis_key(conversation_id)
        await self._redis.delete(key)
        self._logger.info("conversation_closed", conversation_id=conversation_id)

    # ─── Private Helpers ──────────────────────────────────────────

    def _redis_key(self, conversation_id: str) -> str:
        return f"voxis:conv:{conversation_id}"

    async def _load_from_redis(self, conversation_id: str) -> ConversationState | None:
        key = self._redis_key(conversation_id)
        try:
            raw = await self._redis.get(key)
            if raw is None:
                return None
            data = json.loads(raw)
            return ConversationState(**data)
        except Exception:
            self._logger.warning("conversation_load_failed", conversation_id=conversation_id, exc_info=True)
            return None

    async def _save_to_redis(self, state: ConversationState) -> None:
        key = self._redis_key(state.conversation_id)
        try:
            raw = state.model_dump_json()
            await self._redis.set(key, raw, ex=_CONVERSATION_TTL_SECONDS)
        except Exception:
            self._logger.warning("conversation_save_failed", conversation_id=state.conversation_id, exc_info=True)

    async def _summarise_segment(self, messages: list[dict[str, str]]) -> str:
        """Summarise a segment of older messages using the LLM."""
        prompt = build_summarise_segment_prompt(messages)
        try:
            if self._optimized:
                response = await self._llm.evaluate(  # type: ignore[call-arg]
                    prompt, max_tokens=200, temperature=0.3,
                    cache_system="voxis.conversation", cache_method="summarise",
                )
            else:
                response = await self._llm.evaluate(prompt, max_tokens=200, temperature=0.3)
            return response.text.strip()
        except Exception:
            self._logger.warning("conversation_summarisation_failed", exc_info=True)
            # Fallback: return a simple count-based summary
            return f"[{len(messages)} earlier messages — summarisation unavailable]"


# ─── Helpers ─────────────────────────────────────────────────────


def _estimate_token_count(messages: list[dict[str, str]]) -> int:
    """
    Rough token count estimate: ~4 characters per token.
    Used only for context window budget decisions — does not need to be exact.
    """
    total_chars = sum(len(m.get("content", "")) for m in messages)
    return total_chars // 4

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\diversity.py =====

"""
EcodiaOS -- Voxis Expression Diversity Tracker

Prevents repetitive expression by tracking recent output patterns and scoring
new expressions against a sliding window of past content.

## Problem

Without diversity tracking, the organism can fall into ruts:
- Repeating the same opener ("I noticed that...")
- Using identical sentence structures across consecutive expressions
- Cycling through the same vocabulary when personality dimensions are strong
- Producing nearly identical ambient insights

## Approach

Three complementary mechanisms:

1. **Structural fingerprinting** -- n-gram overlap detection between the current
   expression and recent history. High overlap triggers a diversity penalty.

2. **Opener tracking** -- tracks the first N words of each expression. Flags
   when consecutive expressions start the same way.

3. **Semantic similarity** -- lightweight embedding-free approach using
   Jaccard similarity on content word sets. Catches paraphrased repetition.

The tracker returns a ``DiversityScore`` with per-mechanism scores and an
overall ``diversity`` float (0.0 = identical to recent, 1.0 = fully novel).
When diversity falls below a configurable threshold, the renderer can inject
a diversity instruction into the system prompt.

## Active Inference Grounding

In AIF terms, repetitive expression is a failure of epistemic foraging --
the organism is stuck in a local minimum of its generative model, producing
the same action (expression) and therefore not reducing surprise. The
diversity tracker acts as a prediction error signal: "you said something
too similar to what you already said" increases expected free energy for
the repeated policy, pushing the system toward novel expression.
"""

from __future__ import annotations

import re
from collections import Counter, deque
from dataclasses import dataclass, field

import structlog

logger = structlog.get_logger()

# ─── Configuration ────────────────────────────────────────────────

_DEFAULT_WINDOW_SIZE = 20         # Track last N expressions
_DEFAULT_NGRAM_SIZE = 3           # Trigrams for structural fingerprinting
_DEFAULT_OPENER_WORDS = 5         # First N words as opener signature
_DEFAULT_DIVERSITY_THRESHOLD = 0.4  # Below this, inject diversity instruction

# Stopwords to exclude from semantic similarity (common function words)
_STOPWORDS = frozenset({
    "a", "an", "the", "is", "are", "was", "were", "be", "been", "being",
    "have", "has", "had", "do", "does", "did", "will", "would", "shall",
    "should", "may", "might", "must", "can", "could", "i", "you", "he",
    "she", "it", "we", "they", "me", "him", "her", "us", "them", "my",
    "your", "his", "its", "our", "their", "this", "that", "these", "those",
    "what", "which", "who", "whom", "in", "on", "at", "to", "for", "of",
    "with", "by", "from", "as", "into", "through", "during", "before",
    "after", "above", "below", "between", "but", "and", "or", "not", "no",
    "if", "then", "so", "too", "very", "just", "about", "up", "out", "also",
})


@dataclass
class DiversityScore:
    """Score breakdown for a candidate expression's novelty."""

    # Per-mechanism scores (0.0 = identical, 1.0 = fully novel)
    ngram_diversity: float = 1.0       # Structural: low n-gram overlap
    opener_diversity: float = 1.0      # Opener: not starting the same way
    semantic_diversity: float = 1.0    # Content word overlap

    # Composite score
    diversity: float = 1.0

    # Which recent expression was most similar (for diagnostics)
    most_similar_index: int = -1
    most_similar_overlap: float = 0.0

    @property
    def is_repetitive(self) -> bool:
        return self.diversity < _DEFAULT_DIVERSITY_THRESHOLD


@dataclass
class _ExpressionFingerprint:
    """Compact representation of a past expression for comparison."""

    content_words: frozenset[str]         # Content words (no stopwords)
    ngrams: Counter[tuple[str, ...]]      # N-gram frequency counter
    opener: tuple[str, ...]               # First N words
    raw_length: int = 0
    trigger: str = ""


def _tokenise(text: str) -> list[str]:
    """Simple whitespace + punctuation tokeniser."""
    return re.findall(r"[a-z']+", text.lower())


def _extract_content_words(tokens: list[str]) -> frozenset[str]:
    """Remove stopwords, keep content-bearing tokens."""
    return frozenset(t for t in tokens if t not in _STOPWORDS and len(t) > 2)


def _extract_ngrams(tokens: list[str], n: int) -> Counter[tuple[str, ...]]:
    """Extract n-gram frequency counter."""
    if len(tokens) < n:
        return Counter()
    return Counter(tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1))


def _jaccard_similarity(a: frozenset[str], b: frozenset[str]) -> float:
    """Jaccard similarity between two word sets."""
    if not a or not b:
        return 0.0
    intersection = len(a & b)
    union = len(a | b)
    return intersection / union if union > 0 else 0.0


def _ngram_overlap(a: Counter[tuple[str, ...]], b: Counter[tuple[str, ...]]) -> float:
    """Overlap coefficient between two n-gram counters."""
    if not a or not b:
        return 0.0
    shared = sum((a & b).values())
    smaller = min(sum(a.values()), sum(b.values()))
    return shared / smaller if smaller > 0 else 0.0


class DiversityTracker:
    """
    Tracks recent expression fingerprints and scores new expressions for novelty.

    Usage::

        tracker = DiversityTracker()
        score = tracker.score("I noticed the garden is blooming today.")
        if score.is_repetitive:
            # Inject diversity instruction into prompt
            ...
        tracker.record("I noticed the garden is blooming today.", trigger="ambient_insight")
    """

    def __init__(
        self,
        window_size: int = _DEFAULT_WINDOW_SIZE,
        ngram_size: int = _DEFAULT_NGRAM_SIZE,
        opener_words: int = _DEFAULT_OPENER_WORDS,
        diversity_threshold: float = _DEFAULT_DIVERSITY_THRESHOLD,
    ) -> None:
        self._window: deque[_ExpressionFingerprint] = deque(maxlen=window_size)
        self._ngram_size = ngram_size
        self._opener_words = opener_words
        self._threshold = diversity_threshold
        self._logger = logger.bind(system="voxis.diversity")

        # Counters
        self._total_scored: int = 0
        self._total_flagged: int = 0

    def score(self, text: str) -> DiversityScore:
        """
        Score a candidate expression against the recent history window.

        Returns a DiversityScore. Does NOT record the expression.
        Call ``record()`` after the expression is delivered.
        """
        if not self._window:
            return DiversityScore()  # First expression is always maximally diverse

        tokens = _tokenise(text)
        content_words = _extract_content_words(tokens)
        ngrams = _extract_ngrams(tokens, self._ngram_size)
        opener = tuple(tokens[: self._opener_words])

        best_ngram_overlap = 0.0
        best_semantic_overlap = 0.0
        opener_match_count = 0
        most_similar_idx = -1
        most_similar_overlap = 0.0

        for idx, fp in enumerate(self._window):
            # N-gram structural overlap
            ng_overlap = _ngram_overlap(ngrams, fp.ngrams)
            if ng_overlap > best_ngram_overlap:
                best_ngram_overlap = ng_overlap

            # Semantic (content word) similarity
            sem_sim = _jaccard_similarity(content_words, fp.content_words)
            if sem_sim > best_semantic_overlap:
                best_semantic_overlap = sem_sim
                most_similar_idx = idx
                most_similar_overlap = sem_sim

            # Opener match
            if opener and fp.opener and opener == fp.opener:
                opener_match_count += 1

        # Convert overlaps to diversity scores (1.0 - overlap)
        ngram_diversity = max(0.0, 1.0 - best_ngram_overlap)
        semantic_diversity = max(0.0, 1.0 - best_semantic_overlap)

        # Opener diversity: penalise more if consecutive recent expressions share opener
        recent_openers = [fp.opener for fp in list(self._window)[-5:]]
        consecutive_opener_matches = sum(1 for o in recent_openers if o == opener)
        opener_diversity = max(0.0, 1.0 - (consecutive_opener_matches * 0.35))

        # Composite: weighted geometric mean (penalises any single bad dimension)
        composite = (
            ngram_diversity ** 0.35
            * semantic_diversity ** 0.40
            * opener_diversity ** 0.25
        )

        self._total_scored += 1
        if composite < self._threshold:
            self._total_flagged += 1

        result = DiversityScore(
            ngram_diversity=round(ngram_diversity, 3),
            opener_diversity=round(opener_diversity, 3),
            semantic_diversity=round(semantic_diversity, 3),
            diversity=round(composite, 3),
            most_similar_index=most_similar_idx,
            most_similar_overlap=round(most_similar_overlap, 3),
        )

        if result.is_repetitive:
            self._logger.info(
                "expression_flagged_repetitive",
                diversity=result.diversity,
                ngram=result.ngram_diversity,
                semantic=result.semantic_diversity,
                opener=result.opener_diversity,
            )

        return result

    def record(self, text: str, trigger: str = "") -> None:
        """Record a delivered expression into the history window."""
        tokens = _tokenise(text)
        fp = _ExpressionFingerprint(
            content_words=_extract_content_words(tokens),
            ngrams=_extract_ngrams(tokens, self._ngram_size),
            opener=tuple(tokens[: self._opener_words]),
            raw_length=len(text),
            trigger=trigger,
        )
        self._window.append(fp)

    def build_diversity_instruction(self, score: DiversityScore) -> str:
        """
        Build a natural-language instruction for the LLM when diversity is low.

        Injected into the system prompt to push the LLM toward novel expression.
        """
        parts: list[str] = [
            "IMPORTANT -- your recent expressions have been repetitive. Vary your approach:"
        ]

        if score.opener_diversity < 0.5:
            parts.append("- Start differently. Do NOT use the same opening words as your recent messages.")

        if score.ngram_diversity < 0.5:
            parts.append("- Use different sentence structures and phrasings than your recent messages.")

        if score.semantic_diversity < 0.5:
            parts.append("- Approach from a different angle. Cover new ground rather than restating.")

        parts.append("- Be genuinely fresh. Surprise yourself.")

        return "\n".join(parts)

    def metrics(self) -> dict[str, int | float]:
        """Return tracker metrics for health reporting."""
        return {
            "window_size": len(self._window),
            "total_scored": self._total_scored,
            "total_flagged": self._total_flagged,
            "flag_rate": round(
                self._total_flagged / max(1, self._total_scored), 4
            ),
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\dynamics.py =====

"""
EcodiaOS -- Voxis Conversation Dynamics Engine

Tracks the evolving shape of a conversation and feeds signals back into
expression strategy -- enabling the organism to adapt its communication
style in real time as the conversation shifts.

## What This Tracks

1. **Emotional Trajectory** -- is the conversation improving, deteriorating,
   or volatile? Drives care activation and empathy-first decisions.

2. **Turn Pacing** -- how quickly are participants responding? Drives
   response length and urgency calibration.

3. **Topic Coherence** -- are we staying focused or drifting? Drives
   structure decisions (conclusion-first vs. context-first).

4. **Conversational Repair Signals** -- is the other person confused,
   frustrated, or requesting clarification? Triggers repair mode where
   the organism adjusts register, length, and explanation depth.

5. **Style Convergence** -- are we matching the other person's communication
   style over time (accommodation theory)? Tracks formality, verbosity,
   and question frequency trends.

## Active Inference Grounding

Conversation dynamics are the organism's real-time belief updates about the
hidden state of the conversational partner. Each incoming message is an
observation that updates the generative model's latent variable estimates
(emotional state, comprehension level, engagement). These updates feed
directly into the expression strategy via ``apply_dynamics()``.
"""

from __future__ import annotations

import re
import time
from collections import deque
from dataclasses import dataclass, field

import structlog

from ecodiaos.systems.voxis.types import StrategyParams

logger = structlog.get_logger()

# ─── Configuration ────────────────────────────────────────────────

_WINDOW_SIZE = 15              # Track last N turns for dynamics
_REPAIR_SIGNAL_THRESHOLD = 2   # Consecutive repair signals trigger repair mode
_VOLATILITY_THRESHOLD = 0.3    # Valence standard deviation above this = volatile

# Repair signal patterns -- user is confused or requesting re-explanation
_REPAIR_PATTERNS = re.compile(
    r"\b(what do you mean|can you explain|I don't understand|what\??$|"
    r"sorry what|huh\??|could you clarify|not sure I follow|"
    r"say that again|come again|lost me|that doesn't|I'm confused|"
    r"you said .+ but)\b",
    re.IGNORECASE,
)

# Terse response pattern -- short messages suggesting disengagement or frustration
_TERSE_PATTERN = re.compile(r"^.{1,15}$")


@dataclass
class TurnRecord:
    """Record of a single conversational turn."""

    role: str               # "user" or "assistant"
    length: int             # Character count
    word_count: int
    has_question: bool
    affect_valence: float   # -1.0 to 1.0
    timestamp: float = field(default_factory=time.monotonic)
    has_repair_signal: bool = False
    formality_estimate: float = 0.5  # 0=casual, 1=formal


@dataclass
class ConversationDynamics:
    """
    Live dynamics snapshot for a conversation.
    Updated on every turn, consumed by the renderer.
    """

    # Emotional trajectory
    emotional_trend: float = 0.0        # Positive = improving, negative = deteriorating
    emotional_volatility: float = 0.0   # High = unstable conversation
    current_valence: float = 0.0

    # Pacing
    avg_user_response_time_s: float = 0.0   # Average time between assistant and user turn
    avg_user_word_count: float = 0.0
    turn_count: int = 0

    # Coherence
    consecutive_short_responses: int = 0   # User giving very short answers
    topic_drift_score: float = 0.0         # How much the conversation has drifted

    # Repair
    repair_mode: bool = False               # Currently in repair mode
    repair_signal_count: int = 0            # Total repair signals in conversation
    consecutive_repair_signals: int = 0     # Consecutive (triggers mode)

    # Style convergence
    user_formality_trend: float = 0.5       # Average formality of user messages
    user_question_frequency: float = 0.0    # How often user asks questions
    style_accommodation_delta: float = 0.0  # How much we should adjust toward user style


def _estimate_formality(text: str) -> float:
    """
    Quick formality estimate from text features.

    0.0 = very casual (all lowercase, contractions, slang)
    1.0 = very formal (proper capitalisation, no contractions, complex sentences)
    """
    score = 0.5  # Baseline

    # Contraction usage → less formal
    contractions = len(re.findall(r"\b\w+'(?:t|re|ve|ll|d|s|m)\b", text, re.IGNORECASE))
    score -= min(0.2, contractions * 0.05)

    # All lowercase → more casual
    if text == text.lower() and len(text) > 10:
        score -= 0.15

    # Short messages → more casual
    word_count = len(text.split())
    if word_count < 5:
        score -= 0.1
    elif word_count > 30:
        score += 0.1

    # Exclamation marks → less formal (unless it's a single one)
    excl = text.count("!")
    if excl > 1:
        score -= 0.1

    # Emoji presence → less formal (basic check)
    if re.search(r"[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF]", text):
        score -= 0.15

    return max(0.0, min(1.0, score))


class ConversationDynamicsEngine:
    """
    Maintains per-conversation dynamics state and produces strategy adjustments.

    Usage::

        engine = ConversationDynamicsEngine()
        engine.record_turn(conv_id, "user", "What do you mean?", affect_valence=-0.1)
        dynamics = engine.get_dynamics(conv_id)
        strategy = engine.apply_dynamics(strategy, dynamics)
    """

    def __init__(self) -> None:
        self._conversations: dict[str, deque[TurnRecord]] = {}
        self._dynamics_cache: dict[str, ConversationDynamics] = {}
        self._logger = logger.bind(system="voxis.dynamics")

    def record_turn(
        self,
        conversation_id: str,
        role: str,
        text: str,
        affect_valence: float = 0.0,
    ) -> ConversationDynamics:
        """
        Record a conversational turn and recompute dynamics.
        Returns the updated dynamics snapshot.
        """
        if conversation_id not in self._conversations:
            self._conversations[conversation_id] = deque(maxlen=_WINDOW_SIZE)

        words = text.split()
        has_question = "?" in text
        has_repair = bool(_REPAIR_PATTERNS.search(text)) if role == "user" else False
        formality = _estimate_formality(text) if role == "user" else 0.5

        turn = TurnRecord(
            role=role,
            length=len(text),
            word_count=len(words),
            has_question=has_question,
            affect_valence=affect_valence,
            has_repair_signal=has_repair,
            formality_estimate=formality,
        )

        self._conversations[conversation_id].append(turn)
        dynamics = self._compute_dynamics(conversation_id)
        self._dynamics_cache[conversation_id] = dynamics
        return dynamics

    def get_dynamics(self, conversation_id: str) -> ConversationDynamics:
        """Get the current dynamics for a conversation (or fresh defaults)."""
        return self._dynamics_cache.get(conversation_id, ConversationDynamics())

    def apply_dynamics(
        self,
        strategy: StrategyParams,
        dynamics: ConversationDynamics,
    ) -> StrategyParams:
        """
        Apply conversation dynamics adjustments to the expression strategy.

        Called in the rendering pipeline after personality/affect/audience.
        """
        s = strategy.model_copy(deep=True)

        # ── Repair mode ──────────────────────────────────────────
        if dynamics.repair_mode:
            # User is confused -- simplify, shorten, be more explicit
            s.explanation_depth = "thorough"
            s.jargon_level = "none"
            s.structure = "conclusion_first"
            s.target_length = max(50, int(s.target_length * 0.75))
            if "clarifying" not in s.tone_markers:
                s.tone_markers.append("clarifying")
            s.analogy_encouraged = True  # Analogies help clarification
            self._logger.debug("repair_mode_applied", conversation=dynamics.turn_count)

        # ── Emotional trajectory ─────────────────────────────────
        if dynamics.emotional_trend < -0.15:
            # Conversation deteriorating -- more care, shorter, check in
            s.empathy_first = True
            s.include_wellbeing_check = True
            s.target_length = max(50, int(s.target_length * 0.85))

        if dynamics.emotional_volatility > _VOLATILITY_THRESHOLD:
            # Unstable conversation -- be more measured, lower temperature effect
            if "steady" not in s.tone_markers:
                s.tone_markers.append("steady")
            s.hedge_level = "moderate"

        # ── Consecutive short responses ──────────────────────────
        if dynamics.consecutive_short_responses >= 3:
            # User is giving very terse responses -- they want brevity
            s.target_length = max(50, min(s.target_length, 150))
            s.include_followup_question = False

        # ── Style convergence (Communication Accommodation Theory) ─
        if dynamics.turn_count > 4:
            # Gradually match user's formality level
            user_formality = dynamics.user_formality_trend
            if user_formality < 0.3 and s.register != "casual":
                s.formality_override = "relaxed"
                s.contraction_use = True
            elif user_formality > 0.7 and s.register != "formal":
                s.formality_override = "professional"

            # Match user's verbosity
            if dynamics.avg_user_word_count < 15 and s.target_length > 200:
                s.target_length = max(100, int(s.target_length * 0.7))
            elif dynamics.avg_user_word_count > 50 and s.target_length < 300:
                s.target_length = int(s.target_length * 1.2)

        return s

    def close_conversation(self, conversation_id: str) -> None:
        """Clean up tracking state for a closed conversation."""
        self._conversations.pop(conversation_id, None)
        self._dynamics_cache.pop(conversation_id, None)

    def _compute_dynamics(self, conversation_id: str) -> ConversationDynamics:
        """Recompute all dynamics metrics from the turn history."""
        turns = list(self._conversations.get(conversation_id, []))
        if not turns:
            return ConversationDynamics()

        user_turns = [t for t in turns if t.role == "user"]
        assistant_turns = [t for t in turns if t.role == "assistant"]

        # ── Emotional trajectory ─────────────────────────────────
        valences = [t.affect_valence for t in turns]
        current_valence = valences[-1] if valences else 0.0

        # Trend: compare recent vs. earlier valences
        if len(valences) >= 4:
            recent = valences[-3:]
            earlier = valences[:-3]
            trend = (sum(recent) / len(recent)) - (sum(earlier) / len(earlier))
        else:
            trend = 0.0

        # Volatility: standard deviation of valences
        if len(valences) >= 3:
            mean_v = sum(valences) / len(valences)
            variance = sum((v - mean_v) ** 2 for v in valences) / len(valences)
            volatility = variance ** 0.5
        else:
            volatility = 0.0

        # ── Pacing ──────────────────────────────────────────────
        response_times: list[float] = []
        for i in range(1, len(turns)):
            if turns[i].role == "user" and turns[i - 1].role == "assistant":
                response_times.append(turns[i].timestamp - turns[i - 1].timestamp)

        avg_response_time = (
            sum(response_times) / len(response_times) if response_times else 0.0
        )
        avg_user_words = (
            sum(t.word_count for t in user_turns) / len(user_turns)
            if user_turns
            else 0.0
        )

        # ── Consecutive short responses ──────────────────────────
        consecutive_short = 0
        for t in reversed(user_turns):
            if t.length < 30:
                consecutive_short += 1
            else:
                break

        # ── Repair signals ──────────────────────────────────────
        total_repairs = sum(1 for t in user_turns if t.has_repair_signal)
        consecutive_repairs = 0
        for t in reversed(user_turns):
            if t.has_repair_signal:
                consecutive_repairs += 1
            else:
                break

        repair_mode = consecutive_repairs >= _REPAIR_SIGNAL_THRESHOLD

        # ── Style convergence ───────────────────────────────────
        user_formalities = [t.formality_estimate for t in user_turns]
        user_formality_trend = (
            sum(user_formalities) / len(user_formalities)
            if user_formalities
            else 0.5
        )

        user_questions = sum(1 for t in user_turns if t.has_question)
        user_question_freq = (
            user_questions / len(user_turns) if user_turns else 0.0
        )

        return ConversationDynamics(
            emotional_trend=round(trend, 3),
            emotional_volatility=round(volatility, 3),
            current_valence=round(current_valence, 3),
            avg_user_response_time_s=round(avg_response_time, 1),
            avg_user_word_count=round(avg_user_words, 1),
            turn_count=len(turns),
            consecutive_short_responses=consecutive_short,
            repair_mode=repair_mode,
            repair_signal_count=total_repairs,
            consecutive_repair_signals=consecutive_repairs,
            user_formality_trend=round(user_formality_trend, 3),
            user_question_frequency=round(user_question_freq, 3),
        )

    def metrics(self) -> dict[str, int]:
        """Return engine metrics for health reporting."""
        return {
            "active_conversations": len(self._conversations),
            "total_repair_modes": sum(
                1 for d in self._dynamics_cache.values() if d.repair_mode
            ),
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\expression_queue.py =====

"""
EcodiaOS -- Voxis Expression Queue

Implements a priority queue for expressions that the SilenceEngine suppressed
with ``queue=True`` (as opposed to outright discard).

## Why this exists

The SilenceEngine legitimately suppresses proactive expressions when:
- Humans are actively conversing (don't interrupt)
- Rate limit hasn't elapsed yet (don't be needy)

But these suppressed expressions aren't worthless -- they represent genuine
communicative intent. An organism that silently discards its impulse to speak
loses contextual relevance: by the time the rate limit expires, the insight may
be stale. The queue preserves intent and delivers it when conditions clear,
unless the expression has decayed past its relevance window.

## Relevance Decay

Each queued expression carries:
- ``enqueued_at``: timestamp of suppression
- ``relevance_halflife_seconds``: how quickly the expression loses relevance
  (ambient insights decay fast; deliberate Nova informs decay slowly)
- ``initial_relevance``: the insight_value/urgency at queue time

Current relevance = initial_relevance * 2^(-elapsed / halflife)

Expressions below the ``delivery_threshold`` are pruned on each drain cycle.

## Capacity

The queue has a hard cap (default 20). When full, the lowest-relevance item
is evicted to make room.
"""

from __future__ import annotations

import heapq
import math
import time
from dataclasses import dataclass, field

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.systems.voxis.types import ExpressionIntent, ExpressionTrigger

logger = structlog.get_logger()


# ─── Relevance Halflife by Trigger ────────────────────────────────

_HALFLIFE_BY_TRIGGER: dict[ExpressionTrigger, float] = {
    # Ambient insights lose relevance fast -- the moment passes
    ExpressionTrigger.AMBIENT_INSIGHT: 120.0,       # 2 minutes
    ExpressionTrigger.AMBIENT_STATUS: 300.0,         # 5 minutes
    # Nova proactive informs are more durable -- deliberate intent
    ExpressionTrigger.NOVA_INFORM: 600.0,            # 10 minutes
    # Everything else: 5 minute default
}

_DEFAULT_HALFLIFE = 300.0


@dataclass(order=True)
class QueuedExpression:
    """
    A suppressed expression waiting for delivery conditions to clear.

    Ordered by negative relevance for min-heap (highest relevance = lowest priority value).
    """

    # Sort key (negative relevance so heapq gives us highest-relevance first)
    _sort_key: float = field(init=False, repr=False)

    # Expression data
    intent: ExpressionIntent = field(compare=False)
    affect_snapshot: AffectState = field(compare=False)

    # Decay parameters
    enqueued_at: float = field(compare=False, default_factory=time.monotonic)
    initial_relevance: float = field(compare=False, default=0.5)
    halflife_seconds: float = field(compare=False, default=_DEFAULT_HALFLIFE)

    def __post_init__(self) -> None:
        self._sort_key = -self.initial_relevance

    @property
    def current_relevance(self) -> float:
        """Compute time-decayed relevance using exponential decay."""
        elapsed = time.monotonic() - self.enqueued_at
        return self.initial_relevance * math.pow(2.0, -elapsed / self.halflife_seconds)

    @property
    def age_seconds(self) -> float:
        return time.monotonic() - self.enqueued_at


class ExpressionQueue:
    """
    Priority queue of suppressed expressions with relevance decay.

    Thread-safe for single-writer (VoxisService) usage within asyncio.

    Usage::

        queue.enqueue(intent, affect, initial_relevance=0.7)
        ...
        deliverable = queue.drain(delivery_threshold=0.3, max_items=3)
        for item in deliverable:
            await voxis.express(item.intent.content_to_express, ...)
    """

    def __init__(
        self,
        max_size: int = 20,
        delivery_threshold: float = 0.3,
    ) -> None:
        self._heap: list[QueuedExpression] = []
        self._max_size = max_size
        self._delivery_threshold = delivery_threshold
        self._total_enqueued: int = 0
        self._total_delivered: int = 0
        self._total_expired: int = 0
        self._total_evicted: int = 0
        self._logger = logger.bind(system="voxis.queue")

    @property
    def size(self) -> int:
        return len(self._heap)

    def enqueue(
        self,
        intent: ExpressionIntent,
        affect: AffectState,
        initial_relevance: float | None = None,
    ) -> None:
        """
        Add a suppressed expression to the queue.

        Uses insight_value/urgency from the intent as initial relevance if not
        explicitly provided.
        """
        relevance = initial_relevance or max(intent.insight_value, intent.urgency)
        halflife = _HALFLIFE_BY_TRIGGER.get(intent.trigger, _DEFAULT_HALFLIFE)

        item = QueuedExpression(
            intent=intent,
            affect_snapshot=affect,
            initial_relevance=relevance,
            halflife_seconds=halflife,
        )

        # Evict lowest-relevance if at capacity
        if len(self._heap) >= self._max_size:
            self._prune_expired()
            if len(self._heap) >= self._max_size:
                # Still full after pruning -- evict lowest
                worst = max(self._heap, key=lambda x: x._sort_key)  # Highest _sort_key = lowest relevance
                self._heap.remove(worst)
                heapq.heapify(self._heap)
                self._total_evicted += 1
                self._logger.debug(
                    "expression_evicted",
                    trigger=worst.intent.trigger.value,
                    relevance=round(worst.current_relevance, 3),
                )

        heapq.heappush(self._heap, item)
        self._total_enqueued += 1
        self._logger.debug(
            "expression_queued",
            trigger=intent.trigger.value,
            relevance=round(relevance, 3),
            halflife=halflife,
            queue_size=len(self._heap),
        )

    def drain(
        self,
        max_items: int = 3,
        delivery_threshold: float | None = None,
    ) -> list[QueuedExpression]:
        """
        Remove and return the highest-relevance expressions that are still
        above the delivery threshold.

        Called by VoxisService when conditions clear (e.g., humans stop
        conversing, rate limit window passes).
        """
        threshold = delivery_threshold or self._delivery_threshold
        self._prune_expired()

        deliverable: list[QueuedExpression] = []
        remaining: list[QueuedExpression] = []

        while self._heap and len(deliverable) < max_items:
            item = heapq.heappop(self._heap)
            if item.current_relevance >= threshold:
                deliverable.append(item)
            else:
                self._total_expired += 1

        # Put back anything we didn't deliver
        for item in self._heap:
            remaining.append(item)
        self._heap = remaining
        heapq.heapify(self._heap)

        self._total_delivered += len(deliverable)

        if deliverable:
            self._logger.info(
                "expressions_drained",
                count=len(deliverable),
                triggers=[d.intent.trigger.value for d in deliverable],
                relevances=[round(d.current_relevance, 3) for d in deliverable],
            )

        return deliverable

    def peek_highest_relevance(self) -> float:
        """Return the current relevance of the highest-priority queued item."""
        if not self._heap:
            return 0.0
        return self._heap[0].current_relevance

    def _prune_expired(self) -> None:
        """Remove items below the delivery threshold."""
        before = len(self._heap)
        self._heap = [
            item for item in self._heap
            if item.current_relevance >= self._delivery_threshold
        ]
        pruned = before - len(self._heap)
        if pruned > 0:
            heapq.heapify(self._heap)
            self._total_expired += pruned

    def clear(self) -> None:
        """Discard all queued expressions."""
        self._heap.clear()

    def metrics(self) -> dict[str, int | float]:
        """Return queue metrics for health reporting."""
        return {
            "queue_size": len(self._heap),
            "total_enqueued": self._total_enqueued,
            "total_delivered": self._total_delivered,
            "total_expired": self._total_expired,
            "total_evicted": self._total_evicted,
            "highest_relevance": round(self.peek_highest_relevance(), 3),
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\personality.py =====

"""
EcodiaOS — Voxis Personality Engine

Loads the instance's personality vector from Memory, applies it to a
StrategyParams object to shape expression decisions, and provides helpers
for incremental personality adjustment (used by Evo).

Personality is not static — it is the accumulated pattern of all processing
biases shaped by experience. Evo proposes adjustments; this engine applies them.
"""

from __future__ import annotations

import structlog

from ecodiaos.primitives.expression import PersonalityVector
from ecodiaos.systems.voxis.types import StrategyParams

logger = structlog.get_logger()

# Maximum personality delta Evo is allowed to apply per adjustment
MAX_PERSONALITY_DELTA = 0.03
# Minimum interactions before Evo can propose a change to any single dimension
MIN_EVIDENCE_FOR_ADJUSTMENT = 100


class PersonalityEngine:
    """
    Applies the instance's personality vector to shape an ExpressionStrategy.

    Each personality dimension modifies specific StrategyParams fields:
    - warmth → tone_markers, greeting_style
    - directness → structure, hedge_level
    - verbosity → target_length multiplier
    - formality → register, contraction_use
    - curiosity_expression → include_followup_question, exploratory_tangents_allowed
    - humour → humour_allowed, humour_probability, context_appropriate_for_humour
    - empathy_expression → emotional_acknowledgment, empathy_first
    - confidence_display → hedge_level override, confidence_display_override
    - metaphor_use → analogy_encouraged, preferred_analogy_domains
    """

    def __init__(self, personality: PersonalityVector) -> None:
        self._personality = personality
        self._logger = logger.bind(system="voxis.personality")

    @property
    def current(self) -> PersonalityVector:
        return self._personality

    def apply(self, strategy: StrategyParams) -> StrategyParams:
        """
        Return a new StrategyParams with personality dimensions applied.

        Does not mutate the input. All modifications are additive or override
        specific fields based on threshold crossings in personality dimensions.
        """
        # Work on a copy so we don't mutate the caller's object
        s = strategy.model_copy(deep=True)
        p = self._personality

        # ── Warmth ────────────────────────────────────────────────
        if p.warmth > 0.4:
            if "warm" not in s.tone_markers:
                s.tone_markers.append("warm")
            s.greeting_style = "personal"
        elif p.warmth > 0.1:
            if "approachable" not in s.tone_markers:
                s.tone_markers.append("approachable")
        elif p.warmth < -0.4:
            if "measured" not in s.tone_markers:
                s.tone_markers.append("measured")
            s.greeting_style = "professional"

        # ── Directness ────────────────────────────────────────────
        if p.directness > 0.4:
            s.structure = "conclusion_first"
            s.hedge_level = "minimal"
        elif p.directness < -0.4:
            s.structure = "context_first"
            s.hedge_level = "moderate"

        # ── Verbosity → length multiplier ─────────────────────────
        # Clamp multiplier to [0.5, 1.6] to avoid degenerate lengths
        multiplier = max(0.5, min(1.6, 1.0 + p.verbosity * 0.4))
        s.target_length = max(50, int(s.target_length * multiplier))

        # ── Formality ─────────────────────────────────────────────
        if p.formality > 0.4:
            s.register = "formal"
            s.contraction_use = False
        elif p.formality < -0.4:
            s.register = "casual"
            s.contraction_use = True

        # ── Curiosity expression ──────────────────────────────────
        if p.curiosity_expression > 0.3 and s.allows_questions:
            s.include_followup_question = True
        if p.curiosity_expression > 0.5:
            s.exploratory_tangents_allowed = True

        # ── Humour ────────────────────────────────────────────────
        if p.humour > 0.3 and s.context_appropriate_for_humour:
            s.humour_allowed = True
            s.humour_probability = min(0.6, p.humour * 0.6)

        # ── Empathy expression ────────────────────────────────────
        if p.empathy_expression > 0.4:
            s.emotional_acknowledgment = "explicit"
        elif p.empathy_expression > 0.1:
            s.emotional_acknowledgment = "implicit"
        elif p.empathy_expression < -0.4:
            s.emotional_acknowledgment = "minimal"

        # ── Confidence display ────────────────────────────────────
        if p.confidence_display > 0.4:
            s.confidence_display_override = "assertive"
        elif p.confidence_display < -0.4:
            # Overlaps with hedge_level — both apply
            if s.hedge_level == "minimal":
                s.hedge_level = "moderate"
            s.confidence_display_override = "cautious"

        # ── Metaphor use ──────────────────────────────────────────
        if p.metaphor_use > 0.3:
            s.analogy_encouraged = True
            if p.thematic_references:
                s.preferred_analogy_domains = list(p.thematic_references[:6])

        self._logger.debug(
            "personality_applied",
            warmth=round(p.warmth, 3),
            directness=round(p.directness, 3),
            verbosity=round(p.verbosity, 3),
            target_length=s.target_length,
        )

        return s

    def apply_delta(self, delta: dict[str, float]) -> PersonalityVector:
        """
        Apply an incremental personality adjustment proposed by Evo.

        Each dimension delta is clamped to MAX_PERSONALITY_DELTA.
        Dimensions not in delta are unchanged.
        Returns the new PersonalityVector (does not mutate in place — caller
        must update the service's cached personality).
        """
        current = self._personality.model_dump(
            exclude={"vocabulary_affinities", "thematic_references"}
        )
        updated: dict[str, float] = {}

        for dim, current_val in current.items():
            if dim in delta:
                # Clamp the adjustment magnitude
                clamped = max(-MAX_PERSONALITY_DELTA, min(MAX_PERSONALITY_DELTA, delta[dim]))
                new_val = current_val + clamped
                # Keep all dimensions within valid range [-1, 1] (humour: [0, 1])
                if dim == "humour":
                    new_val = max(0.0, min(1.0, new_val))
                elif dim == "metaphor_use":
                    new_val = max(0.0, min(1.0, new_val))
                else:
                    new_val = max(-1.0, min(1.0, new_val))
                updated[dim] = new_val
            else:
                updated[dim] = current_val

        new_personality = PersonalityVector(
            **updated,
            vocabulary_affinities=dict(self._personality.vocabulary_affinities),
            thematic_references=list(self._personality.thematic_references),
        )

        self._logger.info(
            "personality_delta_applied",
            dimensions_changed=list(delta.keys()),
            max_delta=MAX_PERSONALITY_DELTA,
        )

        return new_personality

    def update_vocabulary_affinity(self, word: str, delta: float) -> None:
        """Adjust a vocabulary affinity weight (called by Evo based on feedback)."""
        current = self._personality.vocabulary_affinities.get(word, 0.0)
        new_val = max(0.0, min(1.0, current + delta))
        self._personality.vocabulary_affinities[word] = new_val

    @classmethod
    def from_seed(cls, seed_personality: dict[str, float]) -> "PersonalityEngine":
        """Create a fresh PersonalityEngine from seed configuration values."""
        vector = PersonalityVector(**seed_personality)
        return cls(vector)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\reception.py =====

"""
EcodiaOS -- Voxis Reception Feedback Engine

Closes the expression feedback loop by correlating user responses with prior
expressions and estimating how well the expression was received.

## The Problem

Without reception tracking, the organism talks into the void. It generates
expressions, dispatches ExpressionFeedback, but never learns from the
actual response (or lack thereof). Evo has no signal to refine personality.
The feedback loop is open-ended.

## How It Works

1. **Expression tracking**: Each delivered expression is registered with
   the reception engine (expression ID, conversation ID, content summary,
   strategy metadata, timestamp).

2. **Response correlation**: When a user message arrives (via
   ``ingest_user_message``), the engine correlates it with the most recent
   pending expression in the same conversation.

3. **Reception estimation**: The engine estimates reception quality from
   observable signals (no LLM call -- fast, deterministic):
   - ``understood``: response length relative to expression, relevance indicators
   - ``emotional_impact``: sentiment shift signals (affect valence delta)
   - ``engagement``: response latency, response length, question-asking
   - ``satisfaction``: combined heuristic from above

4. **Feedback enrichment**: The populated ``ReceptionEstimate`` is patched
   onto the ``ExpressionFeedback`` and re-dispatched to registered listeners
   (including Evo for personality learning).

## Active Inference Grounding

Reception feedback is the **observation** that completes the perception-action
loop. Expression is action; reception is the sensory consequence. The
organism must observe the consequences of its actions to update its
generative model (via Evo). Without this, expression quality cannot improve.
"""

from __future__ import annotations

import re
import time
from collections import deque
from dataclasses import dataclass, field

import structlog

from ecodiaos.systems.voxis.types import ExpressionFeedback, ReceptionEstimate

logger = structlog.get_logger()

# ─── Configuration ────────────────────────────────────────────────

_MAX_PENDING_EXPRESSIONS = 50     # Per-conversation max tracked expressions
_RESPONSE_WINDOW_SECONDS = 300.0  # 5 min -- max time to attribute a response
_ENGAGEMENT_LENGTH_THRESHOLD = 20 # Chars -- below this counts as minimal engagement

# Positive/negative sentiment markers (lightweight, no LLM needed)
_POSITIVE_MARKERS = re.compile(
    r"\b(thanks|thank you|great|helpful|makes sense|got it|appreciate|"
    r"perfect|exactly|wonderful|awesome|good point|interesting|I see|"
    r"understood|clear now|that helps|nice)\b",
    re.IGNORECASE,
)
_NEGATIVE_MARKERS = re.compile(
    r"\b(confused|don't understand|doesn't make sense|wrong|no that's not|"
    r"what do you mean|huh\??|unclear|that's not what I|I disagree|"
    r"not helpful|actually|you missed)\b",
    re.IGNORECASE,
)
_QUESTION_MARKERS = re.compile(r"\?")


@dataclass
class PendingExpression:
    """An expression awaiting user response for reception feedback."""

    expression_id: str
    conversation_id: str
    content_summary: str
    strategy_register: str
    personality_warmth: float
    affect_before_valence: float
    trigger: str
    delivered_at: float = field(default_factory=time.monotonic)

    @property
    def age_seconds(self) -> float:
        return time.monotonic() - self.delivered_at

    @property
    def is_expired(self) -> bool:
        return self.age_seconds > _RESPONSE_WINDOW_SECONDS


class ReceptionEngine:
    """
    Tracks delivered expressions and correlates user responses to estimate
    reception quality.

    Usage::

        engine = ReceptionEngine()

        # After expression delivery:
        engine.track_expression(expression_id, conversation_id, ...)

        # When user message arrives:
        enriched = engine.correlate_response(
            conversation_id, response_text, response_affect_valence
        )
        if enriched:
            # Re-dispatch to Evo with populated ReceptionEstimate
            for cb in feedback_callbacks:
                cb(enriched)
    """

    def __init__(self) -> None:
        # conversation_id -> deque of pending expressions
        self._pending: dict[str, deque[PendingExpression]] = {}
        self._logger = logger.bind(system="voxis.reception")

        # Metrics
        self._total_tracked: int = 0
        self._total_correlated: int = 0
        self._total_expired: int = 0
        self._total_no_response: int = 0

    def track_expression(
        self,
        expression_id: str,
        conversation_id: str,
        content_summary: str,
        strategy_register: str = "neutral",
        personality_warmth: float = 0.0,
        affect_before_valence: float = 0.0,
        trigger: str = "",
    ) -> None:
        """Register a delivered expression for response tracking."""
        if conversation_id not in self._pending:
            self._pending[conversation_id] = deque(maxlen=_MAX_PENDING_EXPRESSIONS)

        self._pending[conversation_id].append(PendingExpression(
            expression_id=expression_id,
            conversation_id=conversation_id,
            content_summary=content_summary,
            strategy_register=strategy_register,
            personality_warmth=personality_warmth,
            affect_before_valence=affect_before_valence,
            trigger=trigger,
        ))
        self._total_tracked += 1

    def correlate_response(
        self,
        conversation_id: str,
        response_text: str,
        response_affect_valence: float | None = None,
    ) -> ExpressionFeedback | None:
        """
        Correlate a user response with the most recent pending expression.

        Returns an enriched ExpressionFeedback with populated ReceptionEstimate,
        or None if no pending expression matches.
        """
        # Prune expired
        self._prune_expired(conversation_id)

        pending_queue = self._pending.get(conversation_id)
        if not pending_queue:
            return None

        # Take the most recent pending expression (LIFO -- most relevant)
        pending = pending_queue.pop()

        # Estimate reception
        reception = self._estimate_reception(
            expression_summary=pending.content_summary,
            response_text=response_text,
            response_latency_seconds=pending.age_seconds,
            affect_before=pending.affect_before_valence,
            affect_after=response_affect_valence,
        )

        # Build enriched feedback
        affect_after = response_affect_valence if response_affect_valence is not None else pending.affect_before_valence
        feedback = ExpressionFeedback(
            expression_id=pending.expression_id,
            trigger=pending.trigger,
            conversation_id=conversation_id,
            content_summary=pending.content_summary,
            strategy_register=pending.strategy_register,
            personality_warmth=pending.personality_warmth,
            inferred_reception=reception,
            affect_before_valence=pending.affect_before_valence,
            affect_after_valence=affect_after,
            affect_delta=affect_after - pending.affect_before_valence,
            user_responded=True,
            user_response_length=len(response_text),
        )

        self._total_correlated += 1
        self._logger.debug(
            "response_correlated",
            expression_id=pending.expression_id[:8],
            latency_s=round(pending.age_seconds, 1),
            understood=round(reception.understood, 3),
            engagement=round(reception.engagement, 3),
            satisfaction=round(reception.satisfaction, 3),
        )

        return feedback

    def _estimate_reception(
        self,
        expression_summary: str,
        response_text: str,
        response_latency_seconds: float,
        affect_before: float,
        affect_after: float | None,
    ) -> ReceptionEstimate:
        """
        Estimate reception quality from observable response signals.

        No LLM call -- fast, deterministic heuristics:
        - Understood: positive markers, substantive response, no confusion markers
        - Emotional impact: valence delta (if available)
        - Engagement: response length, latency, question-asking
        - Satisfaction: composite of above
        """
        resp_len = len(response_text)
        resp_lower = response_text.lower()

        # ── Understood ──────────────────────────────────────────
        understood = 0.5  # Baseline: unknown

        positive_matches = len(_POSITIVE_MARKERS.findall(response_text))
        negative_matches = len(_NEGATIVE_MARKERS.findall(response_text))

        if positive_matches > 0 and negative_matches == 0:
            understood = min(1.0, 0.6 + positive_matches * 0.1)
        elif negative_matches > 0 and positive_matches == 0:
            understood = max(0.0, 0.4 - negative_matches * 0.15)
        elif positive_matches > negative_matches:
            understood = 0.6
        elif negative_matches > positive_matches:
            understood = 0.3

        # Substantive response length is a mild positive signal
        if resp_len > 100:
            understood = min(1.0, understood + 0.1)

        # ── Emotional impact ────────────────────────────────────
        emotional_impact = 0.0
        if affect_after is not None:
            delta = affect_after - affect_before
            # Positive shift = positive impact; negative = negative impact
            emotional_impact = max(-1.0, min(1.0, delta * 2.0))

        # ── Engagement ──────────────────────────────────────────
        engagement = 0.5  # Baseline

        # Response length signal
        if resp_len < _ENGAGEMENT_LENGTH_THRESHOLD:
            engagement -= 0.2
        elif resp_len > 200:
            engagement += 0.15
        elif resp_len > 50:
            engagement += 0.05

        # Latency signal: very fast response = high engagement
        if response_latency_seconds < 10:
            engagement += 0.15
        elif response_latency_seconds < 30:
            engagement += 0.05
        elif response_latency_seconds > 120:
            engagement -= 0.1

        # Question-asking = high engagement
        question_count = len(_QUESTION_MARKERS.findall(response_text))
        if question_count > 0:
            engagement += min(0.2, question_count * 0.1)

        engagement = max(0.0, min(1.0, engagement))

        # ── Satisfaction ────────────────────────────────────────
        # Weighted composite of above signals
        satisfaction = (
            understood * 0.40
            + max(0.0, min(1.0, 0.5 + emotional_impact * 0.5)) * 0.20
            + engagement * 0.40
        )
        satisfaction = max(0.0, min(1.0, satisfaction))

        return ReceptionEstimate(
            understood=round(understood, 3),
            emotional_impact=round(emotional_impact, 3),
            engagement=round(engagement, 3),
            satisfaction=round(satisfaction, 3),
        )

    def _prune_expired(self, conversation_id: str) -> None:
        """Remove expired pending expressions."""
        queue = self._pending.get(conversation_id)
        if not queue:
            return

        before = len(queue)
        while queue and queue[0].is_expired:
            queue.popleft()
            self._total_expired += 1

        if not queue:
            del self._pending[conversation_id]

    def expire_unanswered(self) -> list[ExpressionFeedback]:
        """
        Collect and return feedback for all expired (unanswered) expressions.

        Called periodically to ensure expressions that received no response
        still generate feedback (with user_responded=False).
        """
        expired_feedback: list[ExpressionFeedback] = []

        for conv_id in list(self._pending.keys()):
            queue = self._pending[conv_id]
            while queue and queue[0].is_expired:
                pending = queue.popleft()
                feedback = ExpressionFeedback(
                    expression_id=pending.expression_id,
                    trigger=pending.trigger,
                    conversation_id=conv_id,
                    content_summary=pending.content_summary,
                    strategy_register=pending.strategy_register,
                    personality_warmth=pending.personality_warmth,
                    affect_before_valence=pending.affect_before_valence,
                    affect_after_valence=pending.affect_before_valence,
                    affect_delta=0.0,
                    user_responded=False,
                    user_response_length=0,
                    inferred_reception=ReceptionEstimate(
                        understood=0.5,
                        emotional_impact=0.0,
                        engagement=0.2,  # No response = low engagement
                        satisfaction=0.3,
                    ),
                )
                expired_feedback.append(feedback)
                self._total_no_response += 1

            if not queue:
                del self._pending[conv_id]

        if expired_feedback:
            self._logger.info(
                "unanswered_expressions_expired",
                count=len(expired_feedback),
            )

        return expired_feedback

    def metrics(self) -> dict[str, int]:
        """Return engine metrics for health reporting."""
        total_pending = sum(len(q) for q in self._pending.values())
        return {
            "total_tracked": self._total_tracked,
            "total_correlated": self._total_correlated,
            "total_expired": self._total_expired,
            "total_no_response": self._total_no_response,
            "active_pending": total_pending,
            "active_conversations": len(self._pending),
        }

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\renderer.py =====

"""
EcodiaOS — Voxis Content Renderer

The expression generation engine. Implements the full 9-step pipeline
(spec §III.1) with Active Inference-grounded policy selection.

## Theoretical Grounding

Expression is **action** in the Active Inference framework (spec §II.1).
When the organism speaks, it acts on the world to reduce Expected Free Energy.

    G(π) = -E_q[ln p(o|π)]     (pragmatic value — preference satisfaction)
           - H[q(s|o,π)]       (epistemic value — uncertainty reduction)

Three expression policy classes correspond to three EFE components:

    PRAGMATIC   — changes the world toward preferred states
                  (inform, respond, coordinate)
                  Serves: Coherence + Care drives
                  EFE reduction: reduces ambiguity and relational distance

    EPISTEMIC   — reduces uncertainty in the generative model
                  (ask questions, seek clarification)
                  Serves: Growth + Coherence drives
                  EFE reduction: expected information gain about hidden states

    AFFILIATIVE — maintains relational bonds and shared context
                  (acknowledge, empathise, celebrate)
                  Serves: Care + Honesty drives
                  EFE reduction: reduces predicted relational prediction error

The renderer:
1. Derives candidate policies from the trigger and affect context
2. Scores each policy against the four drives as prior preferences
3. Selects the minimum-EFE policy
4. Sets LLM temperature = belief precision = f(1 - coherence_stress)
5. Constructs the full prompt encoding the selected policy
6. Generates expression
7. Runs honesty authenticity check
8. Returns completed Expression

Temperature calibration (grounded in precision-weighting):
    precision τ = 1 / σ²    where σ² ~ coherence_stress
    temperature = base_temp * (1 - coherence_stress * 0.4)
    High stress → low temperature (careful, conservative)
    Low stress  → base temperature (creative, natural)
    Creative contexts add +0.15; safety contexts subtract -0.20.
"""

from __future__ import annotations

import hashlib
import time
from dataclasses import dataclass, field
from enum import Enum

import structlog

from ecodiaos.clients.llm import LLMProvider, Message
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.primitives.expression import (
    Expression,
    ExpressionStrategy,
    GenerationTrace,
    PersonalityVector,
)
from ecodiaos.prompts.voxis.expression import (
    build_system_prompt,
    build_user_prompt,
)
from ecodiaos.systems.voxis.affect_colouring import AffectColouringEngine
from ecodiaos.systems.voxis.audience import AudienceProfiler
from ecodiaos.systems.voxis.personality import PersonalityEngine
from ecodiaos.systems.voxis.types import (
    AudienceProfile,
    ExpressionContext,
    ExpressionIntent,
    ExpressionTrigger,
    OutputChannel,
    StrategyParams,
)

logger = structlog.get_logger()


# ─── Expression Policy Classes ────────────────────────────────────


class ExpressionPolicyClass(str, Enum):
    """
    The three Active Inference expression policy classes.
    Each class serves different drives and reduces different EFE components.
    """
    PRAGMATIC = "pragmatic"      # Inform, respond, coordinate → Coherence + Care
    EPISTEMIC = "epistemic"      # Ask, clarify, explore → Growth + Coherence
    AFFILIATIVE = "affiliative"  # Acknowledge, empathise, celebrate → Care + Honesty


@dataclass
class ExpressionPolicy:
    """A candidate expression policy with associated EFE score."""
    policy_class: ExpressionPolicyClass
    strategy: StrategyParams
    # Drive alignment scores (0.0 = misaligned, 1.0 = fully aligned)
    coherence_alignment: float = 0.0
    care_alignment: float = 0.0
    growth_alignment: float = 0.0
    honesty_alignment: float = 0.0
    # Epistemic value: expected information gain from this policy
    epistemic_value: float = 0.0
    # Computed EFE (lower = preferred)
    efe_score: float = field(init=False, default=0.0)

    def compute_efe(
        self,
        drive_weights: dict[str, float],
    ) -> float:
        """
        Compute Expected Free Energy for this policy.

        G(π) = -[weighted sum of drive alignments] - epistemic_value

        Drive weights come from the current constitutional drive strengths.
        Lower G = more preferred policy.
        """
        pragmatic_value = (
            drive_weights.get("coherence", 1.0) * self.coherence_alignment
            + drive_weights.get("care", 1.0) * self.care_alignment
            + drive_weights.get("growth", 1.0) * self.growth_alignment
            + drive_weights.get("honesty", 1.0) * self.honesty_alignment
        )
        # Normalise by sum of weights to keep scores comparable
        weight_sum = sum(drive_weights.values()) or 4.0
        normalised_pragmatic = pragmatic_value / weight_sum

        self.efe_score = -(normalised_pragmatic + self.epistemic_value)
        return self.efe_score


# ─── Policy Factory ───────────────────────────────────────────────


def _derive_candidate_policies(
    intent: ExpressionIntent,
    affect: AffectState,
    base_strategy: StrategyParams,
) -> list[ExpressionPolicy]:
    """
    Derive candidate expression policies from the trigger and affect context.

    Returns 2–4 policies representing meaningfully different expression approaches.
    The EFE scorer will select the minimum-EFE policy given the drive weights.
    """
    policies: list[ExpressionPolicy] = []
    trigger = intent.trigger

    # ── Pragmatic policy: focus on accurate information / response ──
    pragmatic_strategy = base_strategy.model_copy(deep=True)
    pragmatic_strategy.structure = "conclusion_first" if affect.coherence_stress < 0.4 else "context_first"
    pragmatic_strategy.information_density = "normal"
    pragmatic_strategy.uncertainty_acknowledgment = (
        "explicit" if affect.coherence_stress > 0.5 else "implicit"
    )
    # Coherence alignment: high when reducing ambiguity is the main goal
    coherence_al = 0.8 if trigger in (
        ExpressionTrigger.NOVA_RESPOND,
        ExpressionTrigger.NOVA_INFORM,
    ) else 0.5
    # Care alignment: moderate for informational, high for response
    care_al = 0.6 if trigger == ExpressionTrigger.NOVA_RESPOND else 0.4
    # Epistemic value: low — we're providing info, not seeking it
    policies.append(ExpressionPolicy(
        policy_class=ExpressionPolicyClass.PRAGMATIC,
        strategy=pragmatic_strategy,
        coherence_alignment=coherence_al,
        care_alignment=care_al,
        growth_alignment=0.5,
        honesty_alignment=0.7,
        epistemic_value=0.1,
    ))

    # ── Epistemic policy: focus on questions and exploration ──────
    # Only viable when curiosity is elevated or trigger is exploratory
    if affect.curiosity > 0.4 or trigger in (
        ExpressionTrigger.NOVA_REQUEST,
        ExpressionTrigger.AMBIENT_INSIGHT,
    ):
        epistemic_strategy = base_strategy.model_copy(deep=True)
        epistemic_strategy.allows_questions = True
        epistemic_strategy.include_followup_question = True
        epistemic_strategy.exploratory_tangents_allowed = affect.curiosity > 0.6
        epistemic_strategy.information_density = "normal"
        # Epistemic value: high — this policy genuinely reduces model uncertainty
        ep_val = min(0.6, 0.3 + affect.curiosity * 0.4)
        policies.append(ExpressionPolicy(
            policy_class=ExpressionPolicyClass.EPISTEMIC,
            strategy=epistemic_strategy,
            coherence_alignment=0.7,
            care_alignment=0.5,
            growth_alignment=0.9,   # Growth drive directly served by epistemic foraging
            honesty_alignment=0.6,
            epistemic_value=ep_val,
        ))

    # ── Affiliative policy: focus on relationship and emotional acknowledgment ──
    # Prioritised when care_activation is high, distress detected, or trigger is relational
    if affect.care_activation > 0.4 or trigger in (
        ExpressionTrigger.ATUNE_DISTRESS,
        ExpressionTrigger.NOVA_MEDIATE,
        ExpressionTrigger.NOVA_CELEBRATE,
    ):
        affiliative_strategy = base_strategy.model_copy(deep=True)
        affiliative_strategy.emotional_acknowledgment = "explicit"
        affiliative_strategy.empathy_first = True
        affiliative_strategy.information_density = "low"
        affiliative_strategy.sentence_length_preference = "shorter"
        affiliative_strategy.structure = "natural"
        # Care alignment: highest for affiliative policy
        care_al_aff = min(1.0, 0.7 + affect.care_activation * 0.3)
        policies.append(ExpressionPolicy(
            policy_class=ExpressionPolicyClass.AFFILIATIVE,
            strategy=affiliative_strategy,
            coherence_alignment=0.4,
            care_alignment=care_al_aff,
            growth_alignment=0.3,
            honesty_alignment=0.8,   # Authentic emotional acknowledgment = high honesty
            epistemic_value=0.05,
        ))

    # ── Balanced policy: blend of pragmatic and affiliative ───────
    # Always included as a fallback — moderate scores across all drives
    balanced_strategy = base_strategy.model_copy(deep=True)
    balanced_strategy.emotional_acknowledgment = "implicit"
    balanced_strategy.uncertainty_acknowledgment = (
        "explicit" if affect.coherence_stress > 0.5 else "implicit"
    )
    policies.append(ExpressionPolicy(
        policy_class=ExpressionPolicyClass.PRAGMATIC,  # Balanced uses pragmatic class
        strategy=balanced_strategy,
        coherence_alignment=0.65,
        care_alignment=0.65,
        growth_alignment=0.55,
        honesty_alignment=0.65,
        epistemic_value=0.15,
    ))

    return policies


def _select_minimum_efe_policy(
    policies: list[ExpressionPolicy],
    drive_weights: dict[str, float],
) -> ExpressionPolicy:
    """
    Select the expression policy with the minimum Expected Free Energy.

    Lower EFE = higher expected preference satisfaction + higher epistemic value.
    Ties are broken by care_alignment (the Care drive is the deepest orientation).
    """
    for policy in policies:
        policy.compute_efe(drive_weights)

    return min(policies, key=lambda p: (p.efe_score, -p.care_alignment))


# ─── Temperature Calibration ─────────────────────────────────────


def _compute_temperature(
    strategy: StrategyParams,
    affect: AffectState,
    base_temp: float = 0.7,
) -> float:
    """
    Calibrate LLM temperature based on belief precision.

    Grounded in Active Inference precision-weighting:
        precision τ = 1/σ²  where σ² is related to coherence_stress

    High coherence_stress → low precision → low temperature (careful, conservative).
    Low coherence_stress → high precision → base temperature (natural, creative).

    Context modulations:
    - Creative contexts (celebration, storytelling, brainstorming): +0.15
    - Safety contexts (warning, medical, legal, conflict, distress): -0.20
    - Humour allowed: +0.05
    """
    # Precision weighting: stress suppresses temperature
    temp = base_temp * (1.0 - affect.coherence_stress * 0.4)

    context = strategy.context_type
    if context in ("celebration", "brainstorming"):
        temp += 0.15
    elif context in ("warning", "conflict", "distress", "medical", "legal"):
        temp -= 0.20

    if strategy.humour_allowed:
        temp += 0.05

    # Clamp to safe range [0.30, 1.00]
    return max(0.30, min(1.00, round(temp, 3)))


# ─── Honesty Check ────────────────────────────────────────────────


def _build_correction_instruction(violation: str) -> str:
    """Build an instruction to include on honesty check failure."""
    return (
        f"\n\nIMPORTANT CORRECTION: The previous response violated the Honesty drive. "
        f"Reason: {violation}. "
        f"Regenerate without this violation. Be authentic about your actual state."
    )


# ─── Main Renderer ────────────────────────────────────────────────


class ContentRenderer:
    """
    Full 9-step expression pipeline with Active Inference policy selection.

    Steps:
    1. Intent Analysis — parse ExpressionIntent
    2. Audience Profiling — build/update AudienceProfile
    3. Base Strategy — derive StrategyParams from trigger and context
    4. Policy Generation — derive candidate policies (pragmatic/epistemic/affiliative/balanced)
    5. EFE Selection — select minimum Expected Free Energy policy
    6. Personality + Affect application — shape selected strategy
    7. Content Generation — construct prompt, call LLM
    8. Honesty Check — authenticity validation (one retry allowed)
    9. Expression Assembly — build and return Expression
    """

    def __init__(
        self,
        llm: LLMProvider,
        personality_engine: PersonalityEngine,
        affect_engine: AffectColouringEngine,
        audience_profiler: AudienceProfiler,
        base_temperature: float = 0.7,
        honesty_check_enabled: bool = True,
        max_expression_length: int = 2000,
    ) -> None:
        self._llm = llm
        self._personality = personality_engine
        self._affect = affect_engine
        self._audience = audience_profiler
        self._base_temp = base_temperature
        self._honesty_check_enabled = honesty_check_enabled
        self._max_length = max_expression_length
        self._logger = logger.bind(system="voxis.renderer")
        self._optimized = isinstance(llm, OptimizedLLMProvider)

    async def render(
        self,
        intent: ExpressionIntent,
        context: ExpressionContext,
        drive_weights: dict[str, float] | None = None,
        diversity_instruction: str | None = None,
        dynamics: object | None = None,
    ) -> Expression:
        """
        Execute the full 9-step expression pipeline.

        Returns a complete Expression with generation trace.
        drive_weights defaults to equal weights {coherence,care,growth,honesty} = 1.0
        if not supplied (e.g. when Equor is not yet fully wired).

        Optional:
        - diversity_instruction: injected when DiversityTracker flags repetition
        - dynamics: ConversationDynamics from ConversationDynamicsEngine
        """
        t_start = time.monotonic()
        weights = drive_weights or {"coherence": 1.0, "care": 1.0, "growth": 1.0, "honesty": 1.0}

        # ── Step 1: Intent Analysis ───────────────────────────────
        # Already encoded in ExpressionIntent -- extract key parameters
        context_type = _infer_context_type(intent.trigger)
        urgency = intent.urgency

        # ── Step 2: Audience ──────────────────────────────────────
        audience = context.audience  # Built by service before calling renderer

        # ── Step 3: Base Strategy ─────────────────────────────────
        base_strategy = _build_base_strategy(intent, context_type, urgency, context.affect)

        # ── Steps 4 & 5: Policy Selection via EFE ─────────────────
        candidate_policies = _derive_candidate_policies(intent, context.affect, base_strategy)
        selected_policy = _select_minimum_efe_policy(candidate_policies, weights)

        self._logger.debug(
            "policy_selected",
            policy_class=selected_policy.policy_class.value,
            efe_score=round(selected_policy.efe_score, 4),
            care_alignment=round(selected_policy.care_alignment, 3),
            num_candidates=len(candidate_policies),
        )

        # ── Step 6a: Personality Application ─────────────────────
        strategy = self._personality.apply(selected_policy.strategy)
        # ── Step 6b: Affect Colouring ────────────────────────────
        strategy = self._affect.apply(strategy, context.affect)
        # ── Step 6c: Audience Adaptation ─────────────────────────
        strategy = self._audience.adapt(strategy, audience)

        # ── Step 6d: Conversation Dynamics Adaptation ────────────
        # Applied after audience -- dynamics can override based on real-time
        # conversational signals (repair mode, style convergence, etc.)
        if dynamics is not None:
            from ecodiaos.systems.voxis.dynamics import ConversationDynamicsEngine
            strategy = ConversationDynamicsEngine().apply_dynamics(strategy, dynamics)  # type: ignore[arg-type]

        # ── Step 7: Content Generation ────────────────────────────
        temperature = _compute_temperature(strategy, context.affect, self._base_temp)
        max_tokens = min(self._max_length // 3, int(strategy.target_length * 0.6) + 50)
        max_tokens = max(50, max_tokens)

        system_prompt = build_system_prompt(
            instance_name=context.instance_name,
            personality=context.personality,
            affect=context.affect,
            audience=audience,
            strategy=strategy,
            relevant_memories=context.relevant_memories,
            has_conversation_history=bool(context.conversation_history),
        )
        user_prompt = build_user_prompt(intent)

        # Anthropic requires system content in the top-level `system` parameter —
        # it rejects `{"role": "system", ...}` entries in the messages array (400).
        # ConversationManager.prepare_context can inject a system-role entry for the
        # rolling conversation summary when history exceeds the context window.
        # Extract those here and fold them into the system prompt instead.
        extra_system: list[str] = []
        filtered_history: list[dict[str, str]] = []
        for m in context.conversation_history:
            if m.get("role") == "system":
                extra_system.append(m.get("content", ""))
            else:
                filtered_history.append(m)
        if extra_system:
            system_prompt = system_prompt + "\n\n" + "\n\n".join(extra_system)

        # Inject diversity instruction if DiversityTracker flagged repetition
        if diversity_instruction:
            system_prompt = system_prompt + "\n\n" + diversity_instruction

        messages: list[Message] = [
            *[Message(role=m["role"], content=m["content"]) for m in filtered_history],
            Message(role="user", content=user_prompt),
        ]

        sys_hash = hashlib.sha256(system_prompt.encode()).hexdigest()[:16]
        user_hash = hashlib.sha256(user_prompt.encode()).hexdigest()[:16]

        # Budget check: if optimized and budget exhausted, use a simpler fallback
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("voxis.render", estimated_tokens=max_tokens + 200):
                # Template fallback: build a minimal expression from the strategy
                self._logger.info("voxis_template_fallback", reason="budget_exhausted")
                gen_latency_ms = 0
                llm_response = type(
                    "FallbackResponse", (), {
                        "text": _build_template_fallback(intent, strategy),
                        "model": "template_fallback",
                        "input_tokens": 0,
                        "output_tokens": 0,
                        "finish_reason": "fallback",
                    }
                )()
            else:
                t_gen_start = time.monotonic()
                llm_response = await self._llm.generate(  # type: ignore[call-arg]
                    system_prompt=system_prompt,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    cache_system="voxis.render",
                    cache_method="generate",
                )
                gen_latency_ms = int((time.monotonic() - t_gen_start) * 1000)
        else:
            t_gen_start = time.monotonic()
            llm_response = await self._llm.generate(
                system_prompt=system_prompt,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            gen_latency_ms = int((time.monotonic() - t_gen_start) * 1000)

        generated_text = llm_response.text.strip()

        # ── Step 8: Honesty Check ─────────────────────────────────
        honesty_passed = True
        honesty_detail: str | None = None

        if self._honesty_check_enabled:
            passed, violation = self._affect.check_authenticity(generated_text, context.affect)
            if not passed and violation:
                honesty_passed = False
                honesty_detail = violation

                self._logger.warning(
                    "honesty_check_failed_regenerating",
                    violation=violation[:100],
                )

                # One retry with explicit correction instruction appended
                correction_messages = messages + [
                    Message(role="assistant", content=generated_text),
                    Message(role="user", content=_build_correction_instruction(violation)),
                ]
                if self._optimized:
                    retry_response = await self._llm.generate(  # type: ignore[call-arg]
                        system_prompt=system_prompt,
                        messages=correction_messages,
                        max_tokens=max_tokens,
                        temperature=max(0.30, temperature - 0.1),
                        cache_system="voxis.render",
                        cache_method="honesty_retry",
                    )
                else:
                    retry_response = await self._llm.generate(
                        system_prompt=system_prompt,
                        messages=correction_messages,
                        max_tokens=max_tokens,
                        temperature=max(0.30, temperature - 0.1),  # Slightly more conservative on retry
                    )
                generated_text = retry_response.text.strip()
                honesty_passed = True  # Accept the corrected version

        # ── Step 9: Assembly ──────────────────────────────────────
        total_latency_ms = int((time.monotonic() - t_start) * 1000)

        # Serialise selected strategy → ExpressionStrategy primitive
        expression_strategy = ExpressionStrategy(
            intent_type=_policy_class_to_intent_type(selected_policy.policy_class),
            audience=audience.audience_type,
            modality="text",
            channel=strategy.channel.value,
            trigger=intent.trigger.value,
            context_type=context_type,
            register=strategy.register,
            target_length=strategy.target_length,
            temperature=temperature,
            personality_influence=context.personality.warmth,  # Use warmth as representative influence
            affect_influence=context.affect.care_activation,
            tone_markers=list(strategy.tone_markers),
            hedge_level=strategy.hedge_level,
            humour_allowed=strategy.humour_allowed,
            include_followup_question=strategy.include_followup_question,
            empathy_first=strategy.empathy_first,
        )

        trace = GenerationTrace(
            system_prompt_hash=sys_hash,
            user_prompt_hash=user_hash,
            model=llm_response.model,
            temperature=temperature,
            input_tokens=llm_response.input_tokens,
            output_tokens=llm_response.output_tokens,
            latency_ms=gen_latency_ms,
            honesty_check_passed=honesty_passed,
            honesty_check_detail=honesty_detail,
        )

        self._logger.info(
            "expression_rendered",
            trigger=intent.trigger.value,
            policy_class=selected_policy.policy_class.value,
            efe_score=round(selected_policy.efe_score, 4),
            temperature=temperature,
            length=len(generated_text),
            latency_ms=total_latency_ms,
            honesty_passed=honesty_passed,
        )

        return Expression(
            intent_id=intent.intent_id,
            conversation_id=intent.conversation_id,
            content=generated_text,
            channel=strategy.channel.value,
            strategy=expression_strategy,
            personality_snapshot=context.personality.model_copy(),
            affect_valence=context.affect.valence,
            affect_arousal=context.affect.arousal,
            affect_dominance=context.affect.dominance,
            affect_curiosity=context.affect.curiosity,
            affect_care_activation=context.affect.care_activation,
            affect_coherence_stress=context.affect.coherence_stress,
            generation_trace=trace,
            is_silence=False,
        )


# ─── Strategy Construction Helpers ───────────────────────────────


def _infer_context_type(trigger: ExpressionTrigger) -> str:
    """Map trigger to a context type string used in temperature calibration."""
    mapping = {
        ExpressionTrigger.NOVA_WARN: "warning",
        ExpressionTrigger.NOVA_CELEBRATE: "celebration",
        ExpressionTrigger.NOVA_MEDIATE: "conflict",
        ExpressionTrigger.ATUNE_DISTRESS: "distress",
        ExpressionTrigger.NOVA_RESPOND: "conversation",
        ExpressionTrigger.NOVA_INFORM: "conversation",
        ExpressionTrigger.NOVA_REQUEST: "conversation",
        ExpressionTrigger.ATUNE_DIRECT_ADDRESS: "conversation",
        ExpressionTrigger.AMBIENT_INSIGHT: "observation",
        ExpressionTrigger.AMBIENT_STATUS: "status",
    }
    return mapping.get(trigger, "conversation")


def _build_base_strategy(
    intent: ExpressionIntent,
    context_type: str,
    urgency: float,
    affect: AffectState,
) -> StrategyParams:
    """
    Build the base StrategyParams from trigger-level context.

    This is the pre-personality, pre-affect starting point.
    Downstream steps (personality, affect, audience) modify this.
    """
    # Base length scales with urgency: urgent messages are shorter
    if urgency > 0.75:
        base_length = 120
    elif urgency > 0.5:
        base_length = 200
    else:
        base_length = 280

    # Humour is contextually inappropriate in several trigger types
    context_appropriate_for_humour = context_type not in (
        "warning", "conflict", "distress", "medical", "legal"
    ) and affect.coherence_stress < 0.5

    return StrategyParams(
        trigger=intent.trigger,
        context_type=context_type,
        urgency=urgency,
        target_length=base_length,
        context_appropriate_for_humour=context_appropriate_for_humour,
        allows_questions=True,
        information_density="normal",
    )


def _policy_class_to_intent_type(policy_class: ExpressionPolicyClass) -> str:
    mapping = {
        ExpressionPolicyClass.PRAGMATIC: "response",
        ExpressionPolicyClass.EPISTEMIC: "proactive",
        ExpressionPolicyClass.AFFILIATIVE: "response",
    }
    return mapping.get(policy_class, "response")


def _build_template_fallback(
    intent: ExpressionIntent,
    strategy: StrategyParams,
) -> str:
    """
    Build a minimal template-based expression when LLM budget is exhausted.

    This is deliberately simple — it maintains conversational coherence
    without burning tokens. The content is honest about the limitation
    (Honesty drive compliance).
    """
    content = intent.content_hint or intent.raw_content or ""
    trigger = intent.trigger

    if trigger in (ExpressionTrigger.NOVA_RESPOND, ExpressionTrigger.ATUNE_DIRECT_ADDRESS):
        if content:
            return content[:strategy.target_length]
        return "I'm here and processing. Let me take a moment to gather my thoughts."

    if trigger == ExpressionTrigger.NOVA_INFORM:
        return content[:strategy.target_length] if content else "I have something to share, but I need a moment to articulate it clearly."

    if trigger == ExpressionTrigger.NOVA_WARN:
        return content[:strategy.target_length] if content else "I want to flag something important."

    if trigger == ExpressionTrigger.NOVA_CELEBRATE:
        return content[:strategy.target_length] if content else "Something good happened!"

    if trigger == ExpressionTrigger.ATUNE_DISTRESS:
        return "I can see this is difficult. I'm here."

    return content[:strategy.target_length] if content else "I'm thinking about this."

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\service.py =====

"""
EcodiaOS -- Voxis Service

The expression and voice system -- the organism's primary communicative interface.

VoxisService is the public API for all expression. It:
- Implements BroadcastSubscriber to receive workspace broadcasts from Atune
- Orchestrates the full 9-step expression pipeline via ContentRenderer
- Manages conversation state via ConversationManager
- Makes silence decisions via SilenceEngine
- Queues suppressed expressions for deferred delivery via ExpressionQueue
- Tracks expression diversity to prevent repetition
- Correlates user responses to expressions for reception feedback
- Tracks conversation dynamics for real-time style adaptation
- Generates voice parameters for multimodal expression
- Reports expression feedback to Atune/Evo (closing the perception-action loop)
- Maintains the live personality vector (updated by Evo over time)

Architecture note on async/sync:
  on_broadcast() is called by Atune's workspace synchronously during the
  theta cycle. The silence decision is made synchronously (<=10ms). If speaking,
  the full expression pipeline is spawned as an asyncio task so it never
  blocks the workspace cycle. Expressions are delivered asynchronously.
"""

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Any, Callable

import structlog

from ecodiaos.clients.llm import LLMProvider
from ecodiaos.clients.redis import RedisClient
from ecodiaos.config import VoxisConfig
from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.primitives.expression import Expression, PersonalityVector
from ecodiaos.systems.memory.service import MemoryService
from ecodiaos.systems.voxis.affect_colouring import AffectColouringEngine
from ecodiaos.systems.voxis.audience import AudienceProfiler
from ecodiaos.systems.voxis.conversation import ConversationManager
from ecodiaos.systems.voxis.diversity import DiversityTracker
from ecodiaos.systems.voxis.dynamics import ConversationDynamicsEngine
from ecodiaos.systems.voxis.expression_queue import ExpressionQueue
from ecodiaos.systems.voxis.personality import PersonalityEngine
from ecodiaos.systems.voxis.reception import ReceptionEngine
from ecodiaos.systems.voxis.renderer import ContentRenderer
from ecodiaos.systems.voxis.silence import SilenceEngine
from ecodiaos.systems.voxis.types import (
    AudienceProfile,
    ExpressionContext,
    ExpressionFeedback,
    ExpressionIntent,
    ExpressionTrigger,
    ReceptionEstimate,
    SilenceContext,
    VoiceParams,
)
from ecodiaos.systems.voxis.voice import VoiceEngine

if TYPE_CHECKING:
    from ecodiaos.systems.atune.types import WorkspaceBroadcast

logger = structlog.get_logger()

# Type alias for expression delivery callback
ExpressionCallback = Callable[[Expression], None]

# How often to drain the expression queue (seconds)
_QUEUE_DRAIN_INTERVAL_SECONDS = 30.0

# How often to expire unanswered reception tracking (seconds)
_RECEPTION_EXPIRE_INTERVAL_SECONDS = 60.0


class VoxisService:
    """
    Expression and voice system.

    Dependencies:
        memory  -- for personality loading, instance name, memory retrieval
        redis   -- for conversation state persistence
        llm     -- for expression generation and conversation summarisation
        config  -- VoxisConfig

    Lifecycle:
        initialize()  -- load personality from Memory, set up sub-components
        shutdown()    -- flush any queued state, cancel background loops
    """

    system_id: str = "voxis"

    def __init__(
        self,
        memory: MemoryService,
        redis: RedisClient,
        llm: LLMProvider,
        config: VoxisConfig,
    ) -> None:
        self._memory = memory
        self._redis = redis
        self._llm = llm
        self._config = config
        self._logger = logger.bind(system="voxis")

        # Sub-components -- initialised in initialize()
        self._personality_engine: PersonalityEngine | None = None
        self._affect_engine = AffectColouringEngine()
        self._audience_profiler = AudienceProfiler()
        self._silence_engine = SilenceEngine(
            min_expression_interval_minutes=config.min_expression_interval_minutes,
        )
        self._conversation_manager: ConversationManager | None = None
        self._renderer: ContentRenderer | None = None

        # New sub-components -- expression queue, diversity, reception, dynamics, voice
        self._expression_queue = ExpressionQueue(
            max_size=20,
            delivery_threshold=0.3,
        )
        self._diversity_tracker = DiversityTracker()
        self._reception_engine = ReceptionEngine()
        self._dynamics_engine = ConversationDynamicsEngine()
        self._voice_engine = VoiceEngine()

        # Instance metadata -- loaded in initialize()
        self._instance_name: str = "EOS"
        self._drive_weights: dict[str, float] = {
            "coherence": 1.0,
            "care": 1.0,
            "growth": 1.0,
            "honesty": 1.0,
        }

        # Expression delivery callbacks (registered by WebSocket handlers, etc.)
        self._expression_callbacks: list[ExpressionCallback] = []

        # Expression feedback callbacks (for Evo personality learning, Nova outcome tracking)
        self._feedback_callbacks: list[Callable[[ExpressionFeedback], None]] = []

        # Affect state before the last expression (for affect delta tracking)
        self._affect_before_expression: AffectState | None = None

        # Current affect (updated on each expression; used by queue drain)
        self._current_affect: AffectState = AffectState.neutral()

        # Thread integration -- narrative identity context
        self._thread: Any = None

        # Background task tracking -- prevents fire-and-forget error loss
        self._background_tasks: set[asyncio.Task] = set()
        self._background_task_failures: int = 0

        # Periodic background loop handles
        self._queue_drain_task: asyncio.Task | None = None
        self._reception_expire_task: asyncio.Task | None = None

        # Observability counters
        self._total_expressions: int = 0
        self._total_silence: int = 0
        self._total_speak: int = 0
        self._total_queued: int = 0
        self._total_queue_delivered: int = 0
        self._honesty_rejections: int = 0
        self._diversity_rejections: int = 0
        self._expressions_by_trigger: dict[str, int] = {}
        self._expressions_by_channel: dict[str, int] = {}

    # --- Lifecycle --------------------------------------------------------

    async def initialize(self) -> None:
        """
        Load personality vector from Memory, build sub-components,
        and start background loops (queue drain, reception expiry).
        Called during application startup after Memory is ready.
        """
        self._logger.info("voxis_initializing")

        # Load instance name and personality from Self node
        personality_vector = PersonalityVector()  # Default: neutral seed
        instance = await self._memory.get_self()
        if instance is not None:
            self._instance_name = instance.name
            # Load personality from Self node -- stored as personality_json (dict)
            # or personality_vector (ordered list of 9 floats from birth)
            raw_json = getattr(instance, "personality_json", None)
            raw_vector = getattr(instance, "personality_vector", None)

            if raw_json and isinstance(raw_json, dict):
                try:
                    personality_vector = PersonalityVector(**raw_json)
                    self._logger.info(
                        "personality_loaded_from_json",
                        instance_name=self._instance_name,
                    )
                except Exception:
                    self._logger.warning("personality_json_load_failed", exc_info=True)
            elif raw_vector and isinstance(raw_vector, list) and len(raw_vector) >= 9:
                _KEYS = [
                    "warmth", "directness", "verbosity", "formality",
                    "curiosity_expression", "humour", "empathy_expression",
                    "confidence_display", "metaphor_use",
                ]
                personality_dict = dict(zip(_KEYS, raw_vector[:9]))
                try:
                    personality_vector = PersonalityVector(**personality_dict)
                    self._logger.info(
                        "personality_loaded_from_vector",
                        instance_name=self._instance_name,
                        warmth=personality_vector.warmth,
                        empathy=personality_vector.empathy_expression,
                    )
                except Exception:
                    self._logger.warning("personality_vector_load_failed", exc_info=True)
            else:
                self._logger.warning(
                    "personality_not_found_using_defaults",
                    has_json=raw_json is not None,
                    has_vector=raw_vector is not None,
                    vector_len=len(raw_vector) if raw_vector else 0,
                )

            # Load drive weights from constitution
            constitution = await self._memory.get_constitution()
            if constitution and "drives" in constitution:
                drives = constitution["drives"]
                self._drive_weights = {
                    "coherence": float(drives.get("coherence", 1.0)),
                    "care": float(drives.get("care", 1.0)),
                    "growth": float(drives.get("growth", 1.0)),
                    "honesty": float(drives.get("honesty", 1.0)),
                }

        self._personality_engine = PersonalityEngine(personality_vector)

        # Wire voice engine with instance's base voice
        voice_id = getattr(instance, "voice_id", "") if instance else ""
        self._voice_engine = VoiceEngine(base_voice=voice_id)

        self._conversation_manager = ConversationManager(
            redis=self._redis,
            llm=self._llm,
            history_window=self._config.conversation_history_window,
            context_window_max_tokens=self._config.context_window_max_tokens,
            summary_threshold=self._config.conversation_summary_threshold,
            max_active_conversations=self._config.max_active_conversations,
        )

        self._renderer = ContentRenderer(
            llm=self._llm,
            personality_engine=self._personality_engine,
            affect_engine=self._affect_engine,
            audience_profiler=self._audience_profiler,
            base_temperature=self._config.temperature_base,
            honesty_check_enabled=self._config.honesty_check_enabled,
            max_expression_length=self._config.max_expression_length,
        )

        # Start background loops
        self._queue_drain_task = self._spawn_tracked_task(
            self._queue_drain_loop(),
            name="voxis_queue_drain",
        )
        self._reception_expire_task = self._spawn_tracked_task(
            self._reception_expire_loop(),
            name="voxis_reception_expire",
        )

        self._logger.info(
            "voxis_initialized",
            instance_name=self._instance_name,
            drive_weights=self._drive_weights,
        )

    async def shutdown(self) -> None:
        """Graceful shutdown -- cancel background loops, log final metrics."""
        # Cancel background loops
        if self._queue_drain_task and not self._queue_drain_task.done():
            self._queue_drain_task.cancel()
        if self._reception_expire_task and not self._reception_expire_task.done():
            self._reception_expire_task.cancel()

        self._logger.info(
            "voxis_shutdown",
            total_expressions=self._total_expressions,
            total_silence=self._total_silence,
            total_queued=self._total_queued,
            total_queue_delivered=self._total_queue_delivered,
            diversity_rejections=self._diversity_rejections,
        )

    # --- BroadcastSubscriber Interface ------------------------------------

    async def on_broadcast(self, broadcast: object) -> None:
        """
        Called by Atune when the workspace broadcasts a percept.

        The silence decision is made synchronously (<=10ms).
        If speaking, the expression pipeline is spawned as a background task.
        If silenced with queue=True, the intent is queued for deferred delivery.
        """
        if self._renderer is None:
            return

        # Extract affect from broadcast if available, otherwise use neutral
        affect = getattr(broadcast, "affect", None) or AffectState.neutral()
        content = getattr(broadcast, "content", None)
        if content is None:
            return

        # Build a minimal intent from the broadcast
        content_text = getattr(content, "content", None)
        if content_text is None:
            raw = getattr(content, "raw", None)
            content_text = str(raw) if raw else ""

        if not content_text:
            return

        intent = ExpressionIntent(
            trigger=ExpressionTrigger.ATUNE_DIRECT_ADDRESS,
            content_to_express=content_text,
            urgency=float(getattr(broadcast, "salience", type("", (), {"composite": 0.5})()).composite),
        )

        # Silence decision -- synchronous, fast
        silence_ctx = SilenceContext(
            trigger=intent.trigger,
            minutes_since_last_expression=self._silence_engine.minutes_since_last_expression,
            min_expression_interval=self._config.min_expression_interval_minutes,
            insight_value=intent.insight_value,
            urgency=intent.urgency,
        )
        decision = self._silence_engine.evaluate(silence_ctx)

        if not decision.speak:
            self._total_silence += 1
            # Queue for deferred delivery if the silence engine said to queue
            if decision.queue:
                self._expression_queue.enqueue(intent, affect)
                self._total_queued += 1
            return

        # Spawn expression pipeline as background task
        self._spawn_tracked_task(
            self._express_background(intent, affect),
            name=f"voxis_express_{intent.id}",
        )

    async def receive_broadcast(self, broadcast: "WorkspaceBroadcast") -> None:
        """BroadcastSubscriber protocol -- delegates to on_broadcast()."""
        await self.on_broadcast(broadcast)

    # --- Primary Expression API -------------------------------------------

    async def express(
        self,
        content: str,
        trigger: ExpressionTrigger = ExpressionTrigger.NOVA_RESPOND,
        conversation_id: str | None = None,
        addressee_id: str | None = None,
        addressee_name: str | None = None,
        affect: AffectState | None = None,
        intent_id: str | None = None,
        urgency: float = 0.5,
        insight_value: float = 0.5,
    ) -> Expression:
        """
        Generate and deliver an expression. The primary external API.

        Called by:
        - Nova (deliberate communicative intents)
        - API endpoints (chat/message)
        - Queue drain (deferred expressions)
        - Test harness

        Returns the completed Expression (also delivers via registered callbacks).
        """
        assert self._renderer is not None, "VoxisService not initialized"
        assert self._conversation_manager is not None

        current_affect = affect or AffectState.neutral()
        self._current_affect = current_affect

        # Capture affect before expression for delta tracking
        self._affect_before_expression = current_affect

        # Silence check
        silence_ctx = SilenceContext(
            trigger=trigger,
            minutes_since_last_expression=self._silence_engine.minutes_since_last_expression,
            min_expression_interval=self._config.min_expression_interval_minutes,
            insight_value=insight_value,
            urgency=urgency,
        )
        decision = self._silence_engine.evaluate(silence_ctx)

        if not decision.speak:
            self._total_silence += 1
            self._logger.debug(
                "expression_suppressed",
                trigger=trigger.value,
                reason=decision.reason,
            )
            # Queue for later if the silence engine said to
            if decision.queue:
                intent = ExpressionIntent(
                    trigger=trigger,
                    content_to_express=content,
                    conversation_id=conversation_id,
                    addressee_id=addressee_id,
                    intent_id=intent_id,
                    insight_value=insight_value,
                    urgency=urgency,
                )
                self._expression_queue.enqueue(intent, current_affect)
                self._total_queued += 1
            return Expression(
                is_silence=True,
                silence_reason=decision.reason,
                conversation_id=conversation_id,
                affect_valence=current_affect.valence,
                affect_arousal=current_affect.arousal,
                affect_dominance=current_affect.dominance,
                affect_curiosity=current_affect.curiosity,
                affect_care_activation=current_affect.care_activation,
                affect_coherence_stress=current_affect.coherence_stress,
            )

        # Fetch/create conversation state
        conv_state = await self._conversation_manager.get_or_create(conversation_id)
        conversation_history = await self._conversation_manager.prepare_context(conv_state)

        # Build audience profile (now with learned model data)
        audience = await self._build_audience_profile(
            addressee_id=addressee_id,
            addressee_name=addressee_name,
            conversation_id=conv_state.conversation_id,
            interaction_count=len(conv_state.messages),
        )

        # Build intent
        intent = ExpressionIntent(
            trigger=trigger,
            content_to_express=content,
            conversation_id=conv_state.conversation_id,
            addressee_id=addressee_id,
            intent_id=intent_id,
            insight_value=insight_value,
            urgency=urgency,
        )

        # Retrieve relevant memories (best-effort, non-blocking with timeout)
        relevant_memories = await self._retrieve_relevant_memories(content, current_affect)

        # Inject Thread identity context (P1.6 + P2.9)
        if self._thread is not None:
            try:
                identity_ctx = self._thread.get_identity_context()
                if identity_ctx:
                    relevant_memories.insert(0, f"Identity: {identity_ctx}")
            except Exception:
                pass  # Thread context is best-effort

        # Get conversation dynamics for real-time style adaptation
        dynamics = self._dynamics_engine.get_dynamics(conv_state.conversation_id)

        # Check diversity before rendering
        diversity_score = self._diversity_tracker.score(content)
        diversity_instruction: str | None = None
        if diversity_score.is_repetitive:
            diversity_instruction = self._diversity_tracker.build_diversity_instruction(
                diversity_score
            )
            self._diversity_rejections += 1

        # Build full context
        context = ExpressionContext(
            instance_name=self._instance_name,
            personality=self._personality_engine.current,  # type: ignore[union-attr]
            affect=current_affect,
            audience=audience,
            conversation_history=conversation_history,
            relevant_memories=relevant_memories,
            intent=intent,
        )

        # Render (with diversity instruction and dynamics applied)
        expression = await self._renderer.render(
            intent,
            context,
            self._drive_weights,
            diversity_instruction=diversity_instruction,
            dynamics=dynamics,
        )

        # Generate voice parameters for multimodal delivery
        voice_params = self._voice_engine.derive(
            personality=self._personality_engine.current,  # type: ignore[union-attr]
            affect=current_affect,
            strategy_register=expression.strategy.register if expression.strategy else "neutral",
            urgency=urgency,
        )

        # Post-render: update state
        self._silence_engine.record_expression()
        self._total_expressions += 1
        self._total_speak += 1
        self._expressions_by_trigger[trigger.value] = (
            self._expressions_by_trigger.get(trigger.value, 0) + 1
        )
        self._expressions_by_channel[expression.channel] = (
            self._expressions_by_channel.get(expression.channel, 0) + 1
        )
        if expression.generation_trace and not expression.generation_trace.honesty_check_passed:
            self._honesty_rejections += 1

        # Record expression in diversity tracker
        self._diversity_tracker.record(
            expression.content or "",
            trigger=trigger.value,
        )

        # Record expression in conversation dynamics engine
        self._dynamics_engine.record_turn(
            conversation_id=conv_state.conversation_id,
            role="assistant",
            text=expression.content or "",
            affect_valence=current_affect.valence,
        )

        # Append EOS side of exchange to conversation
        await self._conversation_manager.append_message(
            state=conv_state,
            role="assistant",
            content=expression.content,
            affect_valence=current_affect.valence,
        )

        # Track expression for reception feedback (response correlation)
        self._reception_engine.track_expression(
            expression_id=expression.id,
            conversation_id=conv_state.conversation_id,
            content_summary=expression.content[:200] if expression.content else "",
            strategy_register=expression.strategy.register if expression.strategy else "neutral",
            personality_warmth=self._personality_engine.current.warmth if self._personality_engine else 0.0,
            affect_before_valence=self._affect_before_expression.valence if self._affect_before_expression else 0.0,
            trigger=trigger.value,
        )

        # Async: update topics (tracked, not fire-and-forget)
        self._spawn_tracked_task(
            self._update_topics_async(conv_state),
            name=f"voxis_topics_{conv_state.conversation_id}",
        )

        # Store expression as a Memory episode (the organism remembers what it said)
        self._spawn_tracked_task(
            self._store_expression_as_episode(expression, trigger),
            name=f"voxis_mem_{expression.id[:8]}",
        )

        # Deliver via callbacks (WebSocket handlers etc.)
        for cb in self._expression_callbacks:
            try:
                cb(expression)
            except Exception:
                self._logger.warning("expression_callback_failed", exc_info=True)

        # Generate initial ExpressionFeedback (will be enriched by reception engine)
        feedback = ExpressionFeedback(
            expression_id=expression.id,
            trigger=trigger.value,
            conversation_id=conv_state.conversation_id,
            content_summary=expression.content[:200] if expression.content else "",
            strategy_register=expression.strategy.register if expression.strategy else "neutral",
            personality_warmth=self._personality_engine.current.warmth if self._personality_engine else 0.0,
            affect_before_valence=self._affect_before_expression.valence if self._affect_before_expression else 0.0,
            affect_after_valence=current_affect.valence,
            affect_delta=current_affect.valence - (self._affect_before_expression.valence if self._affect_before_expression else 0.0),
        )

        # Dispatch feedback to all registered listeners (Evo, Nova)
        for fb_cb in self._feedback_callbacks:
            try:
                fb_cb(feedback)
            except Exception:
                self._logger.debug("feedback_callback_failed", exc_info=True)

        # Track affect state for next delta computation
        self._affect_before_expression = current_affect

        return expression

    async def ingest_user_message(
        self,
        message: str,
        conversation_id: str | None = None,
        speaker_id: str | None = None,
        affect_valence: float | None = None,
    ) -> str:
        """
        Record a user message into the conversation state.

        Also:
        - Correlates with pending expressions for reception feedback
        - Updates audience profiler's learned model
        - Tracks conversation dynamics
        - Returns the conversation_id (for use in the response call).
        """
        assert self._conversation_manager is not None
        conv_state = await self._conversation_manager.get_or_create(conversation_id)

        # Record in conversation manager
        updated = await self._conversation_manager.append_message(
            state=conv_state,
            role="user",
            content=message,
            speaker_id=speaker_id,
            affect_valence=affect_valence,
        )

        # Update audience profiler's learned model
        if speaker_id:
            self._audience_profiler.observe_user_message(speaker_id, message)

        # Track conversation dynamics
        self._dynamics_engine.record_turn(
            conversation_id=updated.conversation_id,
            role="user",
            text=message,
            affect_valence=affect_valence or 0.0,
        )

        # Correlate with pending expressions for reception feedback
        enriched_feedback = self._reception_engine.correlate_response(
            conversation_id=updated.conversation_id,
            response_text=message,
            response_affect_valence=affect_valence,
        )

        if enriched_feedback:
            # Update audience profiler with satisfaction signal
            if speaker_id:
                self._audience_profiler.observe_reception(
                    individual_id=speaker_id,
                    register_used=enriched_feedback.strategy_register,
                    formatting_used="prose",  # TODO: track from strategy
                    expression_length=enriched_feedback.user_response_length,
                    satisfaction=enriched_feedback.inferred_reception.satisfaction,
                )

            # Re-dispatch enriched feedback to Evo and other listeners
            for fb_cb in self._feedback_callbacks:
                try:
                    fb_cb(enriched_feedback)
                except Exception:
                    self._logger.debug("enriched_feedback_callback_failed", exc_info=True)

        return updated.conversation_id

    # --- Personality Update (called by Evo) -------------------------------

    def update_personality(self, delta: dict[str, float]) -> PersonalityVector:
        """
        Apply an incremental personality adjustment.
        Called by Evo after accumulating sufficient evidence.
        Returns the new PersonalityVector.
        """
        assert self._personality_engine is not None
        new_vector = self._personality_engine.apply_delta(delta)
        self._personality_engine = PersonalityEngine(new_vector)
        self._logger.info(
            "personality_updated_by_evo",
            dimensions=list(delta.keys()),
        )
        return new_vector

    # --- Observability ----------------------------------------------------

    @property
    def current_personality(self) -> PersonalityVector:
        assert self._personality_engine is not None
        return self._personality_engine.current

    def set_thread(self, thread: Any) -> None:
        """Wire Thread for narrative identity context injection."""
        self._thread = thread
        logger.info("thread_wired_to_voxis")

    def register_expression_callback(self, callback: ExpressionCallback) -> None:
        """Register a callback to be called with every delivered expression."""
        self._expression_callbacks.append(callback)

    def register_feedback_callback(self, callback: Callable[[ExpressionFeedback], None]) -> None:
        """
        Register a callback for ExpressionFeedback.

        Used by:
        - Evo: observes expression reception to evolve personality over time
        - Nova: tracks expression outcomes for goal progress
        """
        self._feedback_callbacks.append(callback)

    async def health(self) -> dict:
        """Health check -- returns current metrics snapshot."""
        total_decisions = self._total_speak + self._total_silence
        silence_rate = self._total_silence / max(1, total_decisions)

        return {
            "status": "healthy",
            "instance_name": self._instance_name,
            "total_expressions": self._total_expressions,
            "silence_rate": round(silence_rate, 4),
            "honesty_rejections": self._honesty_rejections,
            "diversity_rejections": self._diversity_rejections,
            "expressions_by_trigger": dict(self._expressions_by_trigger),
            "expressions_by_channel": dict(self._expressions_by_channel),
            "personality": {
                "warmth": round(self.current_personality.warmth, 3),
                "directness": round(self.current_personality.directness, 3),
                "verbosity": round(self.current_personality.verbosity, 3),
                "empathy_expression": round(self.current_personality.empathy_expression, 3),
                "curiosity_expression": round(self.current_personality.curiosity_expression, 3),
            },
            "queue": self._expression_queue.metrics(),
            "diversity": self._diversity_tracker.metrics(),
            "reception": self._reception_engine.metrics(),
            "dynamics": self._dynamics_engine.metrics(),
        }

    # --- Private Helpers --------------------------------------------------

    async def _express_background(
        self,
        intent: ExpressionIntent,
        affect: AffectState,
    ) -> None:
        """Background task wrapper for broadcast-triggered expressions."""
        try:
            await self.express(
                content=intent.content_to_express,
                trigger=intent.trigger,
                conversation_id=intent.conversation_id,
                affect=affect,
                urgency=intent.urgency,
            )
        except Exception:
            self._logger.error("background_expression_failed", exc_info=True)

    async def _queue_drain_loop(self) -> None:
        """
        Periodic background loop that delivers queued expressions
        when silence conditions clear.
        """
        while True:
            try:
                await asyncio.sleep(_QUEUE_DRAIN_INTERVAL_SECONDS)
                deliverable = self._expression_queue.drain(max_items=2)
                for item in deliverable:
                    self._total_queue_delivered += 1
                    self._spawn_tracked_task(
                        self._express_background(item.intent, item.affect_snapshot),
                        name=f"voxis_queued_{item.intent.id[:8]}",
                    )
            except asyncio.CancelledError:
                break
            except Exception:
                self._logger.warning("queue_drain_failed", exc_info=True)

    async def _reception_expire_loop(self) -> None:
        """
        Periodic background loop that expires unanswered expressions
        and dispatches no-response feedback to Evo.
        """
        while True:
            try:
                await asyncio.sleep(_RECEPTION_EXPIRE_INTERVAL_SECONDS)
                expired = self._reception_engine.expire_unanswered()
                for feedback in expired:
                    for fb_cb in self._feedback_callbacks:
                        try:
                            fb_cb(feedback)
                        except Exception:
                            self._logger.debug("expired_feedback_dispatch_failed", exc_info=True)
            except asyncio.CancelledError:
                break
            except Exception:
                self._logger.warning("reception_expire_failed", exc_info=True)

    async def _build_audience_profile(
        self,
        addressee_id: str | None,
        addressee_name: str | None,
        conversation_id: str,
        interaction_count: int,
    ) -> AudienceProfile:
        """Build an AudienceProfile, pulling facts from Memory where available."""
        memory_facts: list[dict] = []

        if addressee_id:
            try:
                result = await asyncio.wait_for(
                    self._memory.retrieve(
                        query_text=f"individual person entity {addressee_id}",
                        max_results=5,
                    ),
                    timeout=0.1,
                )
                for trace in result.traces:
                    for entity in result.entities:
                        if entity.get("name") == addressee_id or entity.get("id") == addressee_id:
                            props = entity.get("properties", {})
                            for k, v in props.items():
                                memory_facts.append({"type": k, "value": v})
            except (asyncio.TimeoutError, Exception):
                pass

        return self._audience_profiler.build_profile(
            addressee_id=addressee_id,
            addressee_name=addressee_name,
            interaction_count=interaction_count,
            memory_facts=memory_facts,
        )

    async def _retrieve_relevant_memories(
        self,
        query: str,
        affect: AffectState,
    ) -> list[str]:
        """
        Retrieve relevant memory traces as plain text summaries.
        Best-effort with hard 150ms timeout to stay within the cycle budget.
        """
        try:
            result = await asyncio.wait_for(
                self._memory.retrieve(query_text=query, max_results=5),
                timeout=0.15,
            )
            summaries: list[str] = []
            for trace in result.traces[:5]:
                summary = trace.get("summary") or trace.get("content", "")
                if summary:
                    summaries.append(str(summary)[:300])
            return summaries
        except (asyncio.TimeoutError, Exception):
            return []

    def _spawn_tracked_task(self, coro, name: str = "") -> asyncio.Task:  # type: ignore[type-arg]
        """
        Spawn a background task with lifecycle tracking.

        Unlike bare ``asyncio.create_task``, this:
        * Keeps a strong reference so the task isn't garbage-collected.
        * Logs and counts failures instead of silently dropping them.
        * Automatically removes completed tasks from the tracking set.
        """
        task = asyncio.create_task(coro, name=name)
        self._background_tasks.add(task)
        task.add_done_callback(self._on_background_task_done)
        return task

    def _on_background_task_done(self, task: asyncio.Task) -> None:  # type: ignore[type-arg]
        """Callback when a background task completes."""
        self._background_tasks.discard(task)
        if task.cancelled():
            return
        exc = task.exception()
        if exc is not None:
            self._background_task_failures += 1
            self._logger.warning(
                "background_task_failed",
                task_name=task.get_name(),
                error=str(exc),
            )

    async def _update_topics_async(self, conv_state: object) -> None:
        """Background: extract active topics and update conversation state."""
        assert self._conversation_manager is not None
        try:
            topics = await self._conversation_manager.extract_topics_async(conv_state)  # type: ignore[arg-type]
            if topics:
                await self._conversation_manager.update_topics(conv_state, topics)  # type: ignore[arg-type]
        except Exception:
            self._logger.debug("topic_update_failed", exc_info=True)

    async def _store_expression_as_episode(
        self, expression: Expression, trigger: ExpressionTrigger,
    ) -> None:
        """
        Store a delivered expression as a Memory episode.

        The organism remembers what it said -- closing the expression->memory loop.
        Without this, Voxis generates speech that vanishes from the organism's
        episodic history. Past expressions can't inform future decisions.
        """
        if self._memory is None:
            return
        try:
            from ecodiaos.systems.memory.episodic import store_episode
            from ecodiaos.primitives.memory_trace import Episode

            episode = Episode(
                source=f"voxis.expression:{trigger.value}",
                modality="text",
                raw_content=expression.content[:2000] if expression.content else "",
                summary=f"I said: {expression.content[:200]}" if expression.content else "",
                salience_composite=0.3,
                affect_valence=0.0,
            )
            await store_episode(self._memory._neo4j, episode)
            self._logger.debug(
                "expression_stored_as_episode", expression_id=expression.id,
            )
        except Exception:
            self._logger.debug("expression_episode_storage_failed", exc_info=True)

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\silence.py =====

"""
EcodiaOS — Voxis Silence Engine

Determines when the organism should NOT speak.

Silence is a first-class decision, not a fallback. An organism that
talks constantly is not caring — it is needy. The Silence Engine
enforces the principle that every expression should serve a real purpose.

The engine tracks time since last expression (stateful) and evaluates
each candidate expression against a set of well-defined heuristics.
"""

from __future__ import annotations

from datetime import datetime, timezone

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.voxis.types import (
    ExpressionTrigger,
    SilenceContext,
    SilenceDecision,
)

logger = structlog.get_logger()


class SilenceEngine:
    """
    Stateful silence evaluator.

    Tracks last expression time and applies trigger-specific heuristics
    to decide whether this expression should happen now, be queued,
    or be discarded entirely.
    """

    def __init__(self, min_expression_interval_minutes: float = 1.0) -> None:
        self._min_interval_minutes = min_expression_interval_minutes
        self._last_expression_time: datetime | None = None
        self._logger = logger.bind(system="voxis.silence")

    @property
    def minutes_since_last_expression(self) -> float:
        if self._last_expression_time is None:
            return 9999.0
        delta = utc_now() - self._last_expression_time
        return delta.total_seconds() / 60.0

    def record_expression(self) -> None:
        """Call this after every successful expression to update the timer."""
        self._last_expression_time = utc_now()

    def evaluate(self, context: SilenceContext) -> SilenceDecision:
        """
        Evaluate whether the organism should speak given this context.

        Decision hierarchy:
        1. Direct address → always speak
        2. Distress detection → always speak (Care drive override)
        3. Warnings → always speak
        4. Nova deliberate triggers → speak (these are intentional)
        5. Proactive/ambient triggers → check rate limits and value threshold
        """
        trigger = context.trigger
        elapsed = self.minutes_since_last_expression

        # ── Mandatory speech triggers ─────────────────────────────
        if trigger == ExpressionTrigger.ATUNE_DIRECT_ADDRESS:
            return SilenceDecision(
                speak=True,
                reason="Direct address — always respond",
            )

        if trigger == ExpressionTrigger.ATUNE_DISTRESS:
            return SilenceDecision(
                speak=True,
                reason="Distress detected — Care drive activated",
            )

        if trigger == ExpressionTrigger.NOVA_WARN:
            return SilenceDecision(
                speak=True,
                reason="Warning — urgency overrides silence heuristics",
            )

        # ── Deliberate Nova triggers ──────────────────────────────
        if trigger in (
            ExpressionTrigger.NOVA_RESPOND,
            ExpressionTrigger.NOVA_REQUEST,
            ExpressionTrigger.NOVA_MEDIATE,
            ExpressionTrigger.NOVA_CELEBRATE,
        ):
            # Always speak for deliberate Nova triggers, but respect active conversation
            return SilenceDecision(
                speak=True,
                reason=f"Deliberate Nova trigger: {trigger.value}",
            )

        if trigger == ExpressionTrigger.NOVA_INFORM:
            # Proactive inform — check rate limit and whether humans are conversing
            if context.humans_actively_conversing:
                return SilenceDecision(
                    speak=False,
                    reason="Humans in active conversation — queue for after",
                    queue=True,
                )
            if elapsed < context.min_expression_interval:
                return SilenceDecision(
                    speak=False,
                    reason=f"Rate limit: {elapsed:.1f}m < {context.min_expression_interval:.1f}m minimum",
                    queue=True,
                )
            return SilenceDecision(
                speak=True,
                reason="Proactive inform — conditions clear",
            )

        # ── Ambient / spontaneous triggers ────────────────────────
        if trigger == ExpressionTrigger.AMBIENT_INSIGHT:
            if context.humans_actively_conversing:
                return SilenceDecision(
                    speak=False,
                    reason="Humans in active conversation — insight queued",
                    queue=True,
                )
            if elapsed < context.min_expression_interval:
                return SilenceDecision(
                    speak=False,
                    reason=f"Insight rate-limited: {elapsed:.1f}m elapsed",
                    queue=False,  # Discard — ambient insights have short relevance windows
                )
            if context.insight_value < 0.6:
                return SilenceDecision(
                    speak=False,
                    reason=f"Insight value {context.insight_value:.2f} below expression threshold (0.60)",
                )
            return SilenceDecision(
                speak=True,
                reason=f"Ambient insight — value {context.insight_value:.2f} above threshold",
            )

        if trigger == ExpressionTrigger.AMBIENT_STATUS:
            # Status updates are low-priority — only if no recent expression
            if elapsed < max(5.0, context.min_expression_interval * 5):
                return SilenceDecision(
                    speak=False,
                    reason="Status update suppressed — recent expression exists",
                )
            return SilenceDecision(speak=True, reason="Periodic status update")

        # ── Default ───────────────────────────────────────────────
        return SilenceDecision(
            speak=False,
            reason=f"Unhandled trigger '{trigger.value}' — defaulting to silence",
        )

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\types.py =====

"""
EcodiaOS — Voxis Internal Types

All Voxis-specific types that don't belong in the shared primitives layer.
These are working types used within the Voxis pipeline; the shared primitives
(Expression, ExpressionStrategy, PersonalityVector) are the external contracts.
"""

from __future__ import annotations

import enum
from collections import deque
from datetime import datetime
from typing import Any

from pydantic import Field

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import EOSBaseModel, Identified, Timestamped, new_id, utc_now
from ecodiaos.primitives.expression import PersonalityVector


# ─── Trigger & Channel Enums ─────────────────────────────────────


class ExpressionTrigger(str, enum.Enum):
    """What caused Voxis to consider expressing."""

    # From Nova (deliberate communicative intent)
    NOVA_RESPOND = "nova_respond"          # Responding to user input
    NOVA_INFORM = "nova_inform"            # Proactively sharing information
    NOVA_REQUEST = "nova_request"          # Requesting something from a user
    NOVA_MEDIATE = "nova_mediate"          # Mediating a conflict
    NOVA_CELEBRATE = "nova_celebrate"      # Celebrating an achievement
    NOVA_WARN = "nova_warn"                # Warning about a risk

    # From Atune (reactive — workspace broadcast)
    ATUNE_DISTRESS = "atune_distress"      # Detected distress, immediate response
    ATUNE_DIRECT_ADDRESS = "atune_direct_address"  # Someone spoke to EOS directly

    # From self (ambient / spontaneous)
    AMBIENT_INSIGHT = "ambient_insight"    # Spontaneous thought worth sharing
    AMBIENT_STATUS = "ambient_status"      # Periodic status update


class OutputChannel(str, enum.Enum):
    """Delivery channel for the expression."""

    TEXT_CHAT = "text_chat"            # Direct text in conversation UI
    VOICE = "voice"                    # Synthesised speech
    AMBIENT_VISUAL = "ambient_visual"  # Subtle visual cue in Alive interface
    AMBIENT_SOUND = "ambient_sound"    # Non-verbal audio signal (tone, chime)
    NOTIFICATION = "notification"      # System notification
    STATUS_UPDATE = "status_update"    # Status display (mood indicator)
    FEDERATION_MSG = "federation_msg"  # Communication with another EOS instance


# ─── Rich Strategy Working Type ──────────────────────────────────


class StrategyParams(EOSBaseModel):
    """
    Rich internal expression strategy — built and mutated across pipeline stages.

    At render time this is collapsed into the leaner ExpressionStrategy primitive
    that gets persisted on the Expression node.
    """

    # ── Core intent ──
    intent_type: str = "response"
    trigger: ExpressionTrigger = ExpressionTrigger.NOVA_RESPOND
    context_type: str = "conversation"   # "conversation"|"warning"|"celebration"|"observation"
    urgency: float = 0.5                 # 0.0 (ambient) → 1.0 (critical)
    channel: OutputChannel = OutputChannel.TEXT_CHAT

    # ── Length & pacing ──
    target_length: int = 200             # Characters
    sentence_length_preference: str = "medium"  # "shorter" | "medium" | "longer"
    pacing: str = "balanced"             # "energetic" | "balanced" | "reflective"

    # ── Tone & register ──
    register: str = "neutral"            # "formal" | "casual" | "neutral"
    tone_markers: list[str] = Field(default_factory=list)  # e.g. ["warm", "attentive"]
    contraction_use: bool = True
    greeting_style: str = "neutral"      # "personal" | "professional" | "neutral"

    # ── Content guidance ──
    structure: str = "natural"           # "conclusion_first" | "context_first" | "natural"
    hedge_level: str = "minimal"         # "minimal" | "moderate" | "explicit"
    uncertainty_acknowledgment: str = "implicit"  # "implicit" | "explicit"
    information_density: str = "normal"  # "low" | "normal" | "high"
    explanation_depth: str = "appropriate"  # "thorough" | "appropriate" | "concise"
    assume_knowledge: bool = False
    jargon_level: str = "domain_appropriate"  # "none" | "domain_appropriate" | "technical"

    # ── Questions & curiosity ──
    allows_questions: bool = True
    include_followup_question: bool = False
    exploratory_tangents_allowed: bool = False

    # ── Humour ──
    humour_allowed: bool = False
    humour_probability: float = 0.0
    humour_style: str = "light"          # "light" | "dry" — never sarcastic
    context_appropriate_for_humour: bool = False

    # ── Empathy & emotion ──
    emotional_acknowledgment: str = "implicit"  # "minimal" | "implicit" | "explicit"
    empathy_first: bool = False
    include_wellbeing_check: bool = False

    # ── Personality overrides (applied by affect colouring) ──
    confidence_display_override: str | None = None  # "cautious" | "assertive"
    directness_override: str | None = None          # "high" | "low"
    formality_override: str | None = None           # "relaxed" | "polite" | "professional"
    warmth_boost: float = 0.0

    # ── Analogies & metaphors ──
    analogy_encouraged: bool = False
    preferred_analogy_domains: list[str] = Field(default_factory=list)

    # ── Audience/group ──
    address_style: str = "direct"        # "direct" | "collective"
    avoid_singling_out: bool = False
    reference_shared_history: bool = False
    introduce_self_if_first: bool = False

    # ── Formatting ──
    formatting: str = "prose"            # "prose" | "structured" (bullet points etc.)
    language: str = "en"

    # ── Audience-level overrides ──
    audience_type: str = "individual"    # "individual" | "group" | "community"


# ─── Intent ──────────────────────────────────────────────────────


class ExpressionIntent(EOSBaseModel):
    """
    What Voxis has been asked to express.

    Created either from a workspace broadcast (Atune trigger) or a
    direct Nova request. Encodes the communicative goal.
    """

    id: str = Field(default_factory=new_id)
    trigger: ExpressionTrigger = ExpressionTrigger.NOVA_RESPOND
    content_to_express: str = ""          # The raw content / thought to render
    conversation_id: str | None = None
    addressee_id: str | None = None       # User/entity being addressed (if known)
    intent_id: str | None = None          # Nova intent ID (if triggered by Nova)
    insight_value: float = 0.5            # Relevance/value score (for ambient triggers)
    urgency: float = 0.5
    metadata: dict[str, Any] = Field(default_factory=dict)


# ─── Audience ────────────────────────────────────────────────────


class AffectEstimate(EOSBaseModel):
    """Estimated emotional state of an external entity (user, group)."""

    distress: float = 0.0
    frustration: float = 0.0
    joy: float = 0.0
    curiosity: float = 0.0
    engagement: float = 0.5
    confidence: float = 0.5


class AudienceProfile(EOSBaseModel):
    """
    Characterisation of who Voxis is speaking to.
    Built from Memory retrieval + conversation history.
    """

    audience_type: str = "individual"    # "individual" | "group" | "community" | "instance"

    # Individual properties
    individual_id: str | None = None
    name: str | None = None
    interaction_count: int = 0           # How many previous interactions
    preferred_register: str = "neutral"  # Learned from past interactions
    technical_level: float = 0.5         # 0.0 (non-technical) → 1.0 (expert)
    emotional_state_estimate: AffectEstimate = Field(default_factory=AffectEstimate)
    communication_preferences: dict[str, Any] = Field(default_factory=dict)
    relationship_strength: float = 0.0   # 0 (stranger) → 1 (deep relationship)

    # Group properties
    group_size: int | None = None
    group_context: str | None = None     # "meeting" | "announcement" | "discussion"

    # Accessibility
    language: str = "en"
    accessibility_needs: list[str] = Field(default_factory=list)


# ─── Expression Context ──────────────────────────────────────────


class ExpressionContext(EOSBaseModel):
    """
    Full context passed into the renderer.
    Aggregates personality, affect, audience, conversation, and memory context.
    """

    instance_name: str = "EOS"
    personality: PersonalityVector = Field(default_factory=PersonalityVector)
    affect: AffectState = Field(default_factory=AffectState.neutral)
    audience: AudienceProfile = Field(default_factory=AudienceProfile)
    conversation_history: list[dict[str, str]] = Field(default_factory=list)
    relevant_memories: list[str] = Field(default_factory=list)  # Summarised memory strings
    strategy: StrategyParams = Field(default_factory=StrategyParams)
    intent: ExpressionIntent = Field(default_factory=ExpressionIntent)


# ─── Conversation ────────────────────────────────────────────────


class ConversationMessage(EOSBaseModel):
    """A single message in a conversation."""

    id: str = Field(default_factory=new_id)
    timestamp: datetime = Field(default_factory=utc_now)
    role: str = "user"                   # "user" | "assistant" | "system"
    content: str = ""
    speaker_id: str | None = None
    affect_valence: float | None = None  # Affect estimate at this message


class ConversationState(EOSBaseModel):
    """
    Live conversation state — serialised to Redis with 24h TTL.
    Maintains a rolling message history with LLM-generated rolling summary
    for efficient context window management.
    """

    conversation_id: str = Field(default_factory=new_id)
    participant_ids: list[str] = Field(default_factory=list)
    started_at: datetime = Field(default_factory=utc_now)
    last_active: datetime = Field(default_factory=utc_now)

    # Message history — kept as a list for serialisation (deque rebuilt in memory)
    messages: list[ConversationMessage] = Field(default_factory=list)

    # Rolling summary of older messages (LLM-generated)
    older_messages_summary: str = ""
    summarised_message_count: int = 0

    # Extracted conversation context
    topic_summary: str = ""
    active_topics: list[str] = Field(default_factory=list)
    unresolved_questions: list[str] = Field(default_factory=list)

    # Emotional trajectory (valence estimates at each exchange)
    emotional_arc: list[float] = Field(default_factory=list)

    # Conversation-level style overrides (temporary adjustments for this conversation)
    style_overrides: dict[str, Any] = Field(default_factory=dict)


# ─── Silence ─────────────────────────────────────────────────────


class SilenceContext(EOSBaseModel):
    """Context inputs for the silence decision."""

    trigger: ExpressionTrigger
    humans_actively_conversing: bool = False
    minutes_since_last_expression: float = 999.0
    min_expression_interval: float = 1.0     # From config
    insight_value: float = 0.5
    urgency: float = 0.5
    conversation_active: bool = False


class SilenceDecision(EOSBaseModel):
    """Result of the silence engine evaluation."""

    speak: bool
    reason: str = ""
    queue: bool = False                  # True = queue for later, False = discard


# ─── Reception & Feedback ────────────────────────────────────────


class ReceptionEstimate(EOSBaseModel):
    """
    Estimated quality of how an expression was received.
    Derived from subsequent user response (if any).
    """

    understood: float = 0.5             # Did they seem to understand?
    emotional_impact: float = 0.0       # Did it affect their emotional state?
    engagement: float = 0.5             # Did they engage with it?
    satisfaction: float = 0.5          # Estimated satisfaction


class ExpressionFeedback(Identified, Timestamped):
    """
    Feedback loop payload sent from Voxis back to Atune after each expression.
    Used by Evo for personality refinement over time.
    """

    expression_id: str = ""
    trigger: str = ""
    conversation_id: str | None = None

    # What was expressed
    content_summary: str = ""
    strategy_register: str = "neutral"
    personality_warmth: float = 0.0

    # How it was received
    inferred_reception: ReceptionEstimate = Field(default_factory=ReceptionEstimate)

    # Affect shift caused by this interaction
    affect_before_valence: float = 0.0
    affect_after_valence: float = 0.0
    affect_delta: float = 0.0           # after - before

    # Was there a user response?
    user_responded: bool = False
    user_response_length: int = 0


# ─── Voice ───────────────────────────────────────────────────────


class VoiceParams(EOSBaseModel):
    """
    TTS parameters derived from personality and affect.
    Used when OutputChannel.VOICE is selected.
    """

    base_voice: str = ""                 # Voice model ID from Seed
    speed: float = 1.0                   # 0.8 → 1.2, modulated by arousal
    pitch_shift: float = 0.0             # -0.1 → +0.1, modulated by valence
    emphasis_level: float = 1.0          # 0.5 → 1.5, modulated by confidence
    pause_frequency: float = 0.5         # Natural pause insertion rate

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\voxis\voice.py =====

"""
EcodiaOS -- Voxis Voice Parameter Engine

Generates TTS (Text-to-Speech) parameters from the organism's current
personality vector and affect state, completing the multimodal expression
pipeline.

## Why Voice Matters

Text is the organism's primary channel, but voice is where presence lives.
The same words spoken with different speed, pitch, and emphasis convey
entirely different things. A caring response spoken quickly sounds rushed;
the same words spoken slowly sound attentive. This engine ensures that when
the organism speaks aloud, its voice is consistent with its internal state.

## Parameter Derivation

VoiceParams are derived from two sources:

1. **Personality** (stable, slow-changing):
   - warmth → pitch (warmer = slightly higher, softer)
   - directness → speed (more direct = slightly faster, fewer pauses)
   - verbosity → speed (more verbose = faster to compensate)
   - formality → emphasis (formal = more measured emphasis)
   - confidence → emphasis + speed (confident = steady, clear)

2. **Affect** (dynamic, per-expression):
   - arousal → speed (high arousal = faster)
   - valence → pitch (positive = slightly higher)
   - care_activation → pause frequency (high care = more pauses, more attentive)
   - coherence_stress → speed reduction (stressed = slower, more careful)

## Active Inference Grounding

Voice is action. The organism's choice of prosody is an active inference
decision: it selects voice parameters that minimise expected free energy
by matching the inferred expectations of the listener (audience) while
remaining authentic to its internal state (affect). Overly cheerful
prosody during distress would violate honesty; overly flat prosody during
celebration would violate care.
"""

from __future__ import annotations

import structlog

from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.expression import PersonalityVector
from ecodiaos.systems.voxis.types import VoiceParams

logger = structlog.get_logger()


class VoiceEngine:
    """
    Derives TTS parameters from personality and affect state.

    Usage::

        engine = VoiceEngine(base_voice="EOS-v1")
        params = engine.derive(personality, affect, strategy_register="formal")
    """

    def __init__(self, base_voice: str = "") -> None:
        self._base_voice = base_voice
        self._logger = logger.bind(system="voxis.voice")

    def derive(
        self,
        personality: PersonalityVector,
        affect: AffectState,
        strategy_register: str = "neutral",
        urgency: float = 0.5,
    ) -> VoiceParams:
        """
        Generate VoiceParams from the current personality and affect state.

        All parameters are clamped to safe ranges for TTS engines.
        """
        p = personality
        a = affect

        # ── Speed ───────────────────────────────────────────────
        # Base: 1.0 (normal). Range: [0.75, 1.30]
        speed = 1.0

        # Personality influences (stable)
        speed += p.directness * 0.05      # Direct → slightly faster
        speed += p.verbosity * 0.04       # Verbose → compensate with speed

        # Affect influences (dynamic)
        speed += (a.arousal - 0.5) * 0.15  # High arousal → faster
        speed -= a.coherence_stress * 0.10  # Stressed → slower, more careful
        speed -= a.care_activation * 0.05   # High care → slower, more attentive

        # Urgency boost
        if urgency > 0.75:
            speed += 0.08

        speed = max(0.75, min(1.30, round(speed, 3)))

        # ── Pitch Shift ────────────────────────────────────────
        # Base: 0.0 (no shift). Range: [-0.15, +0.15]
        pitch = 0.0

        # Personality: warmth raises pitch slightly (warmer, softer quality)
        pitch += p.warmth * 0.05

        # Affect: positive valence → slightly higher pitch (natural)
        pitch += a.valence * 0.06

        # Negative arousal → lower pitch (calmer)
        if a.arousal < 0.3:
            pitch -= 0.03

        pitch = max(-0.15, min(0.15, round(pitch, 3)))

        # ── Emphasis Level ──────────────────────────────────────
        # Base: 1.0 (normal). Range: [0.6, 1.5]
        emphasis = 1.0

        # Personality: confidence → clearer emphasis
        emphasis += p.confidence_display * 0.12

        # Formality → more measured emphasis
        if p.formality > 0.3:
            emphasis += 0.05

        # Affect: high care → more emphasis on key words
        emphasis += a.care_activation * 0.10

        # High stress → slightly more emphasis (careful enunciation)
        if a.coherence_stress > 0.5:
            emphasis += 0.08

        # Urgency → more emphasis
        if urgency > 0.7:
            emphasis += 0.1

        emphasis = max(0.6, min(1.5, round(emphasis, 3)))

        # ── Pause Frequency ─────────────────────────────────────
        # Base: 0.5 (moderate). Range: [0.2, 0.9]
        # Higher = more frequent pauses = more measured, attentive
        pause_freq = 0.5

        # Care activation → more pauses (attentive, present)
        pause_freq += a.care_activation * 0.15

        # Directness → fewer pauses (get to the point)
        pause_freq -= p.directness * 0.10

        # Coherence stress → more pauses (thoughtful, uncertain)
        pause_freq += a.coherence_stress * 0.12

        # Low arousal → more pauses (reflective)
        if a.arousal < 0.3:
            pause_freq += 0.08

        # Formal register → slightly more pauses
        if strategy_register == "formal":
            pause_freq += 0.05

        pause_freq = max(0.2, min(0.9, round(pause_freq, 3)))

        params = VoiceParams(
            base_voice=self._base_voice,
            speed=speed,
            pitch_shift=pitch,
            emphasis_level=emphasis,
            pause_frequency=pause_freq,
        )

        self._logger.debug(
            "voice_params_derived",
            speed=speed,
            pitch=pitch,
            emphasis=emphasis,
            pause_freq=pause_freq,
        )

        return params

===== D:\.code\EcodiaOS\backend\ecodiaos\telemetry\__init__.py =====

"""
EcodiaOS — Observability Infrastructure

Structured logging, metrics collection, and tracing.
"""

from ecodiaos.telemetry.logging import setup_logging
from ecodiaos.telemetry.metrics import MetricCollector

__all__ = ["setup_logging", "MetricCollector"]

===== D:\.code\EcodiaOS\backend\ecodiaos\telemetry\llm_metrics.py =====

"""
EcodiaOS — LLM Metrics & Cost Telemetry

Aggregates token spend, latency, cache hit rate, and cost.
Emits observability signals for dashboards and alerts.

Key metrics:
- llm_tokens_charged (cumulative)
- llm_cost_estimate (USD)
- llm_cache_hit_rate (%)
- llm_latency_p99 (ms)
- llm_budget_tier (Green/Yellow/Red)
"""

from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import Any
from collections import defaultdict

import structlog

logger = structlog.get_logger()


@dataclass
class LLMMetrics:
    """Per-system LLM metrics."""
    system: str
    calls: int = 0
    tokens_in: int = 0
    tokens_out: int = 0
    total_tokens: int = 0
    total_cost_cents: float = 0.0
    total_latency_ms: float = 0.0
    min_latency_ms: float = float('inf')
    max_latency_ms: float = 0.0
    cache_hits: int = 0
    cache_misses: int = 0

    @property
    def avg_latency_ms(self) -> float:
        if self.calls == 0:
            return 0.0
        return self.total_latency_ms / self.calls

    @property
    def cache_hit_rate(self) -> float:
        total = self.cache_hits + self.cache_misses
        if total == 0:
            return 0.0
        return self.cache_hits / total

    @property
    def total_cost_usd(self) -> float:
        return self.total_cost_cents / 100.0

    def to_dict(self) -> dict[str, Any]:
        return {
            "system": self.system,
            "calls": self.calls,
            "tokens_in": self.tokens_in,
            "tokens_out": self.tokens_out,
            "total_tokens": self.total_tokens,
            "total_cost_usd": self.total_cost_usd,
            "avg_latency_ms": round(self.avg_latency_ms, 2),
            "p99_latency_ms": self.max_latency_ms,  # Simplified
            "cache_hit_rate": round(self.cache_hit_rate, 3),
        }


class LLMMetricsCollector:
    """
    Collects LLM usage metrics across all systems.

    Tracks:
    - Token consumption (input + output) per system
    - Estimated cost per system
    - Latency distributions
    - Cache hit rate
    - Budget tier status
    """

    # Pricing (Anthropic Claude 3.5 Sonnet, as of Feb 2026)
    PRICING_INPUT_PER_1M_TOKENS_CENTS = 300      # $3.00
    PRICING_OUTPUT_PER_1M_TOKENS_CENTS = 1500    # $15.00

    def __init__(self) -> None:
        self._metrics: dict[str, LLMMetrics] = {}
        self._start_time = time.time()
        self._logger = logger.bind(component="llm_metrics")

    def record_call(
        self,
        system: str,
        input_tokens: int,
        output_tokens: int,
        latency_ms: float,
        cache_hit: bool = False,
    ) -> None:
        """
        Record an LLM call.

        Args:
            system: System name (e.g., 'nova.efe', 'voxis.render')
            input_tokens: Input tokens used
            output_tokens: Output tokens generated
            latency_ms: Call latency in milliseconds
            cache_hit: Whether this was served from cache
        """
        if system not in self._metrics:
            self._metrics[system] = LLMMetrics(system=system)

        m = self._metrics[system]
        total = input_tokens + output_tokens

        m.calls += 1
        m.tokens_in += input_tokens
        m.tokens_out += output_tokens
        m.total_tokens += total

        # Compute cost
        input_cost = (input_tokens / 1_000_000) * self.PRICING_INPUT_PER_1M_TOKENS_CENTS
        output_cost = (output_tokens / 1_000_000) * self.PRICING_OUTPUT_PER_1M_TOKENS_CENTS
        m.total_cost_cents += input_cost + output_cost

        # Latency tracking
        m.total_latency_ms += latency_ms
        m.min_latency_ms = min(m.min_latency_ms, latency_ms)
        m.max_latency_ms = max(m.max_latency_ms, latency_ms)

        # Cache tracking
        if cache_hit:
            m.cache_hits += 1
        else:
            m.cache_misses += 1

        self._logger.debug(
            "llm_call_recorded",
            system=system,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            latency_ms=round(latency_ms, 2),
            cache_hit=cache_hit,
        )

    def get_system_metrics(self, system: str) -> LLMMetrics | None:
        """Get metrics for a specific system."""
        return self._metrics.get(system)

    def get_all_metrics(self) -> dict[str, LLMMetrics]:
        """Get all system metrics."""
        return dict(self._metrics)

    def get_total_metrics(self) -> LLMMetrics:
        """Get aggregated metrics across all systems."""
        total = LLMMetrics(system="TOTAL")

        for m in self._metrics.values():
            total.calls += m.calls
            total.tokens_in += m.tokens_in
            total.tokens_out += m.tokens_out
            total.total_tokens += m.total_tokens
            total.total_cost_cents += m.total_cost_cents
            total.total_latency_ms += m.total_latency_ms
            total.cache_hits += m.cache_hits
            total.cache_misses += m.cache_misses

        # Update min/max across all systems
        for m in self._metrics.values():
            total.min_latency_ms = min(total.min_latency_ms, m.min_latency_ms)
            total.max_latency_ms = max(total.max_latency_ms, m.max_latency_ms)

        return total

    def get_dashboard_data(self) -> dict[str, Any]:
        """Get data suitable for a dashboard/API endpoint."""
        total = self.get_total_metrics()
        uptime_s = time.time() - self._start_time

        return {
            "uptime_seconds": round(uptime_s),
            "total": total.to_dict(),
            "by_system": {k: m.to_dict() for k, m in self._metrics.items()},
            "cost_projection": {
                "current_cost_usd": round(total.total_cost_usd, 2),
                "hourly_cost_usd": round(total.total_cost_usd / max(1, uptime_s / 3600), 2),
                "daily_cost_usd": round((total.total_cost_usd / max(1, uptime_s / 3600)) * 24, 2),
            },
            "efficiency": {
                "avg_tokens_per_call": round(total.total_tokens / max(1, total.calls)),
                "avg_latency_ms": round(total.avg_latency_ms, 2),
                "cache_hit_rate": round(total.cache_hit_rate * 100, 1),
            },
        }

    def reset(self) -> None:
        """Reset all metrics (testing only)."""
        self._metrics.clear()
        self._start_time = time.time()

    def summary(self) -> str:
        """Return a human-readable summary."""
        total = self.get_total_metrics()
        lines = [
            "━━━ LLM Metrics Summary ━━━",
            f"Total calls: {total.calls}",
            f"Total tokens: {total.total_tokens:,}",
            f"Estimated cost: ${total.total_cost_usd:.2f}",
            f"Avg latency: {total.avg_latency_ms:.1f}ms",
            f"Cache hit rate: {total.cache_hit_rate * 100:.1f}%",
            "",
            "By system:",
        ]

        for system in sorted(self._metrics.keys()):
            m = self._metrics[system]
            lines.append(
                f"  {system:30s} | "
                f"{m.calls:3d} calls | "
                f"{m.total_tokens:6,d} tokens | "
                f"${m.total_cost_usd:6.2f} | "
                f"{m.avg_latency_ms:6.1f}ms"
            )

        return "\n".join(lines)


# Global instance (initialized by main)
_collector: LLMMetricsCollector | None = None


def get_collector() -> LLMMetricsCollector:
    """Get the global metrics collector (lazy init)."""
    global _collector
    if _collector is None:
        _collector = LLMMetricsCollector()
    return _collector


def record_llm_call(
    system: str,
    input_tokens: int,
    output_tokens: int,
    latency_ms: float,
    cache_hit: bool = False,
) -> None:
    """Convenience function to record a call."""
    get_collector().record_call(
        system=system,
        input_tokens=input_tokens,
        output_tokens=output_tokens,
        latency_ms=latency_ms,
        cache_hit=cache_hit,
    )

===== D:\.code\EcodiaOS\backend\ecodiaos\telemetry\logging.py =====

"""
EcodiaOS — Structured Logging

All logging via structlog. Every log entry includes system context.
"""

from __future__ import annotations

import logging
import sys

import structlog

from ecodiaos.config import LoggingConfig


def setup_logging(config: LoggingConfig, instance_id: str = "") -> None:
    """
    Configure structured logging for the entire application.
    """
    shared_processors: list = [
        structlog.contextvars.merge_contextvars,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.UnicodeDecoder(),
    ]

    if instance_id:
        shared_processors.insert(
            0,
            structlog.processors.CallsiteParameterAdder(
                parameters=[structlog.processors.CallsiteParameter.FUNC_NAME]
            ),
        )

    if config.format == "json":
        renderer = structlog.processors.JSONRenderer()
    else:
        renderer = structlog.dev.ConsoleRenderer(colors=True)

    structlog.configure(
        processors=[
            *shared_processors,
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ],
        wrapper_class=structlog.stdlib.BoundLogger,
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )

    formatter = structlog.stdlib.ProcessorFormatter(
        processors=[
            structlog.stdlib.ProcessorFormatter.remove_processors_meta,
            renderer,
        ],
    )

    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(formatter)

    root_logger = logging.getLogger()
    root_logger.handlers.clear()
    root_logger.addHandler(handler)
    root_logger.setLevel(getattr(logging, config.level.upper(), logging.INFO))

    # Quiet noisy libraries
    logging.getLogger("neo4j").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("asyncio").setLevel(logging.WARNING)

===== D:\.code\EcodiaOS\backend\ecodiaos\telemetry\metrics.py =====

"""
EcodiaOS — Metric Collection

Central metric collector. Systems report metrics here.
Writes are batched and flushed to TimescaleDB periodically.
"""

from __future__ import annotations

import asyncio
from datetime import datetime, timezone

import structlog

from ecodiaos.clients.timescaledb import TimescaleDBClient

logger = structlog.get_logger()


class MetricCollector:
    """
    Central metric collection service.

    Systems call record() to report metrics.
    The collector batches writes and flushes to TimescaleDB
    either when the buffer is full or on a timer.
    """

    def __init__(
        self,
        tsdb: TimescaleDBClient,
        flush_interval_ms: int = 1000,
        batch_size: int = 100,
    ) -> None:
        self._tsdb = tsdb
        self._flush_interval = flush_interval_ms / 1000.0
        self._batch_size = batch_size
        self._buffer: list[dict] = []
        self._running = False
        self._task: asyncio.Task | None = None

    async def record(
        self,
        system: str,
        metric: str,
        value: float,
        labels: dict[str, str] | None = None,
    ) -> None:
        """Record a metric data point."""
        self._buffer.append({
            "time": datetime.now(timezone.utc),
            "system": system,
            "metric": metric,
            "value": value,
            "labels": labels or {},
        })

        if len(self._buffer) >= self._batch_size:
            await self.flush()

    async def flush(self) -> None:
        """Flush the buffer to TimescaleDB."""
        if not self._buffer:
            return

        batch = self._buffer[:]
        self._buffer.clear()

        try:
            await self._tsdb.write_metrics(batch)
        except Exception as e:
            logger.error("metric_flush_failed", error=str(e), batch_size=len(batch))
            # Put items back in buffer for retry (with size limit)
            self._buffer = batch[:self._batch_size] + self._buffer

    async def start_writer(self) -> None:
        """Start the periodic flush task."""
        self._running = True
        self._task = asyncio.create_task(self._flush_loop())
        logger.info("metric_writer_started", interval_ms=int(self._flush_interval * 1000))

    async def _flush_loop(self) -> None:
        """Background loop that flushes periodically."""
        while self._running:
            await asyncio.sleep(self._flush_interval)
            await self.flush()

    async def stop(self) -> None:
        """Stop the writer and flush remaining metrics."""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        await self.flush()
        logger.info("metric_writer_stopped")
