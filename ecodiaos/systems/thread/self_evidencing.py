"""
EcodiaOS — Thread Self-Evidencing Loop

The active inference core of Thread. The organism generates predictions
from its identity schemas about how it SHOULD behave, compares with actual
behaviour, and updates identity or flags dissonance.

This is Friston's self-evidencing at the narrative level: the self is not
a static store of facts but an active inference about the causes of one's
own behaviour patterns.

What makes this genuinely novel: existing active inference implementations
model perceptual inference and action selection. They do not model
*identity inference* — the inference that "I am the kind of entity that..."
This is self-evidencing at a level of abstraction Friston described
theoretically but never operationalized.

Performance:
- Prediction generation: ≤500ms every 100 cycles
- Evidence collection: ≤50ms per episode (embedding comparison, no LLM)
"""

from __future__ import annotations

import structlog

from ecodiaos.systems.thread.identity_schema_engine import cosine_similarity
from ecodiaos.systems.thread.types import (
    Commitment,
    IdentityPrediction,
    IdentitySchema,
    SchemaStrength,
    SelfEvidencingResult,
    ThreadConfig,
)

logger = structlog.get_logger()

# Schema strength → prediction precision
_PRECISION_MAP: dict[SchemaStrength, float] = {
    SchemaStrength.NASCENT: 0.2,
    SchemaStrength.DEVELOPING: 0.4,
    SchemaStrength.ESTABLISHED: 0.6,
    SchemaStrength.CORE: 0.8,
}


class SelfEvidencingLoop:
    """
    Identity-level active inference.

    Generates predictions from schemas and commitments, collects evidence
    from episodes, and computes identity surprise.
    """

    def __init__(self, config: ThreadConfig) -> None:
        self._config = config
        self._logger = logger.bind(system="thread.self_evidencing")
        self._active_predictions: list[IdentityPrediction] = []
        self._cycle_count: int = 0
        self._last_identity_surprise: float = 0.0
        self._recent_context_embedding: list[float] | None = None

    @property
    def active_predictions(self) -> list[IdentityPrediction]:
        return list(self._active_predictions)

    @property
    def last_identity_surprise(self) -> float:
        return self._last_identity_surprise

    def set_context_embedding(self, embedding: list[float]) -> None:
        """Update the recent context embedding for relevance checks."""
        self._recent_context_embedding = embedding

    def generate_identity_predictions(
        self,
        active_schemas: list[IdentitySchema],
        active_commitments: list[Commitment],
    ) -> list[IdentityPrediction]:
        """
        From active schemas and commitments, generate behavioural predictions.

        For each ESTABLISHED or CORE schema:
          - Check if schema is relevant to current context
          - If so, predict the behavioural tendency it would produce
          - Assign precision based on schema strength

        For each ACTIVE commitment:
          - Predict fidelity based on test history

        Performance: ≤500ms (no LLM calls, just embedding comparison).
        """
        predictions: list[IdentityPrediction] = []

        for schema in active_schemas:
            if schema.strength not in (SchemaStrength.ESTABLISHED, SchemaStrength.CORE):
                continue

            # Check relevance to current context
            relevance = 0.5  # Default if no embeddings available
            if self._recent_context_embedding and schema.embedding:
                relevance = cosine_similarity(
                    self._recent_context_embedding,
                    schema.embedding,
                )

            if relevance > self._config.self_evidencing_relevance_threshold:
                predictions.append(IdentityPrediction(
                    schema_id=schema.id,
                    predicted_behavior=schema.behavioral_tendency,
                    predicted_affect=schema.emotional_signature,
                    precision=_PRECISION_MAP.get(schema.strength, 0.5),
                    context_condition=f"relevance={relevance:.2f}",
                ))

        for commitment in active_commitments:
            if commitment.tests_faced >= self._config.commitment_min_tests_for_fidelity:
                predictions.append(IdentityPrediction(
                    commitment_id=commitment.id,
                    predicted_behavior=f"Uphold: {commitment.statement}",
                    precision=commitment.fidelity,
                    context_condition="commitment_active",
                ))

        self._active_predictions = predictions
        self._logger.debug(
            "predictions_generated",
            count=len(predictions),
            from_schemas=sum(1 for p in predictions if p.schema_id),
            from_commitments=sum(1 for p in predictions if p.commitment_id),
        )
        return predictions

    def collect_evidence(
        self,
        episode_id: str,
        episode_embedding: list[float] | None,
        episode_summary: str,
    ) -> SelfEvidencingResult:
        """
        Compare actual behaviour with identity predictions.

        The precision weighting is key (Fristonian): high-confidence
        predictions that are violated generate MORE surprise than
        low-confidence ones. A CORE schema violation is a big deal.
        A NASCENT schema violation is noise.

        Performance: ≤50ms (embedding comparison, no LLM).
        """
        result = SelfEvidencingResult(episode_id=episode_id)

        if not self._active_predictions or episode_embedding is None:
            return result

        surprises: list[float] = []

        for pred in self._active_predictions:
            # Compute similarity between episode and predicted behavior
            # Use the episode embedding as a proxy for actual behavior
            if pred.predicted_behavior and episode_embedding:
                # Simple heuristic: use embedding distance as prediction error
                # In a full implementation, we'd embed the predicted behavior too
                prediction_error = self._estimate_prediction_error(
                    episode_embedding, pred, episode_summary
                )
                weighted_surprise = prediction_error * pred.precision
                surprises.append(weighted_surprise)

                # Classify as confirmation or challenge
                if prediction_error < 0.4:
                    if pred.schema_id:
                        result.schemas_confirmed.append(pred.schema_id)
                elif prediction_error > 0.6:
                    if pred.schema_id:
                        result.schemas_challenged.append(pred.schema_id)
                    if pred.commitment_id:
                        result.commitments_tested.append(pred.commitment_id)

        if surprises:
            result.predictions_evaluated = len(surprises)
            result.mean_prediction_error = sum(surprises) / len(surprises)
            result.identity_surprise = result.mean_prediction_error

        self._last_identity_surprise = result.identity_surprise

        if result.identity_surprise > self._config.identity_surprise_significant:
            self._logger.info(
                "identity_surprise_elevated",
                surprise=round(result.identity_surprise, 4),
                schemas_confirmed=len(result.schemas_confirmed),
                schemas_challenged=len(result.schemas_challenged),
                commitments_tested=len(result.commitments_tested),
            )

        return result

    def classify_surprise(self, result: SelfEvidencingResult) -> str:
        """
        Classify the identity surprise level for routing decisions.

        Returns: "stable" | "mild" | "significant" | "crisis"
        """
        cfg = self._config
        surprise = result.identity_surprise

        if surprise < cfg.identity_surprise_mild:
            return "stable"
        elif surprise < cfg.identity_surprise_significant:
            return "mild"
        elif surprise < cfg.identity_surprise_crisis:
            return "significant"
        else:
            return "crisis"

    def _estimate_prediction_error(
        self,
        episode_embedding: list[float],
        prediction: IdentityPrediction,
        episode_summary: str,
    ) -> float:
        """
        Estimate how much the actual episode deviates from the prediction.

        Returns 0.0 (perfectly consistent) to 1.0 (total surprise).

        Uses a heuristic: keyword overlap between prediction and episode summary,
        combined with any available embedding comparison.
        """
        # Simple keyword-based heuristic for prediction error
        pred_words = set(prediction.predicted_behavior.lower().split())
        episode_words = set(episode_summary.lower().split())

        if not pred_words or not episode_words:
            return 0.5  # Unknown — moderate surprise

        # Jaccard similarity as a proxy for behavioral consistency
        overlap = len(pred_words & episode_words)
        union = len(pred_words | episode_words)
        keyword_similarity = overlap / max(1, union)

        # Invert to get prediction error
        prediction_error = 1.0 - keyword_similarity

        # Clamp to reasonable range — pure keyword match is noisy
        return max(0.1, min(0.9, prediction_error))

    def tick(self) -> bool:
        """
        Called every cognitive cycle. Returns True if it's time
        to regenerate predictions.
        """
        self._cycle_count += 1
        return bool(self._cycle_count % self._config.self_evidencing_interval_cycles == 0)
