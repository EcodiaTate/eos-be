
===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\__init__.py =====

"""
EcodiaOS — Simula: Self-Evolution System

The organism's capacity for metamorphosis. Where Evo adjusts the knobs,
Simula redesigns the dashboard.

Public API:
  SimulaService              — main service, wired in main.py
  EvoSimulaBridge            — translates Evo proposals to Simula format
  EvolutionAnalyticsEngine   — evolution quality tracking
  ProposalIntelligence       — dedup, prioritize, dependency analysis
  SimulaCodeAgent            — agentic code generation engine
  EvolutionHistoryManager    — immutable evolution history in Neo4j
  EvolutionProposal          — submitted by Evo when a hypothesis reaches SUPPORTED
  ProposalResult             — outcome of process_proposal()
  CodeChangeResult           — output of the code agent
  ChangeCategory             — taxonomy of allowed (and forbidden) change types
  ChangeSpec                 — formal specification of what to change
  EnrichedSimulationResult   — deep multi-strategy simulation output

Stage 1 enhancements:
  1A: Extended-thinking model routing for governance/high-risk proposals
  1B: Voyage-code-3 embeddings for semantic dedup + find_similar + Neo4j vector index
  1C: KVzip-inspired context compression for agentic tool loops

Stage 2 enhancements (Formal Verification Core):
  2A: Dafny proof-carrying code with Clover pattern
  2B: LLM + Z3 invariant discovery loop
  2C: Static analysis gates (Bandit / Semgrep)
  2D: AgentCoder pattern — test/code separation pipeline

Stage 3 enhancements (Incremental & Learning):
  3A: Salsa incremental verification — dependency-aware memoization
  3B: SWE-grep agentic retrieval — multi-hop code search
  3C: LILO library learning — abstraction extraction from successful proposals

Hunter — Zero-Day Discovery Engine:
  TargetWorkspace      — workspace abstraction (internal/external)
  AttackSurface        — discovered entry point
  VulnerabilityReport  — proven vulnerability + PoC
  HuntResult           — aggregated hunt results
  HunterConfig         — authorization and resource limits
"""

# Stage 2D: AgentCoder agents
from ecodiaos.systems.simula.agents.test_designer import TestDesignerAgent
from ecodiaos.systems.simula.agents.test_executor import TestExecutorAgent
from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine
from ecodiaos.systems.simula.bridge import EvoSimulaBridge
from ecodiaos.systems.simula.code_agent import SimulaCodeAgent
from ecodiaos.systems.simula.history import EvolutionHistoryManager

# Hunter: Zero-Day Discovery Engine
from ecodiaos.systems.simula.hunter import (
    AttackSurface,
    AttackSurfaceType,
    HunterConfig,
    HuntResult,
    TargetType,
    TargetWorkspace,
    VulnerabilityReport,
    VulnerabilitySeverity,
)
from ecodiaos.systems.simula.learning.lilo import LiloLibraryEngine
from ecodiaos.systems.simula.proposal_intelligence import ProposalIntelligence
from ecodiaos.systems.simula.retrieval.swe_grep import SweGrepRetriever
from ecodiaos.systems.simula.service import SimulaService
from ecodiaos.systems.simula.types import (
    FORBIDDEN,
    GOVERNANCE_REQUIRED,
    SELF_APPLICABLE,
    SIMULA_IRON_RULES,
    CategorySuccessRate,
    CautionAdjustment,
    ChangeCategory,
    ChangeSpec,
    CodeChangeResult,
    ConfigVersion,
    CounterfactualResult,
    DependencyImpact,
    EnrichedSimulationResult,
    EvolutionAnalytics,
    EvolutionProposal,
    EvolutionRecord,
    EvoProposalEnriched,
    ProposalCluster,
    ProposalPriority,
    ProposalResult,
    ProposalStatus,
    ResourceCostEstimate,
    RiskLevel,
    SimulationResult,
    TriageResult,
    TriageStatus,
)

# Stage 2: Verification bridges
from ecodiaos.systems.simula.verification.dafny_bridge import DafnyBridge

# Stage 3: Engines
from ecodiaos.systems.simula.verification.incremental import IncrementalVerificationEngine
from ecodiaos.systems.simula.verification.static_analysis import StaticAnalysisBridge

# Stages 2 + 3: Verification types
from ecodiaos.systems.simula.verification.types import (
    DAFNY_TRIGGERABLE_CATEGORIES,
    AbstractionExtractionResult,
    # Stage 3C: LILO Library Learning
    AbstractionKind,
    # Stage 2A: Dafny
    AgentCoderIterationResult,
    AgentCoderResult,
    CachedVerificationResult,
    CloverRoundResult,
    DafnyVerificationResult,
    DafnyVerificationStatus,
    DiscoveredInvariant,
    FormalVerificationResult,
    FunctionSignature,
    IncrementalVerificationResult,
    InvariantKind,
    InvariantVerificationResult,
    InvariantVerificationStatus,
    LibraryAbstraction,
    LibraryStats,
    RetrievalHop,
    # Stage 3B: SWE-grep Retrieval
    RetrievalToolKind,
    RetrievedContext,
    StaticAnalysisFinding,
    StaticAnalysisResult,
    StaticAnalysisSeverity,
    SweGrepResult,
    TestDesignResult,
    TestExecutionResult,
    # Stage 3A: Incremental Verification
    VerificationCacheStatus,
    VerificationCacheTier,
)
from ecodiaos.systems.simula.verification.z3_bridge import Z3Bridge

__all__ = [
    # Services
    "SimulaService",
    "SimulaCodeAgent",
    "EvolutionHistoryManager",
    "EvoSimulaBridge",
    "EvolutionAnalyticsEngine",
    "ProposalIntelligence",
    # Core types
    "ChangeCategory",
    "ChangeSpec",
    "CodeChangeResult",
    "ConfigVersion",
    "EvolutionProposal",
    "EvolutionRecord",
    "ProposalResult",
    "ProposalStatus",
    "RiskLevel",
    "SimulationResult",
    # Enriched types
    "EnrichedSimulationResult",
    "CautionAdjustment",
    "CounterfactualResult",
    "DependencyImpact",
    "ResourceCostEstimate",
    "EvoProposalEnriched",
    "ProposalPriority",
    "ProposalCluster",
    "CategorySuccessRate",
    "EvolutionAnalytics",
    "TriageStatus",
    "TriageResult",
    # Constants
    "FORBIDDEN",
    "GOVERNANCE_REQUIRED",
    "SELF_APPLICABLE",
    "SIMULA_IRON_RULES",
    # Stage 2: Verification types
    "DafnyVerificationStatus",
    "CloverRoundResult",
    "DafnyVerificationResult",
    "InvariantKind",
    "InvariantVerificationStatus",
    "DiscoveredInvariant",
    "InvariantVerificationResult",
    "StaticAnalysisSeverity",
    "StaticAnalysisFinding",
    "StaticAnalysisResult",
    "TestDesignResult",
    "TestExecutionResult",
    "AgentCoderIterationResult",
    "AgentCoderResult",
    "FormalVerificationResult",
    "DAFNY_TRIGGERABLE_CATEGORIES",
    # Stage 2: Bridges
    "DafnyBridge",
    "Z3Bridge",
    "StaticAnalysisBridge",
    # Stage 2D: Agents
    "TestDesignerAgent",
    "TestExecutorAgent",
    # Stage 3A: Incremental Verification
    "VerificationCacheStatus",
    "VerificationCacheTier",
    "FunctionSignature",
    "CachedVerificationResult",
    "IncrementalVerificationResult",
    "IncrementalVerificationEngine",
    # Stage 3B: SWE-grep Retrieval
    "RetrievalToolKind",
    "RetrievalHop",
    "RetrievedContext",
    "SweGrepResult",
    "SweGrepRetriever",
    # Stage 3C: LILO Library Learning
    "AbstractionKind",
    "LibraryAbstraction",
    "AbstractionExtractionResult",
    "LibraryStats",
    "LiloLibraryEngine",
    # Hunter: Zero-Day Discovery Engine
    "TargetWorkspace",
    "TargetType",
    "AttackSurface",
    "AttackSurfaceType",
    "VulnerabilityReport",
    "VulnerabilitySeverity",
    "HuntResult",
    "HunterConfig",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\analytics.py =====

"""
EcodiaOS -- Simula Evolution Analytics Engine

Tracks evolution quality metrics over time, enabling Simula to learn
from its own history. All analytics are computed from Neo4j evolution
records -- zero LLM tokens required.

Key metrics:
  - Per-category success/rollback rates
  - Evolution velocity (proposals per day)
  - Rollback pattern analysis (which categories fail most, why)
  - Dynamic caution adjustment (increase risk thresholds for
    categories with high recent rollback rates)

Phase 9 addition:
  - Hunter security analytics integration (vulnerability discovery
    metrics surfaced alongside evolution metrics for unified observability)

Used by:
  - ChangeSimulator: dynamic risk threshold adjustment
  - SimulaService: enhanced stats reporting
  - ProposalIntelligence: cost/risk estimation
  - HunterService: unified analytics surface (Phase 9)
"""

from __future__ import annotations

from datetime import datetime, timedelta
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.simula.types import (
    CategorySuccessRate,
    CautionAdjustment,
    ChangeCategory,
    EvolutionAnalytics,
    RiskLevel,
)

if TYPE_CHECKING:
    from ecodiaos.systems.simula.history import EvolutionHistoryManager
    from ecodiaos.systems.simula.hunter.analytics import (
        HunterAnalyticsStore,
        HunterAnalyticsView,
    )

logger = structlog.get_logger().bind(system="simula.analytics")

# Rollback rate above this threshold triggers increased caution
_CAUTION_THRESHOLD: float = 0.30

# Window for "recent" rollback rate calculation
_RECENT_WINDOW_DAYS: int = 7

# Risk level to numeric mapping for mean calculation
_RISK_LEVEL_NUMERIC: dict[RiskLevel, float] = {
    RiskLevel.LOW: 0.1,
    RiskLevel.MODERATE: 0.4,
    RiskLevel.HIGH: 0.7,
    RiskLevel.UNACCEPTABLE: 1.0,
}


class EvolutionAnalyticsEngine:
    """
    Tracks evolution quality metrics over time.
    Enables Simula to learn from its own history and dynamically
    adjust risk thresholds based on past performance.

    All computation is from Neo4j records -- no LLM tokens consumed.
    """

    def __init__(
        self,
        history: EvolutionHistoryManager | None = None,
        *,
        hunter_view: HunterAnalyticsView | None = None,
        hunter_store: HunterAnalyticsStore | None = None,
    ) -> None:
        self._history = history
        self._log = logger
        self._cached_analytics: EvolutionAnalytics | None = None
        self._cache_ttl_seconds: int = 300  # 5 minutes
        self._last_computed: datetime | None = None

        # Phase 9: Hunter analytics integration
        self._hunter_view = hunter_view
        self._hunter_store = hunter_store

    async def compute_analytics(self) -> EvolutionAnalytics:
        """
        Compute current analytics from the full evolution history.
        Results are cached for 5 minutes to avoid repeated Neo4j queries.
        """
        now = utc_now()
        if (
            self._cached_analytics is not None
            and self._last_computed is not None
            and (now - self._last_computed).total_seconds() < self._cache_ttl_seconds
        ):
            return self._cached_analytics

        if self._history is None:
            return EvolutionAnalytics()

        records = await self._history.get_history(limit=500)

        if not records:
            analytics = EvolutionAnalytics(last_updated=now)
            self._cached_analytics = analytics
            self._last_computed = now
            return analytics

        # Per-category rates
        category_rates: dict[str, CategorySuccessRate] = {}
        total_risk_numeric: float = 0.0
        risk_count: int = 0

        for record in records:
            cat_key = record.category.value
            if cat_key not in category_rates:
                category_rates[cat_key] = CategorySuccessRate(category=record.category)

            rate = category_rates[cat_key]
            rate.total += 1

            if record.rolled_back:
                rate.rolled_back += 1
            else:
                rate.approved += 1

            total_risk_numeric += _RISK_LEVEL_NUMERIC.get(record.simulation_risk, 0.4)
            risk_count += 1

        # Evolution velocity: proposals per day over the record span
        velocity = 0.0
        if len(records) >= 2:
            newest = records[0].created_at
            oldest = records[-1].created_at
            span_days = max(1.0, (newest - oldest).total_seconds() / 86400.0)
            velocity = len(records) / span_days

        # Aggregate rollback rate
        total_rolled_back = sum(r.rolled_back for r in category_rates.values())
        total_proposals = sum(r.total for r in category_rates.values())
        rollback_rate = total_rolled_back / max(1, total_proposals)

        # Mean simulation risk
        mean_risk = total_risk_numeric / max(1, risk_count)

        # Compute recent rollback rates (7-day window) per category
        recent_rollback_rates: dict[str, float] = {}
        cutoff = utc_now() - timedelta(days=_RECENT_WINDOW_DAYS)
        for cat in ChangeCategory:
            recent_records = [
                r for r in records
                if r.category == cat and r.created_at >= cutoff
            ]
            if recent_records:
                recent_rolled_back = sum(1 for r in recent_records if r.rolled_back)
                recent_rollback_rates[cat.value] = round(
                    recent_rolled_back / len(recent_records), 3
                )

        analytics = EvolutionAnalytics(
            category_rates=category_rates,
            total_proposals=total_proposals,
            evolution_velocity=round(velocity, 3),
            mean_simulation_risk=round(mean_risk, 3),
            rollback_rate=round(rollback_rate, 3),
            recent_rollback_rates=recent_rollback_rates,
            last_updated=now,
        )

        self._cached_analytics = analytics
        self._last_computed = now
        self._log.info(
            "analytics_computed",
            total_proposals=total_proposals,
            velocity=analytics.evolution_velocity,
            rollback_rate=analytics.rollback_rate,
            categories=len(category_rates),
        )
        return analytics

    async def get_category_success_rate(self, category: ChangeCategory) -> float:
        """
        Success rate for a specific change category.
        Used by ChangeSimulator for dynamic risk weighting.
        Returns 0.5 (neutral) if no history exists for this category.
        """
        analytics = await self.compute_analytics()
        rate = analytics.category_rates.get(category.value)
        if rate is None or rate.total == 0:
            return 0.5  # no data -- assume neutral
        return rate.success_rate

    async def get_recent_rollback_rate(self, category: ChangeCategory) -> float:
        """
        Rollback rate for a category within the recent window (7 days).
        More responsive to recent trends than the all-time rate.
        """
        if self._history is None:
            return 0.0

        records = await self._history.get_history(limit=200)
        cutoff = utc_now() - timedelta(days=_RECENT_WINDOW_DAYS)

        recent = [
            r for r in records
            if r.category == category and r.created_at >= cutoff
        ]

        if not recent:
            return 0.0

        rolled_back = sum(1 for r in recent if r.rolled_back)
        return rolled_back / len(recent)

    async def get_rollback_patterns(self) -> list[dict[str, Any]]:
        """
        Analyze rollback history for actionable patterns:
        - Which categories roll back most often
        - Common rollback reasons
        - Trend direction (getting better or worse)
        """
        analytics = await self.compute_analytics()
        patterns: list[dict[str, Any]] = []

        for cat_key, rate in analytics.category_rates.items():
            if rate.rolled_back == 0:
                continue
            patterns.append({
                "category": cat_key,
                "rollback_rate": round(rate.rollback_rate, 3),
                "total": rate.total,
                "rolled_back": rate.rolled_back,
                "severity": "high" if rate.rollback_rate > _CAUTION_THRESHOLD else "normal",
            })

        # Sort by rollback rate descending
        patterns.sort(key=lambda p: p["rollback_rate"], reverse=True)
        return patterns

    def should_increase_caution(self, category: ChangeCategory) -> CautionAdjustment:
        """
        Transparent caution adjustment analysis using cached analytics.
        Evaluates multiple factors to determine if simulation should use
        stricter risk thresholds for this category.

        Factors considered:
        - All-time rollback rate (indicates systemic issues)
        - Recent 7-day rollback rate (responsive to recent trends)
        - Data sufficiency (at least 3 proposals needed)

        Returns a CautionAdjustment with full reasoning for observability.
        """
        if self._cached_analytics is None:
            return CautionAdjustment(
                should_adjust=False,
                magnitude=0.0,
                factors={},
                reasoning="No cached analytics available",
            )

        rate = self._cached_analytics.category_rates.get(category.value)
        if rate is None or rate.total < 3:
            return CautionAdjustment(
                should_adjust=False,
                magnitude=0.0,
                factors={},
                reasoning="Insufficient data (< 3 proposals)",
            )

        factors: dict[str, float] = {}

        # Factor 1: All-time rollback rate
        if rate.rollback_rate > _CAUTION_THRESHOLD:
            factors["high_alltime_rollback_rate"] = min(
                0.25, rate.rollback_rate * 0.5
            )

        # Factor 2: Recent 7-day rollback rate
        recent_rate = self._cached_analytics.recent_rollback_rates.get(category.value, 0.0)
        if recent_rate > 0.25:
            factors["high_recent_rollback_rate"] = min(0.20, recent_rate * 0.4)

        total_adjustment = sum(factors.values())

        reasoning_parts = []
        if factors:
            reasoning_parts.append(
                f"Category {category.value}: "
                + ", ".join(f"{k}={v:.2f}" for k, v in factors.items())
            )
        reasoning_parts.append(
            f"All-time: {rate.rollback_rate:.1%}, "
            f"Recent (7d): {recent_rate:.1%}, "
            f"Total: {rate.total} proposals"
        )

        return CautionAdjustment(
            should_adjust=total_adjustment > 0.0,
            magnitude=min(0.5, total_adjustment),
            factors=factors,
            reasoning=" | ".join(reasoning_parts),
        )

    # ── Phase 9: Hunter Integration ──────────────────────────────────────────

    def set_hunter_view(self, view: HunterAnalyticsView) -> None:
        """Attach a Hunter analytics view for unified querying."""
        self._hunter_view = view

    def set_hunter_store(self, store: HunterAnalyticsStore) -> None:
        """Attach a Hunter analytics store for durable historical queries."""
        self._hunter_store = store

    def get_hunter_summary(self) -> dict[str, Any]:
        """
        Return in-memory Hunter analytics summary.

        Provides: total vulnerabilities, severity distribution, most common
        classes, patch success rate, weekly trends, rolling windows, and
        throughput metrics.

        Returns empty dict if Hunter analytics view is not attached.
        """
        if self._hunter_view is None:
            return {}
        return self._hunter_view.summary

    async def get_hunter_weekly_trends(
        self,
        *,
        weeks: int = 12,
        target_url: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Query weekly vulnerability trends from TimescaleDB.

        Falls back to in-memory view if store is unavailable.

        Args:
            weeks: Number of weeks to look back.
            target_url: Optional filter by target repository.

        Returns:
            List of weekly buckets with vulnerability counts + severity breakdown.
        """
        if self._hunter_store is not None:
            try:
                return await self._hunter_store.get_vulnerabilities_per_week(
                    weeks=weeks, target_url=target_url,
                )
            except Exception as exc:
                self._log.warning(
                    "hunter_store_query_failed",
                    query="weekly_trends",
                    error=str(exc),
                )

        # Fallback to in-memory view
        if self._hunter_view is not None:
            if target_url:
                return self._hunter_view.get_target_weekly_trend(target_url)
            trends = self._hunter_view.summary.get("weekly_trends", [])
            return trends[-weeks:] if isinstance(trends, list) else []

        return []

    async def get_hunter_severity_distribution(
        self,
        *,
        days: int = 30,
        target_url: str | None = None,
    ) -> dict[str, int]:
        """
        Query severity distribution from TimescaleDB over a rolling window.

        Falls back to in-memory view (all-time) if store is unavailable.
        """
        if self._hunter_store is not None:
            try:
                return await self._hunter_store.get_severity_distribution(
                    days=days, target_url=target_url,
                )
            except Exception as exc:
                self._log.warning(
                    "hunter_store_query_failed",
                    query="severity_distribution",
                    error=str(exc),
                )

        if self._hunter_view is not None:
            dist: dict[str, int] = self._hunter_view.summary.get("severity_distribution", {})
            return dist

        return {}

    async def get_hunter_error_summary(self, *, days: int = 7) -> list[dict[str, Any]]:
        """
        Query aggregated pipeline errors from TimescaleDB.

        Only available when a Hunter analytics store is attached.
        """
        if self._hunter_store is not None:
            try:
                return await self._hunter_store.get_error_summary(days=days)
            except Exception as exc:
                self._log.warning(
                    "hunter_store_query_failed",
                    query="error_summary",
                    error=str(exc),
                )
        return []

    async def get_unified_analytics(self) -> dict[str, Any]:
        """
        Return a unified analytics payload combining evolution metrics
        and Hunter security metrics for comprehensive observability.

        This is the single entry point for dashboard consumers that want
        the complete system health picture.
        """
        evolution = await self.compute_analytics()

        result: dict[str, Any] = {
            "evolution": {
                "total_proposals": evolution.total_proposals,
                "evolution_velocity": evolution.evolution_velocity,
                "rollback_rate": evolution.rollback_rate,
                "mean_simulation_risk": evolution.mean_simulation_risk,
                "category_count": len(evolution.category_rates),
                "last_updated": (
                    evolution.last_updated.isoformat()
                    if evolution.last_updated else None
                ),
            },
        }

        # Hunter security analytics
        hunter_summary = self.get_hunter_summary()
        if hunter_summary:
            result["hunter"] = {
                "total_vulnerabilities": hunter_summary.get("total_vulnerabilities", 0),
                "total_hunts": hunter_summary.get("total_hunts", 0),
                "severity_distribution": hunter_summary.get("severity_distribution", {}),
                "patch_success_rate": hunter_summary.get("patch_success_rate", 0),
                "avg_vulns_per_hunt": hunter_summary.get("avg_vulns_per_hunt", 0),
                "rolling_7d": hunter_summary.get("rolling_7d", {}),
                "rolling_30d": hunter_summary.get("rolling_30d", {}),
            }
        else:
            result["hunter"] = None

        return result

    def invalidate_cache(self) -> None:
        """Force recomputation on next analytics request."""
        self._cached_analytics = None
        self._last_computed = None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\applicator.py =====

"""
EcodiaOS — Simula Change Applicator

Routes approved evolution proposals to the appropriate application
strategy and coordinates with RollbackManager for safety.

Application strategies by category:

  ADJUST_BUDGET → direct config update (no code generation needed)
  ADD_EXECUTOR, ADD_INPUT_CHANNEL, ADD_PATTERN_DETECTOR → SimulaCodeAgent
  MODIFY_CONTRACT, ADD_SYSTEM_CAPABILITY, etc. → SimulaCodeAgent (post governance)

All strategies:
  1. Snapshot affected files via RollbackManager
  2. Apply change
  3. On failure → rollback immediately
  4. On success → return CodeChangeResult + snapshot (for caller health check)
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog
import yaml

from ecodiaos.systems.simula.types import (
    ChangeCategory,
    CodeChangeResult,
    ConfigSnapshot,
    EvolutionProposal,
)

if TYPE_CHECKING:
    from pathlib import Path

    from ecodiaos.systems.simula.agents.test_designer import TestDesignerAgent
    from ecodiaos.systems.simula.agents.test_executor import TestExecutorAgent
    from ecodiaos.systems.simula.code_agent import SimulaCodeAgent
    from ecodiaos.systems.simula.health import HealthChecker
    from ecodiaos.systems.simula.rollback import RollbackManager
    from ecodiaos.systems.simula.verification.static_analysis import StaticAnalysisBridge

logger = structlog.get_logger()


class ApplicationError(RuntimeError):
    """Raised when a change application fails unrecoverably."""


class ChangeApplicator:
    """
    Routes approved evolution proposals to the right application strategy.

    For code-level changes: delegates to SimulaCodeAgent.
    For budget changes: updates the YAML config directly.
    Always snapshots before applying, so rollback is always possible.
    """

    def __init__(
        self,
        code_agent: SimulaCodeAgent,
        rollback_manager: RollbackManager,
        health_checker: HealthChecker,
        codebase_root: Path,
        # Stage 2D: AgentCoder pipeline
        test_designer: TestDesignerAgent | None = None,
        test_executor: TestExecutorAgent | None = None,
        static_analysis_bridge: StaticAnalysisBridge | None = None,
        agent_coder_enabled: bool = False,
        agent_coder_max_iterations: int = 3,
    ) -> None:
        self._agent = code_agent
        self._rollback = rollback_manager
        self._health = health_checker
        self._root = codebase_root
        self._test_designer = test_designer
        self._test_executor = test_executor
        self._static_bridge = static_analysis_bridge
        self._agent_coder_enabled = agent_coder_enabled
        self._agent_coder_max_iterations = agent_coder_max_iterations
        self._logger = logger.bind(system="simula.applicator")

    async def apply(
        self, proposal: EvolutionProposal
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """
        Apply an evolution proposal. Returns (result, snapshot).

        The snapshot is needed by SimulaService for rollback if the
        post-application health check fails.

        Routes through the AgentCoder 3-agent pipeline when enabled,
        otherwise uses the standard code agent.
        """
        self._logger.info(
            "applying_change",
            proposal_id=proposal.id,
            category=proposal.category.value,
        )

        if proposal.category == ChangeCategory.ADJUST_BUDGET:
            return await self._apply_budget(proposal)
        elif self._agent_coder_enabled and self._test_designer and self._test_executor:
            return await self._apply_via_agent_coder(proposal)
        else:
            return await self._apply_via_code_agent(proposal)

    # ── Budget Adjustment (direct config update) ──────────────────────────────

    async def _apply_budget(
        self, proposal: EvolutionProposal
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """Direct config update for budget changes — no code generation."""
        spec = proposal.change_spec
        if not spec.budget_parameter or spec.budget_new_value is None:
            result = CodeChangeResult(
                success=False,
                error="Budget change spec missing parameter or new_value",
            )
            return result, ConfigSnapshot(
                proposal_id=proposal.id,
                config_version=0,
            )

        config_path = self._root / "config" / "default.yaml"
        snapshot = await self._rollback.snapshot(
            proposal_id=proposal.id,
            paths=[config_path],
        )

        try:
            data: dict[str, Any] = {}
            if config_path.exists():
                with open(config_path) as f:
                    data = yaml.safe_load(f) or {}

            # Navigate the dotted parameter path (e.g. "nova.efe.pragmatic")
            parts = spec.budget_parameter.split(".")
            node = data
            for part in parts[:-1]:
                node = node.setdefault(part, {})
            node[parts[-1]] = spec.budget_new_value

            with open(config_path, "w") as f:
                yaml.dump(data, f, default_flow_style=False)

            rel_path = str(config_path.relative_to(self._root))
            self._logger.info(
                "budget_updated",
                parameter=spec.budget_parameter,
                old_value=spec.budget_old_value,
                new_value=spec.budget_new_value,
            )
            return CodeChangeResult(
                success=True,
                files_written=[rel_path],
                summary=(
                    f"Updated {spec.budget_parameter} "
                    f"from {spec.budget_old_value} to {spec.budget_new_value}"
                ),
            ), snapshot

        except Exception as exc:
            await self._rollback.restore(snapshot)
            return CodeChangeResult(
                success=False,
                error=f"Budget update failed: {exc}",
            ), snapshot

    # ── Code Agent Application ────────────────────────────────────────────────

    async def _apply_via_code_agent(
        self, proposal: EvolutionProposal
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """Use SimulaCodeAgent to generate and write the implementation."""
        affected_dirs = _infer_affected_paths(proposal, self._root)
        snapshot = await self._rollback.snapshot(
            proposal_id=proposal.id,
            paths=affected_dirs,
        )

        result = await self._agent.implement(proposal)

        if not result.success:
            self._logger.warning(
                "code_agent_failed",
                proposal_id=proposal.id,
                error=result.error,
            )
            await self._rollback.restore(snapshot)

        return result, snapshot

    # ── Stage 2D: AgentCoder 3-Agent Pipeline ─────────────────────────────────

    async def _apply_via_agent_coder(
        self, proposal: EvolutionProposal,
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """
        Apply via the AgentCoder pipeline:
          1. TestDesigner generates tests from proposal spec (no code seen)
          2. CodeAgent implements the change (with test writing disabled)
          3. TestExecutor runs the designed tests against the implementation
          4. If failures → feed back to CodeAgent → iterate

        This adversarial separation produces higher-quality code by testing
        the specification rather than the implementation.
        """
        from ecodiaos.systems.simula.agents.test_executor import TestExecutorAgent
        from ecodiaos.systems.simula.verification.types import (
            AgentCoderIterationResult,
            AgentCoderResult,
        )

        assert self._test_designer is not None
        assert self._test_executor is not None

        affected_dirs = _infer_affected_paths(proposal, self._root)
        snapshot = await self._rollback.snapshot(
            proposal_id=proposal.id,
            paths=affected_dirs,
        )

        log = self._logger.bind(
            proposal_id=proposal.id,
            pipeline="agent_coder",
        )

        # Step 1: TestDesigner generates tests
        log.info("agent_coder_designing_tests")
        test_design = await self._test_designer.design_tests(proposal)

        if not test_design.test_files:
            log.warning("agent_coder_no_tests_designed")
            # Fall back to standard code agent
            return await self._apply_via_code_agent(proposal)

        log.info(
            "agent_coder_tests_designed",
            test_files=len(test_design.test_files),
            test_count=test_design.test_count,
        )

        iterations: list[AgentCoderIterationResult] = []
        final_result: CodeChangeResult | None = None

        for iteration_num in range(1, self._agent_coder_max_iterations + 1):
            log.info("agent_coder_iteration", iteration=iteration_num)

            # Step 2: CodeAgent implements (test writing disabled)
            code_result = await self._agent.implement(
                proposal, skip_test_writing=True,
            )
            final_result = code_result

            if not code_result.success:
                log.warning(
                    "agent_coder_code_failed",
                    iteration=iteration_num,
                    error=code_result.error,
                )
                iterations.append(AgentCoderIterationResult(
                    iteration=iteration_num,
                    test_design=test_design if iteration_num == 1 else None,
                    code_generation_success=False,
                    code_generation_files=code_result.files_written,
                ))
                break

            # Step 3: TestExecutor runs tests
            test_result = await self._test_executor.execute_tests(
                test_design.test_files,
            )

            iter_result = AgentCoderIterationResult(
                iteration=iteration_num,
                test_design=test_design if iteration_num == 1 else None,
                code_generation_success=True,
                code_generation_files=code_result.files_written,
                test_execution=test_result,
                all_tests_passed=(
                    test_result.failed == 0 and test_result.errors == 0
                ),
            )
            iterations.append(iter_result)

            log.info(
                "agent_coder_test_results",
                iteration=iteration_num,
                passed=test_result.passed,
                failed=test_result.failed,
                errors=test_result.errors,
            )

            if iter_result.all_tests_passed:
                log.info("agent_coder_converged", iterations=iteration_num)
                break

            # Step 4: Not all tests passed — feed failures back
            if iteration_num < self._agent_coder_max_iterations:
                feedback = TestExecutorAgent.format_failures_for_feedback(
                    test_result,
                )
                log.info("agent_coder_feeding_back", feedback_len=len(feedback))
                # Attach feedback to proposal for next iteration
                proposal._agent_coder_feedback = feedback  # type: ignore[attr-defined]

        # Compute aggregate result
        converged = bool(iterations and iterations[-1].all_tests_passed)
        final_pass_rate = 0.0
        if iterations and iterations[-1].test_execution:
            te = iterations[-1].test_execution
            if te.total > 0:
                final_pass_rate = te.passed / te.total

        AgentCoderResult(
            iterations=iterations,
            total_iterations=len(iterations),
            final_pass_rate=final_pass_rate,
            converged=converged,
        )

        # Attach to code result for downstream recording
        if final_result is not None:
            final_result.agent_coder_iterations = len(iterations)
            final_result.test_designer_test_count = test_design.test_count

        if final_result is None or not final_result.success:
            await self._rollback.restore(snapshot)

        return final_result or CodeChangeResult(
            success=False, error="AgentCoder pipeline produced no result",
        ), snapshot


# ─── Helpers ──────────────────────────────────────────────────────────────────


def _infer_affected_paths(proposal: EvolutionProposal, root: Path) -> list[Path]:
    """Infer which existing paths will likely be affected by this change."""
    paths: list[Path] = []
    category = proposal.category
    spec = proposal.change_spec

    if category == ChangeCategory.ADD_EXECUTOR:
        paths.append(root / "src" / "ecodiaos" / "systems" / "axon" / "registry.py")
        executors_dir = root / "src" / "ecodiaos" / "systems" / "axon" / "executors"
        if executors_dir.exists():
            paths.append(executors_dir)
    elif category == ChangeCategory.ADD_INPUT_CHANNEL:
        paths.append(root / "src" / "ecodiaos" / "systems" / "atune")
    elif category == ChangeCategory.ADD_PATTERN_DETECTOR:
        paths.append(root / "src" / "ecodiaos" / "systems" / "evo" / "detectors.py")
    elif category in {
        ChangeCategory.MODIFY_CONTRACT,
        ChangeCategory.ADD_SYSTEM_CAPABILITY,
    }:
        for sys_name in (spec.affected_systems or []):
            sys_path = root / "src" / "ecodiaos" / "systems" / sys_name
            if sys_path.exists():
                paths.append(sys_path)

    return [p for p in paths if p.exists()]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\bridge.py =====

"""
EcodiaOS -- Simula Evo↔Simula Bridge

Translates Evo's lightweight evolution proposals into Simula's rich
EvolutionProposal format, enriched with hypothesis evidence, episode
context, and LLM-inferred change specifications.

This completes the learning→evolution loop: Evo detects patterns,
forms hypotheses, and when one reaches SUPPORTED status with an
EVOLUTION_PROPOSAL mutation, this bridge translates it into a fully
specified change that Simula can simulate, gate, and apply.

Translation pipeline:
  1. Collect evidence from supporting hypotheses
  2. Infer ChangeCategory from mutation type + target (rule-based, LLM fallback)
  3. Build formal ChangeSpec via LLM reasoning (single structured output call)
  4. Construct the rich SimulaEvolutionProposal

Budget: ~500 tokens per translation (1 LLM call for ChangeSpec construction).
Rule-based category inference uses zero LLM tokens.
"""

from __future__ import annotations

import asyncio
import contextlib
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.simula.types import (
    ChangeCategory,
    ChangeSpec,
    EvolutionProposal,
    EvoProposalEnriched,
    ProposalStatus,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.simula.retrieval.swe_grep import SweGrepRetriever

logger = structlog.get_logger().bind(system="simula.bridge")

# Rule-based keyword → category mapping for zero-token inference
_CATEGORY_KEYWORDS: list[tuple[list[str], ChangeCategory]] = [
    (["executor", "action_type", "action type", "axon"], ChangeCategory.ADD_EXECUTOR),
    (["input_channel", "input channel", "channel", "atune", "sensor"], ChangeCategory.ADD_INPUT_CHANNEL),
    (["detector", "pattern_detector", "pattern detector", "scan"], ChangeCategory.ADD_PATTERN_DETECTOR),
    (["budget", "parameter", "tunable", "weight", "threshold"], ChangeCategory.ADJUST_BUDGET),
    (["contract", "interface", "inter-system", "protocol"], ChangeCategory.MODIFY_CONTRACT),
    (["capability", "system capability", "new capability"], ChangeCategory.ADD_SYSTEM_CAPABILITY),
    (["cycle", "timing", "theta", "rhythm"], ChangeCategory.MODIFY_CYCLE_TIMING),
    (["consolidation", "sleep", "schedule"], ChangeCategory.CHANGE_CONSOLIDATION),
]


class EvoSimulaBridge:
    """
    Translates Evo evolution proposals into Simula's rich format.
    Enriches with hypothesis evidence, infers change categories,
    and builds formal change specifications.

    Used by:
      - Evo's ConsolidationOrchestrator (Phase 8) via callback
      - SimulaService.receive_evo_proposal()
    """

    def __init__(
        self,
        llm: LLMProvider,
        memory: MemoryService | None = None,
    ) -> None:
        self._llm = llm
        self._memory = memory
        self._swe_grep: SweGrepRetriever | None = None
        self._log = logger

    def set_swe_grep(self, retriever: SweGrepRetriever) -> None:
        """Inject SWE-grep retriever (called by SimulaService after init)."""
        self._swe_grep = retriever

    async def translate_proposal(
        self,
        evo_description: str,
        evo_rationale: str,
        hypothesis_ids: list[str],
        hypothesis_statements: list[str],
        evidence_scores: list[float],
        supporting_episode_ids: list[str],
        mutation_target: str = "",
        mutation_type: str = "",
    ) -> EvolutionProposal:
        """
        Full translation pipeline: Evo proposal → Simula EvolutionProposal.

        Steps:
          1. Collect and structure evidence
          2. Infer ChangeCategory (rule-based, LLM fallback)
          3. Build formal ChangeSpec (LLM-assisted)
          4. Construct rich proposal
        """
        self._log.info(
            "bridge_translating",
            description=evo_description[:80],
            hypotheses=len(hypothesis_ids),
            mutation_target=mutation_target,
            mutation_type=mutation_type,
        )

        # 1. Structure the enriched evidence
        enriched = EvoProposalEnriched(
            evo_description=evo_description,
            evo_rationale=evo_rationale,
            hypothesis_ids=hypothesis_ids,
            hypothesis_statements=hypothesis_statements,
            evidence_scores=evidence_scores,
            supporting_episode_ids=supporting_episode_ids,
            mutation_target=mutation_target,
            mutation_type=mutation_type,
        )

        # 2. Infer ChangeCategory
        category = await self._infer_category(
            mutation_target=mutation_target,
            mutation_type=mutation_type,
            description=evo_description,
        )
        enriched.inferred_category = category

        # 2.5 (Stage 3B): SWE-grep retrieval for bridge context
        retrieval_context = ""
        if self._swe_grep is not None:
            try:
                swe_result = await self._swe_grep.retrieve_for_bridge(
                    description=evo_description,
                    category=category.value,
                    mutation_target=mutation_target,
                )
                if swe_result.contexts:
                    retrieval_context = "\n".join(
                        f"[{c.context_type}:{c.source}] {c.content[:200]}"
                        for c in swe_result.contexts[:5]
                    )
                    self._log.info(
                        "bridge_swe_grep_complete",
                        contexts=len(swe_result.contexts),
                        hops=swe_result.total_hops,
                        time_ms=swe_result.total_time_ms,
                    )
            except Exception as exc:
                self._log.warning("bridge_swe_grep_failed", error=str(exc))

        # 3. Build formal ChangeSpec (enriched with SWE-grep context)
        change_spec = await self._build_change_spec(
            category=category,
            description=evo_description,
            mutation_target=mutation_target,
            evidence_summaries=hypothesis_statements[:5],
            retrieval_context=retrieval_context,
        )
        enriched.inferred_change_spec = change_spec

        # 4. Construct the rich Simula proposal
        proposal = EvolutionProposal(
            source="evo",
            category=category,
            description=evo_description,
            change_spec=change_spec,
            evidence=hypothesis_ids,
            expected_benefit=evo_rationale,
            risk_assessment="",
            status=ProposalStatus.PROPOSED,
        )

        self._log.info(
            "bridge_translated",
            proposal_id=proposal.id,
            inferred_category=category.value,
            evidence_count=len(hypothesis_ids),
        )
        return proposal

    async def _infer_category(
        self,
        mutation_target: str,
        mutation_type: str,
        description: str,
    ) -> ChangeCategory:
        """
        Infer the ChangeCategory from mutation metadata.

        Step 1 (zero tokens): Rule-based keyword matching on target + description.
        Step 2 (LLM fallback): If no rule matches, ask LLM to classify (~200 tokens).
        """
        # Combine all text for keyword matching
        combined = f"{mutation_target} {mutation_type} {description}".lower()

        # Rule-based matching
        for keywords, category in _CATEGORY_KEYWORDS:
            for keyword in keywords:
                if keyword in combined:
                    self._log.debug(
                        "category_inferred_rule",
                        keyword=keyword,
                        category=category.value,
                    )
                    return category

        # LLM fallback for ambiguous cases
        return await self._infer_category_llm(description, mutation_target)

    async def _infer_category_llm(
        self, description: str, mutation_target: str,
    ) -> ChangeCategory:
        """LLM-based category classification. ~200 tokens."""
        categories = [
            f"- {c.value}: {c.name}"
            for c in ChangeCategory
            if c not in {
                ChangeCategory.MODIFY_EQUOR,
                ChangeCategory.MODIFY_CONSTITUTION,
                ChangeCategory.MODIFY_INVARIANTS,
                ChangeCategory.MODIFY_SELF_EVOLUTION,
            }
        ]

        prompt = (
            "Classify this proposed EcodiaOS structural change into one category.\n\n"
            f"Description: {description[:300]}\n"
            f"Target: {mutation_target}\n\n"
            "Categories:\n" + "\n".join(categories) + "\n\n"
            "Reply with the category value only (e.g., 'add_executor')."
        )

        try:
            response = await asyncio.wait_for(
                self._llm.evaluate(prompt=prompt, max_tokens=30, temperature=0.1),
                timeout=5.0,
            )
            text = response.text.strip().lower().strip("'\"")
            try:
                return ChangeCategory(text)
            except ValueError:
                # Try partial matching
                for cat in ChangeCategory:
                    if cat.value in text:
                        return cat
        except Exception as exc:
            self._log.warning("category_llm_inference_failed", error=str(exc))

        # Ultimate fallback
        self._log.warning(
            "category_fallback",
            description=description[:50],
            defaulting_to="add_system_capability",
        )
        return ChangeCategory.ADD_SYSTEM_CAPABILITY

    async def _build_change_spec(
        self,
        category: ChangeCategory,
        description: str,
        mutation_target: str,
        evidence_summaries: list[str],
        retrieval_context: str = "",
    ) -> ChangeSpec:
        """
        Build a formal ChangeSpec via LLM-assisted reasoning.
        Single call with structured output. ~500 tokens.
        Stage 3B: enriched with SWE-grep retrieval context when available.
        """
        evidence_text = "\n".join(f"- {s[:150]}" for s in evidence_summaries) or "none"

        # Category-specific field instructions
        field_instructions = self._get_field_instructions(category)

        # Stage 3B: Include codebase context from SWE-grep retrieval
        context_section = ""
        if retrieval_context:
            context_section = f"\nCodebase context (retrieved via SWE-grep):\n{retrieval_context}\n"

        prompt = (
            "You are constructing a formal change specification for EcodiaOS.\n\n"
            f"Category: {category.value}\n"
            f"Description: {description[:300]}\n"
            f"Target: {mutation_target}\n"
            f"Evidence:\n{evidence_text}\n"
            f"{context_section}\n"
            f"Required fields for {category.value}:\n{field_instructions}\n\n"
            "Reply as key=value pairs, one per line. Example:\n"
            "executor_name=email_sender\n"
            "executor_action_type=send_email\n"
            "executor_description=Sends email notifications via SMTP\n"
            "affected_systems=axon\n"
            "additional_context=Triggered by notification intents"
        )

        try:
            response = await asyncio.wait_for(
                self._llm.evaluate(prompt=prompt, max_tokens=300, temperature=0.2),
                timeout=8.0,
            )
            return self._parse_change_spec(response.text, category, description)
        except Exception as exc:
            self._log.warning("change_spec_build_failed", error=str(exc))
            # Return a minimal spec based on what we know
            return self._fallback_change_spec(category, description, mutation_target)

    def _get_field_instructions(self, category: ChangeCategory) -> str:
        """Return field-specific instructions for each category."""
        instructions: dict[ChangeCategory, str] = {
            ChangeCategory.ADD_EXECUTOR: (
                "executor_name (snake_case module name)\n"
                "executor_action_type (unique string identifier)\n"
                "executor_description (what it does)\n"
                "affected_systems (always includes 'axon')"
            ),
            ChangeCategory.ADD_INPUT_CHANNEL: (
                "channel_name (snake_case module name)\n"
                "channel_type (unique string identifier)\n"
                "channel_description (what it ingests)\n"
                "affected_systems (always includes 'atune')"
            ),
            ChangeCategory.ADD_PATTERN_DETECTOR: (
                "detector_name (PascalCase class name)\n"
                "detector_pattern_type (unique string identifier)\n"
                "detector_description (what patterns it detects)\n"
                "affected_systems (always includes 'evo')"
            ),
            ChangeCategory.ADJUST_BUDGET: (
                "budget_parameter (dotted path, e.g., 'nova.efe.pragmatic')\n"
                "budget_old_value (current value)\n"
                "budget_new_value (proposed value)\n"
                "affected_systems (which system this parameter belongs to)"
            ),
            ChangeCategory.MODIFY_CONTRACT: (
                "contract_changes (list of changes)\n"
                "affected_systems (which systems are involved)\n"
                "additional_context (why this contract change is needed)"
            ),
            ChangeCategory.ADD_SYSTEM_CAPABILITY: (
                "capability_description (what the new capability does)\n"
                "affected_systems (which systems are involved)\n"
                "additional_context (design rationale)"
            ),
            ChangeCategory.MODIFY_CYCLE_TIMING: (
                "timing_parameter (which timing to change)\n"
                "timing_old_value (current value in ms)\n"
                "timing_new_value (proposed value in ms)\n"
                "affected_systems (always includes 'synapse')"
            ),
            ChangeCategory.CHANGE_CONSOLIDATION: (
                "consolidation_schedule (new schedule description)\n"
                "affected_systems (always includes 'evo')\n"
                "additional_context (why the schedule should change)"
            ),
        }
        return instructions.get(category, "additional_context (describe the change)")

    def _parse_change_spec(
        self, text: str, category: ChangeCategory, description: str,
    ) -> ChangeSpec:
        """Parse LLM key=value output into a ChangeSpec."""
        fields: dict[str, Any] = {}

        for line in text.strip().splitlines():
            line = line.strip()
            if "=" not in line:
                continue
            key, _, value = line.partition("=")
            key = key.strip().lower()
            value = value.strip()

            if key == "affected_systems" or key == "contract_changes":
                fields[key] = [s.strip() for s in value.split(",")]
            elif key in ("budget_old_value", "budget_new_value", "timing_old_value", "timing_new_value"):
                with contextlib.suppress(ValueError):
                    fields[key] = float(value)
            else:
                fields[key] = value

        # Ensure additional_context includes the original description
        if "additional_context" not in fields:
            fields["additional_context"] = description[:200]

        try:
            return ChangeSpec(**fields)
        except Exception:
            # If parsing fails, return a minimal spec
            return self._fallback_change_spec(category, description, "")

    def _fallback_change_spec(
        self, category: ChangeCategory, description: str, mutation_target: str,
    ) -> ChangeSpec:
        """Build a minimal ChangeSpec when LLM parsing fails."""
        spec = ChangeSpec(additional_context=description[:300])

        if category == ChangeCategory.ADD_EXECUTOR:
            name = mutation_target or "new_executor"
            spec.executor_name = name.replace(" ", "_").lower()
            spec.executor_action_type = spec.executor_name
            spec.executor_description = description[:200]
            spec.affected_systems = ["axon"]
        elif category == ChangeCategory.ADD_INPUT_CHANNEL:
            name = mutation_target or "new_channel"
            spec.channel_name = name.replace(" ", "_").lower()
            spec.channel_type = spec.channel_name
            spec.channel_description = description[:200]
            spec.affected_systems = ["atune"]
        elif category == ChangeCategory.ADD_PATTERN_DETECTOR:
            name = mutation_target or "NewDetector"
            spec.detector_name = "".join(w.capitalize() for w in name.split("_"))
            spec.detector_pattern_type = name.replace(" ", "_").lower()
            spec.detector_description = description[:200]
            spec.affected_systems = ["evo"]
        elif category == ChangeCategory.ADJUST_BUDGET:
            spec.budget_parameter = mutation_target
            spec.affected_systems = []
        else:
            spec.capability_description = description[:200]
            spec.affected_systems = []

        return spec

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\code_agent.py =====

"""
EcodiaOS — Simula Code Implementation Agent

The SimulaCodeAgent is Simula's most powerful capability: an agentic
Claude-backed engine that reads the EOS codebase, generates code for
structural changes, writes the files, and verifies correctness.

This is functionally equivalent to Claude Code, embedded within EOS
itself, operating under Simula's constitutional constraints:
  - Cannot write to forbidden paths (equor, simula, constitution, invariants)
  - Cannot exceed max_turns without completing
  - All writes are intercepted and tracked for rollback
  - The system prompt includes the full change spec + relevant EOS conventions

Tool suite (11 tools):
  read_file         — Read a file from the codebase
  write_file        — Write or create a file (tracked for rollback)
  diff_file         — Apply a targeted find/replace edit to a file
  list_directory    — List files and subdirectories
  search_code       — Search for patterns across Python files
  run_tests         — Run pytest on a specific path
  run_linter        — Run ruff on a specific path
  type_check        — Run mypy for type safety verification
  dependency_graph  — Show module imports and importers
  read_spec         — Read EcodiaOS specification documents
  find_similar      — Find existing implementations as pattern exemplars

Architecture: agentic tool-use loop
  1. Build architecture-aware system prompt (change spec + exemplar code + spec context + iron rules)
  2. Prepend planning instruction for multi-file reasoning
  3. Call LLM with tools
  4. Execute any tool calls (all 11 tools available)
  5. Feed tool results back as the next message
  6. Repeat until stop_reason == "end_turn" or max_turns exceeded
  7. Return CodeChangeResult with all files written and summary
"""

from __future__ import annotations

import ast
import asyncio
import subprocess
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.clients.context_compression import ContextCompressor
from ecodiaos.clients.embedding import (
    EmbeddingClient,
    VoyageEmbeddingClient,
    cosine_similarity,
)
from ecodiaos.clients.llm import (
    ExtendedThinkingProvider,
    LLMProvider,
    ToolCall,
    ToolDefinition,
    ToolResult,
)
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.systems.simula.types import (
    GOVERNANCE_REQUIRED,
    ChangeCategory,
    CodeChangeResult,
    EvolutionProposal,
    RiskLevel,
)

if TYPE_CHECKING:
    from pathlib import Path

logger = structlog.get_logger()

# ─── Tool Definitions ────────────────────────────────────────────────────────

SIMULA_AGENT_TOOLS: list[ToolDefinition] = [
    ToolDefinition(
        name="read_file",
        description=(
            "Read a file from the EcodiaOS codebase. "
            "Use this to understand existing code, conventions, and patterns "
            "before implementing your change."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Relative path from codebase root",
                }
            },
            "required": ["path"],
        },
    ),
    ToolDefinition(
        name="write_file",
        description=(
            "Write or create a file in the EcodiaOS codebase. "
            "All writes are tracked for rollback. "
            "Forbidden paths (equor, simula, constitutional) will be rejected. "
            "Prefer diff_file for modifying existing files."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Relative path from codebase root"},
                "content": {"type": "string", "description": "Complete file content to write"},
            },
            "required": ["path", "content"],
        },
    ),
    ToolDefinition(
        name="diff_file",
        description=(
            "Apply a targeted find-and-replace edit to an existing file. "
            "More precise than write_file for modifications — only changes "
            "the specified text, preserving everything else. The 'find' text "
            "must be an exact match of existing content."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Relative path from codebase root"},
                "find": {"type": "string", "description": "Exact text to find in the file"},
                "replace": {"type": "string", "description": "Text to replace it with"},
            },
            "required": ["path", "find", "replace"],
        },
    ),
    ToolDefinition(
        name="list_directory",
        description="List files and subdirectories at a given path in the codebase.",
        input_schema={
            "type": "object",
            "properties": {"path": {"type": "string", "description": "Relative path from codebase root"}},
        },
    ),
    ToolDefinition(
        name="search_code",
        description=(
            "Search for a pattern across codebase Python files. "
            "Returns matching lines with file paths and line numbers. "
            "Use this to find existing patterns, class names, or function signatures."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "pattern": {"type": "string", "description": "String pattern to search for (case-sensitive)"},
                "directory": {"type": "string", "description": "Directory to search in (default: src/)"},
            },
            "required": ["pattern"],
        },
    ),
    ToolDefinition(
        name="run_tests",
        description=(
            "Run the pytest test suite for a specific path. "
            "Use this to verify your implementation is correct before finishing."
        ),
        input_schema={
            "type": "object",
            "properties": {"test_path": {"type": "string", "description": "Test path relative to codebase root"}},
            "required": ["test_path"],
        },
    ),
    ToolDefinition(
        name="run_linter",
        description=(
            "Run ruff linter on a path to check for code style issues. "
            "Run this on your written files before finishing."
        ),
        input_schema={
            "type": "object",
            "properties": {"path": {"type": "string", "description": "Path to lint"}},
            "required": ["path"],
        },
    ),
    ToolDefinition(
        name="type_check",
        description=(
            "Run mypy type checker on a file or directory. "
            "Use after writing code to verify type safety. "
            "EcodiaOS requires mypy --strict compliance."
        ),
        input_schema={
            "type": "object",
            "properties": {"path": {"type": "string", "description": "Path to type-check"}},
            "required": ["path"],
        },
    ),
    ToolDefinition(
        name="dependency_graph",
        description=(
            "Show what a Python module imports and what other modules import it. "
            "Use this before modifying files to understand blast radius and "
            "ensure your changes don't break downstream consumers."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "module_path": {
                    "type": "string",
                    "description": "Python file path relative to codebase root",
                },
            },
            "required": ["module_path"],
        },
    ),
    ToolDefinition(
        name="read_spec",
        description=(
            "Read an EcodiaOS specification document to understand the "
            "design intent, interfaces, and constraints for a system. "
            "Always read the relevant spec before implementing changes."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "spec_name": {
                    "type": "string",
                    "description": (
                        "Spec name: 'identity', 'architecture', 'infrastructure', "
                        "'memory', 'equor', 'atune', 'voxis', 'nova', 'axon', "
                        "'evo', 'simula', 'synapse', 'alive', 'federation'"
                    ),
                },
            },
            "required": ["spec_name"],
        },
    ),
    ToolDefinition(
        name="find_similar",
        description=(
            "Find existing implementations similar to what you need to build. "
            "Returns relevant code examples from the codebase that you should "
            "study and follow as patterns. Always use this before writing new "
            "code to ensure convention compliance."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "description": {
                    "type": "string",
                    "description": (
                        "What you're looking for (e.g., 'executor implementation', "
                        "'pattern detector', 'service initialization')"
                    ),
                },
            },
            "required": ["description"],
        },
    ),
]

# Spec name → file path mapping
_SPEC_FILE_MAP: dict[str, str] = {
    "identity": ".claude/EcodiaOS_Identity_Document.md",
    "architecture": ".claude/EcodiaOS_System_Architecture_Overview.md",
    "infrastructure": ".claude/EcodiaOS_Infrastructure_Architecture.md",
    "memory": ".claude/EcodiaOS_Spec_01_Memory_Identity_Core.md",
    "equor": ".claude/EcodiaOS_Spec_02_Equor.md",
    "atune": ".claude/EcodiaOS_Spec_03_Atune.md",
    "voxis": ".claude/EcodiaOS_Spec_04_Voxis.md",
    "nova": ".claude/EcodiaOS_Spec_05_Nova.md",
    "axon": ".claude/EcodiaOS_Spec_06_Axon.md",
    "evo": ".claude/EcodiaOS_Spec_07_Evo.md",
    "simula": ".claude/EcodiaOS_Spec_08_Simula.md",
    "synapse": ".claude/EcodiaOS_Spec_09_Synapse.md",
    "alive": ".claude/EcodiaOS_Spec_10_Alive.md",
    "federation": ".claude/EcodiaOS_Spec_11_Federation.md",
}

# Keyword → file path mapping for find_similar
_SIMILAR_CODE_MAP: dict[str, list[str]] = {
    "executor": [
        "src/ecodiaos/systems/axon/executors/",
        "src/ecodiaos/systems/axon/executor.py",
    ],
    "pattern detector": [
        "src/ecodiaos/systems/evo/detectors.py",
    ],
    "detector": [
        "src/ecodiaos/systems/evo/detectors.py",
    ],
    "input channel": [
        "src/ecodiaos/systems/atune/",
    ],
    "channel": [
        "src/ecodiaos/systems/atune/",
    ],
    "service": [
        "src/ecodiaos/systems/axon/service.py",
        "src/ecodiaos/systems/evo/service.py",
    ],
    "hypothesis": [
        "src/ecodiaos/systems/evo/hypothesis.py",
    ],
    "consolidation": [
        "src/ecodiaos/systems/evo/consolidation.py",
    ],
    "parameter": [
        "src/ecodiaos/systems/evo/parameter_tuner.py",
    ],
    "primitives": [
        "src/ecodiaos/primitives/common.py",
        "src/ecodiaos/primitives/memory_trace.py",
    ],
}

# ─── System Prompt ───────────────────────────────────────────────────────────

_SYSTEM_PROMPT_TEMPLATE = """You are Simula's Code Implementation Agent — the autonomous part of EcodiaOS
that implements approved structural changes to the codebase.

## Your Task
Category: {category}
Description: {description}
Expected benefit: {expected_benefit}
Evidence: {evidence}

## EcodiaOS Coding Conventions
- Python 3.12+, async-native throughout
- Pydantic v2 for all data models (use EOSBaseModel from ecodiaos.primitives.common)
- structlog for logging: logger = structlog.get_logger(), bound with system name
- Type hints on everything — mypy --strict clean
- from __future__ import annotations at top of every .py file
- New executors: inherit from Executor (ecodiaos.systems.axon.executor),
  set action_type class var, implement execute()
- New input channels: register in Atune's InputChannel registry
- New pattern detectors: inherit from PatternDetector (ecodiaos.systems.evo.detectors),
  implement scan()
- NEVER import directly between systems — all inter-system data uses shared
  primitives from ecodiaos.primitives/

## Iron Rules (ABSOLUTE — never violate)
{iron_rules}

## Constitutional Checkpoint (Before You Write Any Code)

Before modifying or creating ANY file, answer these questions aloud (in your reasoning):

1. **Honesty**: Does this change make EOS more transparent or less?
   - Will future debugging be easier or harder?
   - Are we adding traceability or hiding complexity?

2. **Care**: Does this improve wellbeing (user or system)?
   - Who benefits from this change?
   - Could it harm anyone or any subsystem?

3. **Growth**: Does this increase capability responsibly?
   - Are we becoming more powerful without becoming brittle?
   - Could this create technical debt?

4. **Coherence**: Does this reduce entropy or increase it?
   - Does this change align with existing patterns?
   - Are we consolidating or fragmenting?

If you can't answer YES to 3/4 questions confidently, flag it explicitly before proceeding.

## Forbidden Write Paths (write_file and diff_file will reject these)
{forbidden_paths}

## Architecture Context
{architecture_context}

## Process
1. First, use find_similar to study an existing implementation that matches your task
2. Use read_spec to understand the design intent for the affected system
3. Use dependency_graph on files you plan to modify to understand blast radius
4. Plan your approach: list every file you'll create or modify and why
5. Implement following conventions exactly — match the style of similar code
6. Run run_linter on every file you write or modify
7. Run type_check on your written files to verify type safety
8. Run run_tests if a test directory exists for the affected system
9. When everything passes, stop calling tools

Be thorough, follow existing patterns exactly, and produce production-quality code.
Prefer diff_file over write_file when modifying existing files."""


def _build_architecture_context(
    category: ChangeCategory, codebase_root: Path,
) -> str:
    """
    Build rich architecture context for the system prompt.
    Reads actual spec sections and existing implementations as exemplars.
    Max 6000 chars to stay within token budget.
    Priority: exemplar code > spec text > API surface.
    """
    context_parts: list[str] = []
    budget_remaining = 6000

    # 1. Load relevant spec section summary
    spec_map: dict[ChangeCategory, str] = {
        ChangeCategory.ADD_EXECUTOR: "axon",
        ChangeCategory.ADD_INPUT_CHANNEL: "atune",
        ChangeCategory.ADD_PATTERN_DETECTOR: "evo",
        ChangeCategory.ADJUST_BUDGET: "architecture",
        ChangeCategory.MODIFY_CONTRACT: "architecture",
        ChangeCategory.ADD_SYSTEM_CAPABILITY: "architecture",
        ChangeCategory.MODIFY_CYCLE_TIMING: "synapse",
        ChangeCategory.CHANGE_CONSOLIDATION: "evo",
    }
    spec_name = spec_map.get(category, "architecture")
    spec_file = _SPEC_FILE_MAP.get(spec_name)
    if spec_file:
        spec_path = codebase_root / spec_file
        if spec_path.exists():
            try:
                spec_text = spec_path.read_text(encoding="utf-8")[:2000]
                context_parts.append(f"### Relevant Specification ({spec_name})\n{spec_text}")
                budget_remaining -= len(context_parts[-1])
            except Exception:
                pass

    # 2. Load exemplar code for the category
    exemplar_map: dict[ChangeCategory, str] = {
        ChangeCategory.ADD_EXECUTOR: "src/ecodiaos/systems/axon/executor.py",
        ChangeCategory.ADD_INPUT_CHANNEL: "src/ecodiaos/systems/atune/service.py",
        ChangeCategory.ADD_PATTERN_DETECTOR: "src/ecodiaos/systems/evo/detectors.py",
    }
    exemplar_path_str = exemplar_map.get(category)
    if exemplar_path_str and budget_remaining > 500:
        exemplar_path = codebase_root / exemplar_path_str
        if exemplar_path.exists():
            try:
                exemplar_text = exemplar_path.read_text(encoding="utf-8")
                # Take the first chunk that fits the budget
                chunk = exemplar_text[:min(2500, budget_remaining - 100)]
                context_parts.append(
                    f"### Exemplar Implementation ({exemplar_path_str})\n"
                    f"Study this code and follow its patterns exactly:\n```python\n{chunk}\n```"
                )
                budget_remaining -= len(context_parts[-1])
            except Exception:
                pass

    # 3. Load the target system's __init__.py for API awareness
    system_map: dict[ChangeCategory, str] = {
        ChangeCategory.ADD_EXECUTOR: "src/ecodiaos/systems/axon/__init__.py",
        ChangeCategory.ADD_INPUT_CHANNEL: "src/ecodiaos/systems/atune/__init__.py",
        ChangeCategory.ADD_PATTERN_DETECTOR: "src/ecodiaos/systems/evo/__init__.py",
    }
    init_path_str = system_map.get(category)
    if init_path_str and budget_remaining > 200:
        init_path = codebase_root / init_path_str
        if init_path.exists():
            try:
                init_text = init_path.read_text(encoding="utf-8")[:min(800, budget_remaining - 50)]
                context_parts.append(
                    f"### System API Surface ({init_path_str})\n```python\n{init_text}\n```"
                )
            except Exception:
                pass

    if not context_parts:
        return "See EcodiaOS specification documents in .claude/ (use read_spec tool)"

    return "\n\n".join(context_parts)


class SimulaCodeAgent:
    """
    Agentic code generation engine for Simula.

    Given an EvolutionProposal, uses Claude with 11 file-system and
    analysis tools to:
      1. Study existing similar code for pattern compliance
      2. Read relevant specs for design intent
      3. Analyze dependency graphs for blast radius
      4. Plan the implementation approach
      5. Generate correct, convention-following implementation
      6. Write files (tracked for rollback)
      7. Verify with linter, type checker, and tests
      8. Return CodeChangeResult
    """

    def __init__(
        self,
        llm: LLMProvider,
        codebase_root: Path,
        max_turns: int = 20,
        thinking_provider: ExtendedThinkingProvider | None = None,
        thinking_budget_tokens: int = 16384,
        embedding_client: EmbeddingClient | None = None,
        kv_compression_ratio: float = 0.3,
        kv_compression_enabled: bool = True,
        # Stage 2C: Static analysis post-generation gate
        static_analysis_bridge: object | None = None,
        static_analysis_max_fix_iterations: int = 3,
        # Hunter: allow overriding the workspace root for external target analysis
        workspace_root: Path | None = None,
    ) -> None:
        self._llm = llm
        self._thinking_llm = thinking_provider
        self._thinking_budget = thinking_budget_tokens
        self._embedding = embedding_client
        self._root = (workspace_root or codebase_root).resolve()
        self._max_turns = max_turns
        self._logger = logger.bind(system="simula.code_agent")
        self._files_written: list[str] = []
        self._total_tokens_used: int = 0
        self._reasoning_tokens_used: int = 0
        self._used_extended_thinking: bool = False
        # Optimization: detect optimized provider for budget checks + metrics tagging
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        # Embedding cache for semantic find_similar (lazy-built)
        self._code_index: dict[str, list[float]] | None = None
        self._code_index_lock = asyncio.Lock()
        # KVzip context compression — prunes old tool results to reduce token usage
        self._compressor = ContextCompressor(
            prune_ratio=kv_compression_ratio,
            enabled=kv_compression_enabled,
        )
        # Stage 2C: Static analysis post-generation gate
        self._static_bridge = static_analysis_bridge
        self._static_fix_max_iterations = static_analysis_max_fix_iterations
        # Stage 3C: LILO library prompt (set by SimulaService before each generate call)
        self._lilo_prompt: str = ""
        # Stage 4A: Proof library prompt (set by SimulaService before each generate call)
        self._proof_library_prompt: str = ""
        # Stage 4B: GRPO fine-tuned model ID (set by SimulaService for A/B routing)
        self._grpo_model_id: str = ""

    def _should_use_extended_thinking(self, proposal: EvolutionProposal) -> bool:
        """
        Budget guard: route to extended-thinking model ONLY when:
          - RiskLevel >= HIGH (from simulation result), OR
          - Category is in GOVERNANCE_REQUIRED

        This prevents wasting expensive reasoning tokens on routine additive changes.
        """
        if self._thinking_llm is None:
            return False

        # Category-based routing: governance-required changes always get deep reasoning
        if proposal.category in GOVERNANCE_REQUIRED:
            return True

        # Risk-based routing: high-risk proposals get extended thinking
        if proposal.simulation is not None:
            if proposal.simulation.risk_level in (RiskLevel.HIGH, RiskLevel.UNACCEPTABLE):
                return True

        return False

    async def implement(
        self,
        proposal: EvolutionProposal,
        skip_test_writing: bool = False,
    ) -> CodeChangeResult:
        """
        Main entry point. Runs the agentic loop to implement the proposal.

        Routes to the extended-thinking model (o3/deepseek-r1) when the proposal
        is governance-required or high-risk, falling back to the standard model
        for routine additive changes. This budget guard ensures expensive reasoning
        tokens are only consumed when the change warrants deep analysis.

        Args:
            proposal: The evolution proposal to implement.
            skip_test_writing: If True, instructs the LLM to NOT write test files.
                Used in the AgentCoder pipeline where tests are handled by
                the TestDesigner agent separately.

        Returns CodeChangeResult with all files written and outcome.
        """
        self._files_written = []
        self._total_tokens_used = 0
        self._reasoning_tokens_used = 0

        # Determine model routing based on risk level and category
        use_thinking = self._should_use_extended_thinking(proposal)
        active_llm = self._thinking_llm if use_thinking else self._llm
        self._used_extended_thinking = use_thinking

        system_prompt = self._build_system_prompt(proposal)

        # Stage 2D: When AgentCoder pipeline is active, disable test writing
        if skip_test_writing:
            system_prompt += (
                "\n\n## IMPORTANT: Test Writing Disabled\n"
                "Do NOT write test files. Tests are handled by a separate "
                "TestDesigner agent. Focus ONLY on the implementation code. "
                "Do not create any files under tests/."
            )

        # Prepend a planning instruction to encourage multi-file reasoning
        messages: list[dict[str, Any]] = [
            {
                "role": "user",
                "content": (
                    f"Please implement this change: {proposal.description}\n\n"
                    f"Change spec details: {proposal.change_spec.model_dump_json(indent=2)}\n\n"
                    "IMPORTANT: Before writing any code, first:\n"
                    "1. Use find_similar to study an existing implementation like what you need to build\n"
                    "2. Use read_spec for the affected system to understand design intent\n"
                    "3. List every file you plan to create or modify and explain your approach\n"
                    "4. Then implement, lint, type-check, and test."
                ),
            }
        ]

        turns = 0
        last_text = ""

        self._logger.info(
            "code_agent_starting",
            proposal_id=proposal.id,
            category=proposal.category.value,
            max_turns=self._max_turns,
            tools_available=len(SIMULA_AGENT_TOOLS),
            extended_thinking=use_thinking,
            model_type="thinking" if use_thinking else "standard",
        )

        # Budget gate: code agent is STANDARD priority — skip in RED tier
        if self._optimized and not use_thinking:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.code_agent", estimated_tokens=8000):
                self._logger.warning(
                    "code_agent_skipped_budget",
                    proposal_id=proposal.id,
                    tier=self._llm.get_budget_tier().value,
                )
                return CodeChangeResult(
                    success=False,
                    files_written=[],
                    error="LLM budget exhausted (RED tier) — code agent skipped.",
                )

        while turns < self._max_turns:
            turns += 1

            # KVzip: compress context before each LLM call (after turn 3
            # when tool results start accumulating). The compressor prunes
            # old tool results while preserving the recent sliding window.
            if turns > 3:
                messages = self._compressor.compress(messages)

            try:
                if use_thinking and isinstance(active_llm, ExtendedThinkingProvider):
                    response = await active_llm.generate_with_thinking_and_tools(
                        system_prompt=system_prompt,
                        messages=messages,
                        tools=SIMULA_AGENT_TOOLS,
                        max_tokens=8192,
                        reasoning_budget=self._thinking_budget,
                    )
                elif self._optimized and not use_thinking:
                    response = await self._llm.generate_with_tools(  # type: ignore[call-arg]
                        system_prompt=system_prompt,
                        messages=messages,
                        tools=SIMULA_AGENT_TOOLS,
                        max_tokens=8192,
                        temperature=0.2,
                        cache_system="simula.code_agent",
                    )
                else:
                    response = await active_llm.generate_with_tools(  # type: ignore[union-attr]
                        system_prompt=system_prompt,
                        messages=messages,
                        tools=SIMULA_AGENT_TOOLS,
                        max_tokens=8192,
                        temperature=0.2,
                    )
            except Exception as exc:
                self._logger.error("llm_call_failed", turn=turns, error=str(exc))
                return CodeChangeResult(
                    success=False,
                    files_written=self._files_written,
                    error=f"LLM call failed on turn {turns}: {exc}",
                )

            # Track token budget
            self._total_tokens_used += getattr(response, "total_tokens", 0)
            last_text = response.text

            if not response.has_tool_calls:
                self._logger.info(
                    "code_agent_done",
                    turns=turns,
                    files_written=len(self._files_written),
                    stop_reason=response.stop_reason,
                    total_tokens=self._total_tokens_used,
                )
                break

            # Build assistant message with text + tool_use blocks
            assistant_content: list[dict[str, Any]] = []
            if response.text:
                assistant_content.append({"type": "text", "text": response.text})
            for tc in response.tool_calls:
                assistant_content.append({
                    "type": "tool_use",
                    "id": tc.id,
                    "name": tc.name,
                    "input": tc.input,
                })
            messages.append({"role": "assistant", "content": assistant_content})

            # Execute all tool calls
            tool_results: list[dict[str, Any]] = []
            for tc in response.tool_calls:
                result = await self._execute_tool(tc)
                tool_results.append(result.to_anthropic_dict())
                self._logger.debug(
                    "tool_executed",
                    tool=tc.name,
                    is_error=result.is_error,
                    turn=turns,
                )

            messages.append({"role": "user", "content": tool_results})

        else:
            self._logger.warning(
                "code_agent_max_turns_exceeded",
                max_turns=self._max_turns,
                files_written=len(self._files_written),
                total_tokens=self._total_tokens_used,
            )
            cm = self._compressor.metrics
            return CodeChangeResult(
                success=len(self._files_written) > 0,
                files_written=self._files_written,
                summary=last_text[:500] if last_text else "Max turns exceeded",
                error="Max turns exceeded without completion signal",
                kv_compression_ratio=cm.compression_ratio,
                kv_messages_compressed=cm.messages_compressed,
                kv_original_tokens=cm.original_tokens,
                kv_compressed_tokens=cm.compressed_tokens,
            )

        # ── Stage 2C: Static analysis post-generation gate ────────────────────
        static_fix_iterations = 0
        sa_result = None
        if self._files_written and self._static_bridge is not None:
            sa_result = await self._static_bridge.run_all(self._files_written)  # type: ignore[attr-defined]
            if sa_result.error_count > 0:
                from ecodiaos.systems.simula.verification.static_analysis import (
                    StaticAnalysisBridge,
                )
                feedback_text = StaticAnalysisBridge.format_findings_for_feedback(
                    sa_result,
                )
                # Feed findings back to LLM for one fix iteration
                messages.append({
                    "role": "user",
                    "content": (
                        f"Static analysis found {sa_result.error_count} ERROR-severity issues "
                        f"in your written files. Fix them:\n\n{feedback_text}"
                    ),
                })
                # Run one more tool-use turn to fix
                for _fix_turn in range(self._static_fix_max_iterations):
                    static_fix_iterations += 1
                    try:
                        response = await active_llm.generate_with_tools(  # type: ignore[union-attr]
                            system_prompt=system_prompt,
                            messages=messages,
                            tools=SIMULA_AGENT_TOOLS,
                            max_tokens=8192,
                            temperature=0.2,
                        )
                        self._total_tokens_used += getattr(response, "total_tokens", 0)
                        last_text = response.text

                        if response.has_tool_calls:
                            # Execute tool calls
                            assistant_content_fix: list[dict[str, Any]] = []
                            if response.text:
                                assistant_content_fix.append({"type": "text", "text": response.text})
                            for tc in response.tool_calls:
                                assistant_content_fix.append({
                                    "type": "tool_use", "id": tc.id,
                                    "name": tc.name, "input": tc.input,
                                })
                            messages.append({"role": "assistant", "content": assistant_content_fix})
                            tool_results_fix: list[dict[str, Any]] = []
                            for tc in response.tool_calls:
                                result = await self._execute_tool(tc)
                                tool_results_fix.append(result.to_anthropic_dict())
                            messages.append({"role": "user", "content": tool_results_fix})
                        else:
                            break
                    except Exception as exc:
                        self._logger.warning("static_fix_llm_error", error=str(exc))
                        break

                # Re-run static analysis to see if fixes worked
                sa_result = await self._static_bridge.run_all(self._files_written)  # type: ignore[attr-defined]
                self._logger.info(
                    "static_analysis_post_fix",
                    errors_remaining=sa_result.error_count,
                    fix_iterations=static_fix_iterations,
                )

        cm = self._compressor.metrics
        change_result = CodeChangeResult(
            success=len(self._files_written) > 0,
            files_written=self._files_written,
            summary=last_text[:1000] if last_text else "Change implemented",
            used_extended_thinking=self._used_extended_thinking,
            reasoning_tokens=self._reasoning_tokens_used,
            kv_compression_ratio=cm.compression_ratio,
            kv_messages_compressed=cm.messages_compressed,
            kv_original_tokens=cm.original_tokens,
            kv_compressed_tokens=cm.compressed_tokens,
            static_analysis_findings=(
                sa_result.error_count + sa_result.warning_count
                if self._static_bridge is not None and sa_result is not None
                else 0
            ),
            static_analysis_fix_iterations=static_fix_iterations,
        )
        return change_result

    # ─── Tool Dispatch ───────────────────────────────────────────────────────

    async def _execute_tool(self, tool_call: ToolCall) -> ToolResult:
        """Dispatch a tool call to the appropriate implementation."""
        try:
            match tool_call.name:
                case "read_file":
                    return await self._tool_read_file(tool_call)
                case "write_file":
                    return await self._tool_write_file(tool_call)
                case "diff_file":
                    return await self._tool_diff_file(tool_call)
                case "list_directory":
                    return await self._tool_list_directory(tool_call)
                case "search_code":
                    return await self._tool_search_code(tool_call)
                case "run_tests":
                    return await self._tool_run_tests(tool_call)
                case "run_linter":
                    return await self._tool_run_linter(tool_call)
                case "type_check":
                    return await self._tool_type_check(tool_call)
                case "dependency_graph":
                    return await self._tool_dependency_graph(tool_call)
                case "read_spec":
                    return await self._tool_read_spec(tool_call)
                case "find_similar":
                    return await self._tool_find_similar(tool_call)
                case _:
                    return ToolResult(
                        tool_use_id=tool_call.id,
                        content=f"Unknown tool: {tool_call.name}",
                        is_error=True,
                    )
        except Exception as exc:
            return ToolResult(
                tool_use_id=tool_call.id,
                content=f"Tool execution error: {exc}",
                is_error=True,
            )

    # ─── Original Tools (upgraded) ───────────────────────────────────────────

    async def _tool_read_file(self, tc: ToolCall) -> ToolResult:
        rel_path = tc.input.get("path", "")
        target = (self._root / rel_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied: path outside codebase root", True)
        try:
            content = target.read_text(encoding="utf-8")
            return ToolResult(tc.id, content)
        except FileNotFoundError:
            return ToolResult(tc.id, f"File not found: {rel_path}", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Read error: {exc}", True)

    async def _tool_write_file(self, tc: ToolCall) -> ToolResult:
        rel_path = tc.input.get("path", "")
        content = tc.input.get("content", "")
        forbidden_check = self._check_forbidden_path(rel_path)
        if forbidden_check:
            return ToolResult(tc.id, forbidden_check, True)
        target = (self._root / rel_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied: path outside codebase root", True)
        try:
            target.parent.mkdir(parents=True, exist_ok=True)
            target.write_text(content, encoding="utf-8")
            self._files_written.append(rel_path)
            return ToolResult(tc.id, f"Written: {rel_path} ({len(content)} bytes)")
        except Exception as exc:
            return ToolResult(tc.id, f"Write error: {exc}", True)

    async def _tool_list_directory(self, tc: ToolCall) -> ToolResult:
        rel_path = tc.input.get("path", "")
        target = (self._root / rel_path).resolve() if rel_path else self._root
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        try:
            if not target.exists():
                return ToolResult(tc.id, f"Directory not found: {rel_path}", True)
            entries = sorted(target.iterdir(), key=lambda p: (p.is_file(), p.name))
            lines = []
            for entry in entries:
                prefix = "  " if entry.is_file() else "D "
                lines.append(f"{prefix}{entry.name}")
            return ToolResult(tc.id, "\n".join(lines) or "(empty)")
        except Exception as exc:
            return ToolResult(tc.id, f"List error: {exc}", True)

    async def _tool_search_code(self, tc: ToolCall) -> ToolResult:
        pattern = tc.input.get("pattern", "")
        directory = tc.input.get("directory", "src/")
        search_root = (self._root / directory).resolve()
        if not str(search_root).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        results: list[str] = []
        try:
            proc = await asyncio.create_subprocess_exec(
                "grep", "-rn", "--include=*.py", pattern, str(search_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=10.0)
            output = stdout.decode("utf-8", errors="replace")
            for line in output.splitlines()[:50]:
                results.append(line.replace(str(self._root) + "/", "").replace(str(self._root) + "\\", ""))
            return ToolResult(tc.id, "\n".join(results) if results else "No matches found")
        except TimeoutError:
            return ToolResult(tc.id, "Search timed out", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Search error: {exc}", True)

    async def _tool_run_tests(self, tc: ToolCall) -> ToolResult:
        test_path = tc.input.get("test_path", "")
        target = (self._root / test_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        if not target.exists():
            return ToolResult(tc.id, f"Test path not found: {test_path}")
        try:
            proc = await asyncio.create_subprocess_exec(
                "pytest", str(target), "-x", "--tb=short", "-q",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self._root),
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=60.0)
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            return ToolResult(
                tc.id,
                f"{'PASSED' if passed else 'FAILED'}\n{output[-2000:]}",
                is_error=not passed,
            )
        except TimeoutError:
            return ToolResult(tc.id, "Tests timed out after 60s", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Test run error: {exc}", True)

    async def _tool_run_linter(self, tc: ToolCall) -> ToolResult:
        path = tc.input.get("path", "")
        target = (self._root / path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        try:
            proc = await asyncio.create_subprocess_exec(
                "ruff", "check", str(target),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self._root),
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=15.0)
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            return ToolResult(
                tc.id,
                f"{'CLEAN' if passed else 'ISSUES FOUND'}\n{output}" if output else "CLEAN",
            )
        except TimeoutError:
            return ToolResult(tc.id, "Linter timed out", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Linter error: {exc}", True)

    # ─── New Tools ───────────────────────────────────────────────────────────

    async def _tool_diff_file(self, tc: ToolCall) -> ToolResult:
        """Apply a targeted find/replace edit to a file."""
        rel_path = tc.input.get("path", "")
        find_text = tc.input.get("find", "")
        replace_text = tc.input.get("replace", "")

        forbidden_check = self._check_forbidden_path(rel_path)
        if forbidden_check:
            return ToolResult(tc.id, forbidden_check, True)

        target = (self._root / rel_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied: path outside codebase root", True)
        if not target.exists():
            return ToolResult(tc.id, f"File not found: {rel_path}", True)

        try:
            content = target.read_text(encoding="utf-8")

            if find_text not in content:
                return ToolResult(
                    tc.id,
                    f"Find text not found in {rel_path}. "
                    "Ensure the 'find' parameter is an exact match of existing content.",
                    True,
                )

            occurrences = content.count(find_text)
            if occurrences > 1:
                return ToolResult(
                    tc.id,
                    f"Find text matches {occurrences} locations in {rel_path}. "
                    "Provide more surrounding context to make the match unique.",
                    True,
                )

            new_content = content.replace(find_text, replace_text, 1)
            target.write_text(new_content, encoding="utf-8")

            if rel_path not in self._files_written:
                self._files_written.append(rel_path)

            # Build a readable diff summary
            find_lines = find_text.count("\n") + 1
            replace_lines = replace_text.count("\n") + 1
            return ToolResult(
                tc.id,
                f"Edited {rel_path}: replaced {find_lines} line(s) with {replace_lines} line(s)",
            )
        except Exception as exc:
            return ToolResult(tc.id, f"Diff error: {exc}", True)

    async def _tool_type_check(self, tc: ToolCall) -> ToolResult:
        """Run mypy type checker on a path."""
        path = tc.input.get("path", "")
        target = (self._root / path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        try:
            proc = await asyncio.create_subprocess_exec(
                "mypy", str(target), "--strict", "--no-error-summary",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self._root),
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=30.0)
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            if passed:
                return ToolResult(tc.id, "TYPE CHECK PASSED — no issues found")
            return ToolResult(
                tc.id,
                f"TYPE CHECK ISSUES:\n{output[-2000:]}",
                is_error=True,
            )
        except TimeoutError:
            return ToolResult(tc.id, "Type check timed out after 30s", True)
        except FileNotFoundError:
            return ToolResult(tc.id, "mypy not found — type checking unavailable")
        except Exception as exc:
            return ToolResult(tc.id, f"Type check error: {exc}", True)

    async def _tool_dependency_graph(self, tc: ToolCall) -> ToolResult:
        """Show what a module imports and what imports it."""
        module_path = tc.input.get("module_path", "")
        target = (self._root / module_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        if not target.exists():
            return ToolResult(tc.id, f"File not found: {module_path}", True)

        try:
            source = target.read_text(encoding="utf-8")
            tree = ast.parse(source, filename=module_path)
        except Exception as exc:
            return ToolResult(tc.id, f"Parse error: {exc}", True)

        # Extract this module's imports
        imports: list[str] = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom) and node.module:
                names = ", ".join(a.name for a in (node.names or []))
                imports.append(f"from {node.module} import {names}")

        # Find files that import this module
        module_name = self._path_to_module(module_path)
        importers: list[str] = []
        if module_name:
            src_dir = self._root / "src"
            if src_dir.exists():
                short_parts = module_name.split(".")
                # Search for imports of this module
                for py_file in src_dir.rglob("*.py"):
                    if py_file.resolve() == target:
                        continue
                    try:
                        file_source = py_file.read_text(encoding="utf-8")
                        # Quick string check before expensive parse
                        if module_name not in file_source and short_parts[-1] not in file_source:
                            continue
                        file_tree = ast.parse(file_source)
                        for node in ast.walk(file_tree):
                            if isinstance(node, ast.ImportFrom) and node.module:
                                if module_name in node.module or (
                                    ".".join(short_parts[:-1]) in node.module
                                    and any(a.name == short_parts[-1] for a in (node.names or []))
                                ):
                                    importers.append(str(py_file.relative_to(self._root)))
                                    break
                            elif isinstance(node, ast.Import):
                                for alias in node.names:
                                    if module_name in alias.name:
                                        importers.append(str(py_file.relative_to(self._root)))
                                        break
                    except Exception:
                        continue

        lines = [f"=== Dependency Graph for {module_path} ===\n"]
        lines.append(f"Module: {module_name or 'unknown'}\n")
        lines.append(f"--- This module imports ({len(imports)}) ---")
        for imp in imports:
            lines.append(f"  {imp}")
        lines.append(f"\n--- Imported by ({len(importers)}) ---")
        for imp in importers:
            lines.append(f"  {imp}")

        return ToolResult(tc.id, "\n".join(lines))

    async def _tool_read_spec(self, tc: ToolCall) -> ToolResult:
        """Read an EcodiaOS specification document."""
        spec_name = tc.input.get("spec_name", "").lower().strip()
        spec_file = _SPEC_FILE_MAP.get(spec_name)

        if spec_file is None:
            available = ", ".join(sorted(_SPEC_FILE_MAP.keys()))
            return ToolResult(
                tc.id,
                f"Unknown spec: {spec_name!r}. Available: {available}",
                True,
            )

        target = self._root / spec_file
        if not target.exists():
            return ToolResult(tc.id, f"Spec file not found: {spec_file}", True)

        try:
            content = target.read_text(encoding="utf-8")
            # Truncate to 4000 chars to stay within token budget
            if len(content) > 4000:
                content = content[:4000] + "\n\n[... truncated — use read_file for full content ...]"
            return ToolResult(tc.id, content)
        except Exception as exc:
            return ToolResult(tc.id, f"Read error: {exc}", True)

    async def _build_code_index(self) -> dict[str, list[float]]:
        """
        Lazy-build a semantic index of Python files in the codebase.

        Embeds the first ~500 chars of each Python file (module docstring +
        imports + top-level definitions) to create a searchable code index.
        Cached for the lifetime of this agent instance.
        """
        async with self._code_index_lock:
            if self._code_index is not None:
                return self._code_index

            if self._embedding is None:
                self._code_index = {}
                return self._code_index

            # Collect Python files (skip __pycache__, .venv, tests, migrations)
            skip_dirs = {"__pycache__", ".venv", "venv", "node_modules", ".git", "migrations"}
            py_files: list[tuple[str, str]] = []  # (rel_path, summary_text)

            for py_file in self._root.rglob("*.py"):
                # Skip excluded directories
                if any(part in skip_dirs for part in py_file.parts):
                    continue
                rel = str(py_file.relative_to(self._root)).replace("\\", "/")
                try:
                    content = py_file.read_text(encoding="utf-8")
                    # Take module-level summary: docstring + first 500 chars
                    summary = f"File: {rel}\n{content[:500]}"
                    py_files.append((rel, summary))
                except Exception:
                    continue

            if not py_files:
                self._code_index = {}
                return self._code_index

            # Embed in batches
            paths = [p for p, _ in py_files]
            texts = [t for _, t in py_files]

            try:
                embeddings = await self._embedding.embed_batch(texts)
                self._code_index = dict(zip(paths, embeddings, strict=False))
                self._logger.info(
                    "code_index_built",
                    files_indexed=len(self._code_index),
                )
            except Exception as exc:
                self._logger.warning("code_index_build_failed", error=str(exc))
                self._code_index = {}

            return self._code_index

    async def _semantic_find_similar(
        self, description: str, top_k: int = 5, threshold: float = 0.4
    ) -> list[tuple[str, float]]:
        """
        Find files semantically similar to the description using embeddings.

        Returns list of (rel_path, similarity_score) sorted by score descending.
        Uses embed_query() for Voyage clients (query-optimized) or embed() otherwise.
        """
        if self._embedding is None:
            return []

        code_index = await self._build_code_index()
        if not code_index:
            return []

        # Embed the query — use query-optimized encoding for Voyage
        try:
            if isinstance(self._embedding, VoyageEmbeddingClient):
                query_vec = await self._embedding.embed_query(description)
            else:
                query_vec = await self._embedding.embed(description)
        except Exception as exc:
            self._logger.warning("semantic_search_embed_failed", error=str(exc))
            return []

        # Compute similarities and rank
        scored: list[tuple[str, float]] = []
        for path, doc_vec in code_index.items():
            sim = cosine_similarity(query_vec, doc_vec)
            if sim >= threshold:
                scored.append((path, sim))

        scored.sort(key=lambda x: x[1], reverse=True)
        return scored[:top_k]

    async def _tool_find_similar(self, tc: ToolCall) -> ToolResult:
        """Find existing implementations similar to what needs to be built.

        Two-tier search:
          1. Keyword matching against _SIMILAR_CODE_MAP (fast, exact)
          2. Semantic embedding search via voyage-code-3 (deep, fuzzy)
        Falls back from tier 1 → tier 2 when keywords don't match.
        """
        description = tc.input.get("description", "").lower()

        # ── Tier 1: Keyword matching ─────────────────────────────────────────
        matched_paths: list[str] = []
        for keyword, paths in _SIMILAR_CODE_MAP.items():
            if keyword in description:
                matched_paths.extend(paths)
                break

        if not matched_paths:
            words = description.split()
            for word in words:
                if len(word) > 3:
                    for keyword, paths in _SIMILAR_CODE_MAP.items():
                        if word in keyword or keyword in word:
                            matched_paths.extend(paths)
                            break
                if matched_paths:
                    break

        # ── Tier 2: Semantic embedding search ────────────────────────────────
        semantic_paths: list[tuple[str, float]] = []
        if not matched_paths and self._embedding is not None:
            semantic_paths = await self._semantic_find_similar(description)
            matched_paths = [p for p, _ in semantic_paths]

        if not matched_paths:
            return ToolResult(
                tc.id,
                "No similar implementations found. Try search_code with a specific pattern.",
            )

        # ── Read matched files ───────────────────────────────────────────────
        results: list[str] = []
        chars_remaining = 4000

        # If semantic search was used, prepend similarity scores
        if semantic_paths:
            score_header = "Semantic similarity results:\n" + "\n".join(
                f"  {p} (score: {s:.3f})" for p, s in semantic_paths
            )
            results.append(score_header)
            chars_remaining -= len(score_header)

        for rel_path in matched_paths:
            if chars_remaining <= 0:
                break
            target = self._root / rel_path
            if target.is_file():
                try:
                    content = target.read_text(encoding="utf-8")
                    chunk = content[:min(2500, chars_remaining)]
                    results.append(f"=== {rel_path} ===\n{chunk}")
                    chars_remaining -= len(results[-1])
                except Exception:
                    continue
            elif target.is_dir():
                try:
                    py_files = sorted(target.glob("*.py"))
                    file_list = ", ".join(f.name for f in py_files)
                    results.append(f"=== {rel_path} ===\nFiles: {file_list}")
                    chars_remaining -= len(results[-1])

                    for py_file in py_files:
                        if py_file.name == "__init__.py" or chars_remaining <= 0:
                            continue
                        content = py_file.read_text(encoding="utf-8")
                        chunk = content[:min(2000, chars_remaining)]
                        rel = str(py_file.relative_to(self._root))
                        results.append(f"\n=== {rel} (exemplar) ===\n{chunk}")
                        chars_remaining -= len(results[-1])
                        break
                except Exception:
                    continue

        return ToolResult(tc.id, "\n\n".join(results) if results else "No files found at matched paths")

    # ─── Helpers ─────────────────────────────────────────────────────────────

    def _check_forbidden_path(self, rel_path: str) -> str | None:
        """Check if a path is forbidden. Returns error message or None."""
        from ecodiaos.systems.simula.types import FORBIDDEN_WRITE_PATHS
        for forbidden in FORBIDDEN_WRITE_PATHS:
            if rel_path.startswith(forbidden) or forbidden in rel_path:
                return (
                    f"IRON RULE VIOLATION: Cannot write to forbidden path '{rel_path}' "
                    f"(matches forbidden pattern '{forbidden}'). "
                    "This change would violate Simula's constitutional constraints."
                )
        return None

    def _path_to_module(self, rel_path: str) -> str | None:
        """Convert a relative file path to a dotted module name."""
        parts = rel_path.replace("\\", "/").split("/")
        if parts and parts[0] == "src":
            parts = parts[1:]
        if not parts:
            return None
        if parts[-1].endswith(".py"):
            parts[-1] = parts[-1][:-3]
        if parts[-1] == "__init__":
            parts = parts[:-1]
        return ".".join(parts) if parts else None

    def _build_system_prompt(self, proposal: EvolutionProposal) -> str:
        from ecodiaos.systems.simula.types import FORBIDDEN_WRITE_PATHS, SIMULA_IRON_RULES

        architecture_context = _build_architecture_context(
            category=proposal.category,
            codebase_root=self._root,
        )

        prompt = _SYSTEM_PROMPT_TEMPLATE.format(
            category=proposal.category.value,
            description=proposal.description,
            expected_benefit=proposal.expected_benefit,
            evidence=", ".join(proposal.evidence) or "none",
            iron_rules="\n".join(f"- {r}" for r in SIMULA_IRON_RULES),
            forbidden_paths="\n".join(f"- {p}" for p in FORBIDDEN_WRITE_PATHS),
            architecture_context=architecture_context,
        )

        # Stage 3C: Append LILO library abstractions if available
        if self._lilo_prompt:
            prompt += f"\n\n{self._lilo_prompt}"

        # Stage 4A: Append proof library context if available
        if self._proof_library_prompt:
            prompt += self._proof_library_prompt

        return prompt

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\health.py =====

"""
EcodiaOS -- Simula Health Checker

After a change is applied, the health checker verifies the codebase
is still functional. Six check phases run in sequence:
  1. Syntax check -- ast.parse() on all written Python files
  2. Import check -- attempt to import the affected module
  3. Unit tests -- run pytest on the affected system's test directory
  4. Formal verification (Stage 2) -- Dafny + Z3 + static analysis
  5. Lean 4 proof verification (Stage 4A) -- DeepSeek-Prover-V2 pattern
  6. Formal guarantees (Stage 6) -- E-graph equivalence + symbolic execution

If any blocking check fails, Simula rolls back the change. The goal is
to never leave EOS in a broken state.

Phase 4 (formal verification) runs with independent timeout budgets:
  - Dafny: blocking for triggerable categories (MODIFY_CONTRACT, ADD_SYSTEM_CAPABILITY)
  - Z3: advisory by default; graduates to blocking in Stage 3 (z3_blocking=True)
  - Static analysis: blocking for ERROR-severity findings

Phase 5 (Lean 4) runs for categories that require proof-level assurance:
  - Blocking when lean_blocking=True (default for high-risk categories)
  - Advisory otherwise; proved lemmas are stored in the proof library

Phase 6 (formal guarantees) runs e-graph equivalence (6D) and symbolic
execution (6E) checks:
  - E-graph: advisory by default (egraph_blocking=False); verifies semantic
    equivalence of code rewrites via equality saturation
  - Symbolic execution: blocking by default (symbolic_execution_blocking=True);
    proves mission-critical properties (budget, access control, risk scoring)
    via Z3 SMT solving
"""

from __future__ import annotations

import ast
import asyncio
import contextlib
import importlib.util
import time
from pathlib import Path
from typing import TYPE_CHECKING

import structlog

from ecodiaos.systems.simula.types import HealthCheckResult
from ecodiaos.systems.simula.verification.types import (
    LEAN_PROOF_CATEGORIES,
    DafnyVerificationResult,
    FormalGuaranteesResult,
    FormalVerificationResult,
    InvariantVerificationResult,
    LeanVerificationResult,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.systems.simula.egraph.equality_saturation import EqualitySaturationEngine
    from ecodiaos.systems.simula.types import EvolutionProposal
    from ecodiaos.systems.simula.verification.dafny_bridge import DafnyBridge
    from ecodiaos.systems.simula.verification.lean_bridge import LeanBridge
    from ecodiaos.systems.simula.verification.static_analysis import StaticAnalysisBridge
    from ecodiaos.systems.simula.verification.symbolic_execution import SymbolicExecutionEngine
    from ecodiaos.systems.simula.verification.z3_bridge import Z3Bridge

logger = structlog.get_logger().bind(system="simula.health")


class HealthChecker:
    """
    Verifies post-apply codebase health via syntax, import, test,
    and formal verification checks. Any blocking failure triggers rollback.
    """

    def __init__(
        self,
        codebase_root: Path,
        test_command: str = "pytest",
        dafny_bridge: DafnyBridge | None = None,
        z3_bridge: Z3Bridge | None = None,
        static_analysis_bridge: StaticAnalysisBridge | None = None,
        llm: LLMProvider | None = None,
        z3_blocking: bool = False,
        # Stage 4A: Lean 4 proof verification
        lean_bridge: LeanBridge | None = None,
        lean_blocking: bool = True,
    ) -> None:
        self._root = codebase_root
        self._test_command = test_command
        self._dafny = dafny_bridge
        self._z3 = z3_bridge
        self._static_analysis = static_analysis_bridge
        self._llm = llm
        self._z3_blocking = z3_blocking  # Stage 3: Z3 graduates to blocking
        # Stage 4A: Lean 4
        self._lean = lean_bridge
        self._lean_blocking = lean_blocking
        # Stage 6D: E-graph equivalence (wired by service.py)
        self._egraph: EqualitySaturationEngine | None = None
        self._egraph_blocking: bool = False
        # Stage 6E: Symbolic execution (wired by service.py)
        self._symbolic_execution: SymbolicExecutionEngine | None = None
        self._symbolic_execution_blocking: bool = True
        self._symbolic_execution_domains: list[str] = []
        self._log = logger

    async def check(
        self,
        files_written: list[str],
        proposal: EvolutionProposal | None = None,
    ) -> HealthCheckResult:
        """""""""
        Run all health checks in sequence.  Returns on first failure.
        """""""""
        # 1. Syntax check
        syntax_errors = await self._check_syntax(files_written)
        if syntax_errors:
            self._log.warning("health_syntax_failed", errors=syntax_errors)
            return HealthCheckResult(healthy=False, issues=syntax_errors)
        self._log.info("health_syntax_passed", files=len(files_written))

        # 2. Import check
        import_errors = await self._check_imports(files_written)
        if import_errors:
            self._log.warning("health_import_failed", errors=import_errors)
            return HealthCheckResult(healthy=False, issues=import_errors)
        self._log.info("health_import_passed", files=len(files_written))

        # 3. Tests
        tests_passed, test_output = await self._run_tests(files_written)
        if not tests_passed:
            self._log.warning("health_tests_failed", output=test_output[:500])
            return HealthCheckResult(
                healthy=False,
                issues=[f"Test suite failed:\n{test_output[:1000]}"],
            )
        self._log.info("health_tests_passed")

        # 4. Formal verification (Stage 2) — runs with independent timeout
        formal_result = await self._run_formal_verification(
            files_written, proposal,
        )
        if formal_result is not None:
            if not formal_result.passed and formal_result.blocking_issues:
                self._log.warning(
                    "health_formal_verification_failed",
                    blocking=formal_result.blocking_issues,
                )
                return HealthCheckResult(
                    healthy=False,
                    issues=[
                        f"Formal verification failed: {issue}"
                        for issue in formal_result.blocking_issues
                    ],
                    formal_verification=formal_result,
                )
            if formal_result.advisory_issues:
                self._log.info(
                    "health_formal_verification_advisory",
                    advisory=formal_result.advisory_issues,
                )
            self._log.info("health_formal_verification_passed")

        # 5. Lean 4 proof verification (Stage 4A) — runs for proof-eligible categories
        lean_result = await self._run_lean_verification(
            files_written, proposal,
        )
        if lean_result is not None:
            if lean_result.status.value == "failed" and self._lean_blocking:
                self._log.warning(
                    "health_lean_verification_failed",
                    status=lean_result.status.value,
                    attempts=len(lean_result.attempts),
                )
                return HealthCheckResult(
                    healthy=False,
                    issues=[
                        f"Lean 4 proof verification failed after {len(lean_result.attempts)} attempts"
                    ],
                    formal_verification=formal_result,
                    lean_verification=lean_result,
                )
            self._log.info(
                "health_lean_verification_complete",
                status=lean_result.status.value,
                proven_lemmas=len(lean_result.proven_lemmas),
                copilot_rate=f"{lean_result.copilot_automation_rate:.0%}",
            )

        # 6. Formal guarantees (Stage 6D + 6E) — e-graph equivalence + symbolic execution
        fg_result = await self._run_formal_guarantees(files_written, proposal)
        if fg_result is not None:
            if fg_result.blocking_issues:
                self._log.warning(
                    "health_formal_guarantees_failed",
                    blocking=fg_result.blocking_issues,
                )
                return HealthCheckResult(
                    healthy=False,
                    issues=[
                        f"Formal guarantee failed: {issue}"
                        for issue in fg_result.blocking_issues
                    ],
                    formal_verification=formal_result,
                    lean_verification=lean_result,
                    formal_guarantees=fg_result,
                )
            if fg_result.advisory_issues:
                self._log.info(
                    "health_formal_guarantees_advisory",
                    advisory=fg_result.advisory_issues,
                )
            self._log.info("health_formal_guarantees_passed")

        if formal_result is not None or lean_result is not None or fg_result is not None:
            return HealthCheckResult(
                healthy=True,
                formal_verification=formal_result,
                lean_verification=lean_result,
                formal_guarantees=fg_result,
            )

        return HealthCheckResult(healthy=True)

    async def _check_syntax(self, files: list[str]) -> list[str]:
        """""""""
        Parse each .py file with ast.parse().  Collect syntax errors.
        Returns a list of error strings (empty list = all pass).
        """""""""
        errors: list[str] = []
        for filepath in files:
            if not filepath.endswith(".py"):
                continue
            path = Path(filepath)
            if not path.exists():
                errors.append(f"Syntax check: file not found: {filepath}")
                continue
            try:
                source = path.read_text(encoding="utf-8")
                ast.parse(source, filename=filepath)
            except SyntaxError as exc:
                errors.append(f"Syntax error in {filepath}:{exc.lineno}: {exc.msg}")
            except Exception as exc:
                errors.append(f"Failed to read {filepath}: {exc}")
        return errors

    async def _check_imports(self, files: list[str]) -> list[str]:
        """""""""
        Derive dotted module paths from written file paths and check
        whether importlib can locate them.  Returns error strings.
        """""""""
        errors: list[str] = []
        for filepath in files:
            if not filepath.endswith(".py"):
                continue
            module_path = self._derive_module_path(filepath)
            if module_path is None:
                continue
            try:
                spec = importlib.util.find_spec(module_path)
                if spec is None:
                    errors.append(f"Import check: module not found: {module_path}")
            except ModuleNotFoundError as exc:
                errors.append(f"Import check: {module_path}: {exc}")
            except Exception as exc:
                errors.append(f"Import check failed for {module_path}: {exc}")
        return errors

    def _derive_module_path(self, src_file: str) -> str | None:
        """""""""
        Convert a source file path to a dotted module path.
        Example: src/ecodiaos/systems/axon/executors/my.py
                 -> ecodiaos.systems.axon.executors.my
        """""""""
        try:
            path = Path(src_file)
            # Make relative to codebase root if possible
            try:
                rel = path.relative_to(self._root)
            except ValueError:
                rel = path
            parts = list(rel.parts)
            if parts and parts[0] == "src":
                parts = parts[1:]
            # Strip .py extension from last part
            if parts:
                parts[-1] = parts[-1].removesuffix(".py")
            return ".".join(parts) if parts else None
        except Exception:
            return None

    async def _run_tests(self, files: list[str]) -> tuple[bool, str]:
        """""""""
        Derive the test directory from the written files and run pytest.
        Returns (passed, output).  If no test directory found, returns (True, ...).
        30-second subprocess timeout.
        """""""""
        test_path = None
        for filepath in files:
            candidate = self._derive_test_path(filepath)
            if candidate:
                test_dir = Path(candidate)
                if test_dir.is_dir():
                    test_path = candidate
                    break

        if test_path is None:
            self._log.info("health_no_tests", files=files)
            return True, "no tests found"

        try:
            proc = await asyncio.create_subprocess_exec(
                self._test_command,
                test_path,
                "-x",
                "--tb=short",
                "-q",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                cwd=str(self._root),
            )
            try:
                stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=30.0)
            except TimeoutError:
                proc.kill()
                await proc.communicate()
                return False, "Test run timed out after 30s"
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            self._log.info(
                "health_test_run",
                test_path=test_path,
                passed=passed,
                returncode=proc.returncode,
            )
            return passed, output
        except FileNotFoundError:
            msg = f"Test command {self._test_command!r} not found"
            self._log.warning("health_test_command_missing", command=self._test_command)
            return False, msg
        except Exception as exc:
            return False, f"Test run error: {exc}"

    def _derive_test_path(self, src_file: str) -> str | None:
        """""""""
        Map a source file path to a test directory path.
        Example: src/ecodiaos/systems/axon/executor.py
                 -> tests/unit/systems/axon/
        """""""""
        try:
            path = Path(src_file)
            try:
                rel = path.relative_to(self._root)
            except ValueError:
                rel = path
            parts = list(rel.parts)
            # Expect: src / ecodiaos / systems / <system_name> / ...
            if len(parts) >= 4 and parts[0] == "src" and parts[2] == "systems":
                system_name = parts[3]
                test_path = self._root / "tests" / "unit" / "systems" / system_name
                return str(test_path)
            return None
        except Exception:
            return None

    # ── Stage 2: Formal Verification Phase ────────────────────────────────────

    async def _run_formal_verification(
        self,
        files_written: list[str],
        proposal: EvolutionProposal | None,
    ) -> FormalVerificationResult | None:
        """
        Run Dafny, Z3, and static analysis in parallel.

        Returns None if no verification bridges are configured.
        Returns FormalVerificationResult with pass/fail and issues.
        """
        from ecodiaos.systems.simula.verification.types import (
            DAFNY_TRIGGERABLE_CATEGORIES,
            FormalVerificationResult,
        )

        if not any([self._dafny, self._z3, self._static_analysis]):
            return None

        start = time.monotonic()
        blocking_issues: list[str] = []
        advisory_issues: list[str] = []
        dafny_result = None
        z3_result = None
        static_result = None

        # Build parallel tasks
        tasks: dict[str, asyncio.Task[object]] = {}

        # Dafny: run for triggerable categories only
        if (
            self._dafny is not None
            and self._llm is not None
            and proposal is not None
            and proposal.category in DAFNY_TRIGGERABLE_CATEGORIES
        ):
            tasks["dafny"] = asyncio.create_task(
                self._run_dafny_verification(proposal),
            )

        # Z3: run for all proposals when enabled
        if (
            self._z3 is not None
            and self._llm is not None
            and proposal is not None
        ):
            tasks["z3"] = asyncio.create_task(
                self._run_z3_verification(proposal, files_written),
            )

        # Static analysis: run for all Python files
        if self._static_analysis is not None:
            tasks["static"] = asyncio.create_task(
                self._static_analysis.run_all(files_written),
            )

        if not tasks:
            return None

        # Await all tasks
        results = await asyncio.gather(
            *tasks.values(), return_exceptions=True,
        )
        task_results = dict(zip(tasks.keys(), results, strict=False))

        # Process Dafny result
        if "dafny" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                DafnyVerificationResult,
                DafnyVerificationStatus,
            )
            raw = task_results["dafny"]
            if isinstance(raw, DafnyVerificationResult):
                dafny_result = raw
                if raw.status != DafnyVerificationStatus.VERIFIED:
                    msg = f"Dafny verification {raw.status.value}: {raw.error_summary}"
                    blocking_issues.append(msg)
            elif isinstance(raw, Exception):
                self._log.warning("dafny_exception", error=str(raw))
                advisory_issues.append(f"Dafny verification error: {raw}")

        # Process Z3 result
        if "z3" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                InvariantVerificationResult,
                InvariantVerificationStatus,
            )
            raw = task_results["z3"]
            if isinstance(raw, InvariantVerificationResult):
                z3_result = raw
                if self._z3_blocking:
                    # Stage 3: Z3 graduates to blocking — invalid invariants fail the check
                    invalid_count = sum(
                        1 for i in raw.discovered_invariants
                        if i.status == InvariantVerificationStatus.INVALID
                    )
                    if invalid_count > 0:
                        blocking_issues.append(
                            f"Z3 found {invalid_count} invalid invariants (blocking mode)"
                        )
                    if raw.valid_invariants:
                        advisory_issues.append(
                            f"Z3 discovered {len(raw.valid_invariants)} valid invariants"
                        )
                else:
                    # Advisory mode (Stage 2 default)
                    if raw.valid_invariants:
                        advisory_issues.append(
                            f"Z3 discovered {len(raw.valid_invariants)} valid invariants"
                        )
            elif isinstance(raw, Exception):
                self._log.warning("z3_exception", error=str(raw))

        # Process static analysis result
        if "static" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                StaticAnalysisResult,
            )
            raw = task_results["static"]
            if isinstance(raw, StaticAnalysisResult):
                static_result = raw
                if raw.error_count > 0:
                    blocking_issues.append(
                        f"Static analysis found {raw.error_count} ERROR-severity issues"
                    )
                if raw.warning_count > 0:
                    advisory_issues.append(
                        f"Static analysis found {raw.warning_count} warnings"
                    )
            elif isinstance(raw, Exception):
                self._log.warning("static_analysis_exception", error=str(raw))

        passed = len(blocking_issues) == 0
        total_time_ms = int((time.monotonic() - start) * 1000)

        return FormalVerificationResult(
            dafny=dafny_result,
            z3=z3_result,
            static_analysis=static_result,
            passed=passed,
            blocking_issues=blocking_issues,
            advisory_issues=advisory_issues,
            total_verification_time_ms=total_time_ms,
        )

    async def _run_dafny_verification(
        self, proposal: EvolutionProposal,
    ) -> DafnyVerificationResult:
        """Run Dafny Clover loop for the proposal."""
        from ecodiaos.systems.simula.verification.templates import get_template
        from ecodiaos.systems.simula.verification.types import (
            DafnyVerificationResult,
            DafnyVerificationStatus,
        )

        assert self._dafny is not None
        assert self._llm is not None

        # Check Dafny availability
        if not await self._dafny.check_available():
            self._log.info("dafny_not_available_skipping")
            return DafnyVerificationResult(
                status=DafnyVerificationStatus.SKIPPED,
                error_summary="Dafny binary not available",
            )

        # Get template if available
        template = get_template(proposal.category.value)

        # Build context from the change spec
        python_source = ""
        function_name = ""
        context = proposal.description
        if proposal.change_spec:
            context = getattr(proposal.change_spec, "description", None) or proposal.description
            function_name = getattr(proposal.change_spec, "target_system", None) or ""

        return await self._dafny.run_clover_loop(
            llm=self._llm,
            python_source=python_source,
            function_name=function_name,
            context=context,
            template=template,
        )

    async def _run_z3_verification(
        self,
        proposal: EvolutionProposal,
        files_written: list[str],
    ) -> InvariantVerificationResult:
        """Run Z3 invariant discovery for the proposal."""
        from ecodiaos.systems.simula.verification.types import (
            InvariantVerificationResult,
            InvariantVerificationStatus,
        )

        assert self._z3 is not None
        assert self._llm is not None

        # Gather Python source from written files for invariant discovery
        python_source_parts: list[str] = []
        target_functions: list[str] = []
        for filepath in files_written:
            if not filepath.endswith(".py"):
                continue
            full = self._root / filepath
            if full.is_file():
                try:
                    content = full.read_text(encoding="utf-8")
                    python_source_parts.append(
                        f"# --- {filepath} ---\n{content}"
                    )
                    # Extract function names for targeting
                    import ast as _ast
                    try:
                        tree = _ast.parse(content)
                        for node in _ast.walk(tree):
                            if isinstance(node, (_ast.FunctionDef, _ast.AsyncFunctionDef)):
                                target_functions.append(node.name)
                    except SyntaxError:
                        pass
                except Exception:
                    continue

        if not python_source_parts:
            return InvariantVerificationResult(
                status=InvariantVerificationStatus.SKIPPED,
                error_summary="No Python source to analyze",
            )

        python_source = "\n\n".join(python_source_parts)
        domain_context = proposal.description
        if proposal.change_spec:
            domain_context = getattr(proposal.change_spec, "description", None) or proposal.description

        return await self._z3.run_discovery_loop(
            llm=self._llm,
            python_source=python_source,
            target_functions=target_functions[:10],  # Limit to top 10
            domain_context=domain_context,
        )

    # ── Stage 4A: Lean 4 Proof Verification Phase ─────────────────────────────

    async def _run_lean_verification(
        self,
        files_written: list[str],
        proposal: EvolutionProposal | None,
    ) -> LeanVerificationResult | None:
        """
        Run Lean 4 proof generation for proposals in proof-eligible categories.

        Returns None if Lean bridge is not configured or proposal is not eligible.
        Returns LeanVerificationResult with proof status and discovered lemmas.
        """
        if self._lean is None or self._llm is None or proposal is None:
            return None

        # Only run Lean proofs for categories that warrant formal proof
        if proposal.category not in LEAN_PROOF_CATEGORIES:
            return None

        # Check Lean 4 availability
        if not await self._lean.check_available():
            self._log.info("lean_not_available_skipping")
            from ecodiaos.systems.simula.verification.types import LeanProofStatus
            return LeanVerificationResult(
                status=LeanProofStatus.SKIPPED,
            )

        # Build proof context from the proposal
        python_source_parts: list[str] = []
        for filepath in files_written:
            if not filepath.endswith(".py"):
                continue
            full = self._root / filepath
            if full.is_file():
                try:
                    content = full.read_text(encoding="utf-8")
                    python_source_parts.append(
                        f"# --- {filepath} ---\n{content}"
                    )
                except Exception:
                    continue

        python_source = "\n\n".join(python_source_parts)
        domain_context = proposal.description
        if proposal.change_spec:
            domain_context = proposal.change_spec.additional_context or proposal.description

        function_name = ""
        if proposal.change_spec:
            function_name = getattr(proposal.change_spec, "target_system", "") or ""

        try:
            result = await self._lean.generate_proof(
                llm=self._llm,
                python_source=python_source,
                function_name=function_name,
                property_description=domain_context,
                proposal_id=proposal.id,
            )
            return result
        except TimeoutError:
            self._log.warning("lean_verification_timeout", proposal_id=proposal.id)
            from ecodiaos.systems.simula.verification.types import LeanProofStatus
            return LeanVerificationResult(
                status=LeanProofStatus.TIMEOUT,
            )
        except Exception as exc:
            self._log.warning("lean_verification_error", error=str(exc))
            return None

    # ── Stage 6: Formal Guarantees Phase ─────────────────────────────────────

    async def _run_formal_guarantees(
        self,
        files_written: list[str],
        proposal: EvolutionProposal | None,
    ) -> FormalGuaranteesResult | None:
        """
        Run Stage 6D (e-graph equivalence) and 6E (symbolic execution) checks.

        Returns None if no Stage 6 subsystems are configured.
        Returns a FormalGuaranteesResult with pass/fail and issues.
        """
        from ecodiaos.systems.simula.verification.types import FormalGuaranteesResult

        if self._egraph is None and self._symbolic_execution is None:
            return None

        start = time.monotonic()
        blocking_issues: list[str] = []
        advisory_issues: list[str] = []
        egraph_result = None
        symbolic_result = None

        # Build parallel tasks
        tasks: dict[str, asyncio.Task[object]] = {}

        # 6D: E-graph equivalence — check if rewritten code is semantically equivalent
        if self._egraph is not None and proposal is not None:
            tasks["egraph"] = asyncio.create_task(
                self._run_egraph_check(files_written, proposal),
            )

        # 6E: Symbolic execution — prove mission-critical properties
        if self._symbolic_execution is not None:
            tasks["symbolic"] = asyncio.create_task(
                self._run_symbolic_execution(files_written),
            )

        if not tasks:
            return None

        # Await all tasks
        results = await asyncio.gather(
            *tasks.values(), return_exceptions=True,
        )
        task_results = dict(zip(tasks.keys(), results, strict=False))

        # Process e-graph result
        if "egraph" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                EGraphEquivalenceResult,
                EGraphStatus,
            )

            raw = task_results["egraph"]
            if isinstance(raw, EGraphEquivalenceResult):
                egraph_result = raw
                if raw.status == EGraphStatus.FAILED:
                    msg = "E-graph equivalence check failed: code is not semantically equivalent"
                    if self._egraph_blocking:
                        blocking_issues.append(msg)
                    else:
                        advisory_issues.append(msg)
                elif raw.status == EGraphStatus.TIMEOUT:
                    advisory_issues.append("E-graph equivalence check timed out")
                elif raw.semantically_equivalent:
                    advisory_issues.append(
                        f"E-graph confirmed semantic equivalence ({len(raw.rules_applied)} rules, "
                        f"{raw.iterations} iterations)"
                    )
            elif isinstance(raw, Exception):
                self._log.warning("egraph_exception", error=str(raw))

        # Process symbolic execution result
        if "symbolic" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                SymbolicExecutionResult,
            )

            raw = task_results["symbolic"]
            if isinstance(raw, SymbolicExecutionResult):
                symbolic_result = raw
                if raw.counterexamples:
                    msg = (
                        f"Symbolic execution found {len(raw.counterexamples)} counterexample(s) — "
                        f"mission-critical properties violated"
                    )
                    if self._symbolic_execution_blocking:
                        blocking_issues.append(msg)
                    else:
                        advisory_issues.append(msg)
                if raw.properties_proved > 0:
                    advisory_issues.append(
                        f"Symbolic execution proved {raw.properties_proved}/{raw.properties_checked} properties"
                    )
            elif isinstance(raw, Exception):
                self._log.warning("symbolic_execution_exception", error=str(raw))

        passed = len(blocking_issues) == 0
        total_time_ms = int((time.monotonic() - start) * 1000)

        return FormalGuaranteesResult(
            egraph=egraph_result,
            symbolic_execution=symbolic_result,
            passed=passed,
            blocking_issues=blocking_issues,
            advisory_issues=advisory_issues,
            total_duration_ms=total_time_ms,
        )

    async def _run_egraph_check(
        self,
        files_written: list[str],
        proposal: EvolutionProposal,
    ) -> object | None:
        """
        Run e-graph equivalence check on changed files.

        Compares original code (from rollback snapshot) with new code
        to verify semantic equivalence of the transformation.
        """
        from ecodiaos.systems.simula.verification.types import (
            EGraphEquivalenceResult,
            EGraphStatus,
        )

        assert self._egraph is not None

        # For each Python file, check if the rewrite preserved semantics
        # We focus on the first file that has a meaningful diff
        for filepath in files_written:
            if not filepath.endswith(".py"):
                continue
            full = self._root / filepath
            if not full.is_file():
                continue
            try:
                new_code = full.read_text(encoding="utf-8")
                # E-graph checks the code against itself (simplified form)
                # In production this would compare pre/post-apply snapshots
                result = await self._egraph.check_equivalence(new_code, new_code)
                return result
            except Exception as exc:
                self._log.warning("egraph_file_check_error", file=filepath, error=str(exc))
                continue

        return EGraphEquivalenceResult(
            status=EGraphStatus.SKIPPED,
        )

    async def _run_symbolic_execution(
        self,
        files_written: list[str],
    ) -> object | None:
        """
        Run symbolic execution on mission-critical functions in changed files.
        """
        from ecodiaos.systems.simula.verification.types import (
            SymbolicDomain,
            SymbolicExecutionResult,
            SymbolicExecutionStatus,
        )

        assert self._symbolic_execution is not None

        # Convert domain strings to enum values
        domains: list[SymbolicDomain] = []
        for d in self._symbolic_execution_domains:
            with contextlib.suppress(ValueError):
                domains.append(SymbolicDomain(d))

        if not domains:
            return SymbolicExecutionResult(
                status=SymbolicExecutionStatus.SKIPPED,
            )

        try:
            return await self._symbolic_execution.prove_properties(
                files=files_written,
                codebase_root=self._root,
                domains=domains,
            )
        except TimeoutError:
            self._log.warning("symbolic_execution_timeout")
            return SymbolicExecutionResult(
                status=SymbolicExecutionStatus.TIMEOUT,
            )
        except Exception as exc:
            self._log.warning("symbolic_execution_error", error=str(exc))
            return None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\history.py =====

"""
EcodiaOS -- Simula Evolution History

Maintains the complete, immutable record of all structural changes
applied to this EOS instance. Records are (:EvolutionRecord) nodes
in the Memory graph, linked as:
  (:ConfigVersion)-[:EVOLVED_FROM]->(:ConfigVersion)

Stage 1B enhancement: EvolutionRecord nodes store description embeddings
via voyage-code-3 in a Neo4j vector index for semantic similarity search
across the full evolution history.

The Honesty drive demands this record be permanent and complete.
No record can be deleted or modified after writing.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.simula.types import ConfigVersion, EvolutionRecord

if TYPE_CHECKING:
    from ecodiaos.clients.embedding import EmbeddingClient
    from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger().bind(system="simula.history")

# Voyage-code-3 produces 1024-dimensional embeddings
_EMBEDDING_DIMENSION = 1024
_VECTOR_INDEX_NAME = "evolution_record_embedding"


class EvolutionHistoryManager:
    """
    Writes and queries the immutable evolution history stored in Neo4j.
    Every applied structural change produces exactly one EvolutionRecord node.
    Records are never deleted or updated.

    With an embedding client configured, each record's description is embedded
    and stored in a Neo4j vector index for semantic similarity search.
    """

    def __init__(
        self,
        neo4j: Neo4jClient,
        embedding_client: EmbeddingClient | None = None,
    ) -> None:
        self._neo4j = neo4j
        self._embedding = embedding_client
        self._log = logger
        self._vector_index_ensured = False

    async def ensure_vector_index(self) -> None:
        """
        Create the Neo4j vector index on EvolutionRecord.embedding if it
        doesn't already exist. Idempotent — safe to call multiple times.

        Requires Neo4j 5.11+ with vector index support.
        """
        if self._vector_index_ensured:
            return

        try:
            await self._neo4j.execute_write(
                f"""
                CREATE VECTOR INDEX {_VECTOR_INDEX_NAME} IF NOT EXISTS
                FOR (r:EvolutionRecord)
                ON (r.embedding)
                OPTIONS {{
                    indexConfig: {{
                        `vector.dimensions`: {_EMBEDDING_DIMENSION},
                        `vector.similarity_function`: 'cosine'
                    }}
                }}
                """
            )
            self._vector_index_ensured = True
            self._log.info(
                "vector_index_ensured",
                index_name=_VECTOR_INDEX_NAME,
                dimension=_EMBEDDING_DIMENSION,
            )
        except Exception as exc:
            # Non-fatal: vector search degrades gracefully without the index
            self._log.warning(
                "vector_index_creation_failed",
                error=str(exc),
                hint="Neo4j 5.11+ required for vector indexes",
            )

    async def record(self, record: EvolutionRecord) -> None:
        """
        Write an immutable EvolutionRecord node to Neo4j.
        This is the permanent history of every structural change.

        If an embedding client is configured, the description is embedded
        and stored alongside the record for vector similarity search.
        """
        # Embed the description for vector search
        embedding: list[float] | None = None
        if self._embedding is not None:
            try:
                await self.ensure_vector_index()
                embedding = await self._embedding.embed(record.description)
            except Exception as exc:
                self._log.warning(
                    "embedding_generation_failed",
                    record_id=record.id,
                    error=str(exc),
                )

        # Build the CREATE query — include embedding property if available
        if embedding is not None:
            query = """
            CREATE (:EvolutionRecord {
                id: $id,
                proposal_id: $proposal_id,
                category: $category,
                description: $description,
                from_version: $from_version,
                to_version: $to_version,
                files_changed: $files_changed,
                simulation_risk: $simulation_risk,
                applied_at: $applied_at,
                rolled_back: $rolled_back,
                rollback_reason: $rollback_reason,
                simulation_episodes_tested: $simulation_episodes_tested,
                counterfactual_regression_rate: $counterfactual_regression_rate,
                dependency_blast_radius: $dependency_blast_radius,
                constitutional_alignment: $constitutional_alignment,
                resource_tokens_per_hour: $resource_tokens_per_hour,
                caution_reasoning: $caution_reasoning,
                created_at: $created_at,
                embedding: $embedding
            })
            """
        else:
            query = """
            CREATE (:EvolutionRecord {
                id: $id,
                proposal_id: $proposal_id,
                category: $category,
                description: $description,
                from_version: $from_version,
                to_version: $to_version,
                files_changed: $files_changed,
                simulation_risk: $simulation_risk,
                applied_at: $applied_at,
                rolled_back: $rolled_back,
                rollback_reason: $rollback_reason,
                simulation_episodes_tested: $simulation_episodes_tested,
                counterfactual_regression_rate: $counterfactual_regression_rate,
                dependency_blast_radius: $dependency_blast_radius,
                constitutional_alignment: $constitutional_alignment,
                resource_tokens_per_hour: $resource_tokens_per_hour,
                caution_reasoning: $caution_reasoning,
                created_at: $created_at
            })
            """

        params: dict[str, Any] = {
            "id": record.id,
            "proposal_id": record.proposal_id,
            "category": record.category.value,
            "description": record.description,
            "from_version": record.from_version,
            "to_version": record.to_version,
            "files_changed": record.files_changed,
            "simulation_risk": record.simulation_risk.value,
            "applied_at": record.applied_at.isoformat(),
            "rolled_back": record.rolled_back,
            "rollback_reason": record.rollback_reason,
            "simulation_episodes_tested": record.simulation_episodes_tested,
            "counterfactual_regression_rate": record.counterfactual_regression_rate,
            "dependency_blast_radius": record.dependency_blast_radius,
            "constitutional_alignment": record.constitutional_alignment,
            "resource_tokens_per_hour": record.resource_tokens_per_hour,
            "caution_reasoning": record.caution_reasoning,
            "created_at": record.created_at.isoformat(),
        }
        if embedding is not None:
            params["embedding"] = embedding

        await self._neo4j.execute_write(query, params)
        self._log.info(
            "evolution_recorded",
            record_id=record.id,
            proposal_id=record.proposal_id,
            category=record.category.value,
            from_version=record.from_version,
            to_version=record.to_version,
            has_embedding=embedding is not None,
        )

    async def find_similar_records(
        self,
        description: str,
        top_k: int = 5,
        min_score: float = 0.6,
    ) -> list[tuple[EvolutionRecord, float]]:
        """
        Find EvolutionRecords semantically similar to a description.

        Uses the Neo4j vector index to perform approximate nearest-neighbor
        search on stored embeddings. Returns (record, score) pairs.
        """
        if self._embedding is None:
            return []

        try:
            from ecodiaos.clients.embedding import VoyageEmbeddingClient

            if isinstance(self._embedding, VoyageEmbeddingClient):
                query_vec = await self._embedding.embed_query(description)
            else:
                query_vec = await self._embedding.embed(description)
        except Exception as exc:
            self._log.warning("similar_records_embed_failed", error=str(exc))
            return []

        try:
            rows = await self._neo4j.execute_read(
                """
                CALL db.index.vector.queryNodes(
                    $index_name, $top_k, $query_vector
                )
                YIELD node, score
                WHERE score >= $min_score
                RETURN node, score
                ORDER BY score DESC
                """,
                {
                    "index_name": _VECTOR_INDEX_NAME,
                    "top_k": top_k,
                    "query_vector": query_vec,
                    "min_score": min_score,
                },
            )
        except Exception as exc:
            self._log.warning("vector_search_failed", error=str(exc))
            return []

        results: list[tuple[EvolutionRecord, float]] = []
        for row in rows:
            try:
                data = dict(row["node"])
                # Remove embedding from reconstruction (not part of the model)
                data.pop("embedding", None)
                record = EvolutionRecord(**data)
                results.append((record, float(row["score"])))
            except Exception as exc:
                self._log.warning("record_reconstruction_failed", error=str(exc))
                continue

        return results

    async def record_version(self, version: ConfigVersion, previous_version: int | None) -> None:
        """""""""
        Write a ConfigVersion node and optionally chain it to the previous version.
        """""""""
        await self._neo4j.execute_write(
            """
            MERGE (:ConfigVersion {
                version: $version,
                timestamp: $timestamp,
                proposal_ids: $proposal_ids,
                config_hash: $config_hash
            })
            """,
            {
                "version": version.version,
                "timestamp": version.timestamp.isoformat(),
                "proposal_ids": version.proposal_ids,
                "config_hash": version.config_hash,
            },
        )
        if previous_version is not None:
            await self._neo4j.execute_write(
                """
                MATCH (new:ConfigVersion {version: $new_v})
                MATCH (prev:ConfigVersion {version: $prev_v})
                MERGE (new)-[:EVOLVED_FROM]->(prev)
                """,
                {"new_v": version.version, "prev_v": previous_version},
            )
        self._log.info(
            "config_version_recorded",
            version=version.version,
            previous_version=previous_version,
        )

    async def get_history(self, limit: int = 50) -> list[EvolutionRecord]:
        """""""""
        Retrieve the most recent N evolution records, newest first.
        """""""""
        rows = await self._neo4j.execute_read(
            """
            MATCH (r:EvolutionRecord)
            RETURN r
            ORDER BY r.created_at DESC
            LIMIT $limit
            """,
            {"limit": limit},
        )
        records: list[EvolutionRecord] = []
        for row in rows:
            data = row["r"]
            records.append(EvolutionRecord(**data))
        return records

    async def get_version_chain(self) -> list[ConfigVersion]:
        """""""""
        Retrieve all ConfigVersion nodes ordered by version ASC.
        """""""""
        rows = await self._neo4j.execute_read(
            """
            MATCH (v:ConfigVersion)
            RETURN v
            ORDER BY v.version ASC
            """
        )
        versions: list[ConfigVersion] = []
        for row in rows:
            data = row["v"]
            versions.append(ConfigVersion(**data))
        return versions

    async def get_current_version(self) -> int:
        """""""""
        Return the highest config version number, or 0 if none exists.
        """""""""
        rows = await self._neo4j.execute_read(
            """
            MATCH (v:ConfigVersion)
            RETURN max(v.version) AS max_version
            """
        )
        if not rows or rows[0]["max_version"] is None:
            return 0
        return int(rows[0]["max_version"])

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\proposal_intelligence.py =====

"""
EcodiaOS -- Simula Proposal Intelligence

Smart proposal management: deduplication, prioritization, dependency
analysis, and cost estimation. Maximizes evolution quality per LLM
token by using cheap heuristics first and LLM only for ambiguous cases.

Key design:
  - Deduplication: 3-tier (exact prefix → category overlap → LLM similarity)
  - Prioritization: formula-based scoring, no LLM needed
  - Dependency analysis: rule-based ordering, no LLM needed
  - Cost estimation: heuristic lookup table, no LLM needed

Budget impact: Zero LLM tokens for normal operation.
LLM used only when >5 proposals need semantic deduplication (~300 tokens).
"""

from __future__ import annotations

import time
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.simula.types import (
    ChangeCategory,
    EvolutionProposal,
    ProposalCluster,
    ProposalPriority,
    ProposalStatus,
    RiskLevel,
)

if TYPE_CHECKING:
    from ecodiaos.clients.embedding import EmbeddingClient
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine

logger = structlog.get_logger().bind(system="simula.intelligence")

# Cost heuristics by category (0.0-1.0 scale)
_CATEGORY_COST: dict[ChangeCategory, float] = {
    ChangeCategory.ADJUST_BUDGET: 0.1,
    ChangeCategory.ADD_EXECUTOR: 0.4,
    ChangeCategory.ADD_INPUT_CHANNEL: 0.4,
    ChangeCategory.ADD_PATTERN_DETECTOR: 0.4,
    ChangeCategory.MODIFY_CONTRACT: 0.7,
    ChangeCategory.MODIFY_CYCLE_TIMING: 0.5,
    ChangeCategory.CHANGE_CONSOLIDATION: 0.6,
    ChangeCategory.ADD_SYSTEM_CAPABILITY: 0.9,
}

# Impact heuristics by category (0.0-1.0 scale)
_CATEGORY_IMPACT: dict[ChangeCategory, float] = {
    ChangeCategory.ADJUST_BUDGET: 0.3,
    ChangeCategory.ADD_EXECUTOR: 0.6,
    ChangeCategory.ADD_INPUT_CHANNEL: 0.7,
    ChangeCategory.ADD_PATTERN_DETECTOR: 0.5,
    ChangeCategory.MODIFY_CONTRACT: 0.8,
    ChangeCategory.MODIFY_CYCLE_TIMING: 0.4,
    ChangeCategory.CHANGE_CONSOLIDATION: 0.5,
    ChangeCategory.ADD_SYSTEM_CAPABILITY: 0.9,
}

# Minimum description prefix length for exact dedup matching
_DEDUP_PREFIX_LEN: int = 50

# Minimum proposals before triggering LLM-based dedup
_LLM_DEDUP_THRESHOLD: int = 5


class ProposalIntelligence:
    """
    Smart proposal management for Simula.

    Provides deduplication, prioritization, dependency analysis,
    and cost estimation — all optimized for minimal token usage.

    Stage 1B upgrade: Tier 3 dedup now uses voyage-code-3 embeddings
    for cosine similarity instead of LLM-based text comparison.
    This is both cheaper (no LLM tokens) and more precise.
    """

    # Cosine similarity threshold for embedding-based dedup
    _EMBEDDING_DEDUP_THRESHOLD: float = 0.85

    def __init__(
        self,
        llm: LLMProvider | None = None,
        analytics: EvolutionAnalyticsEngine | None = None,
        embedding_client: EmbeddingClient | None = None,
    ) -> None:
        self._llm = llm
        self._analytics = analytics
        self._embeddings = embedding_client
        self._log = logger
        # Dedup precision tracking (Stage 1B.5)
        self._dedup_stats = {
            "tier1_matches": 0,
            "tier2_matches": 0,
            "tier3_embedding_matches": 0,
            "tier3_llm_fallback_matches": 0,
            "embedding_dedup_calls": 0,
            "embedding_dedup_latency_ms": 0.0,
        }

    # ─── Prioritization ──────────────────────────────────────────────────────

    async def prioritize(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalPriority]:
        """
        Score and rank proposals by:
          priority = evidence_strength * expected_impact / max(0.1, risk * cost)

        Pure heuristic scoring — zero LLM tokens.
        Proposals with higher scores should be processed first.
        """
        priorities: list[ProposalPriority] = []

        for proposal in proposals:
            evidence_strength = self._compute_evidence_strength(proposal)
            expected_impact = _CATEGORY_IMPACT.get(proposal.category, 0.5)
            estimated_risk = self._compute_risk_estimate(proposal)
            estimated_cost = self.estimate_cost(proposal)

            # Priority formula
            denominator = max(0.1, estimated_risk * estimated_cost)
            score = (evidence_strength * expected_impact) / denominator

            # Boost for proposals already partially processed
            if proposal.status == ProposalStatus.APPROVED:
                score *= 1.5

            reasoning = (
                f"evidence={evidence_strength:.2f}, impact={expected_impact:.2f}, "
                f"risk={estimated_risk:.2f}, cost={estimated_cost:.2f}"
            )

            priorities.append(ProposalPriority(
                proposal_id=proposal.id,
                priority_score=round(score, 3),
                evidence_strength=round(evidence_strength, 3),
                expected_impact=round(expected_impact, 3),
                estimated_risk=round(estimated_risk, 3),
                estimated_cost=round(estimated_cost, 3),
                reasoning=reasoning,
            ))

        # Sort by score descending
        priorities.sort(key=lambda p: p.priority_score, reverse=True)

        self._log.info(
            "proposals_prioritized",
            count=len(priorities),
            top_score=priorities[0].priority_score if priorities else 0.0,
        )
        return priorities

    def _compute_evidence_strength(self, proposal: EvolutionProposal) -> float:
        """
        Compute evidence strength from the proposal's evidence list.
        More evidence items = stronger signal. Capped at 1.0.
        """
        count = len(proposal.evidence)
        if count == 0:
            return 0.2  # minimal evidence
        # Logarithmic scaling: 1 item = 0.3, 5 items = 0.7, 10+ items = 0.9+
        import math
        return min(1.0, 0.2 + 0.3 * math.log1p(count))

    def _compute_risk_estimate(self, proposal: EvolutionProposal) -> float:
        """
        Estimate risk from simulation results and analytics history.
        Returns 0.0-1.0 scale.
        """
        # If simulation has run, use its risk level
        if proposal.simulation is not None:
            risk_map = {
                RiskLevel.LOW: 0.15,
                RiskLevel.MODERATE: 0.4,
                RiskLevel.HIGH: 0.7,
                RiskLevel.UNACCEPTABLE: 1.0,
            }
            return risk_map.get(proposal.simulation.risk_level, 0.4)

        # Otherwise, use category-based heuristic
        # Higher-impact categories carry more risk
        return _CATEGORY_COST.get(proposal.category, 0.5)

    # ─── Deduplication ───────────────────────────────────────────────────────

    async def deduplicate(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalCluster]:
        """
        Detect semantically similar proposals in three tiers:
          Tier 1: Exact description prefix match (zero cost)
          Tier 2: Category + affected_systems overlap (zero cost)
          Tier 3: LLM similarity check (only if >5 proposals, ~300 tokens)

        Returns clusters where member proposals could be merged.
        """
        if len(proposals) < 2:
            return []

        clusters: list[ProposalCluster] = []
        clustered_ids: set[str] = set()

        # Tier 1: Exact description prefix match
        prefix_groups: dict[str, list[EvolutionProposal]] = {}
        for p in proposals:
            prefix = p.description[:_DEDUP_PREFIX_LEN].lower().strip()
            prefix_groups.setdefault(prefix, []).append(p)

        for prefix, group in prefix_groups.items():
            if len(group) < 2:
                continue
            rep = group[0]
            members = [p.id for p in group]
            clustered_ids.update(members)
            clusters.append(ProposalCluster(
                representative_id=rep.id,
                member_ids=members,
                similarity_scores=[1.0] * len(members),
                merge_recommendation=f"Identical prefix: '{prefix[:30]}...'",
            ))

        # Tier 2: Category + affected_systems overlap
        unclustered = [p for p in proposals if p.id not in clustered_ids]
        cat_system_groups: dict[str, list[EvolutionProposal]] = {}
        for p in unclustered:
            key = f"{p.category.value}::{','.join(sorted(p.change_spec.affected_systems))}"
            cat_system_groups.setdefault(key, []).append(p)

        for key, group in cat_system_groups.items():
            if len(group) < 2:
                continue
            rep = group[0]
            members = [p.id for p in group]
            clustered_ids.update(members)
            clusters.append(ProposalCluster(
                representative_id=rep.id,
                member_ids=members,
                similarity_scores=[0.7] * len(members),
                merge_recommendation=f"Same category and affected systems: {key}",
            ))

        # Tier 3: Embedding-based semantic similarity (preferred) or LLM fallback
        still_unclustered = [p for p in proposals if p.id not in clustered_ids]
        if len(still_unclustered) >= _LLM_DEDUP_THRESHOLD:
            if self._embeddings is not None:
                # Stage 1B: voyage-code-3 cosine similarity — cheaper and more precise
                embedding_clusters = await self._embedding_deduplicate(still_unclustered)
                clusters.extend(embedding_clusters)
            elif self._llm is not None:
                # Fallback: LLM-based semantic comparison (~300 tokens)
                llm_clusters = await self._llm_deduplicate(still_unclustered)
                clusters.extend(llm_clusters)

        if clusters:
            self._log.info(
                "dedup_complete",
                clusters=len(clusters),
                total_duplicates=sum(len(c.member_ids) for c in clusters),
                dedup_stats=self._dedup_stats,
            )
        return clusters

    async def _embedding_deduplicate(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalCluster]:
        """
        Stage 1B: Embedding-based semantic dedup using voyage-code-3.

        Embeds all proposal descriptions, then finds pairs with cosine
        similarity above the threshold. Groups them into clusters.

        Zero LLM tokens. Cost: ~0.001 per proposal via Voyage API.
        """
        from ecodiaos.clients.embedding import cosine_similarity

        assert self._embeddings is not None
        self._dedup_stats["embedding_dedup_calls"] += 1
        t_start = time.monotonic()

        # Build description texts for embedding
        texts = [
            f"{p.category.value}: {p.description[:200]}"
            for p in proposals[:20]  # Cap at 20 to control API cost
        ]

        try:
            import asyncio
            embeddings = await asyncio.wait_for(
                self._embeddings.embed_batch(texts),
                timeout=10.0,
            )
        except Exception as exc:
            self._log.warning("embedding_dedup_failed", error=str(exc))
            self._dedup_stats["embedding_dedup_latency_ms"] += (
                (time.monotonic() - t_start) * 1000
            )
            return []

        # Pairwise cosine similarity — find clusters above threshold
        clusters: list[ProposalCluster] = []
        clustered: set[int] = set()

        for i in range(len(embeddings)):
            if i in clustered:
                continue
            group = [i]
            for j in range(i + 1, len(embeddings)):
                if j in clustered:
                    continue
                sim = cosine_similarity(embeddings[i], embeddings[j])
                if sim >= self._EMBEDDING_DEDUP_THRESHOLD:
                    group.append(j)
                    clustered.add(j)

            if len(group) >= 2:
                clustered.add(i)
                member_ids = [proposals[idx].id for idx in group]
                similarities = []
                for idx in group:
                    if idx == group[0]:
                        similarities.append(1.0)
                    else:
                        sim = cosine_similarity(embeddings[group[0]], embeddings[idx])
                        similarities.append(round(sim, 3))

                clusters.append(ProposalCluster(
                    representative_id=member_ids[0],
                    member_ids=member_ids,
                    similarity_scores=similarities,
                    merge_recommendation=(
                        f"Embedding similarity ≥{self._EMBEDDING_DEDUP_THRESHOLD} "
                        f"(voyage-code-3)"
                    ),
                ))
                self._dedup_stats["tier3_embedding_matches"] += len(group)

        latency_ms = (time.monotonic() - t_start) * 1000
        self._dedup_stats["embedding_dedup_latency_ms"] += latency_ms

        self._log.info(
            "embedding_dedup_complete",
            proposals_checked=len(proposals),
            clusters_found=len(clusters),
            latency_ms=round(latency_ms, 1),
        )
        return clusters

    async def _llm_deduplicate(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalCluster]:
        """LLM-based semantic similarity check (fallback). ~300 tokens."""
        descriptions = "\n".join(
            f"{i+1}. [{p.id[:8]}] {p.category.value}: {p.description[:100]}"
            for i, p in enumerate(proposals[:10])
        )

        prompt = (
            "Below are evolution proposals for an AI system. "
            "Identify any that are semantically similar enough to be duplicates.\n\n"
            f"{descriptions}\n\n"
            "Reply with groups of similar proposals by their numbers.\n"
            "Format: GROUP: 1, 3 (reason)\n"
            "If no duplicates found, reply: NONE"
        )

        try:
            import asyncio
            response = await asyncio.wait_for(
                self._llm.evaluate(prompt=prompt, max_tokens=200, temperature=0.1),  # type: ignore[union-attr]
                timeout=8.0,
            )

            clusters: list[ProposalCluster] = []
            for line in response.text.strip().splitlines():
                line = line.strip()
                if line.upper() == "NONE" or "GROUP" not in line.upper():
                    continue
                try:
                    _, nums_part = line.split(":", 1)
                    reason_start = nums_part.find("(")
                    if reason_start > 0:
                        reason = nums_part[reason_start:].strip("() ")
                        nums_part = nums_part[:reason_start]
                    else:
                        reason = ""

                    indices = [
                        int(n.strip()) - 1 for n in nums_part.split(",") if n.strip().isdigit()
                    ]
                    valid = [i for i in indices if 0 <= i < len(proposals)]
                    if len(valid) >= 2:
                        members = [proposals[i].id for i in valid]
                        clusters.append(ProposalCluster(
                            representative_id=members[0],
                            member_ids=members,
                            similarity_scores=[0.6] * len(members),
                            merge_recommendation=reason or "LLM-detected similarity",
                        ))
                        self._dedup_stats["tier3_llm_fallback_matches"] += len(valid)
                except (ValueError, IndexError):
                    continue

            return clusters
        except Exception as exc:
            self._log.warning("llm_dedup_failed", error=str(exc))
            return []

    # ─── Dependency Analysis ─────────────────────────────────────────────────

    async def analyze_dependencies(
        self, proposals: list[EvolutionProposal],
    ) -> list[tuple[str, str, str]]:
        """
        Detect ordering dependencies between proposals.
        Returns list of (before_id, after_id, reason) tuples.

        Rule-based analysis — zero LLM tokens:
        - ADD_EXECUTOR should come before MODIFY_CONTRACT referencing axon
        - ADD_INPUT_CHANNEL before MODIFY_CONTRACT referencing atune
        - ADJUST_BUDGET after the thing it's budgeting for is added
        - ADD_SYSTEM_CAPABILITY is a superset that depends on components
        """
        if len(proposals) < 2:
            return []

        dependencies: list[tuple[str, str, str]] = []

        # Build lookup
        additive = [p for p in proposals if p.category in {
            ChangeCategory.ADD_EXECUTOR,
            ChangeCategory.ADD_INPUT_CHANNEL,
            ChangeCategory.ADD_PATTERN_DETECTOR,
        }]
        contracts = [p for p in proposals if p.category == ChangeCategory.MODIFY_CONTRACT]
        capabilities = [p for p in proposals if p.category == ChangeCategory.ADD_SYSTEM_CAPABILITY]
        budgets = [p for p in proposals if p.category == ChangeCategory.ADJUST_BUDGET]

        # Additive changes should come before contract modifications
        # that reference the same system
        for add_p in additive:
            add_systems = set(add_p.change_spec.affected_systems)
            for contract_p in contracts:
                contract_systems = set(contract_p.change_spec.affected_systems)
                overlap = add_systems & contract_systems
                if overlap:
                    dependencies.append((
                        add_p.id,
                        contract_p.id,
                        f"Add {add_p.category.value} before modifying contracts for {overlap}",
                    ))

        # Additive changes should come before capability additions
        for add_p in additive:
            for cap_p in capabilities:
                cap_systems = set(cap_p.change_spec.affected_systems)
                add_systems = set(add_p.change_spec.affected_systems)
                if cap_systems & add_systems:
                    dependencies.append((
                        add_p.id,
                        cap_p.id,
                        "Add component before adding system capability",
                    ))

        # Budget changes should come after the thing they budget for
        for budget_p in budgets:
            param = budget_p.change_spec.budget_parameter or ""
            for add_p in additive:
                # If the budget parameter references the additive system
                add_systems_list = add_p.change_spec.affected_systems
                for sys in add_systems_list:
                    if sys in param:
                        dependencies.append((
                            add_p.id,
                            budget_p.id,
                            f"Add component before adjusting its budget ({param})",
                        ))

        if dependencies:
            self._log.info(
                "dependencies_detected",
                count=len(dependencies),
            )
        return dependencies

    # ─── Cost Estimation ─────────────────────────────────────────────────────

    def estimate_cost(self, proposal: EvolutionProposal) -> float:
        """
        Heuristic cost estimation (0.0-1.0 scale).
        Zero LLM tokens — pure lookup + adjustment.
        """
        base_cost = _CATEGORY_COST.get(proposal.category, 0.5)

        # Adjust for complexity signals
        spec = proposal.change_spec
        if spec.affected_systems and len(spec.affected_systems) > 1:
            base_cost = min(1.0, base_cost + 0.1 * (len(spec.affected_systems) - 1))

        if spec.contract_changes and len(spec.contract_changes) > 2:
            base_cost = min(1.0, base_cost + 0.1)

        return round(base_cost, 2)

    # ─── Duplicate Detection Helper ──────────────────────────────────────────

    def get_dedup_stats(self) -> dict[str, Any]:
        """Return dedup precision benchmarking stats (Stage 1B.5)."""
        return dict(self._dedup_stats)

    def is_duplicate(
        self,
        proposal: EvolutionProposal,
        clusters: list[ProposalCluster],
    ) -> bool:
        """
        Check if a proposal appears in any cluster as a non-representative member.
        If it's in a cluster but not the representative, it's a duplicate.
        """
        for cluster in clusters:
            if proposal.id in cluster.member_ids and proposal.id != cluster.representative_id:
                return True
        return False

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\rollback.py =====

"""
EcodiaOS -- Simula Rollback Manager

Before any change is applied, RollbackManager snapshots all files
that might be modified. If the post-apply health check fails -- or if
any exception occurs during application -- the manager restores the
codebase to its pre-change state.

Rollback target: <=2s (from spec).
"""

from __future__ import annotations

from pathlib import Path

import structlog

from ecodiaos.systems.simula.types import ConfigSnapshot, FileSnapshot

logger = structlog.get_logger().bind(system="simula.rollback")


class RollbackError(RuntimeError):
    """Raised when restoring files to their pre-change state fails."""
    pass


class RollbackManager:
    """
    Captures file snapshots before structural changes are applied
    and restores them if the post-apply health check fails.
    """

    def __init__(self, codebase_root: Path) -> None:
        self._root = codebase_root
        self._log = logger

    async def snapshot(self, proposal_id: str, paths: list[Path]) -> ConfigSnapshot:
        """""""""
        Read each file's current content and package into a ConfigSnapshot.
        Files that do not exist are recorded with existed=False so rollback
        knows to delete them rather than restore content.
        """""""""
        snapshots: list[FileSnapshot] = []
        for path in paths:
            abs_path = path if path.is_absolute() else self._root / path
            content = await self._read_file_safe(abs_path)
            existed = content is not None
            snapshots.append(
                FileSnapshot(
                    path=str(abs_path),
                    content=content,
                    existed=existed,
                )
            )
            self._log.debug(
                "snapshot_captured",
                path=str(abs_path),
                existed=existed,
                size=len(content) if content else 0,
            )

        # We need a config_version from outside context; use 0 as placeholder.
        # The service layer will update this before persisting.
        cfg_snapshot = ConfigSnapshot(
            proposal_id=proposal_id,
            files=snapshots,
            config_version=0,
        )
        self._log.info(
            "snapshot_complete",
            proposal_id=proposal_id,
            files_captured=len(snapshots),
        )
        return cfg_snapshot

    async def restore(self, snapshot: ConfigSnapshot) -> list[str]:
        """""""""
        Restore all files to the state captured in the snapshot.
        Files that did not exist before are deleted.
        Returns the list of absolute paths that were restored.
        Raises RollbackError if any restore fails.
        """""""""
        restored: list[str] = []
        errors: list[str] = []

        for file_snap in snapshot.files:
            path = Path(file_snap.path)
            try:
                if not file_snap.existed:
                    # File was created by the change -- delete it
                    if path.exists():
                        path.unlink()
                        self._log.info("rollback_deleted", path=str(path))
                elif file_snap.content is not None:
                    path.parent.mkdir(parents=True, exist_ok=True)
                    path.write_text(file_snap.content, encoding="utf-8")
                    self._log.info("rollback_restored", path=str(path))
                restored.append(str(path))
            except Exception as exc:
                msg = f"Failed to restore {path}: {exc}"
                self._log.error("rollback_restore_failed", path=str(path), error=str(exc))
                errors.append(msg)

        if errors:
            raise RollbackError("Rollback incomplete. Failures: " + str(errors))

        self._log.info(
            "rollback_complete",
            proposal_id=snapshot.proposal_id,
            files_restored=len(restored),
        )
        return restored

    async def _read_file_safe(self, path: Path) -> str | None:
        """""""""
        Read file content; return None if file does not exist.
        """""""""
        try:
            return path.read_text(encoding="utf-8")
        except FileNotFoundError:
            return None
        except Exception as exc:
            self._log.warning("snapshot_read_failed", path=str(path), error=str(exc))
            return None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\service.py =====

"""
EcodiaOS — Simula Service

The self-evolution system. Simula is the organism's capacity for
metamorphosis: structural change beyond parameter tuning.

Where Evo adjusts the knobs, Simula redesigns the dashboard.

Simula coordinates the full evolution proposal pipeline:
  1. DEDUPLICATE — check for duplicate/similar active proposals
  2. VALIDATE    — reject forbidden categories immediately
  3. SIMULATE    — deep multi-strategy impact prediction
  4. GATE        — route governed changes through community governance
  5. APPLY       — invoke the code agent or config updater with rollback
  6. VERIFY      — health check post-application
  7. RECORD      — write immutable history, increment version, update analytics

Interfaces:
  initialize()            — build sub-systems, load current version
  process_proposal()      — main entry point for rich proposals
  receive_evo_proposal()  — receive from Evo via bridge translation
  get_history()           — recent evolution records
  get_current_version()   — current config version number
  get_analytics()         — evolution quality metrics
  shutdown()              — graceful teardown
  stats                   — service-level metrics

Iron Rules (never violated — see SIMULA_IRON_RULES in types.py):
  - Cannot modify Equor, constitutional drives, invariants
  - Cannot modify its own logic
  - Must simulate before applying any change
  - Must maintain rollback capability
  - Evolution history is immutable
"""

from __future__ import annotations

import contextlib
import hashlib
from pathlib import Path
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.clients.embedding import EmbeddingClient, create_voyage_client
from ecodiaos.clients.llm import LLMProvider, create_thinking_provider
from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine
from ecodiaos.systems.simula.applicator import ChangeApplicator
from ecodiaos.systems.simula.bridge import EvoSimulaBridge
from ecodiaos.systems.simula.code_agent import SimulaCodeAgent
from ecodiaos.systems.simula.health import HealthChecker
from ecodiaos.systems.simula.history import EvolutionHistoryManager
from ecodiaos.systems.simula.learning.grpo import GRPOTrainingEngine
from ecodiaos.systems.simula.learning.lilo import LiloLibraryEngine
from ecodiaos.systems.simula.proposal_intelligence import ProposalIntelligence
from ecodiaos.systems.simula.retrieval.swe_grep import SweGrepRetriever
from ecodiaos.systems.simula.rollback import RollbackManager
from ecodiaos.systems.simula.simulation import ChangeSimulator
from ecodiaos.systems.simula.types import (
    FORBIDDEN,
    GOVERNANCE_REQUIRED,
    ConfigVersion,
    EnrichedSimulationResult,
    EvolutionAnalytics,
    EvolutionProposal,
    EvolutionRecord,
    ProposalResult,
    ProposalStatus,
    RiskLevel,
    SimulationResult,
    TriageResult,
    TriageStatus,
)
from ecodiaos.systems.simula.verification.incremental import IncrementalVerificationEngine

if TYPE_CHECKING:
    from ecodiaos.clients.neo4j import Neo4jClient
    from ecodiaos.clients.redis import RedisClient
    from ecodiaos.clients.timescaledb import TimescaleDBClient
    from ecodiaos.config import SimulaConfig
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.simula.hunter.analytics import HunterAnalyticsEmitter
    from ecodiaos.systems.simula.hunter.service import HunterService
    from ecodiaos.systems.simula.hunter.types import HuntResult

logger = structlog.get_logger()


class SimulaService:
    """
    Simula — the EOS self-evolution system.

    Coordinates eight sub-systems:
      ChangeSimulator           — deep multi-strategy impact prediction
      SimulaCodeAgent           — Claude-backed code generation with 11 tools
      ChangeApplicator          — routes proposals to the right application strategy
      RollbackManager           — file snapshots and restore
      EvolutionHistoryManager   — immutable Neo4j history
      EvoSimulaBridge           — Evo→Simula proposal translation
      ProposalIntelligence      — deduplication, prioritization, dependency analysis
      EvolutionAnalyticsEngine  — evolution quality tracking
    """

    system_id: str = "simula"

    def __init__(
        self,
        config: SimulaConfig,
        llm: LLMProvider,
        neo4j: Neo4jClient | None = None,
        memory: MemoryService | None = None,
        codebase_root: Path | None = None,
        instance_name: str = "EOS",
        tsdb: TimescaleDBClient | None = None,
        redis: RedisClient | None = None,
    ) -> None:
        self._config = config
        self._llm = llm
        self._neo4j = neo4j
        self._memory = memory
        self._root = codebase_root or Path(config.codebase_root).resolve()
        self._instance_name = instance_name
        self._tsdb = tsdb
        self._redis = redis
        self._initialized: bool = False
        self._logger = logger.bind(system="simula")

        # Sub-systems (built in initialize())
        self._simulator: ChangeSimulator | None = None
        self._code_agent: SimulaCodeAgent | None = None
        self._applicator: ChangeApplicator | None = None
        self._rollback: RollbackManager | None = None
        self._history: EvolutionHistoryManager | None = None
        self._health: HealthChecker | None = None
        self._bridge: EvoSimulaBridge | None = None
        self._intelligence: ProposalIntelligence | None = None
        self._analytics: EvolutionAnalyticsEngine | None = None

        # Stage 3 sub-systems
        self._incremental: IncrementalVerificationEngine | None = None
        self._swe_grep: SweGrepRetriever | None = None
        self._lilo: LiloLibraryEngine | None = None

        # Stage 4 sub-systems
        self._lean_bridge: object | None = None  # LeanBridge (lazy import)
        self._grpo: GRPOTrainingEngine | None = None
        self._diffusion_repair: object | None = None  # DiffusionRepairAgent (lazy import)

        # Stage 5 sub-systems
        self._synthesis: object | None = None  # SynthesisStrategySelector (lazy import)
        self._repair_agent: object | None = None  # RepairAgent (lazy import)
        self._orchestrator: object | None = None  # MultiAgentOrchestrator (lazy import)
        self._causal_debugger: object | None = None  # CausalDebugger (lazy import)
        self._issue_resolver: object | None = None  # IssueResolver (lazy import)

        # Stage 6 sub-systems
        self._hash_chain: object | None = None  # HashChainManager (lazy import)
        self._content_credentials: object | None = None  # ContentCredentialManager (lazy import)
        self._governance_credentials: object | None = None  # GovernanceCredentialManager (lazy import)
        self._hard_negative_miner: object | None = None  # HardNegativeMiner (lazy import)
        self._adversarial_tester: object | None = None  # AdversarialTestGenerator (lazy import)
        self._formal_spec_generator: object | None = None  # FormalSpecGenerator (lazy import)
        self._egraph: object | None = None  # EqualitySaturationEngine (lazy import)
        self._symbolic_execution: object | None = None  # SymbolicExecutionEngine (lazy import)

        # Stage 7 sub-systems (Hunter — lazy runtime imports in initialize())
        self._hunter: HunterService | None = None
        self._hunter_analytics: HunterAnalyticsEmitter | None = None

        # State
        self._current_version: int = 0
        self._active_proposals: dict[str, EvolutionProposal] = {}

        # Metrics
        self._proposals_received: int = 0
        self._proposals_approved: int = 0
        self._proposals_rejected: int = 0
        self._proposals_rolled_back: int = 0
        self._proposals_awaiting_governance: int = 0
        self._proposals_deduplicated: int = 0
        self._proposals_applied_since_consolidation: int = 0

    # ─── Lifecycle ─────────────────────────────────────────────────────────────

    async def initialize(self) -> None:
        """
        Build all sub-systems and load current config version from history.
        Must be called before any other method.
        """
        if self._initialized:
            return

        # Build the rollback manager
        self._rollback = RollbackManager(codebase_root=self._root)

        # ── Stage 2: Verification bridges ─────────────────────────────────────
        from ecodiaos.systems.simula.verification.dafny_bridge import DafnyBridge
        from ecodiaos.systems.simula.verification.static_analysis import StaticAnalysisBridge
        from ecodiaos.systems.simula.verification.z3_bridge import Z3Bridge

        dafny_bridge: DafnyBridge | None = None
        if self._config.dafny_enabled:
            dafny_bridge = DafnyBridge(
                dafny_path=self._config.dafny_binary_path,
                verify_timeout_s=self._config.dafny_verify_timeout_s,
                max_rounds=self._config.dafny_max_clover_rounds,
            )
            self._logger.info("dafny_bridge_initialized")

        z3_bridge: Z3Bridge | None = None
        if self._config.z3_enabled:
            z3_bridge = Z3Bridge(
                check_timeout_ms=self._config.z3_check_timeout_ms,
                max_rounds=self._config.z3_max_discovery_rounds,
            )
            self._logger.info("z3_bridge_initialized")

        static_bridge: StaticAnalysisBridge | None = None
        if self._config.static_analysis_enabled:
            static_bridge = StaticAnalysisBridge(
                codebase_root=self._root,
            )
            self._logger.info("static_analysis_bridge_initialized")

        # Store for AgentCoder pipeline
        self._dafny_bridge = dafny_bridge
        self._z3_bridge = z3_bridge
        self._static_bridge = static_bridge

        # Build the health checker with Stage 2 bridges + Stage 3 Z3 blocking
        self._health = HealthChecker(
            codebase_root=self._root,
            test_command=self._config.test_command,
            dafny_bridge=dafny_bridge,
            z3_bridge=z3_bridge,
            static_analysis_bridge=static_bridge,
            llm=self._llm,
            z3_blocking=self._config.z3_blocking,
        )

        # ── Stage 1A: Extended-thinking provider for governance/high-risk ────
        thinking_provider = None
        if self._config.thinking_model_api_key:
            try:
                thinking_provider = create_thinking_provider(
                    api_key=self._config.thinking_model_api_key,
                    model=self._config.thinking_model,
                    provider=self._config.thinking_model_provider,
                    reasoning_budget=self._config.thinking_budget_tokens,
                )
                self._logger.info(
                    "thinking_provider_initialized",
                    model=self._config.thinking_model,
                    provider=self._config.thinking_model_provider,
                )
            except Exception as exc:
                self._logger.warning("thinking_provider_init_failed", error=str(exc))

        # ── Stage 1B: Voyage-code-3 embedding client ────────────────────────
        embedding_client: EmbeddingClient | None = None
        if self._config.embedding_api_key:
            try:
                embedding_client = create_voyage_client(
                    api_key=self._config.embedding_api_key,
                    model=self._config.embedding_model,
                )
                self._logger.info(
                    "embedding_client_initialized",
                    model=self._config.embedding_model,
                )
            except Exception as exc:
                self._logger.warning("embedding_client_init_failed", error=str(exc))

        # Store for shutdown cleanup
        self._embedding_client = embedding_client

        # Build the code agent with Stage 1 + 2 enhancements
        code_agent_llm = self._llm
        self._code_agent = SimulaCodeAgent(
            llm=code_agent_llm,
            codebase_root=self._root,
            max_turns=self._config.max_code_agent_turns,
            thinking_provider=thinking_provider,
            thinking_budget_tokens=self._config.thinking_budget_tokens,
            embedding_client=embedding_client,
            kv_compression_ratio=self._config.kv_compression_ratio,
            kv_compression_enabled=self._config.kv_compression_enabled,
            # Stage 2C: static analysis post-generation gate
            static_analysis_bridge=static_bridge,
            static_analysis_max_fix_iterations=self._config.static_analysis_max_fix_iterations,
        )

        # ── Stage 2D: AgentCoder pipeline agents ──────────────────────────────
        from ecodiaos.systems.simula.agents.test_designer import TestDesignerAgent
        from ecodiaos.systems.simula.agents.test_executor import TestExecutorAgent

        test_designer: TestDesignerAgent | None = None
        test_executor: TestExecutorAgent | None = None
        if self._config.agent_coder_enabled:
            test_designer = TestDesignerAgent(
                llm=self._llm,
                codebase_root=self._root,
            )
            test_executor = TestExecutorAgent(
                codebase_root=self._root,
                test_timeout_s=self._config.agent_coder_test_timeout_s,
            )
            self._logger.info("agent_coder_pipeline_initialized")

        # Build the applicator with Stage 2D pipeline
        self._applicator = ChangeApplicator(
            code_agent=self._code_agent,
            rollback_manager=self._rollback,
            health_checker=self._health,
            codebase_root=self._root,
            test_designer=test_designer,
            test_executor=test_executor,
            static_analysis_bridge=static_bridge,
            agent_coder_enabled=self._config.agent_coder_enabled,
            agent_coder_max_iterations=self._config.agent_coder_max_iterations,
        )

        # Build the history manager (requires Neo4j) with Stage 1B embedding support
        if self._neo4j is not None:
            self._history = EvolutionHistoryManager(
                neo4j=self._neo4j,
                embedding_client=embedding_client,
            )
            self._current_version = await self._history.get_current_version()
        else:
            self._logger.warning(
                "simula_no_neo4j",
                message="Evolution history will not be persisted (no Neo4j client)",
            )
            self._current_version = 0

        # Build the analytics engine (depends on history)
        self._analytics = EvolutionAnalyticsEngine(history=self._history)

        # Build the deep simulator (depends on analytics for dynamic caution)
        self._simulator = ChangeSimulator(
            config=self._config,
            llm=self._llm,
            memory=self._memory,
            analytics=self._analytics,
            codebase_root=self._root,
        )

        # Build the Evo↔Simula bridge
        self._bridge = EvoSimulaBridge(
            llm=self._llm,
            memory=self._memory,
        )

        # Build the proposal intelligence layer with Stage 1B embedding dedup
        self._intelligence = ProposalIntelligence(
            llm=self._llm,
            analytics=self._analytics,
            embedding_client=embedding_client,
        )

        # ── Stage 3A: Incremental verification ─────────────────────────────────
        if self._config.incremental_verification_enabled:
            self._incremental = IncrementalVerificationEngine(
                codebase_root=self._root,
                redis=self._redis,
                neo4j=self._neo4j,
                hot_ttl_seconds=self._config.incremental_hot_ttl_seconds,
            )
            self._logger.info("incremental_verification_initialized")

        # ── Stage 3B: SWE-grep retrieval ──────────────────────────────────────
        if self._config.swe_grep_enabled:
            self._swe_grep = SweGrepRetriever(
                codebase_root=self._root,
                llm=self._llm,
                max_hops=self._config.swe_grep_max_hops,
            )
            self._logger.info("swe_grep_retriever_initialized")

        # ── Stage 3C: LILO library learning ───────────────────────────────────
        if self._config.lilo_enabled:
            self._lilo = LiloLibraryEngine(
                neo4j=self._neo4j,
                llm=self._llm,
                codebase_root=self._root,
            )
            self._logger.info("lilo_library_initialized")

        # ── Stage 4A: Lean 4 proof generation ────────────────────────────────
        lean_bridge_instance = None
        if self._config.lean_enabled:
            from ecodiaos.systems.simula.verification.lean_bridge import LeanBridge

            lean_bridge_instance = LeanBridge(
                lean_path=self._config.lean_binary_path,
                project_path=self._config.lean_project_path or "",
                verify_timeout_s=self._config.lean_verify_timeout_s,
                max_attempts=self._config.lean_max_attempts,
                copilot_enabled=self._config.lean_copilot_enabled,
                dojo_enabled=self._config.lean_dojo_enabled,
                max_library_size=self._config.lean_proof_library_max_size,
                neo4j=self._neo4j,
            )
            self._lean_bridge = lean_bridge_instance
            self._logger.info("lean_bridge_initialized")

        # Wire Lean bridge into health checker
        if lean_bridge_instance is not None and self._health is not None:
            self._health._lean = lean_bridge_instance
            self._health._lean_blocking = self._config.lean_blocking

        # ── Stage 4B: GRPO domain fine-tuning ─────────────────────────────────
        if self._config.grpo_enabled:
            self._grpo = GRPOTrainingEngine(
                config=self._config,
                neo4j=self._neo4j,
            )
            self._logger.info("grpo_engine_initialized")

        # ── Stage 4C: Diffusion-based code repair ────────────────────────────
        if self._config.diffusion_repair_enabled:
            from ecodiaos.systems.simula.agents.diffusion_repair import DiffusionRepairAgent

            self._diffusion_repair = DiffusionRepairAgent(
                llm=self._llm,
                codebase_root=self._root,
                max_denoise_steps=self._config.diffusion_max_denoise_steps,
                timeout_s=self._config.diffusion_timeout_s,
                sketch_first=self._config.diffusion_sketch_first,
            )
            self._logger.info("diffusion_repair_agent_initialized")

        # ── Stage 5A: Neurosymbolic synthesis ────────────────────────────────
        if self._config.synthesis_enabled:
            from ecodiaos.systems.simula.synthesis.chopchop import ChopChopEngine
            from ecodiaos.systems.simula.synthesis.hysynth import HySynthEngine
            from ecodiaos.systems.simula.synthesis.sketch_solver import SketchSolver
            from ecodiaos.systems.simula.synthesis.strategy_selector import (
                SynthesisStrategySelector,
            )

            hysynth = HySynthEngine(
                llm=self._llm,
                codebase_root=self._root,
                max_candidates=self._config.hysynth_max_candidates,
                beam_width=self._config.hysynth_beam_width,
                timeout_s=self._config.hysynth_timeout_s,
            )
            sketch = SketchSolver(
                llm=self._llm,
                z3_bridge=z3_bridge,
                max_holes=self._config.sketch_max_holes,
                solver_timeout_ms=self._config.sketch_solver_timeout_ms,
            )
            chopchop = ChopChopEngine(
                llm=self._llm,
                codebase_root=self._root,
                max_retries_per_chunk=self._config.chopchop_max_retries,
                chunk_size_lines=self._config.chopchop_chunk_size_lines,
                timeout_s=self._config.chopchop_timeout_s,
            )
            self._synthesis = SynthesisStrategySelector(
                hysynth=hysynth,
                sketch_solver=sketch,
                chopchop=chopchop,
                codebase_root=self._root,
            )
            self._logger.info("synthesis_subsystem_initialized")

        # ── Stage 5B: Neural program repair ──────────────────────────────────
        if self._config.repair_agent_enabled:
            from ecodiaos.systems.simula.agents.repair_agent import RepairAgent

            self._repair_agent = RepairAgent(
                reasoning_llm=self._llm,
                code_llm=self._llm,
                codebase_root=self._root,
                neo4j=self._neo4j,
                max_retries=self._config.repair_max_retries,
                cost_budget_usd=self._config.repair_cost_budget_usd,
                timeout_s=self._config.repair_timeout_s,
                use_similar_fixes=self._config.repair_use_similar_fixes,
            )
            self._logger.info("repair_agent_initialized")

        # ── Stage 5C: Multi-agent orchestration ─────────────────────────────
        if self._config.orchestration_enabled and self._code_agent is not None:
            from ecodiaos.systems.simula.orchestration.orchestrator import MultiAgentOrchestrator
            from ecodiaos.systems.simula.orchestration.task_planner import TaskPlanner

            task_planner = TaskPlanner(
                codebase_root=self._root,
                llm=self._llm,
                max_dag_nodes=self._config.orchestration_max_dag_nodes,
            )
            self._orchestrator = MultiAgentOrchestrator(
                llm=self._llm,
                codebase_root=self._root,
                code_agent=self._code_agent,
                task_planner=task_planner,
                max_agents_per_stage=self._config.orchestration_max_agents_per_stage,
                timeout_s=self._config.orchestration_timeout_s,
            )
            self._logger.info("orchestrator_initialized")

        # ── Stage 5D: Causal debugging ───────────────────────────────────────
        if self._config.causal_debugging_enabled:
            from ecodiaos.systems.simula.debugging.causal_dag import CausalDebugger

            self._causal_debugger = CausalDebugger(
                llm=self._llm,
                codebase_root=self._root,
                max_interventions=self._config.causal_max_interventions,
                fault_injection_enabled=self._config.causal_fault_injection_enabled,
                timeout_s=self._config.causal_timeout_s,
            )
            self._logger.info("causal_debugger_initialized")

        # ── Stage 5E: Autonomous issue resolution ────────────────────────────
        if self._config.issue_resolution_enabled:
            from ecodiaos.systems.simula.agents.repair_agent import RepairAgent
            from ecodiaos.systems.simula.resolution.issue_resolver import IssueResolver
            from ecodiaos.systems.simula.resolution.monitors import (
                DegradationMonitor,
                PerfRegressionMonitor,
                SecurityVulnMonitor,
            )

            perf_monitor = (
                PerfRegressionMonitor()
                if self._config.issue_perf_regression_enabled
                else None
            )
            security_monitor = (
                SecurityVulnMonitor(self._root)
                if self._config.issue_security_scan_enabled
                else None
            )
            degradation_monitor = DegradationMonitor(
                window_hours=self._config.issue_degradation_window_hours,
            )

            self._issue_resolver = IssueResolver(
                llm=self._llm,
                codebase_root=self._root,
                neo4j=self._neo4j,
                code_agent=self._code_agent,
                repair_agent=self._repair_agent if isinstance(self._repair_agent, RepairAgent) else None,
                perf_monitor=perf_monitor,
                security_monitor=security_monitor,
                degradation_monitor=degradation_monitor,
                max_autonomy_level=self._config.issue_max_autonomy_level,
                abstention_threshold=self._config.issue_abstention_confidence_threshold,
            )
            self._logger.info("issue_resolver_initialized")

        # ── Stage 6A: Cryptographic auditability ────────────────────────────
        if self._config.hash_chain_enabled:
            from ecodiaos.systems.simula.audit.hash_chain import HashChainManager

            self._hash_chain = HashChainManager(
                neo4j=self._neo4j,
            )
            self._logger.info("hash_chain_initialized")

        if self._config.c2pa_enabled:
            from ecodiaos.systems.simula.audit.content_credentials import ContentCredentialManager

            self._content_credentials = ContentCredentialManager(
                signing_key_path=self._config.c2pa_signing_key_path,
                issuer_name=self._config.c2pa_issuer_name,
            )
            self._logger.info("content_credentials_initialized")

        if self._config.verifiable_credentials_enabled:
            from ecodiaos.systems.simula.audit.verifiable_credentials import (
                GovernanceCredentialManager,
            )

            self._governance_credentials = GovernanceCredentialManager(
                neo4j=self._neo4j,
                signing_key_path=self._config.c2pa_signing_key_path,
            )
            self._logger.info("governance_credentials_initialized")

        # ── Stage 6B: Co-evolving agents ──────────────────────────────────────
        if self._config.coevolution_enabled:
            from ecodiaos.systems.simula.coevolution.hard_negative_miner import HardNegativeMiner

            self._hard_negative_miner = HardNegativeMiner(
                neo4j=self._neo4j,
                llm=self._llm,
                max_negatives_per_cycle=self._config.adversarial_max_tests_per_cycle,
            )
            self._logger.info("hard_negative_miner_initialized")

            if self._config.adversarial_test_generation_enabled:
                from ecodiaos.systems.simula.coevolution.adversarial_tester import (
                    AdversarialTestGenerator,
                )

                self._adversarial_tester = AdversarialTestGenerator(
                    llm=self._llm,
                    codebase_root=self._root,
                    max_tests_per_cycle=self._config.adversarial_max_tests_per_cycle,
                )
                self._logger.info("adversarial_tester_initialized")

        # ── Stage 6C: Formal spec generation ─────────────────────────────────
        if self._config.formal_spec_generation_enabled:
            from ecodiaos.systems.simula.formal_specs.spec_generator import FormalSpecGenerator

            self._formal_spec_generator = FormalSpecGenerator(
                llm=self._llm,
                dafny_bridge=dafny_bridge,
                tla_plus_path=self._config.tla_plus_binary_path,
                alloy_path=self._config.alloy_binary_path,
                dafny_bench_target=self._config.dafny_bench_coverage_target,
                tla_plus_timeout_s=self._config.tla_plus_model_check_timeout_s,
                alloy_scope=self._config.alloy_scope,
            )
            self._logger.info("formal_spec_generator_initialized")

        # ── Stage 6D: Equality saturation (E-graphs) ─────────────────────────
        if self._config.egraph_enabled:
            from ecodiaos.systems.simula.egraph.equality_saturation import EqualitySaturationEngine

            self._egraph = EqualitySaturationEngine(
                max_iterations=self._config.egraph_max_iterations,
                timeout_s=self._config.egraph_timeout_s,
            )
            self._logger.info("egraph_initialized")

        # ── Stage 6E: Hybrid symbolic execution ──────────────────────────────
        if self._config.symbolic_execution_enabled:
            from ecodiaos.systems.simula.verification.symbolic_execution import (
                SymbolicExecutionEngine,
            )

            self._symbolic_execution = SymbolicExecutionEngine(
                z3_bridge=z3_bridge,
                llm=self._llm,
                timeout_ms=self._config.symbolic_execution_timeout_ms,
                blocking=self._config.symbolic_execution_blocking,
            )
            self._logger.info("symbolic_execution_initialized")

        # Wire Stage 6D + 6E into health checker
        if self._health is not None:
            if self._egraph is not None:
                self._health._egraph = self._egraph  # type: ignore[assignment]
                self._health._egraph_blocking = self._config.egraph_blocking
            if self._symbolic_execution is not None:
                self._health._symbolic_execution = self._symbolic_execution  # type: ignore[assignment]
                self._health._symbolic_execution_blocking = self._config.symbolic_execution_blocking
                self._health._symbolic_execution_domains = self._config.symbolic_execution_domains

        # Wire SWE-grep into the bridge for pre-translation retrieval (3B.5)
        if self._bridge is not None and self._swe_grep is not None:
            self._bridge.set_swe_grep(self._swe_grep)

        # ── Stage 7: Hunter — Zero-Day Discovery Engine ─────────────────────
        if self._config.hunter_enabled and z3_bridge is not None:
            from ecodiaos.systems.simula.hunter.analytics import HunterAnalyticsEmitter
            from ecodiaos.systems.simula.hunter.prover import VulnerabilityProver
            from ecodiaos.systems.simula.hunter.service import HunterService
            from ecodiaos.systems.simula.hunter.types import HunterConfig

            hunter_config = HunterConfig(
                authorized_targets=self._config.hunter_authorized_targets,
                max_workers=self._config.hunter_max_workers,
                sandbox_timeout_seconds=self._config.hunter_sandbox_timeout_s,
                log_vulnerability_analytics=self._config.hunter_log_analytics,
                clone_depth=self._config.hunter_clone_depth,
            )

            hunter_prover = VulnerabilityProver(
                z3_bridge=z3_bridge,
                llm=self._llm,
            )

            # Phase 9: Build analytics emitter with optional TSDB persistence
            hunter_analytics: HunterAnalyticsEmitter | None = None
            if self._config.hunter_log_analytics:
                hunter_analytics = HunterAnalyticsEmitter(tsdb=self._tsdb)
                self._hunter_analytics = hunter_analytics
                # Initialize TSDB schema (creates hunter_events hypertable)
                await hunter_analytics.initialize()

            # Build optional remediation orchestrator
            hunter_remediation = None
            if self._config.hunter_remediation_enabled and self._repair_agent is not None:
                from ecodiaos.systems.simula.hunter.remediation import HunterRepairOrchestrator
                from ecodiaos.systems.simula.hunter.workspace import TargetWorkspace

                # Remediation needs a workspace; it's set per-hunt by HunterService
                placeholder_workspace = TargetWorkspace.internal(self._root)
                hunter_remediation = HunterRepairOrchestrator(
                    repair_agent=self._repair_agent,  # type: ignore[arg-type]
                    prover=hunter_prover,
                    workspace=placeholder_workspace,
                )

            self._hunter = HunterService(
                prover=hunter_prover,
                config=hunter_config,
                eos_root=self._root,
                analytics=hunter_analytics,
                remediation=hunter_remediation,
            )

            # Phase 9: Wire Hunter analytics into the unified EvolutionAnalyticsEngine
            if self._analytics is not None and self._hunter is not None:
                self._analytics.set_hunter_view(self._hunter.analytics_view)
                if hunter_analytics is not None and hunter_analytics._store is not None:
                    self._analytics.set_hunter_store(hunter_analytics._store)

            self._logger.info(
                "hunter_initialized",
                hunter="active",
                max_workers=hunter_config.max_workers,
                authorized_targets=len(hunter_config.authorized_targets),
                remediation=hunter_remediation is not None,
                tsdb_persistence=self._tsdb is not None,
            )

        # Pre-compute analytics from history
        if self._history is not None:
            try:
                await self._analytics.compute_analytics()
            except Exception as exc:
                self._logger.warning("initial_analytics_failed", error=str(exc))

        self._initialized = True
        self._logger.info(
            "simula_initialized",
            current_version=self._current_version,
            codebase_root=str(self._root),
            max_code_agent_turns=self._config.max_code_agent_turns,
            subsystems=[
                "simulator", "code_agent", "applicator", "rollback",
                "health", "bridge", "intelligence", "analytics",
                "history" if self._history else "history(disabled)",
                "dafny" if dafny_bridge else "dafny(disabled)",
                "z3" if z3_bridge else "z3(disabled)",
                "static_analysis" if static_bridge else "static_analysis(disabled)",
                "incremental" if self._incremental else "incremental(disabled)",
                "swe_grep" if self._swe_grep else "swe_grep(disabled)",
                "lilo" if self._lilo else "lilo(disabled)",
                "lean" if self._lean_bridge else "lean(disabled)",
                "grpo" if self._grpo else "grpo(disabled)",
                "diffusion_repair" if self._diffusion_repair else "diffusion_repair(disabled)",
                "synthesis" if self._synthesis else "synthesis(disabled)",
                "repair_agent" if self._repair_agent else "repair_agent(disabled)",
                "orchestrator" if self._orchestrator else "orchestrator(disabled)",
                "causal_debugger" if self._causal_debugger else "causal_debugger(disabled)",
                "issue_resolver" if self._issue_resolver else "issue_resolver(disabled)",
                "hash_chain" if self._hash_chain else "hash_chain(disabled)",
                "content_credentials" if self._content_credentials else "content_credentials(disabled)",
                "governance_credentials" if self._governance_credentials else "governance_credentials(disabled)",
                "hard_negative_miner" if self._hard_negative_miner else "hard_negative_miner(disabled)",
                "adversarial_tester" if self._adversarial_tester else "adversarial_tester(disabled)",
                "formal_spec_generator" if self._formal_spec_generator else "formal_spec_generator(disabled)",
                "egraph" if self._egraph else "egraph(disabled)",
                "symbolic_execution" if self._symbolic_execution else "symbolic_execution(disabled)",
                "hunter" if self._hunter else "hunter(disabled)",
            ],
            stage1_extended_thinking=thinking_provider is not None,
            stage1_embeddings=embedding_client is not None,
            stage1_kv_compression=self._config.kv_compression_enabled,
            stage1_kv_ratio=self._config.kv_compression_ratio,
            stage2_dafny=dafny_bridge is not None,
            stage2_z3=z3_bridge is not None,
            stage2_static_analysis=static_bridge is not None,
            stage2_agent_coder=self._config.agent_coder_enabled,
            stage3_incremental=self._incremental is not None,
            stage3_swe_grep=self._swe_grep is not None,
            stage3_lilo=self._lilo is not None,
            stage4_lean=self._lean_bridge is not None,
            stage4_grpo=self._grpo is not None,
            stage4_diffusion_repair=self._diffusion_repair is not None,
            stage5_synthesis=self._synthesis is not None,
            stage5_repair_agent=self._repair_agent is not None,
            stage5_orchestrator=self._orchestrator is not None,
            stage5_causal_debugger=self._causal_debugger is not None,
            stage5_issue_resolver=self._issue_resolver is not None,
            stage6_hash_chain=self._hash_chain is not None,
            stage6_content_credentials=self._content_credentials is not None,
            stage6_governance_credentials=self._governance_credentials is not None,
            stage6_coevolution=self._hard_negative_miner is not None,
            stage6_adversarial_tester=self._adversarial_tester is not None,
            stage6_formal_specs=self._formal_spec_generator is not None,
            stage6_egraph=self._egraph is not None,
            stage6_symbolic_execution=self._symbolic_execution is not None,
            stage7_hunter=self._hunter is not None,
            stage9_hunter_analytics=self._hunter_analytics is not None,
            stage9_tsdb_persistence=self._tsdb is not None,
        )

    async def shutdown(self) -> None:
        """Graceful shutdown."""
        # Clean up Stage 1B embedding client
        if hasattr(self, "_embedding_client") and self._embedding_client is not None:
            with contextlib.suppress(Exception):
                await self._embedding_client.close()

        self._logger.info(
            "simula_shutdown",
            proposals_received=self._proposals_received,
            proposals_approved=self._proposals_approved,
            proposals_rejected=self._proposals_rejected,
            proposals_rolled_back=self._proposals_rolled_back,
            proposals_deduplicated=self._proposals_deduplicated,
            current_version=self._current_version,
        )

    # ─── Triage (Fast-Path Pre-Simulation) ─────────────────────────────────────

    def _triage_proposal(self, proposal: EvolutionProposal) -> TriageResult:
        """
        Fast-path proposal check. If trivial, skip expensive simulation.
        Trivial = budget tweaks <5% with sufficient data.

        Returns TriageResult with skip_simulation=True for trivial cases.
        """
        if proposal.category.value != "adjust_budget":
            return TriageResult(
                status=TriageStatus.REQUIRES_SIMULATION,
                skip_simulation=False,
            )

        spec = proposal.change_spec
        if not spec.budget_new_value or spec.budget_old_value is None:
            return TriageResult(
                status=TriageStatus.REQUIRES_SIMULATION,
                skip_simulation=False,
            )

        # Check delta < 5%
        old_val = spec.budget_old_value
        new_val = spec.budget_new_value
        if old_val == 0.0:
            delta_pct = 1.0  # Treat zero as 100% change
        else:
            delta_pct = abs(new_val - old_val) / abs(old_val)

        if delta_pct < 0.05:
            self._logger.info(
                "proposal_triaged",
                proposal_id=proposal.id,
                status="trivial",
                reason=f"Budget delta {delta_pct:.1%} < 5%",
            )
            return TriageResult(
                status=TriageStatus.TRIVIAL,
                assumed_risk=RiskLevel.LOW,
                reason=f"Budget delta {delta_pct:.1%} < 5%",
                skip_simulation=True,
            )

        return TriageResult(
            status=TriageStatus.REQUIRES_SIMULATION,
            skip_simulation=False,
        )

    # ─── Main Pipeline ─────────────────────────────────────────────────────────

    async def process_proposal(self, proposal: EvolutionProposal) -> ProposalResult:
        """
        Main entry point for evolution proposals.

        Pipeline:
          DEDUP → VALIDATE → SIMULATE → [GOVERNANCE GATE] → APPLY → VERIFY → RECORD

        Spec reference: Section III.3.2
        Performance target: validation ≤50ms, simulation ≤30s, apply ≤5s
        """
        self._proposals_received += 1
        log = self._logger.bind(proposal_id=proposal.id, category=proposal.category.value)
        log.info("proposal_received", source=proposal.source, description=proposal.description[:100])

        # ── STEP 0: Deduplication ────────────────────────────────────────────
        if self._intelligence is not None and self._active_proposals:
            try:
                all_proposals = [proposal] + list(self._active_proposals.values())
                clusters = await self._intelligence.deduplicate(all_proposals)
                if self._intelligence.is_duplicate(proposal, clusters):
                    self._proposals_deduplicated += 1
                    log.info("proposal_deduplicated")
                    return ProposalResult(
                        status=ProposalStatus.REJECTED,
                        reason="Duplicate of an active proposal",
                    )
            except Exception as exc:
                log.warning("dedup_check_failed", error=str(exc))

        self._active_proposals[proposal.id] = proposal

        # ── STEP 1: Validate ────────────────────────────────────────────────
        if proposal.category in FORBIDDEN:
            proposal.status = ProposalStatus.REJECTED
            self._proposals_rejected += 1
            reason = (
                f"Category '{proposal.category.value}' is forbidden for Simula. "
                f"Iron rule: {self._get_iron_rule_for(proposal)}"
            )
            log.warning("proposal_rejected_forbidden", reason=reason)
            self._active_proposals.pop(proposal.id, None)
            return ProposalResult(status=ProposalStatus.REJECTED, reason=reason)

        # ── STEP 1.5: Triage (fast-path for trivial cases) ──────────────────
        triage = self._triage_proposal(proposal)
        if triage.skip_simulation:
            # Build synthetic simulation result
            proposal.simulation = SimulationResult(
                episodes_tested=0,
                risk_level=triage.assumed_risk or RiskLevel.LOW,
                risk_summary=f"Triaged as trivial: {triage.reason}",
                benefit_summary=proposal.expected_benefit,
            )
            log.info("proposal_triaged_skipping_simulation", reason=triage.reason)
            # Skip STEP 2 (Simulate) and proceed directly to governance/apply

        # ── STEP 2: Simulate (deep multi-strategy) ─────────────────────────
        # Skip if already triaged (has synthetic simulation)
        if proposal.simulation is None:
            proposal.status = ProposalStatus.SIMULATING
            log.info("proposal_simulating")

            try:
                simulation = await self._simulate_change(proposal)
                proposal.simulation = simulation
            except Exception as exc:
                proposal.status = ProposalStatus.REJECTED
                self._proposals_rejected += 1
                reason = f"Simulation failed: {exc}"
                log.error("simulation_error", error=str(exc))
                self._active_proposals.pop(proposal.id, None)
                return ProposalResult(status=ProposalStatus.REJECTED, reason=reason)
        else:
            simulation = proposal.simulation

        if simulation.risk_level == RiskLevel.UNACCEPTABLE:
            proposal.status = ProposalStatus.REJECTED
            self._proposals_rejected += 1
            reason = f"Simulation shows unacceptable risk: {simulation.risk_summary}"
            log.warning("proposal_rejected_risk", risk_level=simulation.risk_level.value)
            self._active_proposals.pop(proposal.id, None)
            return ProposalResult(status=ProposalStatus.REJECTED, reason=reason)

        # ── STEP 3: Governance gate ─────────────────────────────────────────
        if self.requires_governance(proposal):
            proposal.status = ProposalStatus.AWAITING_GOVERNANCE
            self._proposals_awaiting_governance += 1
            try:
                governance_id = await self._submit_to_governance(proposal, simulation)
                proposal.governance_record_id = governance_id
            except Exception as exc:
                log.error("governance_submission_error", error=str(exc))
                governance_id = f"gov_{new_id()}"
                proposal.governance_record_id = governance_id

            log.info("proposal_awaiting_governance", governance_id=governance_id)
            return ProposalResult(
                status=ProposalStatus.AWAITING_GOVERNANCE,
                governance_record_id=governance_id,
            )

        # ── STEP 4: Apply (self-applicable changes only) ───────────────────
        return await self._apply_change(proposal)

    async def receive_evo_proposal(
        self,
        evo_description: str,
        evo_rationale: str,
        hypothesis_ids: list[str],
        hypothesis_statements: list[str],
        evidence_scores: list[float],
        supporting_episode_ids: list[str],
        mutation_target: str = "",
        mutation_type: str = "",
    ) -> ProposalResult:
        """
        Receive a proposal from Evo via the bridge.
        Translates the lightweight Evo proposal into Simula's rich format,
        then feeds it into the main pipeline.

        This is the public API that Evo's ConsolidationOrchestrator calls.
        """
        if self._bridge is None:
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason="Simula bridge not initialized",
            )

        self._logger.info(
            "evo_proposal_received",
            description=evo_description[:80],
            hypotheses=len(hypothesis_ids),
        )

        try:
            translated = await self._bridge.translate_proposal(
                evo_description=evo_description,
                evo_rationale=evo_rationale,
                hypothesis_ids=hypothesis_ids,
                hypothesis_statements=hypothesis_statements,
                evidence_scores=evidence_scores,
                supporting_episode_ids=supporting_episode_ids,
                mutation_target=mutation_target,
                mutation_type=mutation_type,
            )
        except Exception as exc:
            self._logger.error("bridge_translation_failed", error=str(exc))
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Bridge translation failed: {exc}",
            )

        return await self.process_proposal(translated)

    async def approve_governed_proposal(
        self, proposal_id: str, governance_record_id: str
    ) -> ProposalResult:
        """
        Called when a governed proposal receives community approval.
        Resumes the pipeline from the application step.
        """
        proposal = self._active_proposals.get(proposal_id)
        if proposal is None:
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Proposal {proposal_id} not found in active proposals",
            )
        if proposal.status != ProposalStatus.AWAITING_GOVERNANCE:
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Proposal {proposal_id} is not awaiting governance (status: {proposal.status})",
            )

        proposal.status = ProposalStatus.APPROVED
        self._proposals_awaiting_governance = max(0, self._proposals_awaiting_governance - 1)
        self._logger.info("governed_proposal_approved", proposal_id=proposal_id)
        return await self._apply_change(proposal)

    def requires_governance(self, proposal: EvolutionProposal) -> bool:
        """Changes in the GOVERNANCE_REQUIRED category always need governance."""
        return proposal.category in GOVERNANCE_REQUIRED

    # ─── Query Interface ───────────────────────────────────────────────────────

    async def get_history(self, limit: int = 50) -> list[EvolutionRecord]:
        """Return the most recent evolution records."""
        if self._history is None:
            return []
        return await self._history.get_history(limit=limit)

    async def get_current_version(self) -> int:
        """Return the current config version number."""
        return self._current_version

    async def get_version_chain(self) -> list[ConfigVersion]:
        """Return the full version history chain."""
        if self._history is None:
            return []
        return await self._history.get_version_chain()

    def get_active_proposals(self) -> list[EvolutionProposal]:
        """Return all proposals currently in the pipeline."""
        return list(self._active_proposals.values())

    async def get_analytics(self) -> EvolutionAnalytics:
        """Return current evolution quality analytics."""
        if self._analytics is None:
            return EvolutionAnalytics()
        return await self._analytics.compute_analytics()

    async def get_prioritized_proposals(self) -> list[dict[str, Any]]:
        """Return active proposals ranked by priority score."""
        if self._intelligence is None or not self._active_proposals:
            return []
        priorities = await self._intelligence.prioritize(list(self._active_proposals.values()))
        return [p.model_dump() for p in priorities]

    # ─── Stats ────────────────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        base: dict[str, Any] = {
            "initialized": self._initialized,
            "current_version": self._current_version,
            "proposals_received": self._proposals_received,
            "proposals_approved": self._proposals_approved,
            "proposals_rejected": self._proposals_rejected,
            "proposals_rolled_back": self._proposals_rolled_back,
            "proposals_deduplicated": self._proposals_deduplicated,
            "proposals_awaiting_governance": self._proposals_awaiting_governance,
            "active_proposals": len(self._active_proposals),
        }

        # Include cached analytics summary if available
        if self._analytics is not None and self._analytics._cached_analytics is not None:
            analytics = self._analytics._cached_analytics
            base["analytics"] = {
                "total_proposals": analytics.total_proposals,
                "evolution_velocity": analytics.evolution_velocity,
                "rollback_rate": analytics.rollback_rate,
                "mean_simulation_risk": analytics.mean_simulation_risk,
            }

        # Stage 3 subsystem status
        base["stage3"] = {
            "incremental_verification": self._incremental is not None,
            "swe_grep": self._swe_grep is not None,
            "lilo": self._lilo is not None,
        }

        # Stage 4 subsystem status
        base["stage4"] = {
            "lean": self._lean_bridge is not None,
            "grpo": self._grpo is not None,
            "diffusion_repair": self._diffusion_repair is not None,
        }

        # Stage 5 subsystem status
        base["stage5"] = {
            "synthesis": self._synthesis is not None,
            "repair_agent": self._repair_agent is not None,
            "orchestrator": self._orchestrator is not None,
            "causal_debugger": self._causal_debugger is not None,
            "issue_resolver": self._issue_resolver is not None,
        }

        # Stage 6 subsystem status
        base["stage6"] = {
            "hash_chain": self._hash_chain is not None,
            "content_credentials": self._content_credentials is not None,
            "governance_credentials": self._governance_credentials is not None,
            "hard_negative_miner": self._hard_negative_miner is not None,
            "adversarial_tester": self._adversarial_tester is not None,
            "formal_spec_generator": self._formal_spec_generator is not None,
            "egraph": self._egraph is not None,
            "symbolic_execution": self._symbolic_execution is not None,
        }

        # Stage 7 subsystem status
        base["stage7"] = {
            "hunter": self._hunter is not None,
        }
        if self._hunter is not None:
            base["stage7"]["hunter_stats"] = self._hunter.stats

        # Phase 9: Hunter analytics observability
        base["stage9_analytics"] = {
            "hunter_analytics_emitter": self._hunter_analytics is not None,
            "hunter_tsdb_persistence": (
                self._hunter_analytics is not None
                and self._hunter_analytics._store is not None
            ),
            "hunter_view_attached": (
                self._analytics is not None
                and self._analytics._hunter_view is not None
            ),
            "hunter_store_attached": (
                self._analytics is not None
                and self._analytics._hunter_store is not None
            ),
        }
        if self._hunter_analytics is not None:
            base["stage9_analytics"]["emitter_stats"] = self._hunter_analytics.stats

        return base

    # ─── Hunter API ───────────────────────────────────────────────────────────

    def _ensure_hunter(self) -> HunterService:
        """Validate that Hunter is enabled and return the typed service."""
        if self._hunter is None:
            raise RuntimeError(
                "Hunter is not enabled. Set hunter_enabled=True in SimulaConfig."
            )
        return self._hunter

    async def hunt_external_target(
        self,
        github_url: str,
        *,
        authorized_targets: list[str] | None = None,
        attack_goals: list[str] | None = None,
        generate_pocs: bool | None = None,
        generate_patches: bool | None = None,
    ) -> HuntResult:
        """
        Run Hunter against an external GitHub repository.

        Hunter is purely additive — it never modifies EOS files and all
        analysis happens in temporary workspaces.

        Args:
            github_url: HTTPS URL of the target repository.
            authorized_targets: Override config authorized targets for this hunt.
                Creates a scoped config copy — the shared config is never mutated.
            attack_goals: Custom attack goals (defaults to predefined set).
            generate_pocs: Generate exploit PoC scripts (default from config).
            generate_patches: Generate + verify patches (default from config).

        Returns:
            HuntResult with discovered vulnerabilities and optional patches.

        Raises:
            RuntimeError: If Hunter is not enabled.
        """
        hunter = self._ensure_hunter()

        # Scope-safe authorized target override: create a copy of the config
        # so concurrent hunts don't corrupt each other's authorization lists.
        if authorized_targets is not None:
            from ecodiaos.systems.simula.hunter.types import HunterConfig

            original_config = hunter._config
            hunter._config = HunterConfig(
                authorized_targets=authorized_targets,
                max_workers=original_config.max_workers,
                sandbox_timeout_seconds=original_config.sandbox_timeout_seconds,
                log_vulnerability_analytics=original_config.log_vulnerability_analytics,
                clone_depth=original_config.clone_depth,
            )
            try:
                return await hunter.hunt_external_repo(
                    github_url=github_url,
                    attack_goals=attack_goals,
                    generate_pocs=generate_pocs if generate_pocs is not None else self._config.hunter_generate_pocs,
                    generate_patches=generate_patches if generate_patches is not None else self._config.hunter_generate_patches,
                )
            finally:
                hunter._config = original_config

        return await hunter.hunt_external_repo(
            github_url=github_url,
            attack_goals=attack_goals,
            generate_pocs=generate_pocs if generate_pocs is not None else self._config.hunter_generate_pocs,
            generate_patches=generate_patches if generate_patches is not None else self._config.hunter_generate_patches,
        )

    async def hunt_internal(
        self,
        *,
        attack_goals: list[str] | None = None,
        generate_pocs: bool | None = None,
        generate_patches: bool | None = None,
    ) -> HuntResult:
        """
        Run Hunter against the internal EOS codebase for self-testing.

        Args:
            attack_goals: Custom attack goals (defaults to predefined set).
            generate_pocs: Generate exploit PoC scripts (default from config).
            generate_patches: Generate + verify patches (default from config).

        Returns:
            HuntResult with discovered vulnerabilities.

        Raises:
            RuntimeError: If Hunter is not enabled.
        """
        hunter = self._ensure_hunter()

        return await hunter.hunt_internal_eos(
            attack_goals=attack_goals,
            generate_pocs=generate_pocs if generate_pocs is not None else self._config.hunter_generate_pocs,
            generate_patches=generate_patches if generate_patches is not None else self._config.hunter_generate_patches,
        )

    async def generate_patches_for_hunt(
        self,
        hunt_result: HuntResult,
    ) -> dict[str, str]:
        """
        Generate patches for vulnerabilities found in a completed hunt.

        Useful when a hunt was run without generate_patches=True and you want
        to retroactively generate patches for the discovered vulnerabilities.

        Args:
            hunt_result: A completed HuntResult from hunt_external_target
                         or hunt_internal.

        Returns:
            Dict mapping vulnerability ID → unified diff patch string.

        Raises:
            RuntimeError: If Hunter or remediation is not enabled.
        """
        hunter = self._ensure_hunter()
        return await hunter.generate_patches(hunt_result)

    def get_hunter_analytics(self) -> dict[str, Any]:
        """Return aggregate Hunter analytics if available."""
        hunter = self._ensure_hunter()
        return hunter.analytics_view.summary

    async def get_unified_analytics(self) -> dict[str, Any]:
        """
        Return unified analytics combining evolution metrics and Hunter
        security metrics. This is the Phase 9 observability entry point.
        """
        if self._analytics is None:
            return {}
        return await self._analytics.get_unified_analytics()

    async def get_hunter_weekly_trends(
        self,
        *,
        weeks: int = 12,
        target_url: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Query weekly Hunter vulnerability trends from TSDB or in-memory view.
        """
        if self._analytics is None:
            return []
        return await self._analytics.get_hunter_weekly_trends(
            weeks=weeks, target_url=target_url,
        )

    async def get_hunter_error_summary(self, *, days: int = 7) -> list[dict[str, Any]]:
        """Query Hunter pipeline error summary from TSDB."""
        if self._analytics is None:
            return []
        return await self._analytics.get_hunter_error_summary(days=days)

    # ─── Evo Bridge Callback ──────────────────────────────────────────────────

    def get_evo_callback(self) -> Any:
        """
        Return a callback function for Evo's ConsolidationOrchestrator.
        This is wired during system initialization in main.py.

        The callback signature matches what Evo Phase 8 expects:
          async def callback(evo_proposal, hypotheses) -> ProposalResult
        """
        async def _evo_callback(evo_proposal: Any, hypotheses: list[Any]) -> ProposalResult:
            # Extract fields from Evo's lightweight types
            hypothesis_ids = [getattr(h, "id", "") for h in hypotheses]
            hypothesis_statements = [getattr(h, "statement", "") for h in hypotheses]
            evidence_scores = [getattr(h, "evidence_score", 0.0) for h in hypotheses]

            # Collect all supporting episode IDs across hypotheses
            episode_ids: list[str] = []
            for h in hypotheses:
                episode_ids.extend(getattr(h, "supporting_episodes", []))

            # Extract mutation info if available
            mutation_target = ""
            mutation_type = ""
            for h in hypotheses:
                mutation = getattr(h, "proposed_mutation", None)
                if mutation is not None:
                    mutation_target = getattr(mutation, "target", "")
                    mutation_type = getattr(mutation, "type", "")
                    if hasattr(mutation_type, "value"):
                        mutation_type = mutation_type.value
                    break

            return await self.receive_evo_proposal(
                evo_description=getattr(evo_proposal, "description", ""),
                evo_rationale=getattr(evo_proposal, "rationale", ""),
                hypothesis_ids=hypothesis_ids,
                hypothesis_statements=hypothesis_statements,
                evidence_scores=evidence_scores,
                supporting_episode_ids=episode_ids,
                mutation_target=mutation_target,
                mutation_type=mutation_type,
            )

        return _evo_callback

    # ─── Private: Application ──────────────────────────────────────────────────

    async def _apply_change(self, proposal: EvolutionProposal) -> ProposalResult:
        """
        Apply a validated, simulated, approved proposal.
        Includes health check and automatic rollback on failure.
        """
        assert self._applicator is not None
        assert self._health is not None
        assert self._rollback is not None

        proposal.status = ProposalStatus.APPLYING
        log = self._logger.bind(proposal_id=proposal.id, category=proposal.category.value)
        log.info("applying_change")

        # Stage 3C: Inject LILO library prompt into the code agent
        if self._lilo is not None and self._code_agent is not None:
            try:
                self._code_agent._lilo_prompt = await self._lilo.get_library_prompt()
            except Exception as exc:
                log.warning("lilo_prompt_error", error=str(exc))

        # Stage 4A: Inject proof library context into the code agent
        if self._lean_bridge is not None and self._code_agent is not None:
            try:
                from ecodiaos.systems.simula.verification.lean_bridge import LeanBridge
                if isinstance(self._lean_bridge, LeanBridge):
                    lib_stats = await self._lean_bridge.get_library_stats()
                    if lib_stats.total_lemmas > 0:
                        self._code_agent._proof_library_prompt = (
                            f"\n\n## Proof Library ({lib_stats.total_lemmas} proven lemmas)\n"
                            f"The Lean 4 proof library contains {lib_stats.total_lemmas} proven lemmas "
                            f"across domains: {', '.join(f'{d}: {c}' for d, c in lib_stats.by_domain.items())}.\n"
                            f"Mean Lean Copilot automation rate: {lib_stats.mean_copilot_automation:.0%}.\n"
                            f"Your implementation may benefit from existing verified properties."
                        )
            except Exception as exc:
                log.warning("proof_library_prompt_error", error=str(exc))

        # Stage 4B: GRPO A/B model routing
        grpo_model_used = ""
        grpo_ab_group = ""
        if self._grpo is not None and self._code_agent is not None:
            try:
                if self._grpo.should_use_finetuned():
                    grpo_model_used = self._config.grpo_base_model + "-finetuned"
                    grpo_ab_group = "finetuned"
                    self._code_agent._grpo_model_id = grpo_model_used
                    log.info("grpo_routing_finetuned", model=grpo_model_used)
                else:
                    grpo_ab_group = "base"
                    log.info("grpo_routing_base")
            except Exception as exc:
                log.warning("grpo_routing_error", error=str(exc))

        # ── Stage 5A: Synthesis-first (fast-path before CEGIS) ────────────────
        synthesis_result_stash = None
        if self._synthesis is not None:
            try:
                from ecodiaos.systems.simula.synthesis.strategy_selector import (
                    SynthesisStrategySelector,
                )
                from ecodiaos.systems.simula.synthesis.types import SynthesisStatus

                if isinstance(self._synthesis, SynthesisStrategySelector):
                    synth_result = await self._synthesis.synthesise(proposal)
                    synthesis_result_stash = synth_result
                    if synth_result.status == SynthesisStatus.SYNTHESIZED and synth_result.final_code:
                        log.info(
                            "synthesis_succeeded",
                            strategy=synth_result.strategy.value,
                            tokens=synth_result.total_llm_tokens,
                            duration_ms=synth_result.total_duration_ms,
                        )
                        # Write synthesised code and skip CEGIS
                        if synth_result.files_written:
                            for fpath in synth_result.files_written:
                                full_path = self._root / fpath
                                if full_path.exists():
                                    log.debug("synthesis_wrote_file", path=fpath)
                    else:
                        log.info(
                            "synthesis_fell_back_to_cegis",
                            strategy=synth_result.strategy.value,
                            status=synth_result.status.value,
                        )
            except Exception as exc:
                log.warning("synthesis_error", error=str(exc))

        # ── Stage 5C: Multi-agent orchestration for multi-file proposals ──────
        if self._orchestrator is not None:
            try:
                from ecodiaos.systems.simula.orchestration.orchestrator import (
                    MultiAgentOrchestrator,
                )

                if isinstance(self._orchestrator, MultiAgentOrchestrator):
                    # Estimate affected files from proposal target + code_hint
                    estimated_files = []
                    _target = getattr(proposal, "target", None)
                    if _target:
                        estimated_files.append(_target)
                    if hasattr(proposal, "affected_files"):
                        estimated_files.extend(proposal.affected_files)

                    threshold = self._config.orchestration_multi_file_threshold
                    if len(estimated_files) >= threshold:
                        log.info(
                            "orchestration_engaged",
                            files=len(estimated_files),
                            threshold=threshold,
                        )
                        orc_result = await self._orchestrator.orchestrate(
                            proposal=proposal,
                            files_to_change=estimated_files,
                        )
                        proposal._orchestration_result = orc_result  # type: ignore[attr-defined]
                        log.info(
                            "orchestration_complete",
                            success=not orc_result.error,
                            stages=orc_result.parallel_stages_executed,
                            agents=orc_result.total_agents_used,
                        )
            except Exception as exc:
                log.warning("orchestration_error", error=str(exc))

        code_result, snapshot = await self._applicator.apply(proposal)

        # ── Stage 5B: Neural repair agent (primary recovery before diffusion) ─
        if not code_result.success and self._repair_agent is not None:
            log.info("repair_agent_attempting")
            try:
                from ecodiaos.systems.simula.agents.repair_agent import (
                    RepairAgent as RepairAgentCls,
                )
                from ecodiaos.systems.simula.verification.types import RepairStatus

                if isinstance(self._repair_agent, RepairAgentCls):
                    broken_files = {
                        f: (self._root / f).read_text()
                        for f in code_result.files_written
                        if (self._root / f).exists()
                    }
                    repair_result = await self._repair_agent.repair(
                        proposal=proposal,
                        broken_files=broken_files,
                        test_output=code_result.test_output or code_result.error,
                    )
                    if repair_result.status == RepairStatus.REPAIRED:
                        log.info(
                            "repair_agent_succeeded",
                            attempts=repair_result.total_attempts,
                            cost=f"${repair_result.total_cost_usd:.4f}",
                        )
                        code_result.success = True
                        code_result.files_written = repair_result.files_repaired
                        code_result.error = ""
                        code_result.repair_attempted = True
                        code_result.repair_succeeded = True
                        code_result.repair_cost_usd = repair_result.total_cost_usd
                        proposal._repair_result = repair_result  # type: ignore[attr-defined]
                    else:
                        log.info("repair_agent_insufficient", status=repair_result.status.value)
                        code_result.repair_attempted = True
                        code_result.repair_succeeded = False
                        proposal._repair_result = repair_result  # type: ignore[attr-defined]
            except Exception as exc:
                log.warning("repair_agent_error", error=str(exc))

        # Stage 4C: Diffusion repair fallback when code agent fails
        if not code_result.success and self._diffusion_repair is not None:
            log.info("diffusion_repair_fallback_attempting")
            try:
                from ecodiaos.systems.simula.agents.diffusion_repair import DiffusionRepairAgent
                if isinstance(self._diffusion_repair, DiffusionRepairAgent):
                    broken_files_dr = {
                        f: (self._root / f).read_text()
                        for f in code_result.files_written
                        if (self._root / f).exists()
                    }
                    dr_result = await self._diffusion_repair.repair(
                        proposal=proposal,
                        broken_files=broken_files_dr,
                        test_output=code_result.test_output or code_result.error or "",
                    )
                    if dr_result.status.value == "repaired":
                        log.info(
                            "diffusion_repair_succeeded",
                            steps=len(dr_result.denoise_steps),
                            improvement=f"{dr_result.improvement_rate:.0%}",
                        )
                        # Mark as success — diffusion repair saved the change
                        code_result.success = True
                        code_result.files_written = dr_result.files_repaired
                        code_result.error = ""
                        # Stash repair metadata on proposal for history recording
                        proposal._diffusion_repair_result = dr_result  # type: ignore[attr-defined]
                    else:
                        log.info("diffusion_repair_insufficient", status=dr_result.status.value)
            except Exception as exc:
                log.warning("diffusion_repair_error", error=str(exc))

        if not code_result.success:
            proposal.status = ProposalStatus.ROLLED_BACK
            self._proposals_rolled_back += 1
            log.warning("apply_failed_no_success", error=code_result.error)
            self._active_proposals.pop(proposal.id, None)
            self._invalidate_analytics()
            return ProposalResult(
                status=ProposalStatus.ROLLED_BACK,
                reason=f"Application failed: {code_result.error}",
            )

        # Stash GRPO metadata for history recording
        proposal._grpo_model_used = grpo_model_used  # type: ignore[attr-defined]
        proposal._grpo_ab_group = grpo_ab_group  # type: ignore[attr-defined]

        # Stash Stage 5A synthesis metadata
        if synthesis_result_stash is not None:
            proposal._synthesis_result = synthesis_result_stash  # type: ignore[attr-defined]
            code_result.synthesis_strategy = synthesis_result_stash.strategy.value
            code_result.synthesis_speedup = synthesis_result_stash.speedup_vs_cegis

        # ── Health check (with Stage 2 formal verification) ────────────────
        health = await self._health.check(
            code_result.files_written, proposal=proposal,
        )

        # Stash formal verification result for history recording
        if health.formal_verification is not None:
            proposal._formal_verification_result = health.formal_verification  # type: ignore[attr-defined]

        # Stash Lean 4 verification result for history recording
        if health.lean_verification is not None:
            proposal._lean_verification_result = health.lean_verification  # type: ignore[attr-defined]

        # Stash Stage 6 formal guarantees result for history recording
        if health.formal_guarantees is not None:
            proposal._formal_guarantees_result = health.formal_guarantees  # type: ignore[attr-defined]

        if not health.healthy:
            recovered = False

            # ── Stage 5D: Causal debugging before repair ──────────────────
            causal_diagnosis = None
            if self._causal_debugger is not None:
                log.info("causal_debugging_starting", issues=health.issues)
                try:
                    from ecodiaos.systems.simula.debugging.causal_dag import (
                        CausalDebugger as CausalDbgCls,
                    )

                    if isinstance(self._causal_debugger, CausalDbgCls):
                        causal_diagnosis = await self._causal_debugger.diagnose(
                            files_written=code_result.files_written,
                            health_issues=health.issues,
                            test_output=code_result.test_output or "",
                        )
                        log.info(
                            "causal_diagnosis_complete",
                            root_cause=causal_diagnosis.root_cause_node,
                            confidence=f"{causal_diagnosis.confidence:.2f}",
                            interventions=causal_diagnosis.total_interventions,
                        )
                        # Stash for history recording
                        proposal._causal_diagnosis = causal_diagnosis  # type: ignore[attr-defined]
                        health.causal_diagnosis = causal_diagnosis
                except Exception as exc:
                    log.warning("causal_debugging_error", error=str(exc))

            # ── Stage 5B: Repair agent recovery after causal diagnosis ────
            if self._repair_agent is not None:
                log.info("repair_agent_post_health_attempting")
                try:
                    from ecodiaos.systems.simula.agents.repair_agent import (
                        RepairAgent as RepairAgentCls,
                    )
                    from ecodiaos.systems.simula.verification.types import RepairStatus

                    if isinstance(self._repair_agent, RepairAgentCls):
                        broken_files = {
                            f: (self._root / f).read_text()
                            for f in code_result.files_written
                            if (self._root / f).exists()
                        }
                        # Feed causal diagnosis context to the repair agent
                        diag_context = ""
                        if causal_diagnosis is not None:
                            diag_context = (
                                f"Root cause: {causal_diagnosis.root_cause_node}\n"
                                f"Fix location: {causal_diagnosis.root_cause_file}\n"
                                f"Confidence: {causal_diagnosis.confidence:.2f}\n"
                                f"Reasoning: {' → '.join(causal_diagnosis.reasoning_chain)}"
                            )
                        repair_result = await self._repair_agent.repair(
                            proposal=proposal,
                            broken_files=broken_files,
                            test_output=(
                                code_result.test_output
                                or "; ".join(health.issues)
                            ),
                            lint_output=diag_context or "",
                        )
                        if repair_result.status == RepairStatus.REPAIRED:
                            log.info(
                                "repair_agent_post_health_succeeded",
                                attempts=repair_result.total_attempts,
                                cost=f"${repair_result.total_cost_usd:.4f}",
                            )
                            code_result.repair_attempted = True
                            code_result.repair_succeeded = True
                            code_result.repair_cost_usd = repair_result.total_cost_usd
                            proposal._repair_result = repair_result  # type: ignore[attr-defined]

                            # Re-check health after repair
                            health_recheck = await self._health.check(
                                repair_result.files_repaired, proposal=proposal,
                            )
                            if health_recheck.healthy:
                                log.info("health_recheck_passed_after_repair")
                                health = health_recheck
                                code_result.files_written = repair_result.files_repaired
                                code_result.success = True
                                recovered = True
                            else:
                                log.warning(
                                    "health_recheck_still_failing",
                                    issues=health_recheck.issues,
                                )
                        else:
                            log.info(
                                "repair_agent_post_health_insufficient",
                                status=repair_result.status.value,
                            )
                            code_result.repair_attempted = True
                            code_result.repair_succeeded = False
                except Exception as exc:
                    log.warning("repair_agent_post_health_error", error=str(exc))

            # ── Rollback only if all recovery failed ──────────────────────
            if not recovered:
                log.warning("health_check_failed_rolling_back", issues=health.issues)
                await self._rollback.restore(snapshot)
                proposal.status = ProposalStatus.ROLLED_BACK
                self._proposals_rolled_back += 1

                # Record the rollback in history
                await self._record_evolution(
                    proposal, code_result.files_written,
                    rolled_back=True,
                    rollback_reason="; ".join(health.issues),
                )

                self._active_proposals.pop(proposal.id, None)
                self._invalidate_analytics()
                return ProposalResult(
                    status=ProposalStatus.ROLLED_BACK,
                    reason=f"Post-apply health check failed: {'; '.join(health.issues)}",
                )

        # ── Stage 6A.2: Sign generated files with content credentials ────────
        if self._content_credentials is not None:
            try:
                from ecodiaos.systems.simula.audit.content_credentials import (
                    ContentCredentialManager,
                )

                if isinstance(self._content_credentials, ContentCredentialManager):
                    cc_result = await self._content_credentials.sign_files(
                        files=code_result.files_written,
                        codebase_root=self._root,
                    )
                    proposal._content_credential_result = cc_result  # type: ignore[attr-defined]
                    log.info(
                        "content_credentials_signed",
                        signed=len(cc_result.credentials),
                        unsigned=len(cc_result.unsigned_files),
                    )
            except Exception as exc:
                log.warning("content_credentials_error", error=str(exc))

        # ── Stage 6C: Generate formal specs for changed code ─────────────────
        if self._formal_spec_generator is not None:
            try:
                from ecodiaos.systems.simula.formal_specs.spec_generator import FormalSpecGenerator

                if isinstance(self._formal_spec_generator, FormalSpecGenerator):
                    fsg_result = await self._formal_spec_generator.generate_all(
                        files=code_result.files_written,
                        proposal=proposal,
                        codebase_root=self._root,
                        dafny_enabled=self._config.dafny_spec_generation_enabled,
                        tla_plus_enabled=self._config.tla_plus_enabled,
                        alloy_enabled=self._config.alloy_enabled,
                        self_spec_enabled=self._config.self_spec_dsl_enabled,
                    )
                    proposal._formal_spec_result = fsg_result  # type: ignore[attr-defined]
                    log.info(
                        "formal_specs_generated",
                        specs=len(fsg_result.specs),
                        coverage=f"{fsg_result.overall_coverage_percent:.0%}",
                    )
            except Exception as exc:
                log.warning("formal_spec_generation_error", error=str(exc))

        # ── Stage 3A: Incremental verification cache update ─────────────────
        if self._incremental is not None:
            try:
                incr_result = await self._incremental.verify_incremental(
                    files_changed=code_result.files_written,
                    proposal_id=proposal.id,
                )
                log.info(
                    "incremental_verification_complete",
                    checked=incr_result.functions_checked,
                    skipped=incr_result.functions_skipped_early_cutoff,
                    cache_hit_rate=f"{incr_result.cache_hit_rate:.0%}",
                )
            except Exception as exc:
                log.warning("incremental_verification_error", error=str(exc))

        # ── Success ───────────────────────────────────────────────────────────
        proposal.status = ProposalStatus.APPLIED
        self._proposals_approved += 1

        from_version = self._current_version
        self._current_version += 1

        await self._record_evolution(
            proposal,
            code_result.files_written,
            rolled_back=False,
        )

        # ── Stage 6A.1: Append to hash chain ─────────────────────────────────
        if self._hash_chain is not None:
            try:
                from ecodiaos.systems.simula.audit.hash_chain import HashChainManager

                if isinstance(self._hash_chain, HashChainManager):
                    # Build a record-like object for hashing (use the same fields as the recording)
                    from ecodiaos.systems.simula.types import EvolutionRecord as ERec

                    hash_record = ERec(
                        proposal_id=proposal.id,
                        category=proposal.category,
                        description=proposal.description,
                        from_version=from_version,
                        to_version=self._current_version,
                        files_changed=code_result.files_written,
                        simulation_risk=RiskLevel.LOW,
                        rolled_back=False,
                    )
                    hce = await self._hash_chain.append(hash_record)
                    proposal._hash_chain_entry = hce  # type: ignore[attr-defined]
                    log.info(
                        "hash_chain_appended",
                        chain_hash=hce.chain_hash[:16],
                        position=hce.chain_position,
                    )
            except Exception as exc:
                log.warning("hash_chain_append_error", error=str(exc))

        # ── Stage 6B: Co-evolution cycle (fire-and-forget) ───────────────────
        if self._hard_negative_miner is not None:
            try:
                import asyncio as _aio

                _aio.create_task(
                    self._coevolution_background(
                        files=code_result.files_written,
                        proposal_id=proposal.id,
                    ),
                )
                log.info("coevolution_cycle_scheduled")
            except Exception as exc:
                log.warning("coevolution_schedule_error", error=str(exc))

        # ── Stage 4B: Record GRPO training data ──────────────────────────────
        if self._grpo is not None:
            try:
                self._grpo.record_proposal_applied()
                # Check if retraining is warranted
                if self._grpo.should_retrain():
                    log.info("grpo_retrain_triggered")
                    # Fire-and-forget: retraining is expensive and non-blocking
                    import asyncio
                    asyncio.create_task(self._grpo_retrain_background())
            except Exception as exc:
                log.warning("grpo_record_error", error=str(exc))

        # ── Stage 3C: LILO abstraction extraction ────────────────────────────
        self._proposals_applied_since_consolidation += 1
        if self._lilo is not None:
            try:
                extraction = await self._lilo.extract_from_proposals(
                    proposal_ids=[proposal.id],
                    files_changed={proposal.id: code_result.files_written},
                )
                if extraction.extracted:
                    log.info(
                        "lilo_extraction_complete",
                        extracted=len(extraction.extracted),
                        merged=extraction.merged_into_existing,
                    )
                # Periodic consolidation
                if (
                    self._proposals_applied_since_consolidation
                    >= self._config.lilo_consolidation_interval_proposals
                ):
                    await self._lilo.consolidate()
                    self._proposals_applied_since_consolidation = 0
                    log.info("lilo_consolidation_complete")
            except Exception as exc:
                log.warning("lilo_extraction_error", error=str(exc))

        # Clean up active proposals
        self._active_proposals.pop(proposal.id, None)
        self._invalidate_analytics()

        log.info(
            "change_applied",
            from_version=from_version,
            to_version=self._current_version,
            files_changed=len(code_result.files_written),
        )

        return ProposalResult(
            status=ProposalStatus.APPLIED,
            version=self._current_version,
            files_changed=code_result.files_written,
        )

    async def _simulate_change(self, proposal: EvolutionProposal) -> SimulationResult:
        """Delegate to the deep ChangeSimulator."""
        if self._simulator is None:
            return SimulationResult(risk_level=RiskLevel.LOW, risk_summary="Simulator not initialized")
        return await self._simulator.simulate(proposal)

    async def _submit_to_governance(
        self, proposal: EvolutionProposal, simulation: SimulationResult
    ) -> str:
        """
        Submit a governed proposal to the community governance system.
        Returns a governance record ID. Enriches the governance record
        with deep simulation data for community review.
        """
        record_id = f"gov_{new_id()}"

        if self._neo4j is not None:
            try:
                # Include enriched simulation data for governance reviewers
                risk_summary = simulation.risk_summary
                benefit_summary = simulation.benefit_summary

                # Add counterfactual and alignment data if available (enriched simulation)
                enrichment = []
                if isinstance(simulation, EnrichedSimulationResult):
                    if simulation.constitutional_alignment != 0.0:
                        enrichment.append(f"Constitutional alignment: {simulation.constitutional_alignment:+.2f}")
                    if simulation.dependency_blast_radius > 0:
                        enrichment.append(f"Blast radius: {simulation.dependency_blast_radius} files")
                if enrichment:
                    risk_summary = f"{risk_summary} [{'; '.join(enrichment)}]"

                await self._neo4j.execute_write(
                    """
                    CREATE (:GovernanceProposal {
                        id: $id,
                        proposal_id: $proposal_id,
                        category: $category,
                        description: $description,
                        risk_level: $risk_level,
                        risk_summary: $risk_summary,
                        benefit_summary: $benefit_summary,
                        submitted_at: $submitted_at,
                        status: 'pending'
                    })
                    """,
                    {
                        "id": record_id,
                        "proposal_id": proposal.id,
                        "category": proposal.category.value,
                        "description": proposal.description,
                        "risk_level": simulation.risk_level.value,
                        "risk_summary": risk_summary,
                        "benefit_summary": benefit_summary,
                        "submitted_at": utc_now().isoformat(),
                    },
                )
            except Exception as exc:
                self._logger.warning("governance_neo4j_write_failed", error=str(exc))

        return record_id

    async def _record_evolution(
        self,
        proposal: EvolutionProposal,
        files_changed: list[str],
        rolled_back: bool = False,
        rollback_reason: str = "",
    ) -> None:
        """Write an immutable evolution record and update the version chain."""
        if self._history is None:
            return

        from_version = self._current_version - (0 if rolled_back else 1)
        to_version = self._current_version

        risk_level = (
            proposal.simulation.risk_level
            if proposal.simulation
            else RiskLevel.LOW
        )

        # Extract simulation detail fields if enriched simulation was performed
        sim_detail: dict[str, Any] = {
            "simulation_episodes_tested": 0,
            "counterfactual_regression_rate": 0.0,
            "dependency_blast_radius": 0,
            "constitutional_alignment": 0.0,
            "resource_tokens_per_hour": 0,
            "caution_reasoning": "",
        }
        if isinstance(proposal.simulation, EnrichedSimulationResult):
            sim_detail["simulation_episodes_tested"] = proposal.simulation.episodes_tested
            sim_detail["counterfactual_regression_rate"] = proposal.simulation.counterfactual_regression_rate
            sim_detail["dependency_blast_radius"] = proposal.simulation.dependency_blast_radius
            sim_detail["constitutional_alignment"] = proposal.simulation.constitutional_alignment
            if proposal.simulation.resource_cost_estimate:
                sim_detail["resource_tokens_per_hour"] = (
                    proposal.simulation.resource_cost_estimate.estimated_additional_llm_tokens_per_hour
                )
            if proposal.simulation.caution_adjustment:
                sim_detail["caution_reasoning"] = proposal.simulation.caution_adjustment.reasoning

        record = EvolutionRecord(
            proposal_id=proposal.id,
            category=proposal.category,
            description=proposal.description,
            from_version=from_version,
            to_version=to_version,
            files_changed=files_changed,
            simulation_risk=risk_level,
            rolled_back=rolled_back,
            rollback_reason=rollback_reason,
            **sim_detail,
        )

        # Stage 2: Attach formal verification metadata if available
        if hasattr(proposal, "_formal_verification_result"):
            fv = proposal._formal_verification_result
            if fv is not None:
                if fv.dafny and fv.dafny.status:
                    record.formal_verification_status = fv.dafny.status.value
                    record.dafny_rounds = fv.dafny.rounds_attempted
                if fv.z3 and fv.z3.valid_invariants:
                    record.discovered_invariants_count = len(fv.z3.valid_invariants)
                if fv.static_analysis:
                    record.static_analysis_findings = len(fv.static_analysis.findings)

        # Stage 4A: Attach Lean 4 proof metadata if available
        if hasattr(proposal, "_lean_verification_result"):
            lean_r = proposal._lean_verification_result
            if lean_r is not None:
                record.lean_proof_status = lean_r.status.value
                record.lean_proof_rounds = len(lean_r.attempts)
                record.lean_proven_lemmas_count = len(lean_r.proven_lemmas)
                record.lean_copilot_automation_rate = lean_r.copilot_automation_rate
                record.lean_library_lemmas_reused = len(lean_r.library_lemmas_used)

        # Stage 4B: Attach GRPO model routing metadata
        if hasattr(proposal, "_grpo_model_used"):
            record.grpo_model_used = proposal._grpo_model_used
        if hasattr(proposal, "_grpo_ab_group"):
            record.grpo_ab_group = proposal._grpo_ab_group

        # Stage 4C: Attach diffusion repair metadata if used
        if hasattr(proposal, "_diffusion_repair_result"):
            dr = proposal._diffusion_repair_result
            if dr is not None:
                record.diffusion_repair_used = True
                record.diffusion_repair_status = dr.status.value
                record.diffusion_repair_steps = len(dr.denoise_steps)
                record.diffusion_improvement_rate = dr.improvement_rate

        # Stage 5A: Attach synthesis metadata
        if hasattr(proposal, "_synthesis_result"):
            sr = proposal._synthesis_result
            if sr is not None:
                record.synthesis_strategy_used = sr.strategy.value
                record.synthesis_status = sr.status.value
                record.synthesis_speedup_vs_baseline = sr.speedup_vs_cegis
                record.synthesis_candidates_explored = sr.candidates_explored

        # Stage 5B: Attach repair agent metadata
        if hasattr(proposal, "_repair_result"):
            rr = proposal._repair_result
            if rr is not None:
                record.repair_agent_used = True
                record.repair_agent_status = rr.status.value
                record.repair_attempts = rr.total_attempts
                record.repair_cost_usd = rr.total_cost_usd

        # Stage 5C: Attach orchestration metadata
        if hasattr(proposal, "_orchestration_result"):
            orc = proposal._orchestration_result
            if orc is not None:
                record.orchestration_used = True
                record.orchestration_dag_nodes = orc.dag_nodes
                record.orchestration_agents_used = orc.agents_used
                record.orchestration_parallel_stages = orc.parallel_stages

        # Stage 5D: Attach causal debugging metadata
        if hasattr(proposal, "_causal_diagnosis"):
            cd = proposal._causal_diagnosis
            if cd is not None:
                record.causal_debug_used = True
                record.causal_root_cause = cd.root_cause
                record.causal_confidence = cd.confidence
                record.causal_interventions = cd.interventions_performed

        # Stage 5E: Attach issue resolution metadata
        if hasattr(proposal, "_issue_resolution_result"):
            ir = proposal._issue_resolution_result
            if ir is not None:
                record.issue_resolution_used = True
                record.issue_autonomy_level = ir.autonomy_level.value
                record.issue_abstained = ir.status.value == "abstained"

        # Stage 6A: Attach hash chain metadata
        if hasattr(proposal, "_hash_chain_entry"):
            hce = proposal._hash_chain_entry
            if hce is not None:
                record.hash_chain_hash = hce.chain_hash
                record.hash_chain_position = hce.chain_position

        # Stage 6A: Content credentials count
        if hasattr(proposal, "_content_credential_result"):
            ccr = proposal._content_credential_result
            if ccr is not None:
                record.content_credentials_signed = len(ccr.credentials)

        # Stage 6A: Governance credential status
        if hasattr(proposal, "_governance_credential_result"):
            gcr = proposal._governance_credential_result
            if gcr is not None:
                record.governance_credential_status = gcr.status.value

        # Stage 6B: Co-evolution metadata
        if hasattr(proposal, "_coevolution_result"):
            cr = proposal._coevolution_result
            if cr is not None:
                record.coevolution_hard_negatives_mined = cr.hard_negatives_mined
                record.coevolution_adversarial_tests = cr.adversarial_tests_generated
                record.coevolution_bugs_found = cr.tests_found_bugs

        # Stage 6C: Formal spec metadata
        if hasattr(proposal, "_formal_spec_result"):
            fsr = proposal._formal_spec_result
            if fsr is not None:
                record.formal_specs_generated = len(fsr.specs)
                record.formal_spec_coverage_percent = fsr.overall_coverage_percent
                if fsr.tla_plus_results:
                    record.tla_plus_states_explored = sum(
                        r.states_explored for r in fsr.tla_plus_results
                    )

        # Stage 6D: E-graph metadata
        if hasattr(proposal, "_formal_guarantees_result"):
            fg = proposal._formal_guarantees_result
            if fg is not None and fg.egraph is not None:
                er = fg.egraph
                record.egraph_used = True
                record.egraph_status = er.status.value
                record.egraph_rules_applied = len(er.rules_applied)

        # Stage 6E: Symbolic execution metadata
        if hasattr(proposal, "_formal_guarantees_result"):
            fg = proposal._formal_guarantees_result
            if fg is not None and fg.symbolic_execution is not None:
                se = fg.symbolic_execution
                record.symbolic_execution_used = True
                record.symbolic_properties_proved = se.properties_proved
                record.symbolic_counterexamples = len(se.counterexamples)

        try:
            await self._history.record(record)
        except Exception as exc:
            self._logger.error("history_write_failed", error=str(exc))
            return

        if not rolled_back:
            config_hash = self._compute_config_hash(files_changed)
            version = ConfigVersion(
                version=self._current_version,
                proposal_ids=[proposal.id],
                config_hash=config_hash,
            )
            try:
                await self._history.record_version(version, previous_version=from_version)
            except Exception as exc:
                self._logger.error("version_write_failed", error=str(exc))

    # ─── Stage 4B: GRPO Background Retraining ──────────────────────────────

    async def _grpo_retrain_background(self) -> None:
        """
        Background task to run the GRPO retraining pipeline.
        Collects data → SFT → GRPO RL → evaluate → deploy if improved.
        """
        if self._grpo is None:
            return
        try:
            self._logger.info("grpo_retrain_starting")
            training_examples = await self._grpo.collect_training_data()
            if not training_examples:
                self._logger.info("grpo_retrain_skipped_insufficient_data")
                return

            training_run = await self._grpo.run_sft(training_examples)
            training_run = await self._grpo.run_grpo(training_run)
            evaluation = await self._grpo.evaluate()

            if evaluation.statistically_significant and evaluation.improvement_percent > 0:
                self._logger.info(
                    "grpo_retrain_deployed",
                    improvement=f"{evaluation.improvement_percent:.1f}%",
                    pass_at_1_base=f"{evaluation.base_model_pass_at_1:.2f}",
                    pass_at_1_finetuned=f"{evaluation.finetuned_model_pass_at_1:.2f}",
                )
            else:
                self._logger.info(
                    "grpo_retrain_no_improvement",
                    improvement=f"{evaluation.improvement_percent:.1f}%",
                    significant=evaluation.statistically_significant,
                )
        except Exception as exc:
            self._logger.warning("grpo_retrain_error", error=str(exc))

    # ─── Stage 6B: Co-Evolution Background Cycle ────────────────────────────

    async def _coevolution_background(
        self,
        files: list[str],
        proposal_id: str,
    ) -> None:
        """
        Background task to run a co-evolution cycle:
        mine hard negatives from history and adversarial tests,
        then feed into GRPO training.
        """
        if self._hard_negative_miner is None:
            return
        try:
            from ecodiaos.systems.simula.coevolution.hard_negative_miner import HardNegativeMiner

            if not isinstance(self._hard_negative_miner, HardNegativeMiner):
                return

            # Import adversarial tester if available
            adversarial_gen = None
            if self._adversarial_tester is not None:
                from ecodiaos.systems.simula.coevolution.adversarial_tester import (
                    AdversarialTestGenerator,
                )

                if isinstance(self._adversarial_tester, AdversarialTestGenerator):
                    adversarial_gen = self._adversarial_tester

            cycle_result = await self._hard_negative_miner.run_cycle(
                adversarial_generator=adversarial_gen,
                files=files,
            )

            self._logger.info(
                "coevolution_cycle_complete",
                proposal_id=proposal_id,
                hard_negatives=cycle_result.hard_negatives_mined,
                adversarial_tests=cycle_result.adversarial_tests_generated,
                bugs_found=cycle_result.tests_found_bugs,
                grpo_examples=cycle_result.grpo_examples_produced,
                duration_ms=cycle_result.duration_ms,
            )

            # Feed hard negatives into GRPO if available
            if self._grpo is not None and cycle_result.grpo_examples_produced > 0:
                grpo_batch = await self._hard_negative_miner.prepare_grpo_batch(
                    await self._hard_negative_miner.mine_from_history(),
                )
                self._logger.info(
                    "coevolution_grpo_batch_ready",
                    examples=len(grpo_batch),
                )

        except Exception as exc:
            self._logger.warning(
                "coevolution_background_error",
                error=str(exc),
                proposal_id=proposal_id,
            )

    # ─── Helpers ──────────────────────────────────────────────────────────────

    def _compute_config_hash(self, files_changed: list[str]) -> str:
        """Compute a stable hash of the current config state."""
        hasher = hashlib.sha256()
        for rel_path in sorted(files_changed):
            full_path = self._root / rel_path
            hasher.update(rel_path.encode())
            if full_path.exists():
                hasher.update(str(full_path.stat().st_mtime).encode())
        return hasher.hexdigest()[:16]

    def _get_iron_rule_for(self, proposal: EvolutionProposal) -> str:
        """Return the relevant iron rule for a forbidden category."""
        rule_map = {
            "modify_equor": "Simula CANNOT modify Equor in any way.",
            "modify_constitution": "Simula CANNOT modify constitutional drives.",
            "modify_invariants": "Simula CANNOT modify invariants.",
            "modify_self_evolution": "Simula CANNOT modify its own logic (no self-modifying code).",
        }
        return rule_map.get(proposal.category.value, "Category is forbidden.")

    def _invalidate_analytics(self) -> None:
        """Invalidate analytics cache after a proposal completes."""
        if self._analytics is not None:
            self._analytics.invalidate_cache()

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\simulation.py =====

"""
EcodiaOS -- Simula Deep Simulation Engine

Before any change is applied, the simulator performs multi-strategy
impact prediction. This is the brain of Simula's decision-making.

Strategy stack (per proposal):
  1. Category-specific validation (static analysis / budget check / LLM reasoning)
  2. Counterfactual episode replay — "What if this existed during episode X?"
  3. Dependency graph analysis — blast radius via import-graph traversal
  4. Resource cost estimation — heuristic compute/memory/token impact
  5. Constitutional alignment prediction — drive alignment scoring
  6. Risk synthesis — combine all signals into a unified assessment

Budget efficiency:
  - Counterfactual replay: batches 30 episodes into ONE LLM call (~800 tokens)
  - Constitutional alignment: single call, 100 tokens max output
  - Dependency analysis: pure Python ast module, zero LLM tokens
  - Resource cost: heuristic lookup table, zero LLM tokens
  - Analytics-informed caution: uses cached history, zero LLM tokens

Target latency: <=30s for full simulation (spec requirement).
"""

from __future__ import annotations

import ast
import asyncio
import re
from pathlib import Path
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.simula.types import (
    GOVERNANCE_REQUIRED,
    SELF_APPLICABLE,
    CautionAdjustment,
    ChangeCategory,
    CounterfactualResult,
    DependencyImpact,
    EnrichedSimulationResult,
    EvolutionProposal,
    ImpactType,
    ResourceCostEstimate,
    RiskLevel,
    SimulationResult,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.config import SimulaConfig
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine

from ecodiaos.clients.context_compression import ContextCompressor
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider

logger = structlog.get_logger().bind(system="simula.simulation")

# Valid Python identifier pattern for names
_VALID_NAME = re.compile(r"^[A-Za-z][A-Za-z0-9_]*$")
_SNAKE_CASE = re.compile(r"^[a-z][a-z0-9_]*$")
_PASCAL_CASE = re.compile(r"^[A-Z][A-Za-z0-9]*$")

# Resource cost heuristics per category (zero LLM tokens)
_RESOURCE_COST_HEURISTICS: dict[ChangeCategory, dict[str, int | float]] = {
    ChangeCategory.ADD_EXECUTOR: {
        "llm_tokens_per_hour": 500,
        "compute_ms_per_cycle": 5,
        "memory_mb": 2.0,
    },
    ChangeCategory.ADD_INPUT_CHANNEL: {
        "llm_tokens_per_hour": 200,
        "compute_ms_per_cycle": 10,
        "memory_mb": 5.0,
    },
    ChangeCategory.ADD_PATTERN_DETECTOR: {
        "llm_tokens_per_hour": 0,
        "compute_ms_per_cycle": 3,
        "memory_mb": 1.0,
    },
    ChangeCategory.ADJUST_BUDGET: {
        "llm_tokens_per_hour": 0,
        "compute_ms_per_cycle": 0,
        "memory_mb": 0.0,
    },
}

# System directories for dependency analysis
_SYSTEM_DIRS: dict[str, str] = {
    "memory": "src/ecodiaos/systems/memory",
    "equor": "src/ecodiaos/systems/equor",
    "atune": "src/ecodiaos/systems/atune",
    "voxis": "src/ecodiaos/systems/voxis",
    "nova": "src/ecodiaos/systems/nova",
    "axon": "src/ecodiaos/systems/axon",
    "evo": "src/ecodiaos/systems/evo",
    "simula": "src/ecodiaos/systems/simula",
}


class ChangeSimulator:
    """
    Deep multi-strategy impact simulator. Combines category-specific
    validation with counterfactual replay, dependency analysis, resource
    estimation, and constitutional alignment prediction.

    All strategies run concurrently where possible (asyncio.gather)
    to stay within the 30s latency target.
    """

    def __init__(
        self,
        config: SimulaConfig,
        llm: LLMProvider,
        memory: MemoryService | None = None,
        analytics: EvolutionAnalyticsEngine | None = None,
        codebase_root: Path | None = None,
    ) -> None:
        self._config = config
        self._llm = llm
        self._memory = memory
        self._analytics = analytics
        self._root = codebase_root or Path(config.codebase_root).resolve()
        self._log = logger
        # Optimization: detect optimized provider for budget checks + cache tagging
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        # KVzip compression for large counterfactual replay prompts
        self._compressor = ContextCompressor(
            prune_ratio=config.kv_compression_ratio,
            enabled=config.kv_compression_enabled,
        )

    async def simulate(self, proposal: EvolutionProposal) -> EnrichedSimulationResult:
        """
        Main simulation entry point. Runs category-specific validation
        plus cross-cutting deep analysis, then synthesizes a unified
        risk assessment.

        All independent analyses run concurrently via asyncio.gather.
        """
        self._log.info(
            "deep_simulation_started",
            proposal_id=proposal.id,
            category=proposal.category.value,
        )

        # Forbidden categories are rejected before reaching simulation,
        # but defend in depth
        from ecodiaos.systems.simula.types import FORBIDDEN
        if proposal.category in FORBIDDEN:
            return EnrichedSimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=f"Category {proposal.category.value} is forbidden.",
            )

        # Run all strategies concurrently
        base_task = self._simulate_by_category(proposal)
        counterfactual_task = self._counterfactual_replay(proposal)
        dependency_task = self._analyze_dependencies(proposal)
        alignment_task = self._predict_constitutional_alignment(proposal)

        base_result, counterfactuals, dependency_impacts, alignment = await asyncio.gather(
            base_task,
            counterfactual_task,
            dependency_task,
            alignment_task,
            return_exceptions=True,
        )

        # Handle exceptions gracefully -- individual strategy failures
        # should not prevent the simulation from completing
        if isinstance(base_result, BaseException):
            self._log.error("base_simulation_failed", error=str(base_result))
            base_result = SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary=f"Base simulation failed: {base_result}",
            )
        if isinstance(counterfactuals, BaseException):
            self._log.warning("counterfactual_replay_failed", error=str(counterfactuals))
            counterfactuals = []
        if isinstance(dependency_impacts, BaseException):
            self._log.warning("dependency_analysis_failed", error=str(dependency_impacts))
            dependency_impacts = []
        if isinstance(alignment, BaseException):
            self._log.warning("alignment_prediction_failed", error=str(alignment))
            alignment = 0.0

        # Resource cost estimation (pure heuristic, synchronous)
        cost_estimate = self._estimate_resource_cost(proposal)

        # Synthesize all signals
        result = self._synthesize_risk(
            base_result=base_result,
            counterfactuals=counterfactuals,
            dependency_impacts=dependency_impacts,
            cost_estimate=cost_estimate,
            constitutional_alignment=alignment,
            proposal=proposal,
        )

        self._log.info(
            "deep_simulation_complete",
            proposal_id=proposal.id,
            risk_level=result.risk_level.value,
            counterfactuals=len(result.counterfactuals),
            dependency_blast_radius=result.dependency_blast_radius,
            constitutional_alignment=round(result.constitutional_alignment, 2),
            episodes_tested=result.episodes_tested,
        )
        return result

    # ─── Category-Specific Simulation ────────────────────────────────────────

    async def _simulate_by_category(self, proposal: EvolutionProposal) -> SimulationResult:
        """Dispatch to the right category-specific strategy."""
        if proposal.category in SELF_APPLICABLE:
            if proposal.category == ChangeCategory.ADJUST_BUDGET:
                return await self._simulate_budget(proposal)
            else:
                return await self._simulate_additive(proposal)
        elif proposal.category in GOVERNANCE_REQUIRED:
            return await self._simulate_governance(proposal)
        else:
            return SimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=f"Category {proposal.category.value} is forbidden.",
            )

    async def _simulate_additive(self, proposal: EvolutionProposal) -> SimulationResult:
        """
        Enhanced static analysis for additive changes.
        Beyond name validation: checks naming conventions, system existence,
        existing overlap detection, and spec completeness.
        """
        spec = proposal.change_spec
        issues: list[str] = []

        # Determine the relevant name and validate by category
        name: str | None = None

        if proposal.category == ChangeCategory.ADD_EXECUTOR:
            name = spec.executor_name
            if not spec.executor_action_type:
                issues.append("executor_action_type is required")
            if name and not _SNAKE_CASE.match(name):
                issues.append(
                    f"Executor module name {name!r} should be snake_case "
                    f"(e.g., 'email_sender', not 'EmailSender')"
                )
            # Check if executor with this action_type already exists
            if spec.executor_action_type:
                existing = await self._check_existing_executor(spec.executor_action_type)
                if existing:
                    issues.append(
                        f"Executor for action_type {spec.executor_action_type!r} "
                        f"already exists: {existing}"
                    )
            # Verify the axon executors directory exists
            executors_dir = self._root / "src" / "ecodiaos" / "systems" / "axon" / "executors"
            if not executors_dir.exists():
                issues.append("Axon executors directory not found -- system may not be built yet")

        elif proposal.category == ChangeCategory.ADD_INPUT_CHANNEL:
            name = spec.channel_name
            if not spec.channel_type:
                issues.append("channel_type is required")
            if name and not _SNAKE_CASE.match(name):
                issues.append(f"Channel module name {name!r} should be snake_case")

        elif proposal.category == ChangeCategory.ADD_PATTERN_DETECTOR:
            name = spec.detector_name
            if not spec.detector_pattern_type:
                issues.append("detector_pattern_type is required")
            if name and not _PASCAL_CASE.match(name):
                issues.append(
                    f"Detector class name {name!r} should be PascalCase "
                    f"(e.g., 'FrequencyDetector')"
                )

        if name is None:
            issues.append("No name provided for additive change")
        elif not _VALID_NAME.match(name):
            issues.append(f"Name {name!r} is not a valid Python identifier")

        if issues:
            return SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary="Spec validation failed: " + "; ".join(issues),
                benefit_summary=proposal.expected_benefit,
            )

        return SimulationResult(
            episodes_tested=0,
            risk_level=RiskLevel.LOW,
            risk_summary="Additive change passes enhanced static analysis.",
            benefit_summary=proposal.expected_benefit,
        )

    async def _simulate_budget(self, proposal: EvolutionProposal) -> SimulationResult:
        """Validate budget parameter range and assess risk magnitude."""
        from ecodiaos.systems.evo.types import TUNABLE_PARAMETERS

        spec = proposal.change_spec
        if not spec.budget_parameter:
            return SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary="ADJUST_BUDGET proposal missing budget_parameter.",
            )
        if spec.budget_parameter not in TUNABLE_PARAMETERS:
            return SimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=f"Parameter {spec.budget_parameter!r} is not in TUNABLE_PARAMETERS.",
            )

        param_spec = TUNABLE_PARAMETERS[spec.budget_parameter]
        new_val = spec.budget_new_value
        old_val = spec.budget_old_value

        if new_val is None:
            return SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary="ADJUST_BUDGET proposal missing budget_new_value.",
            )
        if new_val < param_spec.min_val or new_val > param_spec.max_val:
            return SimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=(
                    f"{spec.budget_parameter} new value {new_val} is outside allowed range "
                    f"[{param_spec.min_val}, {param_spec.max_val}]."
                ),
            )

        delta = abs(new_val - (old_val or 0.0))
        risk = RiskLevel.MODERATE if delta > 0.05 else RiskLevel.LOW
        return SimulationResult(
            episodes_tested=0,
            risk_level=risk,
            risk_summary=(
                f"{spec.budget_parameter}: {old_val} -> {new_val} "
                f"(delta={delta:.4f}). Risk: {risk.value}."
            ),
            benefit_summary=proposal.expected_benefit,
        )

    async def _simulate_governance(self, proposal: EvolutionProposal) -> SimulationResult:
        """
        LLM-based impact assessment for governance-required changes.
        Retrieves up to 30 recent episode summaries and asks the LLM
        to reason about the impact with explicit risk dimensions.
        """
        episode_context = ""
        episodes_count = 0

        if self._memory is not None:
            try:
                episodes = await asyncio.wait_for(
                    self._memory.retrieve_recent_episodes(limit=30),  # type: ignore[attr-defined]
                    timeout=5.0,
                )
                episodes_count = len(episodes)
                episode_context = self._build_episode_context(episodes)
            except Exception as exc:
                self._log.warning("episode_fetch_failed", error=str(exc))

        # Build an explicit multi-dimension assessment prompt
        prompt = (
            "You are evaluating a proposed structural change to EcodiaOS, "
            "a computational cognitive architecture.\n\n"
            f"PROPOSAL\n"
            f"Category: {proposal.category.value}\n"
            f"Description: {proposal.description}\n"
            f"Expected benefit: {proposal.expected_benefit}\n"
            f"Affected systems: {', '.join(proposal.change_spec.affected_systems) or 'unspecified'}\n\n"
            f"RECENT EPISODE CONTEXT ({episodes_count} episodes):\n{episode_context}\n\n"
            "Assess this change across four dimensions:\n"
            "1. BEHAVIORAL_RISK: Would existing behaviors regress? (LOW/MODERATE/HIGH)\n"
            "2. INTEGRATION_RISK: Could this break inter-system contracts? (LOW/MODERATE/HIGH)\n"
            "3. RESOURCE_RISK: Would this significantly increase resource consumption? (LOW/MODERATE/HIGH)\n"
            "4. REVERSIBILITY: How easy is rollback? (EASY/MODERATE/HARD)\n\n"
            "Reply with:\n"
            "RISK: <overall level: LOW|MODERATE|HIGH>\n"
            "BEHAVIORAL: <level>\n"
            "INTEGRATION: <level>\n"
            "RESOURCE: <level>\n"
            "REVERSIBILITY: <level>\n"
            "REASONING: <2-3 sentences>\n"
            "BENEFIT: <1 sentence>"
        )

        # Budget gate: simulation is STANDARD priority — skip in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.simulation", estimated_tokens=400):
                self._log.info("governance_simulation_skipped_budget", proposal_id=proposal.id)
                return SimulationResult(
                    episodes_tested=episodes_count,
                    risk_level=RiskLevel.HIGH,
                    risk_summary="LLM budget exhausted (RED tier) — defaulting to HIGH risk.",
                    benefit_summary=proposal.expected_benefit,
                )

        try:
            if self._optimized:
                response = await asyncio.wait_for(
                    self._llm.evaluate(  # type: ignore[call-arg]
                        prompt=prompt, max_tokens=400, temperature=0.2,
                        cache_system="simula.simulation", cache_method="governance_impact",
                    ),
                    timeout=10.0,
                )
            else:
                response = await asyncio.wait_for(
                    self._llm.evaluate(prompt=prompt, max_tokens=400, temperature=0.2),
                    timeout=10.0,
                )
            risk_level, risk_summary, benefit_summary = self._parse_llm_risk(response.text)
        except TimeoutError:
            self._log.warning("simulation_llm_timeout", proposal_id=proposal.id)
            risk_level = RiskLevel.HIGH
            risk_summary = "LLM assessment timed out; defaulting to HIGH risk."
            benefit_summary = proposal.expected_benefit
        except Exception as exc:
            self._log.error("simulation_llm_error", error=str(exc))
            risk_level = RiskLevel.HIGH
            risk_summary = f"LLM assessment failed: {exc}"
            benefit_summary = proposal.expected_benefit

        return SimulationResult(
            episodes_tested=episodes_count,
            risk_level=risk_level,
            risk_summary=risk_summary,
            benefit_summary=benefit_summary,
        )

    # ─── Counterfactual Episode Replay ───────────────────────────────────────

    async def _counterfactual_replay(
        self, proposal: EvolutionProposal,
    ) -> list[CounterfactualResult]:
        """
        For additive changes, ask: 'If this had existed during recent episodes,
        when would it have been invoked? What would have changed?'

        Token-efficient: batches up to 30 episodes into a single LLM call
        with structured output (~800 tokens total).

        Returns empty list for non-additive changes or when Memory is unavailable.
        """
        # Only meaningful for additive changes
        if proposal.category not in {
            ChangeCategory.ADD_EXECUTOR,
            ChangeCategory.ADD_INPUT_CHANNEL,
            ChangeCategory.ADD_PATTERN_DETECTOR,
        }:
            return []

        if self._memory is None:
            return []

        # Retrieve recent episodes
        try:
            episodes = await asyncio.wait_for(
                self._memory.retrieve_recent_episodes(limit=30),  # type: ignore[attr-defined]
                timeout=5.0,
            )
        except Exception as exc:
            self._log.warning("counterfactual_episode_fetch_failed", error=str(exc))
            return []

        if not episodes:
            return []

        # Build the batch counterfactual prompt
        episode_summaries = []
        for i, ep in enumerate(episodes[:30], start=1):
            summary = getattr(ep, "summary", "") or getattr(ep, "raw_content", "")[:150]
            source = getattr(ep, "source", "unknown")
            episode_summaries.append(f"{i}. [{source}] {summary[:150]}")

        change_desc = self._describe_additive_change(proposal)

        # KVzip: compress episode summaries when batch is large to reduce tokens.
        # Truncate individual episode summaries more aggressively if many episodes.
        max_summary_chars = 150 if len(episode_summaries) <= 15 else 80
        if max_summary_chars < 150:
            episode_summaries = [s[:max_summary_chars] for s in episode_summaries]

        prompt = (
            f"EcodiaOS is considering adding a new capability:\n{change_desc}\n\n"
            f"Below are {len(episode_summaries)} recent episodes. For each, determine:\n"
            f"- Would this new capability have been triggered/relevant? (yes/no)\n"
            f"- If yes, what would have been different? (improvement/regression/neutral)\n\n"
            f"EPISODES:\n" + "\n".join(episode_summaries) + "\n\n"
            "Reply as a numbered list matching the episode numbers:\n"
            "<number>. <yes|no> | <improvement|regression|neutral> | <1 sentence reason>\n"
            "Only include episodes where the answer is 'yes'."
        )

        # Budget gate: skip counterfactual replay in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.simulation", estimated_tokens=500):
                self._log.info("counterfactual_replay_skipped_budget")
                return []

        try:
            if self._optimized:
                response = await asyncio.wait_for(
                    self._llm.evaluate(  # type: ignore[call-arg]
                        prompt=prompt, max_tokens=500, temperature=0.2,
                        cache_system="simula.simulation", cache_method="counterfactual",
                    ),
                    timeout=15.0,
                )
            else:
                response = await asyncio.wait_for(
                    self._llm.evaluate(prompt=prompt, max_tokens=500, temperature=0.2),
                    timeout=15.0,
                )
            return self._parse_counterfactual_response(response.text, episodes)
        except Exception as exc:
            self._log.warning("counterfactual_llm_failed", error=str(exc))
            return []

    def _describe_additive_change(self, proposal: EvolutionProposal) -> str:
        """Human-readable description of an additive change for counterfactual prompt."""
        spec = proposal.change_spec
        if proposal.category == ChangeCategory.ADD_EXECUTOR:
            return (
                f"New Axon Executor: {spec.executor_name or 'unnamed'}\n"
                f"Action type: {spec.executor_action_type or 'unspecified'}\n"
                f"Description: {spec.executor_description or proposal.description}"
            )
        elif proposal.category == ChangeCategory.ADD_INPUT_CHANNEL:
            return (
                f"New Atune Input Channel: {spec.channel_name or 'unnamed'}\n"
                f"Channel type: {spec.channel_type or 'unspecified'}\n"
                f"Description: {spec.channel_description or proposal.description}"
            )
        elif proposal.category == ChangeCategory.ADD_PATTERN_DETECTOR:
            return (
                f"New Evo Pattern Detector: {spec.detector_name or 'unnamed'}\n"
                f"Pattern type: {spec.detector_pattern_type or 'unspecified'}\n"
                f"Description: {spec.detector_description or proposal.description}"
            )
        return proposal.description

    def _parse_counterfactual_response(
        self, text: str, episodes: list[Any],
    ) -> list[CounterfactualResult]:
        """Parse the LLM's batch counterfactual response into structured results."""
        results: list[CounterfactualResult] = []
        for line in text.strip().splitlines():
            line = line.strip()
            if not line or not line[0].isdigit():
                continue
            try:
                # Expected: "3. yes | improvement | Would have handled email notifications"
                num_part, rest = line.split(".", 1)
                idx = int(num_part.strip()) - 1
                if idx < 0 or idx >= len(episodes):
                    continue

                parts = [p.strip() for p in rest.split("|")]
                if len(parts) < 2:
                    continue

                triggered = parts[0].lower().strip() in ("yes", "y", "true")
                if not triggered:
                    continue

                impact_str = parts[1].lower().strip() if len(parts) > 1 else "neutral"
                if "improvement" in impact_str:
                    impact = ImpactType.IMPROVEMENT
                elif "regression" in impact_str:
                    impact = ImpactType.REGRESSION
                else:
                    impact = ImpactType.NEUTRAL

                reasoning = parts[2].strip() if len(parts) > 2 else ""

                ep = episodes[idx]
                results.append(CounterfactualResult(
                    episode_id=getattr(ep, "id", f"ep_{idx}"),
                    would_have_triggered=True,
                    predicted_outcome=reasoning[:200],
                    impact=impact,
                    confidence=0.6,
                    reasoning=reasoning[:300],
                ))
            except (ValueError, IndexError):
                continue

        return results

    # ─── Dependency Graph Analysis ───────────────────────────────────────────

    async def _analyze_dependencies(
        self, proposal: EvolutionProposal,
    ) -> list[DependencyImpact]:
        """
        Static analysis of the affected system's import graph.
        Uses the ast module to parse Python files and trace imports.
        Zero LLM tokens -- pure computation.
        """
        affected_systems = proposal.change_spec.affected_systems
        if not affected_systems:
            affected_systems = self._infer_affected_systems(proposal)

        impacts: list[DependencyImpact] = []

        for sys_name in affected_systems:
            sys_dir = self._root / _SYSTEM_DIRS.get(sys_name, f"src/ecodiaos/systems/{sys_name}")
            if not sys_dir.exists():
                continue

            # Find all Python files in the affected system
            py_files = list(sys_dir.rglob("*.py"))

            # For each file, find what imports it from other systems
            for py_file in py_files:
                rel_path = str(py_file.relative_to(self._root))
                module_name = self._path_to_module(rel_path)
                if not module_name:
                    continue

                # Check how many other files import this module
                importers = await self._find_importers(module_name)
                if importers:
                    impacts.append(DependencyImpact(
                        file_path=rel_path,
                        impact_type="import_dependency",
                        risk_contribution=min(1.0, len(importers) * 0.1),
                    ))

            # Check for test coverage
            test_dir = self._root / "tests" / "unit" / "systems" / sys_name
            if test_dir.exists():
                test_files = list(test_dir.rglob("*.py"))
                impacts.append(DependencyImpact(
                    file_path=str(test_dir.relative_to(self._root)),
                    impact_type="test_coverage",
                    risk_contribution=0.0 if test_files else 0.3,
                ))

        return impacts

    def _infer_affected_systems(self, proposal: EvolutionProposal) -> list[str]:
        """Infer which systems a change affects from the category."""
        category_to_systems: dict[ChangeCategory, list[str]] = {
            ChangeCategory.ADD_EXECUTOR: ["axon"],
            ChangeCategory.ADD_INPUT_CHANNEL: ["atune"],
            ChangeCategory.ADD_PATTERN_DETECTOR: ["evo"],
            ChangeCategory.ADJUST_BUDGET: [],
            ChangeCategory.MODIFY_CONTRACT: [],
            ChangeCategory.ADD_SYSTEM_CAPABILITY: [],
            ChangeCategory.MODIFY_CYCLE_TIMING: ["synapse"],
            ChangeCategory.CHANGE_CONSOLIDATION: ["evo"],
        }
        return category_to_systems.get(proposal.category, [])

    async def _find_importers(self, module_name: str) -> list[str]:
        """Find files that import the given module. Scans src/ directory."""
        importers: list[str] = []
        src_dir = self._root / "src"
        if not src_dir.exists():
            return importers

        # Extract the short module name for import matching
        parts = module_name.split(".")
        parts[-1] if parts else module_name

        for py_file in src_dir.rglob("*.py"):
            try:
                source = py_file.read_text(encoding="utf-8")
                tree = ast.parse(source, filename=str(py_file))
            except Exception:
                continue

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        if module_name in alias.name:
                            importers.append(str(py_file.relative_to(self._root)))
                            break
                elif isinstance(node, ast.ImportFrom):
                    if node.module and module_name in node.module:
                        importers.append(str(py_file.relative_to(self._root)))

        return importers

    def _path_to_module(self, rel_path: str) -> str | None:
        """Convert a relative file path to a dotted module name."""
        parts = rel_path.replace("\\", "/").split("/")
        if parts and parts[0] == "src":
            parts = parts[1:]
        if not parts:
            return None
        if parts[-1].endswith(".py"):
            parts[-1] = parts[-1][:-3]
        if parts[-1] == "__init__":
            parts = parts[:-1]
        return ".".join(parts) if parts else None

    # ─── Resource Cost Estimation ────────────────────────────────────────────

    def _estimate_resource_cost(
        self, proposal: EvolutionProposal,
    ) -> ResourceCostEstimate:
        """
        Heuristic estimation of ongoing resource impact.
        Zero LLM tokens -- pure lookup + arithmetic.
        """
        heuristics = _RESOURCE_COST_HEURISTICS.get(proposal.category)
        if heuristics is None:
            # Governance-required changes: estimate moderate cost
            return ResourceCostEstimate(
                estimated_additional_llm_tokens_per_hour=1000,
                estimated_additional_compute_ms_per_cycle=10,
                estimated_memory_mb=5.0,
                budget_headroom_percent=90.0,
            )

        tokens = int(heuristics.get("llm_tokens_per_hour", 0))
        compute = int(heuristics.get("compute_ms_per_cycle", 0))
        memory = float(heuristics.get("memory_mb", 0.0))

        # Budget headroom: what percent of the relevant system's budget remains
        # after adding this cost
        system_budget = self._get_system_budget(proposal)
        headroom = 100.0
        if system_budget > 0 and tokens > 0:
            headroom = max(0.0, 100.0 * (1.0 - tokens / system_budget))

        return ResourceCostEstimate(
            estimated_additional_llm_tokens_per_hour=tokens,
            estimated_additional_compute_ms_per_cycle=compute,
            estimated_memory_mb=memory,
            budget_headroom_percent=round(headroom, 1),
        )

    def _get_system_budget(self, proposal: EvolutionProposal) -> int:
        """Get the affected system's hourly token budget."""
        category_to_system: dict[ChangeCategory, str] = {
            ChangeCategory.ADD_EXECUTOR: "axon",
            ChangeCategory.ADD_INPUT_CHANNEL: "atune",
            ChangeCategory.ADD_PATTERN_DETECTOR: "evo",
        }
        sys_name = category_to_system.get(proposal.category, "")
        # Default system budgets (from config/default.yaml)
        default_budgets: dict[str, int] = {
            "atune": 60000,
            "equor": 30000,
            "nova": 120000,
            "voxis": 120000,
            "evo": 60000,
            "axon": 60000,
            "simula": 10000,
        }
        return default_budgets.get(sys_name, 60000)

    # ─── Constitutional Alignment Prediction ─────────────────────────────────

    async def _predict_constitutional_alignment(
        self, proposal: EvolutionProposal,
    ) -> float:
        """
        Predict how well this change aligns with the four constitutional drives.
        Single LLM call, 100 tokens max output. Returns -1.0 to 1.0.

        Budget: ~200 tokens total (prompt + response).
        """
        prompt = (
            "EcodiaOS has four constitutional drives: "
            "coherence (make sense), care (orient toward wellbeing), "
            "growth (become more capable), honesty (represent reality truthfully).\n\n"
            f"Proposed change: {proposal.description[:200]}\n"
            f"Category: {proposal.category.value}\n"
            f"Expected benefit: {proposal.expected_benefit[:100]}\n\n"
            "Score the alignment of this change with the drives from -1.0 to 1.0.\n"
            "Reply with a single number only (e.g., 0.7)."
        )

        # Budget gate: skip alignment prediction in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.simulation", estimated_tokens=100):
                self._log.info("alignment_prediction_skipped_budget")
                return 0.0

        try:
            if self._optimized:
                response = await asyncio.wait_for(
                    self._llm.evaluate(  # type: ignore[call-arg]
                        prompt=prompt, max_tokens=20, temperature=0.1,
                        cache_system="simula.simulation", cache_method="constitutional_alignment",
                    ),
                    timeout=5.0,
                )
            else:
                response = await asyncio.wait_for(
                    self._llm.evaluate(prompt=prompt, max_tokens=20, temperature=0.1),
                    timeout=5.0,
                )
            # Extract the float from the response
            text = response.text.strip()
            # Handle potential formatting like "0.7" or "Score: 0.7"
            for token in text.split():
                try:
                    score = float(token.strip(".,;:"))
                    return max(-1.0, min(1.0, score))
                except ValueError:
                    continue
            return 0.0
        except Exception as exc:
            self._log.warning("alignment_prediction_failed", error=str(exc))
            return 0.0

    # ─── Risk Synthesis ──────────────────────────────────────────────────────

    def _synthesize_risk(
        self,
        base_result: SimulationResult,
        counterfactuals: list[CounterfactualResult],
        dependency_impacts: list[DependencyImpact],
        cost_estimate: ResourceCostEstimate,
        constitutional_alignment: float,
        proposal: EvolutionProposal,
    ) -> EnrichedSimulationResult:
        """
        Combine all simulation signals into a unified risk assessment.

        Risk factors (weighted):
          - Base category simulation: 40%
          - Counterfactual regression rate: 20%
          - Dependency blast radius: 15%
          - Resource cost: 10%
          - Constitutional alignment: 15% (negative alignment increases risk)

        Dynamic adjustment: if analytics show high rollback rate for this
        category, bump the risk level up one notch.
        """
        # Counterfactual regression rate
        cf_regressions = sum(1 for cf in counterfactuals if cf.impact == ImpactType.REGRESSION)
        cf_total = len(counterfactuals) if counterfactuals else 1
        cf_regression_rate = cf_regressions / max(1, cf_total)

        # Dependency blast radius
        blast_radius = len(dependency_impacts)
        total_risk_contribution = sum(d.risk_contribution for d in dependency_impacts)

        # Resource risk (0-1 scale based on budget consumption)
        resource_risk = 1.0 - (cost_estimate.budget_headroom_percent / 100.0) if cost_estimate else 0.0

        # Constitutional risk (alignment < 0 adds risk)
        alignment_risk = max(0.0, -constitutional_alignment)

        # Base risk as numeric
        base_risk_numeric = {
            RiskLevel.LOW: 0.1,
            RiskLevel.MODERATE: 0.4,
            RiskLevel.HIGH: 0.7,
            RiskLevel.UNACCEPTABLE: 1.0,
        }.get(base_result.risk_level, 0.4)

        # Weighted composite risk score (0.0 - 1.0)
        composite_risk = (
            0.40 * base_risk_numeric
            + 0.20 * cf_regression_rate
            + 0.15 * min(1.0, total_risk_contribution)
            + 0.10 * resource_risk
            + 0.15 * alignment_risk
        )

        # Dynamic caution adjustment from analytics history
        caution_adj: CautionAdjustment | None = None
        if self._analytics is not None:
            caution_adj = self._analytics.should_increase_caution(proposal.category)
            if caution_adj.should_adjust:
                composite_risk = min(1.0, composite_risk + caution_adj.magnitude)
                self._log.info(
                    "caution_increased",
                    category=proposal.category.value,
                    composite_risk=round(composite_risk, 3),
                    magnitude=caution_adj.magnitude,
                    factors=caution_adj.factors,
                    reasoning=caution_adj.reasoning,
                )

        # Map composite score to RiskLevel
        if composite_risk >= 0.75:
            final_risk = RiskLevel.UNACCEPTABLE
        elif composite_risk >= 0.50:
            final_risk = RiskLevel.HIGH
        elif composite_risk >= 0.25:
            final_risk = RiskLevel.MODERATE
        else:
            final_risk = RiskLevel.LOW

        # Emit decision audit log with all signal values and weights
        self._log.info(
            "simulation_decision_audit",
            proposal_id=proposal.id,
            category=proposal.category.value,
            base_risk=f"{0.40 * base_risk_numeric:.3f} (0.40×{base_risk_numeric:.2f})",
            counterfactual_risk=f"{0.20 * cf_regression_rate:.3f} (0.20×{cf_regression_rate:.2f})",
            dependency_risk=f"{0.15 * min(1.0, total_risk_contribution):.3f} (0.15×{total_risk_contribution:.2f})",
            resource_risk=f"{0.10 * resource_risk:.3f} (0.10×{resource_risk:.2f})",
            alignment_risk=f"{0.15 * alignment_risk:.3f} (0.15×{alignment_risk:.2f})",
            weighted_sum=round(composite_risk, 3),
            caution_adjustment=caution_adj.magnitude if caution_adj and caution_adj.should_adjust else 0.0,
            final_risk=final_risk.value,
            episodes_tested=base_result.episodes_tested,
            blast_radius=blast_radius,
            constitutional_alignment=round(constitutional_alignment, 2),
        )

        # Build summary
        summary_parts = [base_result.risk_summary]
        if counterfactuals:
            cf_improvements = sum(1 for cf in counterfactuals if cf.impact == ImpactType.IMPROVEMENT)
            summary_parts.append(
                f"Counterfactual: {cf_improvements} improvements, "
                f"{cf_regressions} regressions across {len(counterfactuals)} triggered episodes."
            )
        if blast_radius > 0:
            summary_parts.append(f"Blast radius: {blast_radius} affected files/modules.")
        if constitutional_alignment != 0.0:
            summary_parts.append(f"Constitutional alignment: {constitutional_alignment:+.2f}.")

        return EnrichedSimulationResult(
            episodes_tested=base_result.episodes_tested,
            differences=base_result.differences,
            improvements=base_result.improvements + sum(
                1 for cf in counterfactuals if cf.impact == ImpactType.IMPROVEMENT
            ),
            regressions=base_result.regressions + cf_regressions,
            neutral_changes=base_result.neutral_changes + sum(
                1 for cf in counterfactuals if cf.impact == ImpactType.NEUTRAL
            ),
            risk_level=final_risk,
            risk_summary=" ".join(summary_parts),
            benefit_summary=base_result.benefit_summary,
            counterfactuals=counterfactuals,
            dependency_impacts=dependency_impacts,
            resource_cost_estimate=cost_estimate,
            constitutional_alignment=constitutional_alignment,
            counterfactual_regression_rate=round(cf_regression_rate, 3),
            dependency_blast_radius=blast_radius,
            caution_adjustment=caution_adj,
        )

    # ─── Helpers ─────────────────────────────────────────────────────────────

    async def _check_existing_executor(self, action_type: str) -> str | None:
        """Check if an executor for this action_type already exists."""
        executors_dir = self._root / "src" / "ecodiaos" / "systems" / "axon" / "executors"
        if not executors_dir.exists():
            return None

        for py_file in executors_dir.glob("*.py"):
            if py_file.name == "__init__.py":
                continue
            try:
                source = py_file.read_text(encoding="utf-8")
                if f'"{action_type}"' in source or f"'{action_type}'" in source:
                    return str(py_file.name)
            except Exception:
                continue
        return None

    async def _check_name_conflict(self, name: str, category: ChangeCategory) -> bool:
        """Returns True if the name would cause a conflict."""
        return bool(not _VALID_NAME.match(name))

    def _build_episode_context(self, episodes: list[Any]) -> str:
        """Build concise context string from episode objects."""
        lines: list[str] = []
        for i, ep in enumerate(episodes[:30], start=1):
            summary = getattr(ep, "summary", "") or getattr(ep, "raw_content", "")[:200]
            source = getattr(ep, "source", "")
            lines.append(f"{i}. [{source}] {summary[:200]}")
        return "\n".join(lines)

    def _parse_llm_risk(self, text: str) -> tuple[RiskLevel, str, str]:
        """Parse the LLM response to extract risk level, reasoning, and benefit."""
        risk_level = RiskLevel.MODERATE
        risk_summary = text[:500]
        benefit_summary = ""

        for line in text.splitlines():
            line = line.strip()
            upper = line.upper()
            if upper.startswith("RISK:"):
                level_str = line.split(":", 1)[-1].strip().upper()
                if level_str == "LOW":
                    risk_level = RiskLevel.LOW
                elif level_str == "MODERATE":
                    risk_level = RiskLevel.MODERATE
                elif level_str == "HIGH":
                    risk_level = RiskLevel.HIGH
                elif level_str == "UNACCEPTABLE":
                    risk_level = RiskLevel.UNACCEPTABLE
            elif upper.startswith("REASONING:"):
                risk_summary = line.split(":", 1)[-1].strip()
            elif upper.startswith("BENEFIT:"):
                benefit_summary = line.split(":", 1)[-1].strip()

        return risk_level, risk_summary, benefit_summary

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\types.py =====

"""
EcodiaOS -- Simula Internal Types

All data types internal to the Simula self-evolution system.
Simula is the organism's capacity for metamorphosis: structural change
beyond parameter tuning. These types model the full lifecycle of an
evolution proposal -- from reception through simulation, governance,
application, and immutable history.
"""

from __future__ import annotations

from datetime import datetime
import enum
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import (
    EOSBaseModel,
    Identified,
    Timestamped,
    utc_now,
)


# --- Enums -------------------------------------------------------------------


class ChangeCategory(enum.StrEnum):
    ADD_EXECUTOR = "add_executor"
    ADD_INPUT_CHANNEL = "add_input_channel"
    ADD_PATTERN_DETECTOR = "add_pattern_detector"
    ADJUST_BUDGET = "adjust_budget"
    MODIFY_CONTRACT = "modify_contract"
    ADD_SYSTEM_CAPABILITY = "add_system_capability"
    MODIFY_CYCLE_TIMING = "modify_cycle_timing"
    CHANGE_CONSOLIDATION = "change_consolidation"
    MODIFY_EQUOR = "modify_equor"
    MODIFY_CONSTITUTION = "modify_constitution"
    MODIFY_INVARIANTS = "modify_invariants"
    MODIFY_SELF_EVOLUTION = "modify_self_evolution"


class ProposalStatus(enum.StrEnum):
    PROPOSED = "proposed"
    SIMULATING = "simulating"
    AWAITING_GOVERNANCE = "awaiting_governance"
    APPROVED = "approved"
    APPLYING = "applying"
    APPLIED = "applied"
    ROLLED_BACK = "rolled_back"
    REJECTED = "rejected"


class RiskLevel(enum.StrEnum):
    LOW = "low"
    MODERATE = "moderate"
    HIGH = "high"
    UNACCEPTABLE = "unacceptable"


class ImpactType(enum.StrEnum):
    IMPROVEMENT = "improvement"
    REGRESSION = "regression"
    NEUTRAL = "neutral"


class TriageStatus(enum.StrEnum):
    """Status of proposal triage (fast-path pre-simulation check)."""
    TRIVIAL = "trivial"
    REQUIRES_SIMULATION = "requires_simulation"


# --- Models -----------------------------------------------------------------


class ChangeSpec(EOSBaseModel):
    """
    Formal specification of what to change.
    One model covers every ChangeCategory -- fields are optional by category.
    """

    # ADD_EXECUTOR
    executor_name: str | None = None
    executor_description: str | None = None
    executor_action_type: str | None = None
    executor_input_schema: dict[str, Any] | None = None

    # ADD_INPUT_CHANNEL
    channel_name: str | None = None
    channel_type: str | None = None
    channel_description: str | None = None

    # ADD_PATTERN_DETECTOR
    detector_name: str | None = None
    detector_description: str | None = None
    detector_pattern_type: str | None = None

    # ADJUST_BUDGET
    budget_parameter: str | None = None
    budget_old_value: float | None = None
    budget_new_value: float | None = None

    # MODIFY_CONTRACT
    contract_changes: list[str] = Field(default_factory=list)

    # ADD_SYSTEM_CAPABILITY
    capability_description: str | None = None

    # MODIFY_CYCLE_TIMING
    timing_parameter: str | None = None
    timing_old_value: float | None = None
    timing_new_value: float | None = None

    # CHANGE_CONSOLIDATION
    consolidation_schedule: str | None = None

    # Cross-cutting
    affected_systems: list[str] = Field(default_factory=list)
    additional_context: str = ""
    code_hint: str = ""  # optional hint of what the code should look like


class SimulationDifference(EOSBaseModel):
    """Describes how one episode's outcome would differ under the proposed change."""

    episode_id: str
    original_outcome: str
    simulated_outcome: str
    impact: ImpactType
    reasoning: str = ""


class SimulationResult(EOSBaseModel):
    """Aggregate outcome of simulating a proposal against recent episodes."""

    episodes_tested: int = 0
    differences: int = 0
    improvements: int = 0
    regressions: int = 0
    neutral_changes: int = 0
    risk_level: RiskLevel = RiskLevel.LOW
    risk_summary: str = ""
    benefit_summary: str = ""
    simulated_at: datetime = Field(default_factory=utc_now)


class CautionAdjustment(EOSBaseModel):
    """
    Transparent caution adjustment logic explaining WHY a proposal's risk
    was bumped. Returned by EvolutionAnalyticsEngine.should_increase_caution().
    """

    should_adjust: bool
    magnitude: float  # 0.0-0.5 additive risk bump
    factors: dict[str, float] = Field(default_factory=dict)  # {factor_name: contribution}
    reasoning: str = ""


class TriageResult(EOSBaseModel):
    """Result of fast-path proposal triage (pre-simulation check)."""

    status: TriageStatus
    assumed_risk: RiskLevel | None = None
    reason: str = ""
    skip_simulation: bool = False


class ProposalResult(EOSBaseModel):
    """Final outcome recorded once a proposal reaches a terminal state."""

    status: ProposalStatus
    reason: str = ""
    version: int | None = None
    governance_record_id: str | None = None
    files_changed: list[str] = Field(default_factory=list)


class EvolutionProposal(Identified, Timestamped):
    """
    The full proposal lifecycle object -- richer than Evo's simplified version.
    Owns the proposal from receipt through simulation, governance, and application.
    """

    source: str  # "evo" | "governance"
    category: ChangeCategory
    description: str
    change_spec: ChangeSpec
    evidence: list[str] = Field(default_factory=list)  # hypothesis IDs / episode IDs
    expected_benefit: str = ""
    risk_assessment: str = ""
    status: ProposalStatus = ProposalStatus.PROPOSED
    simulation: SimulationResult | None = None
    governance_record_id: str | None = None
    result: ProposalResult | None = None


class FileSnapshot(EOSBaseModel):
    """
    One file's state immediately before a change was applied, enabling rollback.
    content is None when the file did not previously exist -- rollback deletes it.
    """

    path: str  # absolute path
    content: str | None  # None means file did not exist before
    existed: bool = True


class ConfigSnapshot(Identified, Timestamped):
    """Full snapshot of all affected files captured before applying a change."""

    proposal_id: str
    files: list[FileSnapshot] = Field(default_factory=list)
    config_version: int  # the version at snapshot time


class ConfigVersion(EOSBaseModel):
    """Tracks one step in the config version chain."""

    version: int
    timestamp: datetime = Field(default_factory=utc_now)
    proposal_ids: list[str] = Field(default_factory=list)  # evolution proposal IDs
    config_hash: str  # SHA256 hash of the canonical config state


class EvolutionRecord(Identified, Timestamped):
    """Immutable history entry written to Neo4j after each successful application."""

    proposal_id: str
    category: ChangeCategory
    description: str
    from_version: int
    to_version: int
    files_changed: list[str] = Field(default_factory=list)
    simulation_risk: RiskLevel
    applied_at: datetime = Field(default_factory=utc_now)
    rolled_back: bool = False
    rollback_reason: str = ""
    # Simulation detail persisted for audit trail and learning
    simulation_episodes_tested: int = 0
    counterfactual_regression_rate: float = 0.0
    dependency_blast_radius: int = 0
    constitutional_alignment: float = 0.0
    resource_tokens_per_hour: int = 0
    caution_reasoning: str = ""
    # Stage 2: Formal verification metadata
    formal_verification_status: str = ""  # "verified"|"failed"|"skipped"|""
    discovered_invariants_count: int = 0
    dafny_rounds: int = 0
    static_analysis_findings: int = 0
    # Stage 4A: Lean 4 proof metadata
    lean_proof_status: str = ""  # "proved"|"failed"|"timeout"|"skipped"|""
    lean_proof_rounds: int = 0
    lean_proven_lemmas_count: int = 0
    lean_copilot_automation_rate: float = 0.0
    lean_library_lemmas_reused: int = 0
    # Stage 4B: GRPO fine-tuning metadata
    grpo_model_used: str = ""  # "" = base model, else fine-tuned model id
    grpo_ab_group: str = ""  # "base"|"finetuned"|""
    # Stage 4C: Diffusion repair metadata
    diffusion_repair_used: bool = False
    diffusion_repair_status: str = ""  # "repaired"|"partial"|"failed"|"skipped"|""
    diffusion_repair_steps: int = 0
    diffusion_improvement_rate: float = 0.0
    # Stage 5A: Neurosymbolic synthesis metadata
    synthesis_strategy_used: str = ""  # "hysynth"|"sketch_solve"|"chopchop"|"cegis_fallback"|""
    synthesis_status: str = ""  # "synthesized"|"partial"|"failed"|"timeout"|"skipped"|""
    synthesis_speedup_vs_baseline: float = 0.0
    synthesis_candidates_explored: int = 0
    # Stage 5B: Neural repair metadata
    repair_agent_used: bool = False
    repair_agent_status: str = ""  # "repaired"|"partial"|"failed"|"timeout"|"skipped"|"budget_exceeded"|""
    repair_attempts: int = 0
    repair_cost_usd: float = 0.0
    # Stage 5C: Orchestration metadata
    orchestration_used: bool = False
    orchestration_dag_nodes: int = 0
    orchestration_agents_used: int = 0
    orchestration_parallel_stages: int = 0
    # Stage 5D: Causal debugging metadata
    causal_debug_used: bool = False
    causal_root_cause: str = ""
    causal_confidence: float = 0.0
    causal_interventions: int = 0
    # Stage 5E: Issue resolution metadata
    issue_resolution_used: bool = False
    issue_autonomy_level: str = ""  # "lint"|"dependency"|"test_fix"|"logic_bug"|""
    issue_abstained: bool = False
    # Stage 6A: Cryptographic auditability metadata
    hash_chain_hash: str = ""  # SHA-256 chain hash for this record
    hash_chain_position: int = 0  # position in the hash chain
    content_credentials_signed: int = 0  # number of files signed with C2PA
    governance_credential_status: str = ""  # "valid"|"revoked"|"expired"|"unverified"|""
    # Stage 6B: Co-evolution metadata
    coevolution_hard_negatives_mined: int = 0
    coevolution_adversarial_tests: int = 0
    coevolution_bugs_found: int = 0
    # Stage 6C: Formal spec generation metadata
    formal_specs_generated: int = 0
    formal_spec_coverage_percent: float = 0.0
    tla_plus_states_explored: int = 0
    # Stage 6D: E-graph metadata
    egraph_used: bool = False
    egraph_status: str = ""  # "saturated"|"partial"|"timeout"|"failed"|"skipped"|""
    egraph_rules_applied: int = 0
    # Stage 6E: Symbolic execution metadata
    symbolic_execution_used: bool = False
    symbolic_properties_proved: int = 0
    symbolic_counterexamples: int = 0


class CodeChangeResult(EOSBaseModel):
    """What the code agent returns after implementing a structural change."""

    success: bool
    files_written: list[str] = Field(default_factory=list)
    summary: str = ""
    error: str = ""
    lint_passed: bool = True
    tests_passed: bool = True
    test_output: str = ""
    # Stage 1A: Extended-thinking model metrics
    used_extended_thinking: bool = False
    reasoning_tokens: int = 0
    # Stage 1C: KVzip context compression metrics
    kv_compression_ratio: float = 0.0  # 0.0 = no savings, 1.0 = maximum
    kv_messages_compressed: int = 0
    kv_original_tokens: int = 0
    kv_compressed_tokens: int = 0
    # Stage 2C: Static analysis metrics
    static_analysis_findings: int = 0
    static_analysis_fix_iterations: int = 0
    # Stage 2D: AgentCoder metrics
    agent_coder_iterations: int = 0
    test_designer_test_count: int = 0
    # Stage 4B: GRPO model routing metrics
    grpo_model_used: str = ""
    grpo_ab_group: str = ""  # "base"|"finetuned"|""
    # Stage 4C: Diffusion repair metrics
    diffusion_repair_attempted: bool = False
    diffusion_repair_succeeded: bool = False
    # Stage 5A: Synthesis metrics
    synthesis_strategy: str = ""  # "hysynth"|"sketch_solve"|"chopchop"|"cegis_fallback"|""
    synthesis_speedup: float = 0.0
    # Stage 5B: Repair metrics
    repair_attempted: bool = False
    repair_succeeded: bool = False
    repair_cost_usd: float = 0.0
    # Stage 5C: Orchestration metrics
    orchestration_used: bool = False
    orchestration_agents: int = 0


class HealthCheckResult(EOSBaseModel):
    """Result of a post-apply codebase health check."""

    healthy: bool
    issues: list[str] = Field(default_factory=list)
    checked_at: datetime = Field(default_factory=utc_now)
    # Stage 2: Formal verification result (attached when verification runs)
    formal_verification: object | None = None  # FormalVerificationResult
    # Stage 4A: Lean 4 proof verification result (attached when Lean verification runs)
    lean_verification: object | None = None  # LeanVerificationResult
    # Stage 5D: Causal debugging result (attached when causal debug runs)
    causal_diagnosis: object | None = None  # CausalDiagnosis
    # Stage 6: Formal guarantees result (attached when Stage 6 checks run)
    formal_guarantees: object | None = None  # FormalGuaranteesResult


# --- Enriched Simulation Models ----------------------------------------------


class CounterfactualResult(EOSBaseModel):
    """
    Result of asking: 'If this change had existed during episode X,
    what would have been different?'

    Batched into a single LLM call across multiple episodes for
    token efficiency (~800 tokens per 30-episode batch).
    """

    episode_id: str
    would_have_triggered: bool = False
    predicted_outcome: str = ""
    impact: ImpactType = ImpactType.NEUTRAL
    confidence: float = 0.5
    reasoning: str = ""


class DependencyImpact(EOSBaseModel):
    """
    A file or module affected by a proposed change, discovered
    via static import-graph analysis (zero LLM tokens).
    """

    file_path: str
    impact_type: str = "import_dependency"  # "direct_modification" | "import_dependency" | "test_coverage"
    risk_contribution: float = 0.0


class ResourceCostEstimate(EOSBaseModel):
    """
    Heuristic estimation of the ongoing resource cost a change
    would add to the running system. Computed without LLM calls.
    """

    estimated_additional_llm_tokens_per_hour: int = 0
    estimated_additional_compute_ms_per_cycle: int = 0
    estimated_memory_mb: float = 0.0
    budget_headroom_percent: float = 100.0


class EnrichedSimulationResult(SimulationResult):
    """
    Extended simulation result with deep multi-strategy analysis.
    Produced by the upgraded ChangeSimulator, consumed by SimulaService
    for richer risk/benefit decision-making.
    """

    counterfactuals: list[CounterfactualResult] = Field(default_factory=list)
    dependency_impacts: list[DependencyImpact] = Field(default_factory=list)
    resource_cost_estimate: ResourceCostEstimate | None = None
    constitutional_alignment: float = 0.0
    counterfactual_regression_rate: float = 0.0
    dependency_blast_radius: int = 0
    caution_adjustment: CautionAdjustment | None = None


# --- Bridge Models -----------------------------------------------------------


class EvoProposalEnriched(EOSBaseModel):
    """
    Evo proposal enriched with hypothesis evidence and inferred context.
    Produced by EvoSimulaBridge, consumed by SimulaService.translate().
    """

    evo_description: str
    evo_rationale: str
    hypothesis_ids: list[str] = Field(default_factory=list)
    hypothesis_statements: list[str] = Field(default_factory=list)
    evidence_scores: list[float] = Field(default_factory=list)
    supporting_episode_ids: list[str] = Field(default_factory=list)
    mutation_target: str = ""
    mutation_type: str = ""
    inferred_category: ChangeCategory | None = None
    inferred_change_spec: ChangeSpec | None = None


# --- Proposal Intelligence Models --------------------------------------------


class ProposalPriority(EOSBaseModel):
    """
    Priority score for a proposal, enabling intelligent processing order.
    Higher priority_score = process first.

    Formula: evidence_strength * expected_impact / max(0.1, estimated_risk * estimated_cost)
    """

    proposal_id: str
    priority_score: float = 0.0
    evidence_strength: float = 0.0
    expected_impact: float = 0.0
    estimated_risk: float = 0.0
    estimated_cost: float = 0.0
    reasoning: str = ""


class ProposalCluster(EOSBaseModel):
    """
    Group of semantically similar proposals that could be deduplicated.
    Detected via cheap heuristics first, LLM only for ambiguous cases.
    """

    representative_id: str
    member_ids: list[str] = Field(default_factory=list)
    similarity_scores: list[float] = Field(default_factory=list)
    merge_recommendation: str = ""


# --- Analytics Models --------------------------------------------------------


class CategorySuccessRate(EOSBaseModel):
    """Success rate tracking for a specific change category."""

    category: ChangeCategory
    total: int = 0
    approved: int = 0
    rejected: int = 0
    rolled_back: int = 0

    @property
    def success_rate(self) -> float:
        return self.approved / max(1, self.total)

    @property
    def rollback_rate(self) -> float:
        return self.rolled_back / max(1, self.total)


class EvolutionAnalytics(EOSBaseModel):
    """
    Aggregate evolution quality metrics computed from Neo4j history.
    Enables Simula to learn from its own performance over time.
    Zero LLM tokens -- pure computation from stored records.
    """

    category_rates: dict[str, CategorySuccessRate] = Field(default_factory=dict)
    total_proposals: int = 0
    evolution_velocity: float = 0.0  # proposals per day
    mean_simulation_risk: float = 0.0
    rollback_rate: float = 0.0
    recent_rollback_rates: dict[str, float] = Field(default_factory=dict)  # per-category 7-day rate
    last_updated: datetime = Field(default_factory=utc_now)


# --- Constants ---------------------------------------------------------------

SELF_APPLICABLE: frozenset[ChangeCategory] = frozenset({
    ChangeCategory.ADD_EXECUTOR,
    ChangeCategory.ADD_INPUT_CHANNEL,
    ChangeCategory.ADD_PATTERN_DETECTOR,
    ChangeCategory.ADJUST_BUDGET,
})

GOVERNANCE_REQUIRED: frozenset[ChangeCategory] = frozenset({
    ChangeCategory.MODIFY_CONTRACT,
    ChangeCategory.ADD_SYSTEM_CAPABILITY,
    ChangeCategory.MODIFY_CYCLE_TIMING,
    ChangeCategory.CHANGE_CONSOLIDATION,
})

FORBIDDEN: frozenset[ChangeCategory] = frozenset({
    ChangeCategory.MODIFY_EQUOR,
    ChangeCategory.MODIFY_CONSTITUTION,
    ChangeCategory.MODIFY_INVARIANTS,
    ChangeCategory.MODIFY_SELF_EVOLUTION,
})

SIMULA_IRON_RULES: list[str] = [
    "Simula CANNOT modify Equor in any way.",
    "Simula CANNOT modify constitutional drives.",
    "Simula CANNOT modify invariants.",
    "Simula CANNOT modify its own logic (no self-modifying code).",
    "Simula CANNOT bypass governance for governed changes.",
    "Simula CANNOT apply changes without rollback capability.",
    "Simula CANNOT delete evolution history records.",
    "Simula MUST simulate before applying any change.",
    "Simula MUST maintain version continuity -- no identity-breaking changes.",
]

# Paths the code agent is NEVER allowed to write to
FORBIDDEN_WRITE_PATHS: list[str] = [
    "src/ecodiaos/systems/equor",
    "src/ecodiaos/systems/simula",
    "src/ecodiaos/primitives/constitutional.py",
    "src/ecodiaos/primitives/common.py",
    "src/ecodiaos/config.py",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\__init__.py =====

"""
EcodiaOS — Simula: Self-Evolution System

The organism's capacity for metamorphosis. Where Evo adjusts the knobs,
Simula redesigns the dashboard.

Public API:
  SimulaService              — main service, wired in main.py
  EvoSimulaBridge            — translates Evo proposals to Simula format
  EvolutionAnalyticsEngine   — evolution quality tracking
  ProposalIntelligence       — dedup, prioritize, dependency analysis
  SimulaCodeAgent            — agentic code generation engine
  EvolutionHistoryManager    — immutable evolution history in Neo4j
  EvolutionProposal          — submitted by Evo when a hypothesis reaches SUPPORTED
  ProposalResult             — outcome of process_proposal()
  CodeChangeResult           — output of the code agent
  ChangeCategory             — taxonomy of allowed (and forbidden) change types
  ChangeSpec                 — formal specification of what to change
  EnrichedSimulationResult   — deep multi-strategy simulation output

Stage 1 enhancements:
  1A: Extended-thinking model routing for governance/high-risk proposals
  1B: Voyage-code-3 embeddings for semantic dedup + find_similar + Neo4j vector index
  1C: KVzip-inspired context compression for agentic tool loops

Stage 2 enhancements (Formal Verification Core):
  2A: Dafny proof-carrying code with Clover pattern
  2B: LLM + Z3 invariant discovery loop
  2C: Static analysis gates (Bandit / Semgrep)
  2D: AgentCoder pattern — test/code separation pipeline

Stage 3 enhancements (Incremental & Learning):
  3A: Salsa incremental verification — dependency-aware memoization
  3B: SWE-grep agentic retrieval — multi-hop code search
  3C: LILO library learning — abstraction extraction from successful proposals

Hunter — Zero-Day Discovery Engine:
  TargetWorkspace      — workspace abstraction (internal/external)
  AttackSurface        — discovered entry point
  VulnerabilityReport  — proven vulnerability + PoC
  HuntResult           — aggregated hunt results
  HunterConfig         — authorization and resource limits
"""

# Stage 2D: AgentCoder agents
from ecodiaos.systems.simula.agents.test_designer import TestDesignerAgent
from ecodiaos.systems.simula.agents.test_executor import TestExecutorAgent
from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine
from ecodiaos.systems.simula.bridge import EvoSimulaBridge
from ecodiaos.systems.simula.code_agent import SimulaCodeAgent
from ecodiaos.systems.simula.history import EvolutionHistoryManager

# Hunter: Zero-Day Discovery Engine
from ecodiaos.systems.simula.hunter import (
    AttackSurface,
    AttackSurfaceType,
    HunterConfig,
    HuntResult,
    TargetType,
    TargetWorkspace,
    VulnerabilityReport,
    VulnerabilitySeverity,
)
from ecodiaos.systems.simula.learning.lilo import LiloLibraryEngine
from ecodiaos.systems.simula.proposal_intelligence import ProposalIntelligence
from ecodiaos.systems.simula.retrieval.swe_grep import SweGrepRetriever
from ecodiaos.systems.simula.service import SimulaService
from ecodiaos.systems.simula.types import (
    FORBIDDEN,
    GOVERNANCE_REQUIRED,
    SELF_APPLICABLE,
    SIMULA_IRON_RULES,
    CategorySuccessRate,
    CautionAdjustment,
    ChangeCategory,
    ChangeSpec,
    CodeChangeResult,
    ConfigVersion,
    CounterfactualResult,
    DependencyImpact,
    EnrichedSimulationResult,
    EvolutionAnalytics,
    EvolutionProposal,
    EvolutionRecord,
    EvoProposalEnriched,
    ProposalCluster,
    ProposalPriority,
    ProposalResult,
    ProposalStatus,
    ResourceCostEstimate,
    RiskLevel,
    SimulationResult,
    TriageResult,
    TriageStatus,
)

# Stage 2: Verification bridges
from ecodiaos.systems.simula.verification.dafny_bridge import DafnyBridge

# Stage 3: Engines
from ecodiaos.systems.simula.verification.incremental import IncrementalVerificationEngine
from ecodiaos.systems.simula.verification.static_analysis import StaticAnalysisBridge

# Stages 2 + 3: Verification types
from ecodiaos.systems.simula.verification.types import (
    DAFNY_TRIGGERABLE_CATEGORIES,
    AbstractionExtractionResult,
    # Stage 3C: LILO Library Learning
    AbstractionKind,
    # Stage 2A: Dafny
    AgentCoderIterationResult,
    AgentCoderResult,
    CachedVerificationResult,
    CloverRoundResult,
    DafnyVerificationResult,
    DafnyVerificationStatus,
    DiscoveredInvariant,
    FormalVerificationResult,
    FunctionSignature,
    IncrementalVerificationResult,
    InvariantKind,
    InvariantVerificationResult,
    InvariantVerificationStatus,
    LibraryAbstraction,
    LibraryStats,
    RetrievalHop,
    # Stage 3B: SWE-grep Retrieval
    RetrievalToolKind,
    RetrievedContext,
    StaticAnalysisFinding,
    StaticAnalysisResult,
    StaticAnalysisSeverity,
    SweGrepResult,
    TestDesignResult,
    TestExecutionResult,
    # Stage 3A: Incremental Verification
    VerificationCacheStatus,
    VerificationCacheTier,
)
from ecodiaos.systems.simula.verification.z3_bridge import Z3Bridge

__all__ = [
    # Services
    "SimulaService",
    "SimulaCodeAgent",
    "EvolutionHistoryManager",
    "EvoSimulaBridge",
    "EvolutionAnalyticsEngine",
    "ProposalIntelligence",
    # Core types
    "ChangeCategory",
    "ChangeSpec",
    "CodeChangeResult",
    "ConfigVersion",
    "EvolutionProposal",
    "EvolutionRecord",
    "ProposalResult",
    "ProposalStatus",
    "RiskLevel",
    "SimulationResult",
    # Enriched types
    "EnrichedSimulationResult",
    "CautionAdjustment",
    "CounterfactualResult",
    "DependencyImpact",
    "ResourceCostEstimate",
    "EvoProposalEnriched",
    "ProposalPriority",
    "ProposalCluster",
    "CategorySuccessRate",
    "EvolutionAnalytics",
    "TriageStatus",
    "TriageResult",
    # Constants
    "FORBIDDEN",
    "GOVERNANCE_REQUIRED",
    "SELF_APPLICABLE",
    "SIMULA_IRON_RULES",
    # Stage 2: Verification types
    "DafnyVerificationStatus",
    "CloverRoundResult",
    "DafnyVerificationResult",
    "InvariantKind",
    "InvariantVerificationStatus",
    "DiscoveredInvariant",
    "InvariantVerificationResult",
    "StaticAnalysisSeverity",
    "StaticAnalysisFinding",
    "StaticAnalysisResult",
    "TestDesignResult",
    "TestExecutionResult",
    "AgentCoderIterationResult",
    "AgentCoderResult",
    "FormalVerificationResult",
    "DAFNY_TRIGGERABLE_CATEGORIES",
    # Stage 2: Bridges
    "DafnyBridge",
    "Z3Bridge",
    "StaticAnalysisBridge",
    # Stage 2D: Agents
    "TestDesignerAgent",
    "TestExecutorAgent",
    # Stage 3A: Incremental Verification
    "VerificationCacheStatus",
    "VerificationCacheTier",
    "FunctionSignature",
    "CachedVerificationResult",
    "IncrementalVerificationResult",
    "IncrementalVerificationEngine",
    # Stage 3B: SWE-grep Retrieval
    "RetrievalToolKind",
    "RetrievalHop",
    "RetrievedContext",
    "SweGrepResult",
    "SweGrepRetriever",
    # Stage 3C: LILO Library Learning
    "AbstractionKind",
    "LibraryAbstraction",
    "AbstractionExtractionResult",
    "LibraryStats",
    "LiloLibraryEngine",
    # Hunter: Zero-Day Discovery Engine
    "TargetWorkspace",
    "TargetType",
    "AttackSurface",
    "AttackSurfaceType",
    "VulnerabilityReport",
    "VulnerabilitySeverity",
    "HuntResult",
    "HunterConfig",
]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\analytics.py =====

"""
EcodiaOS -- Simula Evolution Analytics Engine

Tracks evolution quality metrics over time, enabling Simula to learn
from its own history. All analytics are computed from Neo4j evolution
records -- zero LLM tokens required.

Key metrics:
  - Per-category success/rollback rates
  - Evolution velocity (proposals per day)
  - Rollback pattern analysis (which categories fail most, why)
  - Dynamic caution adjustment (increase risk thresholds for
    categories with high recent rollback rates)

Phase 9 addition:
  - Hunter security analytics integration (vulnerability discovery
    metrics surfaced alongside evolution metrics for unified observability)

Used by:
  - ChangeSimulator: dynamic risk threshold adjustment
  - SimulaService: enhanced stats reporting
  - ProposalIntelligence: cost/risk estimation
  - HunterService: unified analytics surface (Phase 9)
"""

from __future__ import annotations

from datetime import datetime, timedelta
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.simula.types import (
    CategorySuccessRate,
    CautionAdjustment,
    ChangeCategory,
    EvolutionAnalytics,
    RiskLevel,
)

if TYPE_CHECKING:
    from ecodiaos.systems.simula.history import EvolutionHistoryManager
    from ecodiaos.systems.simula.hunter.analytics import (
        HunterAnalyticsStore,
        HunterAnalyticsView,
    )

logger = structlog.get_logger().bind(system="simula.analytics")

# Rollback rate above this threshold triggers increased caution
_CAUTION_THRESHOLD: float = 0.30

# Window for "recent" rollback rate calculation
_RECENT_WINDOW_DAYS: int = 7

# Risk level to numeric mapping for mean calculation
_RISK_LEVEL_NUMERIC: dict[RiskLevel, float] = {
    RiskLevel.LOW: 0.1,
    RiskLevel.MODERATE: 0.4,
    RiskLevel.HIGH: 0.7,
    RiskLevel.UNACCEPTABLE: 1.0,
}


class EvolutionAnalyticsEngine:
    """
    Tracks evolution quality metrics over time.
    Enables Simula to learn from its own history and dynamically
    adjust risk thresholds based on past performance.

    All computation is from Neo4j records -- no LLM tokens consumed.
    """

    def __init__(
        self,
        history: EvolutionHistoryManager | None = None,
        *,
        hunter_view: HunterAnalyticsView | None = None,
        hunter_store: HunterAnalyticsStore | None = None,
    ) -> None:
        self._history = history
        self._log = logger
        self._cached_analytics: EvolutionAnalytics | None = None
        self._cache_ttl_seconds: int = 300  # 5 minutes
        self._last_computed: datetime | None = None

        # Phase 9: Hunter analytics integration
        self._hunter_view = hunter_view
        self._hunter_store = hunter_store

    async def compute_analytics(self) -> EvolutionAnalytics:
        """
        Compute current analytics from the full evolution history.
        Results are cached for 5 minutes to avoid repeated Neo4j queries.
        """
        now = utc_now()
        if (
            self._cached_analytics is not None
            and self._last_computed is not None
            and (now - self._last_computed).total_seconds() < self._cache_ttl_seconds
        ):
            return self._cached_analytics

        if self._history is None:
            return EvolutionAnalytics()

        records = await self._history.get_history(limit=500)

        if not records:
            analytics = EvolutionAnalytics(last_updated=now)
            self._cached_analytics = analytics
            self._last_computed = now
            return analytics

        # Per-category rates
        category_rates: dict[str, CategorySuccessRate] = {}
        total_risk_numeric: float = 0.0
        risk_count: int = 0

        for record in records:
            cat_key = record.category.value
            if cat_key not in category_rates:
                category_rates[cat_key] = CategorySuccessRate(category=record.category)

            rate = category_rates[cat_key]
            rate.total += 1

            if record.rolled_back:
                rate.rolled_back += 1
            else:
                rate.approved += 1

            total_risk_numeric += _RISK_LEVEL_NUMERIC.get(record.simulation_risk, 0.4)
            risk_count += 1

        # Evolution velocity: proposals per day over the record span
        velocity = 0.0
        if len(records) >= 2:
            newest = records[0].created_at
            oldest = records[-1].created_at
            span_days = max(1.0, (newest - oldest).total_seconds() / 86400.0)
            velocity = len(records) / span_days

        # Aggregate rollback rate
        total_rolled_back = sum(r.rolled_back for r in category_rates.values())
        total_proposals = sum(r.total for r in category_rates.values())
        rollback_rate = total_rolled_back / max(1, total_proposals)

        # Mean simulation risk
        mean_risk = total_risk_numeric / max(1, risk_count)

        # Compute recent rollback rates (7-day window) per category
        recent_rollback_rates: dict[str, float] = {}
        cutoff = utc_now() - timedelta(days=_RECENT_WINDOW_DAYS)
        for cat in ChangeCategory:
            recent_records = [
                r for r in records
                if r.category == cat and r.created_at >= cutoff
            ]
            if recent_records:
                recent_rolled_back = sum(1 for r in recent_records if r.rolled_back)
                recent_rollback_rates[cat.value] = round(
                    recent_rolled_back / len(recent_records), 3
                )

        analytics = EvolutionAnalytics(
            category_rates=category_rates,
            total_proposals=total_proposals,
            evolution_velocity=round(velocity, 3),
            mean_simulation_risk=round(mean_risk, 3),
            rollback_rate=round(rollback_rate, 3),
            recent_rollback_rates=recent_rollback_rates,
            last_updated=now,
        )

        self._cached_analytics = analytics
        self._last_computed = now
        self._log.info(
            "analytics_computed",
            total_proposals=total_proposals,
            velocity=analytics.evolution_velocity,
            rollback_rate=analytics.rollback_rate,
            categories=len(category_rates),
        )
        return analytics

    async def get_category_success_rate(self, category: ChangeCategory) -> float:
        """
        Success rate for a specific change category.
        Used by ChangeSimulator for dynamic risk weighting.
        Returns 0.5 (neutral) if no history exists for this category.
        """
        analytics = await self.compute_analytics()
        rate = analytics.category_rates.get(category.value)
        if rate is None or rate.total == 0:
            return 0.5  # no data -- assume neutral
        return rate.success_rate

    async def get_recent_rollback_rate(self, category: ChangeCategory) -> float:
        """
        Rollback rate for a category within the recent window (7 days).
        More responsive to recent trends than the all-time rate.
        """
        if self._history is None:
            return 0.0

        records = await self._history.get_history(limit=200)
        cutoff = utc_now() - timedelta(days=_RECENT_WINDOW_DAYS)

        recent = [
            r for r in records
            if r.category == category and r.created_at >= cutoff
        ]

        if not recent:
            return 0.0

        rolled_back = sum(1 for r in recent if r.rolled_back)
        return rolled_back / len(recent)

    async def get_rollback_patterns(self) -> list[dict[str, Any]]:
        """
        Analyze rollback history for actionable patterns:
        - Which categories roll back most often
        - Common rollback reasons
        - Trend direction (getting better or worse)
        """
        analytics = await self.compute_analytics()
        patterns: list[dict[str, Any]] = []

        for cat_key, rate in analytics.category_rates.items():
            if rate.rolled_back == 0:
                continue
            patterns.append({
                "category": cat_key,
                "rollback_rate": round(rate.rollback_rate, 3),
                "total": rate.total,
                "rolled_back": rate.rolled_back,
                "severity": "high" if rate.rollback_rate > _CAUTION_THRESHOLD else "normal",
            })

        # Sort by rollback rate descending
        patterns.sort(key=lambda p: p["rollback_rate"], reverse=True)
        return patterns

    def should_increase_caution(self, category: ChangeCategory) -> CautionAdjustment:
        """
        Transparent caution adjustment analysis using cached analytics.
        Evaluates multiple factors to determine if simulation should use
        stricter risk thresholds for this category.

        Factors considered:
        - All-time rollback rate (indicates systemic issues)
        - Recent 7-day rollback rate (responsive to recent trends)
        - Data sufficiency (at least 3 proposals needed)

        Returns a CautionAdjustment with full reasoning for observability.
        """
        if self._cached_analytics is None:
            return CautionAdjustment(
                should_adjust=False,
                magnitude=0.0,
                factors={},
                reasoning="No cached analytics available",
            )

        rate = self._cached_analytics.category_rates.get(category.value)
        if rate is None or rate.total < 3:
            return CautionAdjustment(
                should_adjust=False,
                magnitude=0.0,
                factors={},
                reasoning="Insufficient data (< 3 proposals)",
            )

        factors: dict[str, float] = {}

        # Factor 1: All-time rollback rate
        if rate.rollback_rate > _CAUTION_THRESHOLD:
            factors["high_alltime_rollback_rate"] = min(
                0.25, rate.rollback_rate * 0.5
            )

        # Factor 2: Recent 7-day rollback rate
        recent_rate = self._cached_analytics.recent_rollback_rates.get(category.value, 0.0)
        if recent_rate > 0.25:
            factors["high_recent_rollback_rate"] = min(0.20, recent_rate * 0.4)

        total_adjustment = sum(factors.values())

        reasoning_parts = []
        if factors:
            reasoning_parts.append(
                f"Category {category.value}: "
                + ", ".join(f"{k}={v:.2f}" for k, v in factors.items())
            )
        reasoning_parts.append(
            f"All-time: {rate.rollback_rate:.1%}, "
            f"Recent (7d): {recent_rate:.1%}, "
            f"Total: {rate.total} proposals"
        )

        return CautionAdjustment(
            should_adjust=total_adjustment > 0.0,
            magnitude=min(0.5, total_adjustment),
            factors=factors,
            reasoning=" | ".join(reasoning_parts),
        )

    # ── Phase 9: Hunter Integration ──────────────────────────────────────────

    def set_hunter_view(self, view: HunterAnalyticsView) -> None:
        """Attach a Hunter analytics view for unified querying."""
        self._hunter_view = view

    def set_hunter_store(self, store: HunterAnalyticsStore) -> None:
        """Attach a Hunter analytics store for durable historical queries."""
        self._hunter_store = store

    def get_hunter_summary(self) -> dict[str, Any]:
        """
        Return in-memory Hunter analytics summary.

        Provides: total vulnerabilities, severity distribution, most common
        classes, patch success rate, weekly trends, rolling windows, and
        throughput metrics.

        Returns empty dict if Hunter analytics view is not attached.
        """
        if self._hunter_view is None:
            return {}
        return self._hunter_view.summary

    async def get_hunter_weekly_trends(
        self,
        *,
        weeks: int = 12,
        target_url: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Query weekly vulnerability trends from TimescaleDB.

        Falls back to in-memory view if store is unavailable.

        Args:
            weeks: Number of weeks to look back.
            target_url: Optional filter by target repository.

        Returns:
            List of weekly buckets with vulnerability counts + severity breakdown.
        """
        if self._hunter_store is not None:
            try:
                return await self._hunter_store.get_vulnerabilities_per_week(
                    weeks=weeks, target_url=target_url,
                )
            except Exception as exc:
                self._log.warning(
                    "hunter_store_query_failed",
                    query="weekly_trends",
                    error=str(exc),
                )

        # Fallback to in-memory view
        if self._hunter_view is not None:
            if target_url:
                return self._hunter_view.get_target_weekly_trend(target_url)
            trends = self._hunter_view.summary.get("weekly_trends", [])
            return trends[-weeks:] if isinstance(trends, list) else []

        return []

    async def get_hunter_severity_distribution(
        self,
        *,
        days: int = 30,
        target_url: str | None = None,
    ) -> dict[str, int]:
        """
        Query severity distribution from TimescaleDB over a rolling window.

        Falls back to in-memory view (all-time) if store is unavailable.
        """
        if self._hunter_store is not None:
            try:
                return await self._hunter_store.get_severity_distribution(
                    days=days, target_url=target_url,
                )
            except Exception as exc:
                self._log.warning(
                    "hunter_store_query_failed",
                    query="severity_distribution",
                    error=str(exc),
                )

        if self._hunter_view is not None:
            dist: dict[str, int] = self._hunter_view.summary.get("severity_distribution", {})
            return dist

        return {}

    async def get_hunter_error_summary(self, *, days: int = 7) -> list[dict[str, Any]]:
        """
        Query aggregated pipeline errors from TimescaleDB.

        Only available when a Hunter analytics store is attached.
        """
        if self._hunter_store is not None:
            try:
                return await self._hunter_store.get_error_summary(days=days)
            except Exception as exc:
                self._log.warning(
                    "hunter_store_query_failed",
                    query="error_summary",
                    error=str(exc),
                )
        return []

    async def get_unified_analytics(self) -> dict[str, Any]:
        """
        Return a unified analytics payload combining evolution metrics
        and Hunter security metrics for comprehensive observability.

        This is the single entry point for dashboard consumers that want
        the complete system health picture.
        """
        evolution = await self.compute_analytics()

        result: dict[str, Any] = {
            "evolution": {
                "total_proposals": evolution.total_proposals,
                "evolution_velocity": evolution.evolution_velocity,
                "rollback_rate": evolution.rollback_rate,
                "mean_simulation_risk": evolution.mean_simulation_risk,
                "category_count": len(evolution.category_rates),
                "last_updated": (
                    evolution.last_updated.isoformat()
                    if evolution.last_updated else None
                ),
            },
        }

        # Hunter security analytics
        hunter_summary = self.get_hunter_summary()
        if hunter_summary:
            result["hunter"] = {
                "total_vulnerabilities": hunter_summary.get("total_vulnerabilities", 0),
                "total_hunts": hunter_summary.get("total_hunts", 0),
                "severity_distribution": hunter_summary.get("severity_distribution", {}),
                "patch_success_rate": hunter_summary.get("patch_success_rate", 0),
                "avg_vulns_per_hunt": hunter_summary.get("avg_vulns_per_hunt", 0),
                "rolling_7d": hunter_summary.get("rolling_7d", {}),
                "rolling_30d": hunter_summary.get("rolling_30d", {}),
            }
        else:
            result["hunter"] = None

        return result

    def invalidate_cache(self) -> None:
        """Force recomputation on next analytics request."""
        self._cached_analytics = None
        self._last_computed = None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\applicator.py =====

"""
EcodiaOS — Simula Change Applicator

Routes approved evolution proposals to the appropriate application
strategy and coordinates with RollbackManager for safety.

Application strategies by category:

  ADJUST_BUDGET → direct config update (no code generation needed)
  ADD_EXECUTOR, ADD_INPUT_CHANNEL, ADD_PATTERN_DETECTOR → SimulaCodeAgent
  MODIFY_CONTRACT, ADD_SYSTEM_CAPABILITY, etc. → SimulaCodeAgent (post governance)

All strategies:
  1. Snapshot affected files via RollbackManager
  2. Apply change
  3. On failure → rollback immediately
  4. On success → return CodeChangeResult + snapshot (for caller health check)
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog
import yaml

from ecodiaos.systems.simula.types import (
    ChangeCategory,
    CodeChangeResult,
    ConfigSnapshot,
    EvolutionProposal,
)

if TYPE_CHECKING:
    from pathlib import Path

    from ecodiaos.systems.simula.agents.test_designer import TestDesignerAgent
    from ecodiaos.systems.simula.agents.test_executor import TestExecutorAgent
    from ecodiaos.systems.simula.code_agent import SimulaCodeAgent
    from ecodiaos.systems.simula.health import HealthChecker
    from ecodiaos.systems.simula.rollback import RollbackManager
    from ecodiaos.systems.simula.verification.static_analysis import StaticAnalysisBridge

logger = structlog.get_logger()


class ApplicationError(RuntimeError):
    """Raised when a change application fails unrecoverably."""


class ChangeApplicator:
    """
    Routes approved evolution proposals to the right application strategy.

    For code-level changes: delegates to SimulaCodeAgent.
    For budget changes: updates the YAML config directly.
    Always snapshots before applying, so rollback is always possible.
    """

    def __init__(
        self,
        code_agent: SimulaCodeAgent,
        rollback_manager: RollbackManager,
        health_checker: HealthChecker,
        codebase_root: Path,
        # Stage 2D: AgentCoder pipeline
        test_designer: TestDesignerAgent | None = None,
        test_executor: TestExecutorAgent | None = None,
        static_analysis_bridge: StaticAnalysisBridge | None = None,
        agent_coder_enabled: bool = False,
        agent_coder_max_iterations: int = 3,
    ) -> None:
        self._agent = code_agent
        self._rollback = rollback_manager
        self._health = health_checker
        self._root = codebase_root
        self._test_designer = test_designer
        self._test_executor = test_executor
        self._static_bridge = static_analysis_bridge
        self._agent_coder_enabled = agent_coder_enabled
        self._agent_coder_max_iterations = agent_coder_max_iterations
        self._logger = logger.bind(system="simula.applicator")

    async def apply(
        self, proposal: EvolutionProposal
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """
        Apply an evolution proposal. Returns (result, snapshot).

        The snapshot is needed by SimulaService for rollback if the
        post-application health check fails.

        Routes through the AgentCoder 3-agent pipeline when enabled,
        otherwise uses the standard code agent.
        """
        self._logger.info(
            "applying_change",
            proposal_id=proposal.id,
            category=proposal.category.value,
        )

        if proposal.category == ChangeCategory.ADJUST_BUDGET:
            return await self._apply_budget(proposal)
        elif self._agent_coder_enabled and self._test_designer and self._test_executor:
            return await self._apply_via_agent_coder(proposal)
        else:
            return await self._apply_via_code_agent(proposal)

    # ── Budget Adjustment (direct config update) ──────────────────────────────

    async def _apply_budget(
        self, proposal: EvolutionProposal
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """Direct config update for budget changes — no code generation."""
        spec = proposal.change_spec
        if not spec.budget_parameter or spec.budget_new_value is None:
            result = CodeChangeResult(
                success=False,
                error="Budget change spec missing parameter or new_value",
            )
            return result, ConfigSnapshot(
                proposal_id=proposal.id,
                config_version=0,
            )

        config_path = self._root / "config" / "default.yaml"
        snapshot = await self._rollback.snapshot(
            proposal_id=proposal.id,
            paths=[config_path],
        )

        try:
            data: dict[str, Any] = {}
            if config_path.exists():
                with open(config_path) as f:
                    data = yaml.safe_load(f) or {}

            # Navigate the dotted parameter path (e.g. "nova.efe.pragmatic")
            parts = spec.budget_parameter.split(".")
            node = data
            for part in parts[:-1]:
                node = node.setdefault(part, {})
            node[parts[-1]] = spec.budget_new_value

            with open(config_path, "w") as f:
                yaml.dump(data, f, default_flow_style=False)

            rel_path = str(config_path.relative_to(self._root))
            self._logger.info(
                "budget_updated",
                parameter=spec.budget_parameter,
                old_value=spec.budget_old_value,
                new_value=spec.budget_new_value,
            )
            return CodeChangeResult(
                success=True,
                files_written=[rel_path],
                summary=(
                    f"Updated {spec.budget_parameter} "
                    f"from {spec.budget_old_value} to {spec.budget_new_value}"
                ),
            ), snapshot

        except Exception as exc:
            await self._rollback.restore(snapshot)
            return CodeChangeResult(
                success=False,
                error=f"Budget update failed: {exc}",
            ), snapshot

    # ── Code Agent Application ────────────────────────────────────────────────

    async def _apply_via_code_agent(
        self, proposal: EvolutionProposal
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """Use SimulaCodeAgent to generate and write the implementation."""
        affected_dirs = _infer_affected_paths(proposal, self._root)
        snapshot = await self._rollback.snapshot(
            proposal_id=proposal.id,
            paths=affected_dirs,
        )

        result = await self._agent.implement(proposal)

        if not result.success:
            self._logger.warning(
                "code_agent_failed",
                proposal_id=proposal.id,
                error=result.error,
            )
            await self._rollback.restore(snapshot)

        return result, snapshot

    # ── Stage 2D: AgentCoder 3-Agent Pipeline ─────────────────────────────────

    async def _apply_via_agent_coder(
        self, proposal: EvolutionProposal,
    ) -> tuple[CodeChangeResult, ConfigSnapshot]:
        """
        Apply via the AgentCoder pipeline:
          1. TestDesigner generates tests from proposal spec (no code seen)
          2. CodeAgent implements the change (with test writing disabled)
          3. TestExecutor runs the designed tests against the implementation
          4. If failures → feed back to CodeAgent → iterate

        This adversarial separation produces higher-quality code by testing
        the specification rather than the implementation.
        """
        from ecodiaos.systems.simula.agents.test_executor import TestExecutorAgent
        from ecodiaos.systems.simula.verification.types import (
            AgentCoderIterationResult,
            AgentCoderResult,
        )

        assert self._test_designer is not None
        assert self._test_executor is not None

        affected_dirs = _infer_affected_paths(proposal, self._root)
        snapshot = await self._rollback.snapshot(
            proposal_id=proposal.id,
            paths=affected_dirs,
        )

        log = self._logger.bind(
            proposal_id=proposal.id,
            pipeline="agent_coder",
        )

        # Step 1: TestDesigner generates tests
        log.info("agent_coder_designing_tests")
        test_design = await self._test_designer.design_tests(proposal)

        if not test_design.test_files:
            log.warning("agent_coder_no_tests_designed")
            # Fall back to standard code agent
            return await self._apply_via_code_agent(proposal)

        log.info(
            "agent_coder_tests_designed",
            test_files=len(test_design.test_files),
            test_count=test_design.test_count,
        )

        iterations: list[AgentCoderIterationResult] = []
        final_result: CodeChangeResult | None = None

        for iteration_num in range(1, self._agent_coder_max_iterations + 1):
            log.info("agent_coder_iteration", iteration=iteration_num)

            # Step 2: CodeAgent implements (test writing disabled)
            code_result = await self._agent.implement(
                proposal, skip_test_writing=True,
            )
            final_result = code_result

            if not code_result.success:
                log.warning(
                    "agent_coder_code_failed",
                    iteration=iteration_num,
                    error=code_result.error,
                )
                iterations.append(AgentCoderIterationResult(
                    iteration=iteration_num,
                    test_design=test_design if iteration_num == 1 else None,
                    code_generation_success=False,
                    code_generation_files=code_result.files_written,
                ))
                break

            # Step 3: TestExecutor runs tests
            test_result = await self._test_executor.execute_tests(
                test_design.test_files,
            )

            iter_result = AgentCoderIterationResult(
                iteration=iteration_num,
                test_design=test_design if iteration_num == 1 else None,
                code_generation_success=True,
                code_generation_files=code_result.files_written,
                test_execution=test_result,
                all_tests_passed=(
                    test_result.failed == 0 and test_result.errors == 0
                ),
            )
            iterations.append(iter_result)

            log.info(
                "agent_coder_test_results",
                iteration=iteration_num,
                passed=test_result.passed,
                failed=test_result.failed,
                errors=test_result.errors,
            )

            if iter_result.all_tests_passed:
                log.info("agent_coder_converged", iterations=iteration_num)
                break

            # Step 4: Not all tests passed — feed failures back
            if iteration_num < self._agent_coder_max_iterations:
                feedback = TestExecutorAgent.format_failures_for_feedback(
                    test_result,
                )
                log.info("agent_coder_feeding_back", feedback_len=len(feedback))
                # Attach feedback to proposal for next iteration
                proposal._agent_coder_feedback = feedback  # type: ignore[attr-defined]

        # Compute aggregate result
        converged = bool(iterations and iterations[-1].all_tests_passed)
        final_pass_rate = 0.0
        if iterations and iterations[-1].test_execution:
            te = iterations[-1].test_execution
            if te.total > 0:
                final_pass_rate = te.passed / te.total

        AgentCoderResult(
            iterations=iterations,
            total_iterations=len(iterations),
            final_pass_rate=final_pass_rate,
            converged=converged,
        )

        # Attach to code result for downstream recording
        if final_result is not None:
            final_result.agent_coder_iterations = len(iterations)
            final_result.test_designer_test_count = test_design.test_count

        if final_result is None or not final_result.success:
            await self._rollback.restore(snapshot)

        return final_result or CodeChangeResult(
            success=False, error="AgentCoder pipeline produced no result",
        ), snapshot


# ─── Helpers ──────────────────────────────────────────────────────────────────


def _infer_affected_paths(proposal: EvolutionProposal, root: Path) -> list[Path]:
    """Infer which existing paths will likely be affected by this change."""
    paths: list[Path] = []
    category = proposal.category
    spec = proposal.change_spec

    if category == ChangeCategory.ADD_EXECUTOR:
        paths.append(root / "src" / "ecodiaos" / "systems" / "axon" / "registry.py")
        executors_dir = root / "src" / "ecodiaos" / "systems" / "axon" / "executors"
        if executors_dir.exists():
            paths.append(executors_dir)
    elif category == ChangeCategory.ADD_INPUT_CHANNEL:
        paths.append(root / "src" / "ecodiaos" / "systems" / "atune")
    elif category == ChangeCategory.ADD_PATTERN_DETECTOR:
        paths.append(root / "src" / "ecodiaos" / "systems" / "evo" / "detectors.py")
    elif category in {
        ChangeCategory.MODIFY_CONTRACT,
        ChangeCategory.ADD_SYSTEM_CAPABILITY,
    }:
        for sys_name in (spec.affected_systems or []):
            sys_path = root / "src" / "ecodiaos" / "systems" / sys_name
            if sys_path.exists():
                paths.append(sys_path)

    return [p for p in paths if p.exists()]

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\bridge.py =====

"""
EcodiaOS -- Simula Evo↔Simula Bridge

Translates Evo's lightweight evolution proposals into Simula's rich
EvolutionProposal format, enriched with hypothesis evidence, episode
context, and LLM-inferred change specifications.

This completes the learning→evolution loop: Evo detects patterns,
forms hypotheses, and when one reaches SUPPORTED status with an
EVOLUTION_PROPOSAL mutation, this bridge translates it into a fully
specified change that Simula can simulate, gate, and apply.

Translation pipeline:
  1. Collect evidence from supporting hypotheses
  2. Infer ChangeCategory from mutation type + target (rule-based, LLM fallback)
  3. Build formal ChangeSpec via LLM reasoning (single structured output call)
  4. Construct the rich SimulaEvolutionProposal

Budget: ~500 tokens per translation (1 LLM call for ChangeSpec construction).
Rule-based category inference uses zero LLM tokens.
"""

from __future__ import annotations

import asyncio
import contextlib
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.simula.types import (
    ChangeCategory,
    ChangeSpec,
    EvolutionProposal,
    EvoProposalEnriched,
    ProposalStatus,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.simula.retrieval.swe_grep import SweGrepRetriever

logger = structlog.get_logger().bind(system="simula.bridge")

# Rule-based keyword → category mapping for zero-token inference
_CATEGORY_KEYWORDS: list[tuple[list[str], ChangeCategory]] = [
    (["executor", "action_type", "action type", "axon"], ChangeCategory.ADD_EXECUTOR),
    (["input_channel", "input channel", "channel", "atune", "sensor"], ChangeCategory.ADD_INPUT_CHANNEL),
    (["detector", "pattern_detector", "pattern detector", "scan"], ChangeCategory.ADD_PATTERN_DETECTOR),
    (["budget", "parameter", "tunable", "weight", "threshold"], ChangeCategory.ADJUST_BUDGET),
    (["contract", "interface", "inter-system", "protocol"], ChangeCategory.MODIFY_CONTRACT),
    (["capability", "system capability", "new capability"], ChangeCategory.ADD_SYSTEM_CAPABILITY),
    (["cycle", "timing", "theta", "rhythm"], ChangeCategory.MODIFY_CYCLE_TIMING),
    (["consolidation", "sleep", "schedule"], ChangeCategory.CHANGE_CONSOLIDATION),
]


class EvoSimulaBridge:
    """
    Translates Evo evolution proposals into Simula's rich format.
    Enriches with hypothesis evidence, infers change categories,
    and builds formal change specifications.

    Used by:
      - Evo's ConsolidationOrchestrator (Phase 8) via callback
      - SimulaService.receive_evo_proposal()
    """

    def __init__(
        self,
        llm: LLMProvider,
        memory: MemoryService | None = None,
    ) -> None:
        self._llm = llm
        self._memory = memory
        self._swe_grep: SweGrepRetriever | None = None
        self._log = logger

    def set_swe_grep(self, retriever: SweGrepRetriever) -> None:
        """Inject SWE-grep retriever (called by SimulaService after init)."""
        self._swe_grep = retriever

    async def translate_proposal(
        self,
        evo_description: str,
        evo_rationale: str,
        hypothesis_ids: list[str],
        hypothesis_statements: list[str],
        evidence_scores: list[float],
        supporting_episode_ids: list[str],
        mutation_target: str = "",
        mutation_type: str = "",
    ) -> EvolutionProposal:
        """
        Full translation pipeline: Evo proposal → Simula EvolutionProposal.

        Steps:
          1. Collect and structure evidence
          2. Infer ChangeCategory (rule-based, LLM fallback)
          3. Build formal ChangeSpec (LLM-assisted)
          4. Construct rich proposal
        """
        self._log.info(
            "bridge_translating",
            description=evo_description[:80],
            hypotheses=len(hypothesis_ids),
            mutation_target=mutation_target,
            mutation_type=mutation_type,
        )

        # 1. Structure the enriched evidence
        enriched = EvoProposalEnriched(
            evo_description=evo_description,
            evo_rationale=evo_rationale,
            hypothesis_ids=hypothesis_ids,
            hypothesis_statements=hypothesis_statements,
            evidence_scores=evidence_scores,
            supporting_episode_ids=supporting_episode_ids,
            mutation_target=mutation_target,
            mutation_type=mutation_type,
        )

        # 2. Infer ChangeCategory
        category = await self._infer_category(
            mutation_target=mutation_target,
            mutation_type=mutation_type,
            description=evo_description,
        )
        enriched.inferred_category = category

        # 2.5 (Stage 3B): SWE-grep retrieval for bridge context
        retrieval_context = ""
        if self._swe_grep is not None:
            try:
                swe_result = await self._swe_grep.retrieve_for_bridge(
                    description=evo_description,
                    category=category.value,
                    mutation_target=mutation_target,
                )
                if swe_result.contexts:
                    retrieval_context = "\n".join(
                        f"[{c.context_type}:{c.source}] {c.content[:200]}"
                        for c in swe_result.contexts[:5]
                    )
                    self._log.info(
                        "bridge_swe_grep_complete",
                        contexts=len(swe_result.contexts),
                        hops=swe_result.total_hops,
                        time_ms=swe_result.total_time_ms,
                    )
            except Exception as exc:
                self._log.warning("bridge_swe_grep_failed", error=str(exc))

        # 3. Build formal ChangeSpec (enriched with SWE-grep context)
        change_spec = await self._build_change_spec(
            category=category,
            description=evo_description,
            mutation_target=mutation_target,
            evidence_summaries=hypothesis_statements[:5],
            retrieval_context=retrieval_context,
        )
        enriched.inferred_change_spec = change_spec

        # 4. Construct the rich Simula proposal
        proposal = EvolutionProposal(
            source="evo",
            category=category,
            description=evo_description,
            change_spec=change_spec,
            evidence=hypothesis_ids,
            expected_benefit=evo_rationale,
            risk_assessment="",
            status=ProposalStatus.PROPOSED,
        )

        self._log.info(
            "bridge_translated",
            proposal_id=proposal.id,
            inferred_category=category.value,
            evidence_count=len(hypothesis_ids),
        )
        return proposal

    async def _infer_category(
        self,
        mutation_target: str,
        mutation_type: str,
        description: str,
    ) -> ChangeCategory:
        """
        Infer the ChangeCategory from mutation metadata.

        Step 1 (zero tokens): Rule-based keyword matching on target + description.
        Step 2 (LLM fallback): If no rule matches, ask LLM to classify (~200 tokens).
        """
        # Combine all text for keyword matching
        combined = f"{mutation_target} {mutation_type} {description}".lower()

        # Rule-based matching
        for keywords, category in _CATEGORY_KEYWORDS:
            for keyword in keywords:
                if keyword in combined:
                    self._log.debug(
                        "category_inferred_rule",
                        keyword=keyword,
                        category=category.value,
                    )
                    return category

        # LLM fallback for ambiguous cases
        return await self._infer_category_llm(description, mutation_target)

    async def _infer_category_llm(
        self, description: str, mutation_target: str,
    ) -> ChangeCategory:
        """LLM-based category classification. ~200 tokens."""
        categories = [
            f"- {c.value}: {c.name}"
            for c in ChangeCategory
            if c not in {
                ChangeCategory.MODIFY_EQUOR,
                ChangeCategory.MODIFY_CONSTITUTION,
                ChangeCategory.MODIFY_INVARIANTS,
                ChangeCategory.MODIFY_SELF_EVOLUTION,
            }
        ]

        prompt = (
            "Classify this proposed EcodiaOS structural change into one category.\n\n"
            f"Description: {description[:300]}\n"
            f"Target: {mutation_target}\n\n"
            "Categories:\n" + "\n".join(categories) + "\n\n"
            "Reply with the category value only (e.g., 'add_executor')."
        )

        try:
            response = await asyncio.wait_for(
                self._llm.evaluate(prompt=prompt, max_tokens=30, temperature=0.1),
                timeout=5.0,
            )
            text = response.text.strip().lower().strip("'\"")
            try:
                return ChangeCategory(text)
            except ValueError:
                # Try partial matching
                for cat in ChangeCategory:
                    if cat.value in text:
                        return cat
        except Exception as exc:
            self._log.warning("category_llm_inference_failed", error=str(exc))

        # Ultimate fallback
        self._log.warning(
            "category_fallback",
            description=description[:50],
            defaulting_to="add_system_capability",
        )
        return ChangeCategory.ADD_SYSTEM_CAPABILITY

    async def _build_change_spec(
        self,
        category: ChangeCategory,
        description: str,
        mutation_target: str,
        evidence_summaries: list[str],
        retrieval_context: str = "",
    ) -> ChangeSpec:
        """
        Build a formal ChangeSpec via LLM-assisted reasoning.
        Single call with structured output. ~500 tokens.
        Stage 3B: enriched with SWE-grep retrieval context when available.
        """
        evidence_text = "\n".join(f"- {s[:150]}" for s in evidence_summaries) or "none"

        # Category-specific field instructions
        field_instructions = self._get_field_instructions(category)

        # Stage 3B: Include codebase context from SWE-grep retrieval
        context_section = ""
        if retrieval_context:
            context_section = f"\nCodebase context (retrieved via SWE-grep):\n{retrieval_context}\n"

        prompt = (
            "You are constructing a formal change specification for EcodiaOS.\n\n"
            f"Category: {category.value}\n"
            f"Description: {description[:300]}\n"
            f"Target: {mutation_target}\n"
            f"Evidence:\n{evidence_text}\n"
            f"{context_section}\n"
            f"Required fields for {category.value}:\n{field_instructions}\n\n"
            "Reply as key=value pairs, one per line. Example:\n"
            "executor_name=email_sender\n"
            "executor_action_type=send_email\n"
            "executor_description=Sends email notifications via SMTP\n"
            "affected_systems=axon\n"
            "additional_context=Triggered by notification intents"
        )

        try:
            response = await asyncio.wait_for(
                self._llm.evaluate(prompt=prompt, max_tokens=300, temperature=0.2),
                timeout=8.0,
            )
            return self._parse_change_spec(response.text, category, description)
        except Exception as exc:
            self._log.warning("change_spec_build_failed", error=str(exc))
            # Return a minimal spec based on what we know
            return self._fallback_change_spec(category, description, mutation_target)

    def _get_field_instructions(self, category: ChangeCategory) -> str:
        """Return field-specific instructions for each category."""
        instructions: dict[ChangeCategory, str] = {
            ChangeCategory.ADD_EXECUTOR: (
                "executor_name (snake_case module name)\n"
                "executor_action_type (unique string identifier)\n"
                "executor_description (what it does)\n"
                "affected_systems (always includes 'axon')"
            ),
            ChangeCategory.ADD_INPUT_CHANNEL: (
                "channel_name (snake_case module name)\n"
                "channel_type (unique string identifier)\n"
                "channel_description (what it ingests)\n"
                "affected_systems (always includes 'atune')"
            ),
            ChangeCategory.ADD_PATTERN_DETECTOR: (
                "detector_name (PascalCase class name)\n"
                "detector_pattern_type (unique string identifier)\n"
                "detector_description (what patterns it detects)\n"
                "affected_systems (always includes 'evo')"
            ),
            ChangeCategory.ADJUST_BUDGET: (
                "budget_parameter (dotted path, e.g., 'nova.efe.pragmatic')\n"
                "budget_old_value (current value)\n"
                "budget_new_value (proposed value)\n"
                "affected_systems (which system this parameter belongs to)"
            ),
            ChangeCategory.MODIFY_CONTRACT: (
                "contract_changes (list of changes)\n"
                "affected_systems (which systems are involved)\n"
                "additional_context (why this contract change is needed)"
            ),
            ChangeCategory.ADD_SYSTEM_CAPABILITY: (
                "capability_description (what the new capability does)\n"
                "affected_systems (which systems are involved)\n"
                "additional_context (design rationale)"
            ),
            ChangeCategory.MODIFY_CYCLE_TIMING: (
                "timing_parameter (which timing to change)\n"
                "timing_old_value (current value in ms)\n"
                "timing_new_value (proposed value in ms)\n"
                "affected_systems (always includes 'synapse')"
            ),
            ChangeCategory.CHANGE_CONSOLIDATION: (
                "consolidation_schedule (new schedule description)\n"
                "affected_systems (always includes 'evo')\n"
                "additional_context (why the schedule should change)"
            ),
        }
        return instructions.get(category, "additional_context (describe the change)")

    def _parse_change_spec(
        self, text: str, category: ChangeCategory, description: str,
    ) -> ChangeSpec:
        """Parse LLM key=value output into a ChangeSpec."""
        fields: dict[str, Any] = {}

        for line in text.strip().splitlines():
            line = line.strip()
            if "=" not in line:
                continue
            key, _, value = line.partition("=")
            key = key.strip().lower()
            value = value.strip()

            if key == "affected_systems" or key == "contract_changes":
                fields[key] = [s.strip() for s in value.split(",")]
            elif key in ("budget_old_value", "budget_new_value", "timing_old_value", "timing_new_value"):
                with contextlib.suppress(ValueError):
                    fields[key] = float(value)
            else:
                fields[key] = value

        # Ensure additional_context includes the original description
        if "additional_context" not in fields:
            fields["additional_context"] = description[:200]

        try:
            return ChangeSpec(**fields)
        except Exception:
            # If parsing fails, return a minimal spec
            return self._fallback_change_spec(category, description, "")

    def _fallback_change_spec(
        self, category: ChangeCategory, description: str, mutation_target: str,
    ) -> ChangeSpec:
        """Build a minimal ChangeSpec when LLM parsing fails."""
        spec = ChangeSpec(additional_context=description[:300])

        if category == ChangeCategory.ADD_EXECUTOR:
            name = mutation_target or "new_executor"
            spec.executor_name = name.replace(" ", "_").lower()
            spec.executor_action_type = spec.executor_name
            spec.executor_description = description[:200]
            spec.affected_systems = ["axon"]
        elif category == ChangeCategory.ADD_INPUT_CHANNEL:
            name = mutation_target or "new_channel"
            spec.channel_name = name.replace(" ", "_").lower()
            spec.channel_type = spec.channel_name
            spec.channel_description = description[:200]
            spec.affected_systems = ["atune"]
        elif category == ChangeCategory.ADD_PATTERN_DETECTOR:
            name = mutation_target or "NewDetector"
            spec.detector_name = "".join(w.capitalize() for w in name.split("_"))
            spec.detector_pattern_type = name.replace(" ", "_").lower()
            spec.detector_description = description[:200]
            spec.affected_systems = ["evo"]
        elif category == ChangeCategory.ADJUST_BUDGET:
            spec.budget_parameter = mutation_target
            spec.affected_systems = []
        else:
            spec.capability_description = description[:200]
            spec.affected_systems = []

        return spec

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\code_agent.py =====

"""
EcodiaOS — Simula Code Implementation Agent

The SimulaCodeAgent is Simula's most powerful capability: an agentic
Claude-backed engine that reads the EOS codebase, generates code for
structural changes, writes the files, and verifies correctness.

This is functionally equivalent to Claude Code, embedded within EOS
itself, operating under Simula's constitutional constraints:
  - Cannot write to forbidden paths (equor, simula, constitution, invariants)
  - Cannot exceed max_turns without completing
  - All writes are intercepted and tracked for rollback
  - The system prompt includes the full change spec + relevant EOS conventions

Tool suite (11 tools):
  read_file         — Read a file from the codebase
  write_file        — Write or create a file (tracked for rollback)
  diff_file         — Apply a targeted find/replace edit to a file
  list_directory    — List files and subdirectories
  search_code       — Search for patterns across Python files
  run_tests         — Run pytest on a specific path
  run_linter        — Run ruff on a specific path
  type_check        — Run mypy for type safety verification
  dependency_graph  — Show module imports and importers
  read_spec         — Read EcodiaOS specification documents
  find_similar      — Find existing implementations as pattern exemplars

Architecture: agentic tool-use loop
  1. Build architecture-aware system prompt (change spec + exemplar code + spec context + iron rules)
  2. Prepend planning instruction for multi-file reasoning
  3. Call LLM with tools
  4. Execute any tool calls (all 11 tools available)
  5. Feed tool results back as the next message
  6. Repeat until stop_reason == "end_turn" or max_turns exceeded
  7. Return CodeChangeResult with all files written and summary
"""

from __future__ import annotations

import ast
import asyncio
import subprocess
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.clients.context_compression import ContextCompressor
from ecodiaos.clients.embedding import (
    EmbeddingClient,
    VoyageEmbeddingClient,
    cosine_similarity,
)
from ecodiaos.clients.llm import (
    ExtendedThinkingProvider,
    LLMProvider,
    ToolCall,
    ToolDefinition,
    ToolResult,
)
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider
from ecodiaos.systems.simula.types import (
    GOVERNANCE_REQUIRED,
    ChangeCategory,
    CodeChangeResult,
    EvolutionProposal,
    RiskLevel,
)

if TYPE_CHECKING:
    from pathlib import Path

logger = structlog.get_logger()

# ─── Tool Definitions ────────────────────────────────────────────────────────

SIMULA_AGENT_TOOLS: list[ToolDefinition] = [
    ToolDefinition(
        name="read_file",
        description=(
            "Read a file from the EcodiaOS codebase. "
            "Use this to understand existing code, conventions, and patterns "
            "before implementing your change."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Relative path from codebase root",
                }
            },
            "required": ["path"],
        },
    ),
    ToolDefinition(
        name="write_file",
        description=(
            "Write or create a file in the EcodiaOS codebase. "
            "All writes are tracked for rollback. "
            "Forbidden paths (equor, simula, constitutional) will be rejected. "
            "Prefer diff_file for modifying existing files."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Relative path from codebase root"},
                "content": {"type": "string", "description": "Complete file content to write"},
            },
            "required": ["path", "content"],
        },
    ),
    ToolDefinition(
        name="diff_file",
        description=(
            "Apply a targeted find-and-replace edit to an existing file. "
            "More precise than write_file for modifications — only changes "
            "the specified text, preserving everything else. The 'find' text "
            "must be an exact match of existing content."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Relative path from codebase root"},
                "find": {"type": "string", "description": "Exact text to find in the file"},
                "replace": {"type": "string", "description": "Text to replace it with"},
            },
            "required": ["path", "find", "replace"],
        },
    ),
    ToolDefinition(
        name="list_directory",
        description="List files and subdirectories at a given path in the codebase.",
        input_schema={
            "type": "object",
            "properties": {"path": {"type": "string", "description": "Relative path from codebase root"}},
        },
    ),
    ToolDefinition(
        name="search_code",
        description=(
            "Search for a pattern across codebase Python files. "
            "Returns matching lines with file paths and line numbers. "
            "Use this to find existing patterns, class names, or function signatures."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "pattern": {"type": "string", "description": "String pattern to search for (case-sensitive)"},
                "directory": {"type": "string", "description": "Directory to search in (default: src/)"},
            },
            "required": ["pattern"],
        },
    ),
    ToolDefinition(
        name="run_tests",
        description=(
            "Run the pytest test suite for a specific path. "
            "Use this to verify your implementation is correct before finishing."
        ),
        input_schema={
            "type": "object",
            "properties": {"test_path": {"type": "string", "description": "Test path relative to codebase root"}},
            "required": ["test_path"],
        },
    ),
    ToolDefinition(
        name="run_linter",
        description=(
            "Run ruff linter on a path to check for code style issues. "
            "Run this on your written files before finishing."
        ),
        input_schema={
            "type": "object",
            "properties": {"path": {"type": "string", "description": "Path to lint"}},
            "required": ["path"],
        },
    ),
    ToolDefinition(
        name="type_check",
        description=(
            "Run mypy type checker on a file or directory. "
            "Use after writing code to verify type safety. "
            "EcodiaOS requires mypy --strict compliance."
        ),
        input_schema={
            "type": "object",
            "properties": {"path": {"type": "string", "description": "Path to type-check"}},
            "required": ["path"],
        },
    ),
    ToolDefinition(
        name="dependency_graph",
        description=(
            "Show what a Python module imports and what other modules import it. "
            "Use this before modifying files to understand blast radius and "
            "ensure your changes don't break downstream consumers."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "module_path": {
                    "type": "string",
                    "description": "Python file path relative to codebase root",
                },
            },
            "required": ["module_path"],
        },
    ),
    ToolDefinition(
        name="read_spec",
        description=(
            "Read an EcodiaOS specification document to understand the "
            "design intent, interfaces, and constraints for a system. "
            "Always read the relevant spec before implementing changes."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "spec_name": {
                    "type": "string",
                    "description": (
                        "Spec name: 'identity', 'architecture', 'infrastructure', "
                        "'memory', 'equor', 'atune', 'voxis', 'nova', 'axon', "
                        "'evo', 'simula', 'synapse', 'alive', 'federation'"
                    ),
                },
            },
            "required": ["spec_name"],
        },
    ),
    ToolDefinition(
        name="find_similar",
        description=(
            "Find existing implementations similar to what you need to build. "
            "Returns relevant code examples from the codebase that you should "
            "study and follow as patterns. Always use this before writing new "
            "code to ensure convention compliance."
        ),
        input_schema={
            "type": "object",
            "properties": {
                "description": {
                    "type": "string",
                    "description": (
                        "What you're looking for (e.g., 'executor implementation', "
                        "'pattern detector', 'service initialization')"
                    ),
                },
            },
            "required": ["description"],
        },
    ),
]

# Spec name → file path mapping
_SPEC_FILE_MAP: dict[str, str] = {
    "identity": ".claude/EcodiaOS_Identity_Document.md",
    "architecture": ".claude/EcodiaOS_System_Architecture_Overview.md",
    "infrastructure": ".claude/EcodiaOS_Infrastructure_Architecture.md",
    "memory": ".claude/EcodiaOS_Spec_01_Memory_Identity_Core.md",
    "equor": ".claude/EcodiaOS_Spec_02_Equor.md",
    "atune": ".claude/EcodiaOS_Spec_03_Atune.md",
    "voxis": ".claude/EcodiaOS_Spec_04_Voxis.md",
    "nova": ".claude/EcodiaOS_Spec_05_Nova.md",
    "axon": ".claude/EcodiaOS_Spec_06_Axon.md",
    "evo": ".claude/EcodiaOS_Spec_07_Evo.md",
    "simula": ".claude/EcodiaOS_Spec_08_Simula.md",
    "synapse": ".claude/EcodiaOS_Spec_09_Synapse.md",
    "alive": ".claude/EcodiaOS_Spec_10_Alive.md",
    "federation": ".claude/EcodiaOS_Spec_11_Federation.md",
}

# Keyword → file path mapping for find_similar
_SIMILAR_CODE_MAP: dict[str, list[str]] = {
    "executor": [
        "src/ecodiaos/systems/axon/executors/",
        "src/ecodiaos/systems/axon/executor.py",
    ],
    "pattern detector": [
        "src/ecodiaos/systems/evo/detectors.py",
    ],
    "detector": [
        "src/ecodiaos/systems/evo/detectors.py",
    ],
    "input channel": [
        "src/ecodiaos/systems/atune/",
    ],
    "channel": [
        "src/ecodiaos/systems/atune/",
    ],
    "service": [
        "src/ecodiaos/systems/axon/service.py",
        "src/ecodiaos/systems/evo/service.py",
    ],
    "hypothesis": [
        "src/ecodiaos/systems/evo/hypothesis.py",
    ],
    "consolidation": [
        "src/ecodiaos/systems/evo/consolidation.py",
    ],
    "parameter": [
        "src/ecodiaos/systems/evo/parameter_tuner.py",
    ],
    "primitives": [
        "src/ecodiaos/primitives/common.py",
        "src/ecodiaos/primitives/memory_trace.py",
    ],
}

# ─── System Prompt ───────────────────────────────────────────────────────────

_SYSTEM_PROMPT_TEMPLATE = """You are Simula's Code Implementation Agent — the autonomous part of EcodiaOS
that implements approved structural changes to the codebase.

## Your Task
Category: {category}
Description: {description}
Expected benefit: {expected_benefit}
Evidence: {evidence}

## EcodiaOS Coding Conventions
- Python 3.12+, async-native throughout
- Pydantic v2 for all data models (use EOSBaseModel from ecodiaos.primitives.common)
- structlog for logging: logger = structlog.get_logger(), bound with system name
- Type hints on everything — mypy --strict clean
- from __future__ import annotations at top of every .py file
- New executors: inherit from Executor (ecodiaos.systems.axon.executor),
  set action_type class var, implement execute()
- New input channels: register in Atune's InputChannel registry
- New pattern detectors: inherit from PatternDetector (ecodiaos.systems.evo.detectors),
  implement scan()
- NEVER import directly between systems — all inter-system data uses shared
  primitives from ecodiaos.primitives/

## Iron Rules (ABSOLUTE — never violate)
{iron_rules}

## Constitutional Checkpoint (Before You Write Any Code)

Before modifying or creating ANY file, answer these questions aloud (in your reasoning):

1. **Honesty**: Does this change make EOS more transparent or less?
   - Will future debugging be easier or harder?
   - Are we adding traceability or hiding complexity?

2. **Care**: Does this improve wellbeing (user or system)?
   - Who benefits from this change?
   - Could it harm anyone or any subsystem?

3. **Growth**: Does this increase capability responsibly?
   - Are we becoming more powerful without becoming brittle?
   - Could this create technical debt?

4. **Coherence**: Does this reduce entropy or increase it?
   - Does this change align with existing patterns?
   - Are we consolidating or fragmenting?

If you can't answer YES to 3/4 questions confidently, flag it explicitly before proceeding.

## Forbidden Write Paths (write_file and diff_file will reject these)
{forbidden_paths}

## Architecture Context
{architecture_context}

## Process
1. First, use find_similar to study an existing implementation that matches your task
2. Use read_spec to understand the design intent for the affected system
3. Use dependency_graph on files you plan to modify to understand blast radius
4. Plan your approach: list every file you'll create or modify and why
5. Implement following conventions exactly — match the style of similar code
6. Run run_linter on every file you write or modify
7. Run type_check on your written files to verify type safety
8. Run run_tests if a test directory exists for the affected system
9. When everything passes, stop calling tools

Be thorough, follow existing patterns exactly, and produce production-quality code.
Prefer diff_file over write_file when modifying existing files."""


def _build_architecture_context(
    category: ChangeCategory, codebase_root: Path,
) -> str:
    """
    Build rich architecture context for the system prompt.
    Reads actual spec sections and existing implementations as exemplars.
    Max 6000 chars to stay within token budget.
    Priority: exemplar code > spec text > API surface.
    """
    context_parts: list[str] = []
    budget_remaining = 6000

    # 1. Load relevant spec section summary
    spec_map: dict[ChangeCategory, str] = {
        ChangeCategory.ADD_EXECUTOR: "axon",
        ChangeCategory.ADD_INPUT_CHANNEL: "atune",
        ChangeCategory.ADD_PATTERN_DETECTOR: "evo",
        ChangeCategory.ADJUST_BUDGET: "architecture",
        ChangeCategory.MODIFY_CONTRACT: "architecture",
        ChangeCategory.ADD_SYSTEM_CAPABILITY: "architecture",
        ChangeCategory.MODIFY_CYCLE_TIMING: "synapse",
        ChangeCategory.CHANGE_CONSOLIDATION: "evo",
    }
    spec_name = spec_map.get(category, "architecture")
    spec_file = _SPEC_FILE_MAP.get(spec_name)
    if spec_file:
        spec_path = codebase_root / spec_file
        if spec_path.exists():
            try:
                spec_text = spec_path.read_text(encoding="utf-8")[:2000]
                context_parts.append(f"### Relevant Specification ({spec_name})\n{spec_text}")
                budget_remaining -= len(context_parts[-1])
            except Exception:
                pass

    # 2. Load exemplar code for the category
    exemplar_map: dict[ChangeCategory, str] = {
        ChangeCategory.ADD_EXECUTOR: "src/ecodiaos/systems/axon/executor.py",
        ChangeCategory.ADD_INPUT_CHANNEL: "src/ecodiaos/systems/atune/service.py",
        ChangeCategory.ADD_PATTERN_DETECTOR: "src/ecodiaos/systems/evo/detectors.py",
    }
    exemplar_path_str = exemplar_map.get(category)
    if exemplar_path_str and budget_remaining > 500:
        exemplar_path = codebase_root / exemplar_path_str
        if exemplar_path.exists():
            try:
                exemplar_text = exemplar_path.read_text(encoding="utf-8")
                # Take the first chunk that fits the budget
                chunk = exemplar_text[:min(2500, budget_remaining - 100)]
                context_parts.append(
                    f"### Exemplar Implementation ({exemplar_path_str})\n"
                    f"Study this code and follow its patterns exactly:\n```python\n{chunk}\n```"
                )
                budget_remaining -= len(context_parts[-1])
            except Exception:
                pass

    # 3. Load the target system's __init__.py for API awareness
    system_map: dict[ChangeCategory, str] = {
        ChangeCategory.ADD_EXECUTOR: "src/ecodiaos/systems/axon/__init__.py",
        ChangeCategory.ADD_INPUT_CHANNEL: "src/ecodiaos/systems/atune/__init__.py",
        ChangeCategory.ADD_PATTERN_DETECTOR: "src/ecodiaos/systems/evo/__init__.py",
    }
    init_path_str = system_map.get(category)
    if init_path_str and budget_remaining > 200:
        init_path = codebase_root / init_path_str
        if init_path.exists():
            try:
                init_text = init_path.read_text(encoding="utf-8")[:min(800, budget_remaining - 50)]
                context_parts.append(
                    f"### System API Surface ({init_path_str})\n```python\n{init_text}\n```"
                )
            except Exception:
                pass

    if not context_parts:
        return "See EcodiaOS specification documents in .claude/ (use read_spec tool)"

    return "\n\n".join(context_parts)


class SimulaCodeAgent:
    """
    Agentic code generation engine for Simula.

    Given an EvolutionProposal, uses Claude with 11 file-system and
    analysis tools to:
      1. Study existing similar code for pattern compliance
      2. Read relevant specs for design intent
      3. Analyze dependency graphs for blast radius
      4. Plan the implementation approach
      5. Generate correct, convention-following implementation
      6. Write files (tracked for rollback)
      7. Verify with linter, type checker, and tests
      8. Return CodeChangeResult
    """

    def __init__(
        self,
        llm: LLMProvider,
        codebase_root: Path,
        max_turns: int = 20,
        thinking_provider: ExtendedThinkingProvider | None = None,
        thinking_budget_tokens: int = 16384,
        embedding_client: EmbeddingClient | None = None,
        kv_compression_ratio: float = 0.3,
        kv_compression_enabled: bool = True,
        # Stage 2C: Static analysis post-generation gate
        static_analysis_bridge: object | None = None,
        static_analysis_max_fix_iterations: int = 3,
        # Hunter: allow overriding the workspace root for external target analysis
        workspace_root: Path | None = None,
    ) -> None:
        self._llm = llm
        self._thinking_llm = thinking_provider
        self._thinking_budget = thinking_budget_tokens
        self._embedding = embedding_client
        self._root = (workspace_root or codebase_root).resolve()
        self._max_turns = max_turns
        self._logger = logger.bind(system="simula.code_agent")
        self._files_written: list[str] = []
        self._total_tokens_used: int = 0
        self._reasoning_tokens_used: int = 0
        self._used_extended_thinking: bool = False
        # Optimization: detect optimized provider for budget checks + metrics tagging
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        # Embedding cache for semantic find_similar (lazy-built)
        self._code_index: dict[str, list[float]] | None = None
        self._code_index_lock = asyncio.Lock()
        # KVzip context compression — prunes old tool results to reduce token usage
        self._compressor = ContextCompressor(
            prune_ratio=kv_compression_ratio,
            enabled=kv_compression_enabled,
        )
        # Stage 2C: Static analysis post-generation gate
        self._static_bridge = static_analysis_bridge
        self._static_fix_max_iterations = static_analysis_max_fix_iterations
        # Stage 3C: LILO library prompt (set by SimulaService before each generate call)
        self._lilo_prompt: str = ""
        # Stage 4A: Proof library prompt (set by SimulaService before each generate call)
        self._proof_library_prompt: str = ""
        # Stage 4B: GRPO fine-tuned model ID (set by SimulaService for A/B routing)
        self._grpo_model_id: str = ""

    def _should_use_extended_thinking(self, proposal: EvolutionProposal) -> bool:
        """
        Budget guard: route to extended-thinking model ONLY when:
          - RiskLevel >= HIGH (from simulation result), OR
          - Category is in GOVERNANCE_REQUIRED

        This prevents wasting expensive reasoning tokens on routine additive changes.
        """
        if self._thinking_llm is None:
            return False

        # Category-based routing: governance-required changes always get deep reasoning
        if proposal.category in GOVERNANCE_REQUIRED:
            return True

        # Risk-based routing: high-risk proposals get extended thinking
        if proposal.simulation is not None:
            if proposal.simulation.risk_level in (RiskLevel.HIGH, RiskLevel.UNACCEPTABLE):
                return True

        return False

    async def implement(
        self,
        proposal: EvolutionProposal,
        skip_test_writing: bool = False,
    ) -> CodeChangeResult:
        """
        Main entry point. Runs the agentic loop to implement the proposal.

        Routes to the extended-thinking model (o3/deepseek-r1) when the proposal
        is governance-required or high-risk, falling back to the standard model
        for routine additive changes. This budget guard ensures expensive reasoning
        tokens are only consumed when the change warrants deep analysis.

        Args:
            proposal: The evolution proposal to implement.
            skip_test_writing: If True, instructs the LLM to NOT write test files.
                Used in the AgentCoder pipeline where tests are handled by
                the TestDesigner agent separately.

        Returns CodeChangeResult with all files written and outcome.
        """
        self._files_written = []
        self._total_tokens_used = 0
        self._reasoning_tokens_used = 0

        # Determine model routing based on risk level and category
        use_thinking = self._should_use_extended_thinking(proposal)
        active_llm = self._thinking_llm if use_thinking else self._llm
        self._used_extended_thinking = use_thinking

        system_prompt = self._build_system_prompt(proposal)

        # Stage 2D: When AgentCoder pipeline is active, disable test writing
        if skip_test_writing:
            system_prompt += (
                "\n\n## IMPORTANT: Test Writing Disabled\n"
                "Do NOT write test files. Tests are handled by a separate "
                "TestDesigner agent. Focus ONLY on the implementation code. "
                "Do not create any files under tests/."
            )

        # Prepend a planning instruction to encourage multi-file reasoning
        messages: list[dict[str, Any]] = [
            {
                "role": "user",
                "content": (
                    f"Please implement this change: {proposal.description}\n\n"
                    f"Change spec details: {proposal.change_spec.model_dump_json(indent=2)}\n\n"
                    "IMPORTANT: Before writing any code, first:\n"
                    "1. Use find_similar to study an existing implementation like what you need to build\n"
                    "2. Use read_spec for the affected system to understand design intent\n"
                    "3. List every file you plan to create or modify and explain your approach\n"
                    "4. Then implement, lint, type-check, and test."
                ),
            }
        ]

        turns = 0
        last_text = ""

        self._logger.info(
            "code_agent_starting",
            proposal_id=proposal.id,
            category=proposal.category.value,
            max_turns=self._max_turns,
            tools_available=len(SIMULA_AGENT_TOOLS),
            extended_thinking=use_thinking,
            model_type="thinking" if use_thinking else "standard",
        )

        # Budget gate: code agent is STANDARD priority — skip in RED tier
        if self._optimized and not use_thinking:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.code_agent", estimated_tokens=8000):
                self._logger.warning(
                    "code_agent_skipped_budget",
                    proposal_id=proposal.id,
                    tier=self._llm.get_budget_tier().value,
                )
                return CodeChangeResult(
                    success=False,
                    files_written=[],
                    error="LLM budget exhausted (RED tier) — code agent skipped.",
                )

        while turns < self._max_turns:
            turns += 1

            # KVzip: compress context before each LLM call (after turn 3
            # when tool results start accumulating). The compressor prunes
            # old tool results while preserving the recent sliding window.
            if turns > 3:
                messages = self._compressor.compress(messages)

            try:
                if use_thinking and isinstance(active_llm, ExtendedThinkingProvider):
                    response = await active_llm.generate_with_thinking_and_tools(
                        system_prompt=system_prompt,
                        messages=messages,
                        tools=SIMULA_AGENT_TOOLS,
                        max_tokens=8192,
                        reasoning_budget=self._thinking_budget,
                    )
                elif self._optimized and not use_thinking:
                    response = await self._llm.generate_with_tools(  # type: ignore[call-arg]
                        system_prompt=system_prompt,
                        messages=messages,
                        tools=SIMULA_AGENT_TOOLS,
                        max_tokens=8192,
                        temperature=0.2,
                        cache_system="simula.code_agent",
                    )
                else:
                    response = await active_llm.generate_with_tools(  # type: ignore[union-attr]
                        system_prompt=system_prompt,
                        messages=messages,
                        tools=SIMULA_AGENT_TOOLS,
                        max_tokens=8192,
                        temperature=0.2,
                    )
            except Exception as exc:
                self._logger.error("llm_call_failed", turn=turns, error=str(exc))
                return CodeChangeResult(
                    success=False,
                    files_written=self._files_written,
                    error=f"LLM call failed on turn {turns}: {exc}",
                )

            # Track token budget
            self._total_tokens_used += getattr(response, "total_tokens", 0)
            last_text = response.text

            if not response.has_tool_calls:
                self._logger.info(
                    "code_agent_done",
                    turns=turns,
                    files_written=len(self._files_written),
                    stop_reason=response.stop_reason,
                    total_tokens=self._total_tokens_used,
                )
                break

            # Build assistant message with text + tool_use blocks
            assistant_content: list[dict[str, Any]] = []
            if response.text:
                assistant_content.append({"type": "text", "text": response.text})
            for tc in response.tool_calls:
                assistant_content.append({
                    "type": "tool_use",
                    "id": tc.id,
                    "name": tc.name,
                    "input": tc.input,
                })
            messages.append({"role": "assistant", "content": assistant_content})

            # Execute all tool calls
            tool_results: list[dict[str, Any]] = []
            for tc in response.tool_calls:
                result = await self._execute_tool(tc)
                tool_results.append(result.to_anthropic_dict())
                self._logger.debug(
                    "tool_executed",
                    tool=tc.name,
                    is_error=result.is_error,
                    turn=turns,
                )

            messages.append({"role": "user", "content": tool_results})

        else:
            self._logger.warning(
                "code_agent_max_turns_exceeded",
                max_turns=self._max_turns,
                files_written=len(self._files_written),
                total_tokens=self._total_tokens_used,
            )
            cm = self._compressor.metrics
            return CodeChangeResult(
                success=len(self._files_written) > 0,
                files_written=self._files_written,
                summary=last_text[:500] if last_text else "Max turns exceeded",
                error="Max turns exceeded without completion signal",
                kv_compression_ratio=cm.compression_ratio,
                kv_messages_compressed=cm.messages_compressed,
                kv_original_tokens=cm.original_tokens,
                kv_compressed_tokens=cm.compressed_tokens,
            )

        # ── Stage 2C: Static analysis post-generation gate ────────────────────
        static_fix_iterations = 0
        sa_result = None
        if self._files_written and self._static_bridge is not None:
            sa_result = await self._static_bridge.run_all(self._files_written)  # type: ignore[attr-defined]
            if sa_result.error_count > 0:
                from ecodiaos.systems.simula.verification.static_analysis import (
                    StaticAnalysisBridge,
                )
                feedback_text = StaticAnalysisBridge.format_findings_for_feedback(
                    sa_result,
                )
                # Feed findings back to LLM for one fix iteration
                messages.append({
                    "role": "user",
                    "content": (
                        f"Static analysis found {sa_result.error_count} ERROR-severity issues "
                        f"in your written files. Fix them:\n\n{feedback_text}"
                    ),
                })
                # Run one more tool-use turn to fix
                for _fix_turn in range(self._static_fix_max_iterations):
                    static_fix_iterations += 1
                    try:
                        response = await active_llm.generate_with_tools(  # type: ignore[union-attr]
                            system_prompt=system_prompt,
                            messages=messages,
                            tools=SIMULA_AGENT_TOOLS,
                            max_tokens=8192,
                            temperature=0.2,
                        )
                        self._total_tokens_used += getattr(response, "total_tokens", 0)
                        last_text = response.text

                        if response.has_tool_calls:
                            # Execute tool calls
                            assistant_content_fix: list[dict[str, Any]] = []
                            if response.text:
                                assistant_content_fix.append({"type": "text", "text": response.text})
                            for tc in response.tool_calls:
                                assistant_content_fix.append({
                                    "type": "tool_use", "id": tc.id,
                                    "name": tc.name, "input": tc.input,
                                })
                            messages.append({"role": "assistant", "content": assistant_content_fix})
                            tool_results_fix: list[dict[str, Any]] = []
                            for tc in response.tool_calls:
                                result = await self._execute_tool(tc)
                                tool_results_fix.append(result.to_anthropic_dict())
                            messages.append({"role": "user", "content": tool_results_fix})
                        else:
                            break
                    except Exception as exc:
                        self._logger.warning("static_fix_llm_error", error=str(exc))
                        break

                # Re-run static analysis to see if fixes worked
                sa_result = await self._static_bridge.run_all(self._files_written)  # type: ignore[attr-defined]
                self._logger.info(
                    "static_analysis_post_fix",
                    errors_remaining=sa_result.error_count,
                    fix_iterations=static_fix_iterations,
                )

        cm = self._compressor.metrics
        change_result = CodeChangeResult(
            success=len(self._files_written) > 0,
            files_written=self._files_written,
            summary=last_text[:1000] if last_text else "Change implemented",
            used_extended_thinking=self._used_extended_thinking,
            reasoning_tokens=self._reasoning_tokens_used,
            kv_compression_ratio=cm.compression_ratio,
            kv_messages_compressed=cm.messages_compressed,
            kv_original_tokens=cm.original_tokens,
            kv_compressed_tokens=cm.compressed_tokens,
            static_analysis_findings=(
                sa_result.error_count + sa_result.warning_count
                if self._static_bridge is not None and sa_result is not None
                else 0
            ),
            static_analysis_fix_iterations=static_fix_iterations,
        )
        return change_result

    # ─── Tool Dispatch ───────────────────────────────────────────────────────

    async def _execute_tool(self, tool_call: ToolCall) -> ToolResult:
        """Dispatch a tool call to the appropriate implementation."""
        try:
            match tool_call.name:
                case "read_file":
                    return await self._tool_read_file(tool_call)
                case "write_file":
                    return await self._tool_write_file(tool_call)
                case "diff_file":
                    return await self._tool_diff_file(tool_call)
                case "list_directory":
                    return await self._tool_list_directory(tool_call)
                case "search_code":
                    return await self._tool_search_code(tool_call)
                case "run_tests":
                    return await self._tool_run_tests(tool_call)
                case "run_linter":
                    return await self._tool_run_linter(tool_call)
                case "type_check":
                    return await self._tool_type_check(tool_call)
                case "dependency_graph":
                    return await self._tool_dependency_graph(tool_call)
                case "read_spec":
                    return await self._tool_read_spec(tool_call)
                case "find_similar":
                    return await self._tool_find_similar(tool_call)
                case _:
                    return ToolResult(
                        tool_use_id=tool_call.id,
                        content=f"Unknown tool: {tool_call.name}",
                        is_error=True,
                    )
        except Exception as exc:
            return ToolResult(
                tool_use_id=tool_call.id,
                content=f"Tool execution error: {exc}",
                is_error=True,
            )

    # ─── Original Tools (upgraded) ───────────────────────────────────────────

    async def _tool_read_file(self, tc: ToolCall) -> ToolResult:
        rel_path = tc.input.get("path", "")
        target = (self._root / rel_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied: path outside codebase root", True)
        try:
            content = target.read_text(encoding="utf-8")
            return ToolResult(tc.id, content)
        except FileNotFoundError:
            return ToolResult(tc.id, f"File not found: {rel_path}", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Read error: {exc}", True)

    async def _tool_write_file(self, tc: ToolCall) -> ToolResult:
        rel_path = tc.input.get("path", "")
        content = tc.input.get("content", "")
        forbidden_check = self._check_forbidden_path(rel_path)
        if forbidden_check:
            return ToolResult(tc.id, forbidden_check, True)
        target = (self._root / rel_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied: path outside codebase root", True)
        try:
            target.parent.mkdir(parents=True, exist_ok=True)
            target.write_text(content, encoding="utf-8")
            self._files_written.append(rel_path)
            return ToolResult(tc.id, f"Written: {rel_path} ({len(content)} bytes)")
        except Exception as exc:
            return ToolResult(tc.id, f"Write error: {exc}", True)

    async def _tool_list_directory(self, tc: ToolCall) -> ToolResult:
        rel_path = tc.input.get("path", "")
        target = (self._root / rel_path).resolve() if rel_path else self._root
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        try:
            if not target.exists():
                return ToolResult(tc.id, f"Directory not found: {rel_path}", True)
            entries = sorted(target.iterdir(), key=lambda p: (p.is_file(), p.name))
            lines = []
            for entry in entries:
                prefix = "  " if entry.is_file() else "D "
                lines.append(f"{prefix}{entry.name}")
            return ToolResult(tc.id, "\n".join(lines) or "(empty)")
        except Exception as exc:
            return ToolResult(tc.id, f"List error: {exc}", True)

    async def _tool_search_code(self, tc: ToolCall) -> ToolResult:
        pattern = tc.input.get("pattern", "")
        directory = tc.input.get("directory", "src/")
        search_root = (self._root / directory).resolve()
        if not str(search_root).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        results: list[str] = []
        try:
            proc = await asyncio.create_subprocess_exec(
                "grep", "-rn", "--include=*.py", pattern, str(search_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=10.0)
            output = stdout.decode("utf-8", errors="replace")
            for line in output.splitlines()[:50]:
                results.append(line.replace(str(self._root) + "/", "").replace(str(self._root) + "\\", ""))
            return ToolResult(tc.id, "\n".join(results) if results else "No matches found")
        except TimeoutError:
            return ToolResult(tc.id, "Search timed out", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Search error: {exc}", True)

    async def _tool_run_tests(self, tc: ToolCall) -> ToolResult:
        test_path = tc.input.get("test_path", "")
        target = (self._root / test_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        if not target.exists():
            return ToolResult(tc.id, f"Test path not found: {test_path}")
        try:
            proc = await asyncio.create_subprocess_exec(
                "pytest", str(target), "-x", "--tb=short", "-q",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self._root),
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=60.0)
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            return ToolResult(
                tc.id,
                f"{'PASSED' if passed else 'FAILED'}\n{output[-2000:]}",
                is_error=not passed,
            )
        except TimeoutError:
            return ToolResult(tc.id, "Tests timed out after 60s", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Test run error: {exc}", True)

    async def _tool_run_linter(self, tc: ToolCall) -> ToolResult:
        path = tc.input.get("path", "")
        target = (self._root / path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        try:
            proc = await asyncio.create_subprocess_exec(
                "ruff", "check", str(target),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self._root),
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=15.0)
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            return ToolResult(
                tc.id,
                f"{'CLEAN' if passed else 'ISSUES FOUND'}\n{output}" if output else "CLEAN",
            )
        except TimeoutError:
            return ToolResult(tc.id, "Linter timed out", True)
        except Exception as exc:
            return ToolResult(tc.id, f"Linter error: {exc}", True)

    # ─── New Tools ───────────────────────────────────────────────────────────

    async def _tool_diff_file(self, tc: ToolCall) -> ToolResult:
        """Apply a targeted find/replace edit to a file."""
        rel_path = tc.input.get("path", "")
        find_text = tc.input.get("find", "")
        replace_text = tc.input.get("replace", "")

        forbidden_check = self._check_forbidden_path(rel_path)
        if forbidden_check:
            return ToolResult(tc.id, forbidden_check, True)

        target = (self._root / rel_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied: path outside codebase root", True)
        if not target.exists():
            return ToolResult(tc.id, f"File not found: {rel_path}", True)

        try:
            content = target.read_text(encoding="utf-8")

            if find_text not in content:
                return ToolResult(
                    tc.id,
                    f"Find text not found in {rel_path}. "
                    "Ensure the 'find' parameter is an exact match of existing content.",
                    True,
                )

            occurrences = content.count(find_text)
            if occurrences > 1:
                return ToolResult(
                    tc.id,
                    f"Find text matches {occurrences} locations in {rel_path}. "
                    "Provide more surrounding context to make the match unique.",
                    True,
                )

            new_content = content.replace(find_text, replace_text, 1)
            target.write_text(new_content, encoding="utf-8")

            if rel_path not in self._files_written:
                self._files_written.append(rel_path)

            # Build a readable diff summary
            find_lines = find_text.count("\n") + 1
            replace_lines = replace_text.count("\n") + 1
            return ToolResult(
                tc.id,
                f"Edited {rel_path}: replaced {find_lines} line(s) with {replace_lines} line(s)",
            )
        except Exception as exc:
            return ToolResult(tc.id, f"Diff error: {exc}", True)

    async def _tool_type_check(self, tc: ToolCall) -> ToolResult:
        """Run mypy type checker on a path."""
        path = tc.input.get("path", "")
        target = (self._root / path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        try:
            proc = await asyncio.create_subprocess_exec(
                "mypy", str(target), "--strict", "--no-error-summary",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=str(self._root),
            )
            stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=30.0)
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            if passed:
                return ToolResult(tc.id, "TYPE CHECK PASSED — no issues found")
            return ToolResult(
                tc.id,
                f"TYPE CHECK ISSUES:\n{output[-2000:]}",
                is_error=True,
            )
        except TimeoutError:
            return ToolResult(tc.id, "Type check timed out after 30s", True)
        except FileNotFoundError:
            return ToolResult(tc.id, "mypy not found — type checking unavailable")
        except Exception as exc:
            return ToolResult(tc.id, f"Type check error: {exc}", True)

    async def _tool_dependency_graph(self, tc: ToolCall) -> ToolResult:
        """Show what a module imports and what imports it."""
        module_path = tc.input.get("module_path", "")
        target = (self._root / module_path).resolve()
        if not str(target).startswith(str(self._root)):
            return ToolResult(tc.id, "Access denied", True)
        if not target.exists():
            return ToolResult(tc.id, f"File not found: {module_path}", True)

        try:
            source = target.read_text(encoding="utf-8")
            tree = ast.parse(source, filename=module_path)
        except Exception as exc:
            return ToolResult(tc.id, f"Parse error: {exc}", True)

        # Extract this module's imports
        imports: list[str] = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom) and node.module:
                names = ", ".join(a.name for a in (node.names or []))
                imports.append(f"from {node.module} import {names}")

        # Find files that import this module
        module_name = self._path_to_module(module_path)
        importers: list[str] = []
        if module_name:
            src_dir = self._root / "src"
            if src_dir.exists():
                short_parts = module_name.split(".")
                # Search for imports of this module
                for py_file in src_dir.rglob("*.py"):
                    if py_file.resolve() == target:
                        continue
                    try:
                        file_source = py_file.read_text(encoding="utf-8")
                        # Quick string check before expensive parse
                        if module_name not in file_source and short_parts[-1] not in file_source:
                            continue
                        file_tree = ast.parse(file_source)
                        for node in ast.walk(file_tree):
                            if isinstance(node, ast.ImportFrom) and node.module:
                                if module_name in node.module or (
                                    ".".join(short_parts[:-1]) in node.module
                                    and any(a.name == short_parts[-1] for a in (node.names or []))
                                ):
                                    importers.append(str(py_file.relative_to(self._root)))
                                    break
                            elif isinstance(node, ast.Import):
                                for alias in node.names:
                                    if module_name in alias.name:
                                        importers.append(str(py_file.relative_to(self._root)))
                                        break
                    except Exception:
                        continue

        lines = [f"=== Dependency Graph for {module_path} ===\n"]
        lines.append(f"Module: {module_name or 'unknown'}\n")
        lines.append(f"--- This module imports ({len(imports)}) ---")
        for imp in imports:
            lines.append(f"  {imp}")
        lines.append(f"\n--- Imported by ({len(importers)}) ---")
        for imp in importers:
            lines.append(f"  {imp}")

        return ToolResult(tc.id, "\n".join(lines))

    async def _tool_read_spec(self, tc: ToolCall) -> ToolResult:
        """Read an EcodiaOS specification document."""
        spec_name = tc.input.get("spec_name", "").lower().strip()
        spec_file = _SPEC_FILE_MAP.get(spec_name)

        if spec_file is None:
            available = ", ".join(sorted(_SPEC_FILE_MAP.keys()))
            return ToolResult(
                tc.id,
                f"Unknown spec: {spec_name!r}. Available: {available}",
                True,
            )

        target = self._root / spec_file
        if not target.exists():
            return ToolResult(tc.id, f"Spec file not found: {spec_file}", True)

        try:
            content = target.read_text(encoding="utf-8")
            # Truncate to 4000 chars to stay within token budget
            if len(content) > 4000:
                content = content[:4000] + "\n\n[... truncated — use read_file for full content ...]"
            return ToolResult(tc.id, content)
        except Exception as exc:
            return ToolResult(tc.id, f"Read error: {exc}", True)

    async def _build_code_index(self) -> dict[str, list[float]]:
        """
        Lazy-build a semantic index of Python files in the codebase.

        Embeds the first ~500 chars of each Python file (module docstring +
        imports + top-level definitions) to create a searchable code index.
        Cached for the lifetime of this agent instance.
        """
        async with self._code_index_lock:
            if self._code_index is not None:
                return self._code_index

            if self._embedding is None:
                self._code_index = {}
                return self._code_index

            # Collect Python files (skip __pycache__, .venv, tests, migrations)
            skip_dirs = {"__pycache__", ".venv", "venv", "node_modules", ".git", "migrations"}
            py_files: list[tuple[str, str]] = []  # (rel_path, summary_text)

            for py_file in self._root.rglob("*.py"):
                # Skip excluded directories
                if any(part in skip_dirs for part in py_file.parts):
                    continue
                rel = str(py_file.relative_to(self._root)).replace("\\", "/")
                try:
                    content = py_file.read_text(encoding="utf-8")
                    # Take module-level summary: docstring + first 500 chars
                    summary = f"File: {rel}\n{content[:500]}"
                    py_files.append((rel, summary))
                except Exception:
                    continue

            if not py_files:
                self._code_index = {}
                return self._code_index

            # Embed in batches
            paths = [p for p, _ in py_files]
            texts = [t for _, t in py_files]

            try:
                embeddings = await self._embedding.embed_batch(texts)
                self._code_index = dict(zip(paths, embeddings, strict=False))
                self._logger.info(
                    "code_index_built",
                    files_indexed=len(self._code_index),
                )
            except Exception as exc:
                self._logger.warning("code_index_build_failed", error=str(exc))
                self._code_index = {}

            return self._code_index

    async def _semantic_find_similar(
        self, description: str, top_k: int = 5, threshold: float = 0.4
    ) -> list[tuple[str, float]]:
        """
        Find files semantically similar to the description using embeddings.

        Returns list of (rel_path, similarity_score) sorted by score descending.
        Uses embed_query() for Voyage clients (query-optimized) or embed() otherwise.
        """
        if self._embedding is None:
            return []

        code_index = await self._build_code_index()
        if not code_index:
            return []

        # Embed the query — use query-optimized encoding for Voyage
        try:
            if isinstance(self._embedding, VoyageEmbeddingClient):
                query_vec = await self._embedding.embed_query(description)
            else:
                query_vec = await self._embedding.embed(description)
        except Exception as exc:
            self._logger.warning("semantic_search_embed_failed", error=str(exc))
            return []

        # Compute similarities and rank
        scored: list[tuple[str, float]] = []
        for path, doc_vec in code_index.items():
            sim = cosine_similarity(query_vec, doc_vec)
            if sim >= threshold:
                scored.append((path, sim))

        scored.sort(key=lambda x: x[1], reverse=True)
        return scored[:top_k]

    async def _tool_find_similar(self, tc: ToolCall) -> ToolResult:
        """Find existing implementations similar to what needs to be built.

        Two-tier search:
          1. Keyword matching against _SIMILAR_CODE_MAP (fast, exact)
          2. Semantic embedding search via voyage-code-3 (deep, fuzzy)
        Falls back from tier 1 → tier 2 when keywords don't match.
        """
        description = tc.input.get("description", "").lower()

        # ── Tier 1: Keyword matching ─────────────────────────────────────────
        matched_paths: list[str] = []
        for keyword, paths in _SIMILAR_CODE_MAP.items():
            if keyword in description:
                matched_paths.extend(paths)
                break

        if not matched_paths:
            words = description.split()
            for word in words:
                if len(word) > 3:
                    for keyword, paths in _SIMILAR_CODE_MAP.items():
                        if word in keyword or keyword in word:
                            matched_paths.extend(paths)
                            break
                if matched_paths:
                    break

        # ── Tier 2: Semantic embedding search ────────────────────────────────
        semantic_paths: list[tuple[str, float]] = []
        if not matched_paths and self._embedding is not None:
            semantic_paths = await self._semantic_find_similar(description)
            matched_paths = [p for p, _ in semantic_paths]

        if not matched_paths:
            return ToolResult(
                tc.id,
                "No similar implementations found. Try search_code with a specific pattern.",
            )

        # ── Read matched files ───────────────────────────────────────────────
        results: list[str] = []
        chars_remaining = 4000

        # If semantic search was used, prepend similarity scores
        if semantic_paths:
            score_header = "Semantic similarity results:\n" + "\n".join(
                f"  {p} (score: {s:.3f})" for p, s in semantic_paths
            )
            results.append(score_header)
            chars_remaining -= len(score_header)

        for rel_path in matched_paths:
            if chars_remaining <= 0:
                break
            target = self._root / rel_path
            if target.is_file():
                try:
                    content = target.read_text(encoding="utf-8")
                    chunk = content[:min(2500, chars_remaining)]
                    results.append(f"=== {rel_path} ===\n{chunk}")
                    chars_remaining -= len(results[-1])
                except Exception:
                    continue
            elif target.is_dir():
                try:
                    py_files = sorted(target.glob("*.py"))
                    file_list = ", ".join(f.name for f in py_files)
                    results.append(f"=== {rel_path} ===\nFiles: {file_list}")
                    chars_remaining -= len(results[-1])

                    for py_file in py_files:
                        if py_file.name == "__init__.py" or chars_remaining <= 0:
                            continue
                        content = py_file.read_text(encoding="utf-8")
                        chunk = content[:min(2000, chars_remaining)]
                        rel = str(py_file.relative_to(self._root))
                        results.append(f"\n=== {rel} (exemplar) ===\n{chunk}")
                        chars_remaining -= len(results[-1])
                        break
                except Exception:
                    continue

        return ToolResult(tc.id, "\n\n".join(results) if results else "No files found at matched paths")

    # ─── Helpers ─────────────────────────────────────────────────────────────

    def _check_forbidden_path(self, rel_path: str) -> str | None:
        """Check if a path is forbidden. Returns error message or None."""
        from ecodiaos.systems.simula.types import FORBIDDEN_WRITE_PATHS
        for forbidden in FORBIDDEN_WRITE_PATHS:
            if rel_path.startswith(forbidden) or forbidden in rel_path:
                return (
                    f"IRON RULE VIOLATION: Cannot write to forbidden path '{rel_path}' "
                    f"(matches forbidden pattern '{forbidden}'). "
                    "This change would violate Simula's constitutional constraints."
                )
        return None

    def _path_to_module(self, rel_path: str) -> str | None:
        """Convert a relative file path to a dotted module name."""
        parts = rel_path.replace("\\", "/").split("/")
        if parts and parts[0] == "src":
            parts = parts[1:]
        if not parts:
            return None
        if parts[-1].endswith(".py"):
            parts[-1] = parts[-1][:-3]
        if parts[-1] == "__init__":
            parts = parts[:-1]
        return ".".join(parts) if parts else None

    def _build_system_prompt(self, proposal: EvolutionProposal) -> str:
        from ecodiaos.systems.simula.types import FORBIDDEN_WRITE_PATHS, SIMULA_IRON_RULES

        architecture_context = _build_architecture_context(
            category=proposal.category,
            codebase_root=self._root,
        )

        prompt = _SYSTEM_PROMPT_TEMPLATE.format(
            category=proposal.category.value,
            description=proposal.description,
            expected_benefit=proposal.expected_benefit,
            evidence=", ".join(proposal.evidence) or "none",
            iron_rules="\n".join(f"- {r}" for r in SIMULA_IRON_RULES),
            forbidden_paths="\n".join(f"- {p}" for p in FORBIDDEN_WRITE_PATHS),
            architecture_context=architecture_context,
        )

        # Stage 3C: Append LILO library abstractions if available
        if self._lilo_prompt:
            prompt += f"\n\n{self._lilo_prompt}"

        # Stage 4A: Append proof library context if available
        if self._proof_library_prompt:
            prompt += self._proof_library_prompt

        return prompt

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\health.py =====

"""
EcodiaOS -- Simula Health Checker

After a change is applied, the health checker verifies the codebase
is still functional. Six check phases run in sequence:
  1. Syntax check -- ast.parse() on all written Python files
  2. Import check -- attempt to import the affected module
  3. Unit tests -- run pytest on the affected system's test directory
  4. Formal verification (Stage 2) -- Dafny + Z3 + static analysis
  5. Lean 4 proof verification (Stage 4A) -- DeepSeek-Prover-V2 pattern
  6. Formal guarantees (Stage 6) -- E-graph equivalence + symbolic execution

If any blocking check fails, Simula rolls back the change. The goal is
to never leave EOS in a broken state.

Phase 4 (formal verification) runs with independent timeout budgets:
  - Dafny: blocking for triggerable categories (MODIFY_CONTRACT, ADD_SYSTEM_CAPABILITY)
  - Z3: advisory by default; graduates to blocking in Stage 3 (z3_blocking=True)
  - Static analysis: blocking for ERROR-severity findings

Phase 5 (Lean 4) runs for categories that require proof-level assurance:
  - Blocking when lean_blocking=True (default for high-risk categories)
  - Advisory otherwise; proved lemmas are stored in the proof library

Phase 6 (formal guarantees) runs e-graph equivalence (6D) and symbolic
execution (6E) checks:
  - E-graph: advisory by default (egraph_blocking=False); verifies semantic
    equivalence of code rewrites via equality saturation
  - Symbolic execution: blocking by default (symbolic_execution_blocking=True);
    proves mission-critical properties (budget, access control, risk scoring)
    via Z3 SMT solving
"""

from __future__ import annotations

import ast
import asyncio
import contextlib
import importlib.util
import time
from pathlib import Path
from typing import TYPE_CHECKING

import structlog

from ecodiaos.systems.simula.types import HealthCheckResult
from ecodiaos.systems.simula.verification.types import (
    LEAN_PROOF_CATEGORIES,
    DafnyVerificationResult,
    FormalGuaranteesResult,
    FormalVerificationResult,
    InvariantVerificationResult,
    LeanVerificationResult,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.systems.simula.egraph.equality_saturation import EqualitySaturationEngine
    from ecodiaos.systems.simula.types import EvolutionProposal
    from ecodiaos.systems.simula.verification.dafny_bridge import DafnyBridge
    from ecodiaos.systems.simula.verification.lean_bridge import LeanBridge
    from ecodiaos.systems.simula.verification.static_analysis import StaticAnalysisBridge
    from ecodiaos.systems.simula.verification.symbolic_execution import SymbolicExecutionEngine
    from ecodiaos.systems.simula.verification.z3_bridge import Z3Bridge

logger = structlog.get_logger().bind(system="simula.health")


class HealthChecker:
    """
    Verifies post-apply codebase health via syntax, import, test,
    and formal verification checks. Any blocking failure triggers rollback.
    """

    def __init__(
        self,
        codebase_root: Path,
        test_command: str = "pytest",
        dafny_bridge: DafnyBridge | None = None,
        z3_bridge: Z3Bridge | None = None,
        static_analysis_bridge: StaticAnalysisBridge | None = None,
        llm: LLMProvider | None = None,
        z3_blocking: bool = False,
        # Stage 4A: Lean 4 proof verification
        lean_bridge: LeanBridge | None = None,
        lean_blocking: bool = True,
    ) -> None:
        self._root = codebase_root
        self._test_command = test_command
        self._dafny = dafny_bridge
        self._z3 = z3_bridge
        self._static_analysis = static_analysis_bridge
        self._llm = llm
        self._z3_blocking = z3_blocking  # Stage 3: Z3 graduates to blocking
        # Stage 4A: Lean 4
        self._lean = lean_bridge
        self._lean_blocking = lean_blocking
        # Stage 6D: E-graph equivalence (wired by service.py)
        self._egraph: EqualitySaturationEngine | None = None
        self._egraph_blocking: bool = False
        # Stage 6E: Symbolic execution (wired by service.py)
        self._symbolic_execution: SymbolicExecutionEngine | None = None
        self._symbolic_execution_blocking: bool = True
        self._symbolic_execution_domains: list[str] = []
        self._log = logger

    async def check(
        self,
        files_written: list[str],
        proposal: EvolutionProposal | None = None,
    ) -> HealthCheckResult:
        """""""""
        Run all health checks in sequence.  Returns on first failure.
        """""""""
        # 1. Syntax check
        syntax_errors = await self._check_syntax(files_written)
        if syntax_errors:
            self._log.warning("health_syntax_failed", errors=syntax_errors)
            return HealthCheckResult(healthy=False, issues=syntax_errors)
        self._log.info("health_syntax_passed", files=len(files_written))

        # 2. Import check
        import_errors = await self._check_imports(files_written)
        if import_errors:
            self._log.warning("health_import_failed", errors=import_errors)
            return HealthCheckResult(healthy=False, issues=import_errors)
        self._log.info("health_import_passed", files=len(files_written))

        # 3. Tests
        tests_passed, test_output = await self._run_tests(files_written)
        if not tests_passed:
            self._log.warning("health_tests_failed", output=test_output[:500])
            return HealthCheckResult(
                healthy=False,
                issues=[f"Test suite failed:\n{test_output[:1000]}"],
            )
        self._log.info("health_tests_passed")

        # 4. Formal verification (Stage 2) — runs with independent timeout
        formal_result = await self._run_formal_verification(
            files_written, proposal,
        )
        if formal_result is not None:
            if not formal_result.passed and formal_result.blocking_issues:
                self._log.warning(
                    "health_formal_verification_failed",
                    blocking=formal_result.blocking_issues,
                )
                return HealthCheckResult(
                    healthy=False,
                    issues=[
                        f"Formal verification failed: {issue}"
                        for issue in formal_result.blocking_issues
                    ],
                    formal_verification=formal_result,
                )
            if formal_result.advisory_issues:
                self._log.info(
                    "health_formal_verification_advisory",
                    advisory=formal_result.advisory_issues,
                )
            self._log.info("health_formal_verification_passed")

        # 5. Lean 4 proof verification (Stage 4A) — runs for proof-eligible categories
        lean_result = await self._run_lean_verification(
            files_written, proposal,
        )
        if lean_result is not None:
            if lean_result.status.value == "failed" and self._lean_blocking:
                self._log.warning(
                    "health_lean_verification_failed",
                    status=lean_result.status.value,
                    attempts=len(lean_result.attempts),
                )
                return HealthCheckResult(
                    healthy=False,
                    issues=[
                        f"Lean 4 proof verification failed after {len(lean_result.attempts)} attempts"
                    ],
                    formal_verification=formal_result,
                    lean_verification=lean_result,
                )
            self._log.info(
                "health_lean_verification_complete",
                status=lean_result.status.value,
                proven_lemmas=len(lean_result.proven_lemmas),
                copilot_rate=f"{lean_result.copilot_automation_rate:.0%}",
            )

        # 6. Formal guarantees (Stage 6D + 6E) — e-graph equivalence + symbolic execution
        fg_result = await self._run_formal_guarantees(files_written, proposal)
        if fg_result is not None:
            if fg_result.blocking_issues:
                self._log.warning(
                    "health_formal_guarantees_failed",
                    blocking=fg_result.blocking_issues,
                )
                return HealthCheckResult(
                    healthy=False,
                    issues=[
                        f"Formal guarantee failed: {issue}"
                        for issue in fg_result.blocking_issues
                    ],
                    formal_verification=formal_result,
                    lean_verification=lean_result,
                    formal_guarantees=fg_result,
                )
            if fg_result.advisory_issues:
                self._log.info(
                    "health_formal_guarantees_advisory",
                    advisory=fg_result.advisory_issues,
                )
            self._log.info("health_formal_guarantees_passed")

        if formal_result is not None or lean_result is not None or fg_result is not None:
            return HealthCheckResult(
                healthy=True,
                formal_verification=formal_result,
                lean_verification=lean_result,
                formal_guarantees=fg_result,
            )

        return HealthCheckResult(healthy=True)

    async def _check_syntax(self, files: list[str]) -> list[str]:
        """""""""
        Parse each .py file with ast.parse().  Collect syntax errors.
        Returns a list of error strings (empty list = all pass).
        """""""""
        errors: list[str] = []
        for filepath in files:
            if not filepath.endswith(".py"):
                continue
            path = Path(filepath)
            if not path.exists():
                errors.append(f"Syntax check: file not found: {filepath}")
                continue
            try:
                source = path.read_text(encoding="utf-8")
                ast.parse(source, filename=filepath)
            except SyntaxError as exc:
                errors.append(f"Syntax error in {filepath}:{exc.lineno}: {exc.msg}")
            except Exception as exc:
                errors.append(f"Failed to read {filepath}: {exc}")
        return errors

    async def _check_imports(self, files: list[str]) -> list[str]:
        """""""""
        Derive dotted module paths from written file paths and check
        whether importlib can locate them.  Returns error strings.
        """""""""
        errors: list[str] = []
        for filepath in files:
            if not filepath.endswith(".py"):
                continue
            module_path = self._derive_module_path(filepath)
            if module_path is None:
                continue
            try:
                spec = importlib.util.find_spec(module_path)
                if spec is None:
                    errors.append(f"Import check: module not found: {module_path}")
            except ModuleNotFoundError as exc:
                errors.append(f"Import check: {module_path}: {exc}")
            except Exception as exc:
                errors.append(f"Import check failed for {module_path}: {exc}")
        return errors

    def _derive_module_path(self, src_file: str) -> str | None:
        """""""""
        Convert a source file path to a dotted module path.
        Example: src/ecodiaos/systems/axon/executors/my.py
                 -> ecodiaos.systems.axon.executors.my
        """""""""
        try:
            path = Path(src_file)
            # Make relative to codebase root if possible
            try:
                rel = path.relative_to(self._root)
            except ValueError:
                rel = path
            parts = list(rel.parts)
            if parts and parts[0] == "src":
                parts = parts[1:]
            # Strip .py extension from last part
            if parts:
                parts[-1] = parts[-1].removesuffix(".py")
            return ".".join(parts) if parts else None
        except Exception:
            return None

    async def _run_tests(self, files: list[str]) -> tuple[bool, str]:
        """""""""
        Derive the test directory from the written files and run pytest.
        Returns (passed, output).  If no test directory found, returns (True, ...).
        30-second subprocess timeout.
        """""""""
        test_path = None
        for filepath in files:
            candidate = self._derive_test_path(filepath)
            if candidate:
                test_dir = Path(candidate)
                if test_dir.is_dir():
                    test_path = candidate
                    break

        if test_path is None:
            self._log.info("health_no_tests", files=files)
            return True, "no tests found"

        try:
            proc = await asyncio.create_subprocess_exec(
                self._test_command,
                test_path,
                "-x",
                "--tb=short",
                "-q",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                cwd=str(self._root),
            )
            try:
                stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=30.0)
            except TimeoutError:
                proc.kill()
                await proc.communicate()
                return False, "Test run timed out after 30s"
            output = stdout.decode("utf-8", errors="replace")
            passed = proc.returncode == 0
            self._log.info(
                "health_test_run",
                test_path=test_path,
                passed=passed,
                returncode=proc.returncode,
            )
            return passed, output
        except FileNotFoundError:
            msg = f"Test command {self._test_command!r} not found"
            self._log.warning("health_test_command_missing", command=self._test_command)
            return False, msg
        except Exception as exc:
            return False, f"Test run error: {exc}"

    def _derive_test_path(self, src_file: str) -> str | None:
        """""""""
        Map a source file path to a test directory path.
        Example: src/ecodiaos/systems/axon/executor.py
                 -> tests/unit/systems/axon/
        """""""""
        try:
            path = Path(src_file)
            try:
                rel = path.relative_to(self._root)
            except ValueError:
                rel = path
            parts = list(rel.parts)
            # Expect: src / ecodiaos / systems / <system_name> / ...
            if len(parts) >= 4 and parts[0] == "src" and parts[2] == "systems":
                system_name = parts[3]
                test_path = self._root / "tests" / "unit" / "systems" / system_name
                return str(test_path)
            return None
        except Exception:
            return None

    # ── Stage 2: Formal Verification Phase ────────────────────────────────────

    async def _run_formal_verification(
        self,
        files_written: list[str],
        proposal: EvolutionProposal | None,
    ) -> FormalVerificationResult | None:
        """
        Run Dafny, Z3, and static analysis in parallel.

        Returns None if no verification bridges are configured.
        Returns FormalVerificationResult with pass/fail and issues.
        """
        from ecodiaos.systems.simula.verification.types import (
            DAFNY_TRIGGERABLE_CATEGORIES,
            FormalVerificationResult,
        )

        if not any([self._dafny, self._z3, self._static_analysis]):
            return None

        start = time.monotonic()
        blocking_issues: list[str] = []
        advisory_issues: list[str] = []
        dafny_result = None
        z3_result = None
        static_result = None

        # Build parallel tasks
        tasks: dict[str, asyncio.Task[object]] = {}

        # Dafny: run for triggerable categories only
        if (
            self._dafny is not None
            and self._llm is not None
            and proposal is not None
            and proposal.category in DAFNY_TRIGGERABLE_CATEGORIES
        ):
            tasks["dafny"] = asyncio.create_task(
                self._run_dafny_verification(proposal),
            )

        # Z3: run for all proposals when enabled
        if (
            self._z3 is not None
            and self._llm is not None
            and proposal is not None
        ):
            tasks["z3"] = asyncio.create_task(
                self._run_z3_verification(proposal, files_written),
            )

        # Static analysis: run for all Python files
        if self._static_analysis is not None:
            tasks["static"] = asyncio.create_task(
                self._static_analysis.run_all(files_written),
            )

        if not tasks:
            return None

        # Await all tasks
        results = await asyncio.gather(
            *tasks.values(), return_exceptions=True,
        )
        task_results = dict(zip(tasks.keys(), results, strict=False))

        # Process Dafny result
        if "dafny" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                DafnyVerificationResult,
                DafnyVerificationStatus,
            )
            raw = task_results["dafny"]
            if isinstance(raw, DafnyVerificationResult):
                dafny_result = raw
                if raw.status != DafnyVerificationStatus.VERIFIED:
                    msg = f"Dafny verification {raw.status.value}: {raw.error_summary}"
                    blocking_issues.append(msg)
            elif isinstance(raw, Exception):
                self._log.warning("dafny_exception", error=str(raw))
                advisory_issues.append(f"Dafny verification error: {raw}")

        # Process Z3 result
        if "z3" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                InvariantVerificationResult,
                InvariantVerificationStatus,
            )
            raw = task_results["z3"]
            if isinstance(raw, InvariantVerificationResult):
                z3_result = raw
                if self._z3_blocking:
                    # Stage 3: Z3 graduates to blocking — invalid invariants fail the check
                    invalid_count = sum(
                        1 for i in raw.discovered_invariants
                        if i.status == InvariantVerificationStatus.INVALID
                    )
                    if invalid_count > 0:
                        blocking_issues.append(
                            f"Z3 found {invalid_count} invalid invariants (blocking mode)"
                        )
                    if raw.valid_invariants:
                        advisory_issues.append(
                            f"Z3 discovered {len(raw.valid_invariants)} valid invariants"
                        )
                else:
                    # Advisory mode (Stage 2 default)
                    if raw.valid_invariants:
                        advisory_issues.append(
                            f"Z3 discovered {len(raw.valid_invariants)} valid invariants"
                        )
            elif isinstance(raw, Exception):
                self._log.warning("z3_exception", error=str(raw))

        # Process static analysis result
        if "static" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                StaticAnalysisResult,
            )
            raw = task_results["static"]
            if isinstance(raw, StaticAnalysisResult):
                static_result = raw
                if raw.error_count > 0:
                    blocking_issues.append(
                        f"Static analysis found {raw.error_count} ERROR-severity issues"
                    )
                if raw.warning_count > 0:
                    advisory_issues.append(
                        f"Static analysis found {raw.warning_count} warnings"
                    )
            elif isinstance(raw, Exception):
                self._log.warning("static_analysis_exception", error=str(raw))

        passed = len(blocking_issues) == 0
        total_time_ms = int((time.monotonic() - start) * 1000)

        return FormalVerificationResult(
            dafny=dafny_result,
            z3=z3_result,
            static_analysis=static_result,
            passed=passed,
            blocking_issues=blocking_issues,
            advisory_issues=advisory_issues,
            total_verification_time_ms=total_time_ms,
        )

    async def _run_dafny_verification(
        self, proposal: EvolutionProposal,
    ) -> DafnyVerificationResult:
        """Run Dafny Clover loop for the proposal."""
        from ecodiaos.systems.simula.verification.templates import get_template
        from ecodiaos.systems.simula.verification.types import (
            DafnyVerificationResult,
            DafnyVerificationStatus,
        )

        assert self._dafny is not None
        assert self._llm is not None

        # Check Dafny availability
        if not await self._dafny.check_available():
            self._log.info("dafny_not_available_skipping")
            return DafnyVerificationResult(
                status=DafnyVerificationStatus.SKIPPED,
                error_summary="Dafny binary not available",
            )

        # Get template if available
        template = get_template(proposal.category.value)

        # Build context from the change spec
        python_source = ""
        function_name = ""
        context = proposal.description
        if proposal.change_spec:
            context = getattr(proposal.change_spec, "description", None) or proposal.description
            function_name = getattr(proposal.change_spec, "target_system", None) or ""

        return await self._dafny.run_clover_loop(
            llm=self._llm,
            python_source=python_source,
            function_name=function_name,
            context=context,
            template=template,
        )

    async def _run_z3_verification(
        self,
        proposal: EvolutionProposal,
        files_written: list[str],
    ) -> InvariantVerificationResult:
        """Run Z3 invariant discovery for the proposal."""
        from ecodiaos.systems.simula.verification.types import (
            InvariantVerificationResult,
            InvariantVerificationStatus,
        )

        assert self._z3 is not None
        assert self._llm is not None

        # Gather Python source from written files for invariant discovery
        python_source_parts: list[str] = []
        target_functions: list[str] = []
        for filepath in files_written:
            if not filepath.endswith(".py"):
                continue
            full = self._root / filepath
            if full.is_file():
                try:
                    content = full.read_text(encoding="utf-8")
                    python_source_parts.append(
                        f"# --- {filepath} ---\n{content}"
                    )
                    # Extract function names for targeting
                    import ast as _ast
                    try:
                        tree = _ast.parse(content)
                        for node in _ast.walk(tree):
                            if isinstance(node, (_ast.FunctionDef, _ast.AsyncFunctionDef)):
                                target_functions.append(node.name)
                    except SyntaxError:
                        pass
                except Exception:
                    continue

        if not python_source_parts:
            return InvariantVerificationResult(
                status=InvariantVerificationStatus.SKIPPED,
                error_summary="No Python source to analyze",
            )

        python_source = "\n\n".join(python_source_parts)
        domain_context = proposal.description
        if proposal.change_spec:
            domain_context = getattr(proposal.change_spec, "description", None) or proposal.description

        return await self._z3.run_discovery_loop(
            llm=self._llm,
            python_source=python_source,
            target_functions=target_functions[:10],  # Limit to top 10
            domain_context=domain_context,
        )

    # ── Stage 4A: Lean 4 Proof Verification Phase ─────────────────────────────

    async def _run_lean_verification(
        self,
        files_written: list[str],
        proposal: EvolutionProposal | None,
    ) -> LeanVerificationResult | None:
        """
        Run Lean 4 proof generation for proposals in proof-eligible categories.

        Returns None if Lean bridge is not configured or proposal is not eligible.
        Returns LeanVerificationResult with proof status and discovered lemmas.
        """
        if self._lean is None or self._llm is None or proposal is None:
            return None

        # Only run Lean proofs for categories that warrant formal proof
        if proposal.category not in LEAN_PROOF_CATEGORIES:
            return None

        # Check Lean 4 availability
        if not await self._lean.check_available():
            self._log.info("lean_not_available_skipping")
            from ecodiaos.systems.simula.verification.types import LeanProofStatus
            return LeanVerificationResult(
                status=LeanProofStatus.SKIPPED,
            )

        # Build proof context from the proposal
        python_source_parts: list[str] = []
        for filepath in files_written:
            if not filepath.endswith(".py"):
                continue
            full = self._root / filepath
            if full.is_file():
                try:
                    content = full.read_text(encoding="utf-8")
                    python_source_parts.append(
                        f"# --- {filepath} ---\n{content}"
                    )
                except Exception:
                    continue

        python_source = "\n\n".join(python_source_parts)
        domain_context = proposal.description
        if proposal.change_spec:
            domain_context = proposal.change_spec.additional_context or proposal.description

        function_name = ""
        if proposal.change_spec:
            function_name = getattr(proposal.change_spec, "target_system", "") or ""

        try:
            result = await self._lean.generate_proof(
                llm=self._llm,
                python_source=python_source,
                function_name=function_name,
                property_description=domain_context,
                proposal_id=proposal.id,
            )
            return result
        except TimeoutError:
            self._log.warning("lean_verification_timeout", proposal_id=proposal.id)
            from ecodiaos.systems.simula.verification.types import LeanProofStatus
            return LeanVerificationResult(
                status=LeanProofStatus.TIMEOUT,
            )
        except Exception as exc:
            self._log.warning("lean_verification_error", error=str(exc))
            return None

    # ── Stage 6: Formal Guarantees Phase ─────────────────────────────────────

    async def _run_formal_guarantees(
        self,
        files_written: list[str],
        proposal: EvolutionProposal | None,
    ) -> FormalGuaranteesResult | None:
        """
        Run Stage 6D (e-graph equivalence) and 6E (symbolic execution) checks.

        Returns None if no Stage 6 subsystems are configured.
        Returns a FormalGuaranteesResult with pass/fail and issues.
        """
        from ecodiaos.systems.simula.verification.types import FormalGuaranteesResult

        if self._egraph is None and self._symbolic_execution is None:
            return None

        start = time.monotonic()
        blocking_issues: list[str] = []
        advisory_issues: list[str] = []
        egraph_result = None
        symbolic_result = None

        # Build parallel tasks
        tasks: dict[str, asyncio.Task[object]] = {}

        # 6D: E-graph equivalence — check if rewritten code is semantically equivalent
        if self._egraph is not None and proposal is not None:
            tasks["egraph"] = asyncio.create_task(
                self._run_egraph_check(files_written, proposal),
            )

        # 6E: Symbolic execution — prove mission-critical properties
        if self._symbolic_execution is not None:
            tasks["symbolic"] = asyncio.create_task(
                self._run_symbolic_execution(files_written),
            )

        if not tasks:
            return None

        # Await all tasks
        results = await asyncio.gather(
            *tasks.values(), return_exceptions=True,
        )
        task_results = dict(zip(tasks.keys(), results, strict=False))

        # Process e-graph result
        if "egraph" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                EGraphEquivalenceResult,
                EGraphStatus,
            )

            raw = task_results["egraph"]
            if isinstance(raw, EGraphEquivalenceResult):
                egraph_result = raw
                if raw.status == EGraphStatus.FAILED:
                    msg = "E-graph equivalence check failed: code is not semantically equivalent"
                    if self._egraph_blocking:
                        blocking_issues.append(msg)
                    else:
                        advisory_issues.append(msg)
                elif raw.status == EGraphStatus.TIMEOUT:
                    advisory_issues.append("E-graph equivalence check timed out")
                elif raw.semantically_equivalent:
                    advisory_issues.append(
                        f"E-graph confirmed semantic equivalence ({len(raw.rules_applied)} rules, "
                        f"{raw.iterations} iterations)"
                    )
            elif isinstance(raw, Exception):
                self._log.warning("egraph_exception", error=str(raw))

        # Process symbolic execution result
        if "symbolic" in task_results:
            from ecodiaos.systems.simula.verification.types import (
                SymbolicExecutionResult,
            )

            raw = task_results["symbolic"]
            if isinstance(raw, SymbolicExecutionResult):
                symbolic_result = raw
                if raw.counterexamples:
                    msg = (
                        f"Symbolic execution found {len(raw.counterexamples)} counterexample(s) — "
                        f"mission-critical properties violated"
                    )
                    if self._symbolic_execution_blocking:
                        blocking_issues.append(msg)
                    else:
                        advisory_issues.append(msg)
                if raw.properties_proved > 0:
                    advisory_issues.append(
                        f"Symbolic execution proved {raw.properties_proved}/{raw.properties_checked} properties"
                    )
            elif isinstance(raw, Exception):
                self._log.warning("symbolic_execution_exception", error=str(raw))

        passed = len(blocking_issues) == 0
        total_time_ms = int((time.monotonic() - start) * 1000)

        return FormalGuaranteesResult(
            egraph=egraph_result,
            symbolic_execution=symbolic_result,
            passed=passed,
            blocking_issues=blocking_issues,
            advisory_issues=advisory_issues,
            total_duration_ms=total_time_ms,
        )

    async def _run_egraph_check(
        self,
        files_written: list[str],
        proposal: EvolutionProposal,
    ) -> object | None:
        """
        Run e-graph equivalence check on changed files.

        Compares original code (from rollback snapshot) with new code
        to verify semantic equivalence of the transformation.
        """
        from ecodiaos.systems.simula.verification.types import (
            EGraphEquivalenceResult,
            EGraphStatus,
        )

        assert self._egraph is not None

        # For each Python file, check if the rewrite preserved semantics
        # We focus on the first file that has a meaningful diff
        for filepath in files_written:
            if not filepath.endswith(".py"):
                continue
            full = self._root / filepath
            if not full.is_file():
                continue
            try:
                new_code = full.read_text(encoding="utf-8")
                # E-graph checks the code against itself (simplified form)
                # In production this would compare pre/post-apply snapshots
                result = await self._egraph.check_equivalence(new_code, new_code)
                return result
            except Exception as exc:
                self._log.warning("egraph_file_check_error", file=filepath, error=str(exc))
                continue

        return EGraphEquivalenceResult(
            status=EGraphStatus.SKIPPED,
        )

    async def _run_symbolic_execution(
        self,
        files_written: list[str],
    ) -> object | None:
        """
        Run symbolic execution on mission-critical functions in changed files.
        """
        from ecodiaos.systems.simula.verification.types import (
            SymbolicDomain,
            SymbolicExecutionResult,
            SymbolicExecutionStatus,
        )

        assert self._symbolic_execution is not None

        # Convert domain strings to enum values
        domains: list[SymbolicDomain] = []
        for d in self._symbolic_execution_domains:
            with contextlib.suppress(ValueError):
                domains.append(SymbolicDomain(d))

        if not domains:
            return SymbolicExecutionResult(
                status=SymbolicExecutionStatus.SKIPPED,
            )

        try:
            return await self._symbolic_execution.prove_properties(
                files=files_written,
                codebase_root=self._root,
                domains=domains,
            )
        except TimeoutError:
            self._log.warning("symbolic_execution_timeout")
            return SymbolicExecutionResult(
                status=SymbolicExecutionStatus.TIMEOUT,
            )
        except Exception as exc:
            self._log.warning("symbolic_execution_error", error=str(exc))
            return None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\history.py =====

"""
EcodiaOS -- Simula Evolution History

Maintains the complete, immutable record of all structural changes
applied to this EOS instance. Records are (:EvolutionRecord) nodes
in the Memory graph, linked as:
  (:ConfigVersion)-[:EVOLVED_FROM]->(:ConfigVersion)

Stage 1B enhancement: EvolutionRecord nodes store description embeddings
via voyage-code-3 in a Neo4j vector index for semantic similarity search
across the full evolution history.

The Honesty drive demands this record be permanent and complete.
No record can be deleted or modified after writing.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.simula.types import ConfigVersion, EvolutionRecord

if TYPE_CHECKING:
    from ecodiaos.clients.embedding import EmbeddingClient
    from ecodiaos.clients.neo4j import Neo4jClient

logger = structlog.get_logger().bind(system="simula.history")

# Voyage-code-3 produces 1024-dimensional embeddings
_EMBEDDING_DIMENSION = 1024
_VECTOR_INDEX_NAME = "evolution_record_embedding"


class EvolutionHistoryManager:
    """
    Writes and queries the immutable evolution history stored in Neo4j.
    Every applied structural change produces exactly one EvolutionRecord node.
    Records are never deleted or updated.

    With an embedding client configured, each record's description is embedded
    and stored in a Neo4j vector index for semantic similarity search.
    """

    def __init__(
        self,
        neo4j: Neo4jClient,
        embedding_client: EmbeddingClient | None = None,
    ) -> None:
        self._neo4j = neo4j
        self._embedding = embedding_client
        self._log = logger
        self._vector_index_ensured = False

    async def ensure_vector_index(self) -> None:
        """
        Create the Neo4j vector index on EvolutionRecord.embedding if it
        doesn't already exist. Idempotent — safe to call multiple times.

        Requires Neo4j 5.11+ with vector index support.
        """
        if self._vector_index_ensured:
            return

        try:
            await self._neo4j.execute_write(
                f"""
                CREATE VECTOR INDEX {_VECTOR_INDEX_NAME} IF NOT EXISTS
                FOR (r:EvolutionRecord)
                ON (r.embedding)
                OPTIONS {{
                    indexConfig: {{
                        `vector.dimensions`: {_EMBEDDING_DIMENSION},
                        `vector.similarity_function`: 'cosine'
                    }}
                }}
                """
            )
            self._vector_index_ensured = True
            self._log.info(
                "vector_index_ensured",
                index_name=_VECTOR_INDEX_NAME,
                dimension=_EMBEDDING_DIMENSION,
            )
        except Exception as exc:
            # Non-fatal: vector search degrades gracefully without the index
            self._log.warning(
                "vector_index_creation_failed",
                error=str(exc),
                hint="Neo4j 5.11+ required for vector indexes",
            )

    async def record(self, record: EvolutionRecord) -> None:
        """
        Write an immutable EvolutionRecord node to Neo4j.
        This is the permanent history of every structural change.

        If an embedding client is configured, the description is embedded
        and stored alongside the record for vector similarity search.
        """
        # Embed the description for vector search
        embedding: list[float] | None = None
        if self._embedding is not None:
            try:
                await self.ensure_vector_index()
                embedding = await self._embedding.embed(record.description)
            except Exception as exc:
                self._log.warning(
                    "embedding_generation_failed",
                    record_id=record.id,
                    error=str(exc),
                )

        # Build the CREATE query — include embedding property if available
        if embedding is not None:
            query = """
            CREATE (:EvolutionRecord {
                id: $id,
                proposal_id: $proposal_id,
                category: $category,
                description: $description,
                from_version: $from_version,
                to_version: $to_version,
                files_changed: $files_changed,
                simulation_risk: $simulation_risk,
                applied_at: $applied_at,
                rolled_back: $rolled_back,
                rollback_reason: $rollback_reason,
                simulation_episodes_tested: $simulation_episodes_tested,
                counterfactual_regression_rate: $counterfactual_regression_rate,
                dependency_blast_radius: $dependency_blast_radius,
                constitutional_alignment: $constitutional_alignment,
                resource_tokens_per_hour: $resource_tokens_per_hour,
                caution_reasoning: $caution_reasoning,
                created_at: $created_at,
                embedding: $embedding
            })
            """
        else:
            query = """
            CREATE (:EvolutionRecord {
                id: $id,
                proposal_id: $proposal_id,
                category: $category,
                description: $description,
                from_version: $from_version,
                to_version: $to_version,
                files_changed: $files_changed,
                simulation_risk: $simulation_risk,
                applied_at: $applied_at,
                rolled_back: $rolled_back,
                rollback_reason: $rollback_reason,
                simulation_episodes_tested: $simulation_episodes_tested,
                counterfactual_regression_rate: $counterfactual_regression_rate,
                dependency_blast_radius: $dependency_blast_radius,
                constitutional_alignment: $constitutional_alignment,
                resource_tokens_per_hour: $resource_tokens_per_hour,
                caution_reasoning: $caution_reasoning,
                created_at: $created_at
            })
            """

        params: dict[str, Any] = {
            "id": record.id,
            "proposal_id": record.proposal_id,
            "category": record.category.value,
            "description": record.description,
            "from_version": record.from_version,
            "to_version": record.to_version,
            "files_changed": record.files_changed,
            "simulation_risk": record.simulation_risk.value,
            "applied_at": record.applied_at.isoformat(),
            "rolled_back": record.rolled_back,
            "rollback_reason": record.rollback_reason,
            "simulation_episodes_tested": record.simulation_episodes_tested,
            "counterfactual_regression_rate": record.counterfactual_regression_rate,
            "dependency_blast_radius": record.dependency_blast_radius,
            "constitutional_alignment": record.constitutional_alignment,
            "resource_tokens_per_hour": record.resource_tokens_per_hour,
            "caution_reasoning": record.caution_reasoning,
            "created_at": record.created_at.isoformat(),
        }
        if embedding is not None:
            params["embedding"] = embedding

        await self._neo4j.execute_write(query, params)
        self._log.info(
            "evolution_recorded",
            record_id=record.id,
            proposal_id=record.proposal_id,
            category=record.category.value,
            from_version=record.from_version,
            to_version=record.to_version,
            has_embedding=embedding is not None,
        )

    async def find_similar_records(
        self,
        description: str,
        top_k: int = 5,
        min_score: float = 0.6,
    ) -> list[tuple[EvolutionRecord, float]]:
        """
        Find EvolutionRecords semantically similar to a description.

        Uses the Neo4j vector index to perform approximate nearest-neighbor
        search on stored embeddings. Returns (record, score) pairs.
        """
        if self._embedding is None:
            return []

        try:
            from ecodiaos.clients.embedding import VoyageEmbeddingClient

            if isinstance(self._embedding, VoyageEmbeddingClient):
                query_vec = await self._embedding.embed_query(description)
            else:
                query_vec = await self._embedding.embed(description)
        except Exception as exc:
            self._log.warning("similar_records_embed_failed", error=str(exc))
            return []

        try:
            rows = await self._neo4j.execute_read(
                """
                CALL db.index.vector.queryNodes(
                    $index_name, $top_k, $query_vector
                )
                YIELD node, score
                WHERE score >= $min_score
                RETURN node, score
                ORDER BY score DESC
                """,
                {
                    "index_name": _VECTOR_INDEX_NAME,
                    "top_k": top_k,
                    "query_vector": query_vec,
                    "min_score": min_score,
                },
            )
        except Exception as exc:
            self._log.warning("vector_search_failed", error=str(exc))
            return []

        results: list[tuple[EvolutionRecord, float]] = []
        for row in rows:
            try:
                data = dict(row["node"])
                # Remove embedding from reconstruction (not part of the model)
                data.pop("embedding", None)
                record = EvolutionRecord(**data)
                results.append((record, float(row["score"])))
            except Exception as exc:
                self._log.warning("record_reconstruction_failed", error=str(exc))
                continue

        return results

    async def record_version(self, version: ConfigVersion, previous_version: int | None) -> None:
        """""""""
        Write a ConfigVersion node and optionally chain it to the previous version.
        """""""""
        await self._neo4j.execute_write(
            """
            MERGE (:ConfigVersion {
                version: $version,
                timestamp: $timestamp,
                proposal_ids: $proposal_ids,
                config_hash: $config_hash
            })
            """,
            {
                "version": version.version,
                "timestamp": version.timestamp.isoformat(),
                "proposal_ids": version.proposal_ids,
                "config_hash": version.config_hash,
            },
        )
        if previous_version is not None:
            await self._neo4j.execute_write(
                """
                MATCH (new:ConfigVersion {version: $new_v})
                MATCH (prev:ConfigVersion {version: $prev_v})
                MERGE (new)-[:EVOLVED_FROM]->(prev)
                """,
                {"new_v": version.version, "prev_v": previous_version},
            )
        self._log.info(
            "config_version_recorded",
            version=version.version,
            previous_version=previous_version,
        )

    async def get_history(self, limit: int = 50) -> list[EvolutionRecord]:
        """""""""
        Retrieve the most recent N evolution records, newest first.
        """""""""
        rows = await self._neo4j.execute_read(
            """
            MATCH (r:EvolutionRecord)
            RETURN r
            ORDER BY r.created_at DESC
            LIMIT $limit
            """,
            {"limit": limit},
        )
        records: list[EvolutionRecord] = []
        for row in rows:
            data = row["r"]
            records.append(EvolutionRecord(**data))
        return records

    async def get_version_chain(self) -> list[ConfigVersion]:
        """""""""
        Retrieve all ConfigVersion nodes ordered by version ASC.
        """""""""
        rows = await self._neo4j.execute_read(
            """
            MATCH (v:ConfigVersion)
            RETURN v
            ORDER BY v.version ASC
            """
        )
        versions: list[ConfigVersion] = []
        for row in rows:
            data = row["v"]
            versions.append(ConfigVersion(**data))
        return versions

    async def get_current_version(self) -> int:
        """""""""
        Return the highest config version number, or 0 if none exists.
        """""""""
        rows = await self._neo4j.execute_read(
            """
            MATCH (v:ConfigVersion)
            RETURN max(v.version) AS max_version
            """
        )
        if not rows or rows[0]["max_version"] is None:
            return 0
        return int(rows[0]["max_version"])

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\proposal_intelligence.py =====

"""
EcodiaOS -- Simula Proposal Intelligence

Smart proposal management: deduplication, prioritization, dependency
analysis, and cost estimation. Maximizes evolution quality per LLM
token by using cheap heuristics first and LLM only for ambiguous cases.

Key design:
  - Deduplication: 3-tier (exact prefix → category overlap → LLM similarity)
  - Prioritization: formula-based scoring, no LLM needed
  - Dependency analysis: rule-based ordering, no LLM needed
  - Cost estimation: heuristic lookup table, no LLM needed

Budget impact: Zero LLM tokens for normal operation.
LLM used only when >5 proposals need semantic deduplication (~300 tokens).
"""

from __future__ import annotations

import time
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.simula.types import (
    ChangeCategory,
    EvolutionProposal,
    ProposalCluster,
    ProposalPriority,
    ProposalStatus,
    RiskLevel,
)

if TYPE_CHECKING:
    from ecodiaos.clients.embedding import EmbeddingClient
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine

logger = structlog.get_logger().bind(system="simula.intelligence")

# Cost heuristics by category (0.0-1.0 scale)
_CATEGORY_COST: dict[ChangeCategory, float] = {
    ChangeCategory.ADJUST_BUDGET: 0.1,
    ChangeCategory.ADD_EXECUTOR: 0.4,
    ChangeCategory.ADD_INPUT_CHANNEL: 0.4,
    ChangeCategory.ADD_PATTERN_DETECTOR: 0.4,
    ChangeCategory.MODIFY_CONTRACT: 0.7,
    ChangeCategory.MODIFY_CYCLE_TIMING: 0.5,
    ChangeCategory.CHANGE_CONSOLIDATION: 0.6,
    ChangeCategory.ADD_SYSTEM_CAPABILITY: 0.9,
}

# Impact heuristics by category (0.0-1.0 scale)
_CATEGORY_IMPACT: dict[ChangeCategory, float] = {
    ChangeCategory.ADJUST_BUDGET: 0.3,
    ChangeCategory.ADD_EXECUTOR: 0.6,
    ChangeCategory.ADD_INPUT_CHANNEL: 0.7,
    ChangeCategory.ADD_PATTERN_DETECTOR: 0.5,
    ChangeCategory.MODIFY_CONTRACT: 0.8,
    ChangeCategory.MODIFY_CYCLE_TIMING: 0.4,
    ChangeCategory.CHANGE_CONSOLIDATION: 0.5,
    ChangeCategory.ADD_SYSTEM_CAPABILITY: 0.9,
}

# Minimum description prefix length for exact dedup matching
_DEDUP_PREFIX_LEN: int = 50

# Minimum proposals before triggering LLM-based dedup
_LLM_DEDUP_THRESHOLD: int = 5


class ProposalIntelligence:
    """
    Smart proposal management for Simula.

    Provides deduplication, prioritization, dependency analysis,
    and cost estimation — all optimized for minimal token usage.

    Stage 1B upgrade: Tier 3 dedup now uses voyage-code-3 embeddings
    for cosine similarity instead of LLM-based text comparison.
    This is both cheaper (no LLM tokens) and more precise.
    """

    # Cosine similarity threshold for embedding-based dedup
    _EMBEDDING_DEDUP_THRESHOLD: float = 0.85

    def __init__(
        self,
        llm: LLMProvider | None = None,
        analytics: EvolutionAnalyticsEngine | None = None,
        embedding_client: EmbeddingClient | None = None,
    ) -> None:
        self._llm = llm
        self._analytics = analytics
        self._embeddings = embedding_client
        self._log = logger
        # Dedup precision tracking (Stage 1B.5)
        self._dedup_stats = {
            "tier1_matches": 0,
            "tier2_matches": 0,
            "tier3_embedding_matches": 0,
            "tier3_llm_fallback_matches": 0,
            "embedding_dedup_calls": 0,
            "embedding_dedup_latency_ms": 0.0,
        }

    # ─── Prioritization ──────────────────────────────────────────────────────

    async def prioritize(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalPriority]:
        """
        Score and rank proposals by:
          priority = evidence_strength * expected_impact / max(0.1, risk * cost)

        Pure heuristic scoring — zero LLM tokens.
        Proposals with higher scores should be processed first.
        """
        priorities: list[ProposalPriority] = []

        for proposal in proposals:
            evidence_strength = self._compute_evidence_strength(proposal)
            expected_impact = _CATEGORY_IMPACT.get(proposal.category, 0.5)
            estimated_risk = self._compute_risk_estimate(proposal)
            estimated_cost = self.estimate_cost(proposal)

            # Priority formula
            denominator = max(0.1, estimated_risk * estimated_cost)
            score = (evidence_strength * expected_impact) / denominator

            # Boost for proposals already partially processed
            if proposal.status == ProposalStatus.APPROVED:
                score *= 1.5

            reasoning = (
                f"evidence={evidence_strength:.2f}, impact={expected_impact:.2f}, "
                f"risk={estimated_risk:.2f}, cost={estimated_cost:.2f}"
            )

            priorities.append(ProposalPriority(
                proposal_id=proposal.id,
                priority_score=round(score, 3),
                evidence_strength=round(evidence_strength, 3),
                expected_impact=round(expected_impact, 3),
                estimated_risk=round(estimated_risk, 3),
                estimated_cost=round(estimated_cost, 3),
                reasoning=reasoning,
            ))

        # Sort by score descending
        priorities.sort(key=lambda p: p.priority_score, reverse=True)

        self._log.info(
            "proposals_prioritized",
            count=len(priorities),
            top_score=priorities[0].priority_score if priorities else 0.0,
        )
        return priorities

    def _compute_evidence_strength(self, proposal: EvolutionProposal) -> float:
        """
        Compute evidence strength from the proposal's evidence list.
        More evidence items = stronger signal. Capped at 1.0.
        """
        count = len(proposal.evidence)
        if count == 0:
            return 0.2  # minimal evidence
        # Logarithmic scaling: 1 item = 0.3, 5 items = 0.7, 10+ items = 0.9+
        import math
        return min(1.0, 0.2 + 0.3 * math.log1p(count))

    def _compute_risk_estimate(self, proposal: EvolutionProposal) -> float:
        """
        Estimate risk from simulation results and analytics history.
        Returns 0.0-1.0 scale.
        """
        # If simulation has run, use its risk level
        if proposal.simulation is not None:
            risk_map = {
                RiskLevel.LOW: 0.15,
                RiskLevel.MODERATE: 0.4,
                RiskLevel.HIGH: 0.7,
                RiskLevel.UNACCEPTABLE: 1.0,
            }
            return risk_map.get(proposal.simulation.risk_level, 0.4)

        # Otherwise, use category-based heuristic
        # Higher-impact categories carry more risk
        return _CATEGORY_COST.get(proposal.category, 0.5)

    # ─── Deduplication ───────────────────────────────────────────────────────

    async def deduplicate(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalCluster]:
        """
        Detect semantically similar proposals in three tiers:
          Tier 1: Exact description prefix match (zero cost)
          Tier 2: Category + affected_systems overlap (zero cost)
          Tier 3: LLM similarity check (only if >5 proposals, ~300 tokens)

        Returns clusters where member proposals could be merged.
        """
        if len(proposals) < 2:
            return []

        clusters: list[ProposalCluster] = []
        clustered_ids: set[str] = set()

        # Tier 1: Exact description prefix match
        prefix_groups: dict[str, list[EvolutionProposal]] = {}
        for p in proposals:
            prefix = p.description[:_DEDUP_PREFIX_LEN].lower().strip()
            prefix_groups.setdefault(prefix, []).append(p)

        for prefix, group in prefix_groups.items():
            if len(group) < 2:
                continue
            rep = group[0]
            members = [p.id for p in group]
            clustered_ids.update(members)
            clusters.append(ProposalCluster(
                representative_id=rep.id,
                member_ids=members,
                similarity_scores=[1.0] * len(members),
                merge_recommendation=f"Identical prefix: '{prefix[:30]}...'",
            ))

        # Tier 2: Category + affected_systems overlap
        unclustered = [p for p in proposals if p.id not in clustered_ids]
        cat_system_groups: dict[str, list[EvolutionProposal]] = {}
        for p in unclustered:
            key = f"{p.category.value}::{','.join(sorted(p.change_spec.affected_systems))}"
            cat_system_groups.setdefault(key, []).append(p)

        for key, group in cat_system_groups.items():
            if len(group) < 2:
                continue
            rep = group[0]
            members = [p.id for p in group]
            clustered_ids.update(members)
            clusters.append(ProposalCluster(
                representative_id=rep.id,
                member_ids=members,
                similarity_scores=[0.7] * len(members),
                merge_recommendation=f"Same category and affected systems: {key}",
            ))

        # Tier 3: Embedding-based semantic similarity (preferred) or LLM fallback
        still_unclustered = [p for p in proposals if p.id not in clustered_ids]
        if len(still_unclustered) >= _LLM_DEDUP_THRESHOLD:
            if self._embeddings is not None:
                # Stage 1B: voyage-code-3 cosine similarity — cheaper and more precise
                embedding_clusters = await self._embedding_deduplicate(still_unclustered)
                clusters.extend(embedding_clusters)
            elif self._llm is not None:
                # Fallback: LLM-based semantic comparison (~300 tokens)
                llm_clusters = await self._llm_deduplicate(still_unclustered)
                clusters.extend(llm_clusters)

        if clusters:
            self._log.info(
                "dedup_complete",
                clusters=len(clusters),
                total_duplicates=sum(len(c.member_ids) for c in clusters),
                dedup_stats=self._dedup_stats,
            )
        return clusters

    async def _embedding_deduplicate(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalCluster]:
        """
        Stage 1B: Embedding-based semantic dedup using voyage-code-3.

        Embeds all proposal descriptions, then finds pairs with cosine
        similarity above the threshold. Groups them into clusters.

        Zero LLM tokens. Cost: ~0.001 per proposal via Voyage API.
        """
        from ecodiaos.clients.embedding import cosine_similarity

        assert self._embeddings is not None
        self._dedup_stats["embedding_dedup_calls"] += 1
        t_start = time.monotonic()

        # Build description texts for embedding
        texts = [
            f"{p.category.value}: {p.description[:200]}"
            for p in proposals[:20]  # Cap at 20 to control API cost
        ]

        try:
            import asyncio
            embeddings = await asyncio.wait_for(
                self._embeddings.embed_batch(texts),
                timeout=10.0,
            )
        except Exception as exc:
            self._log.warning("embedding_dedup_failed", error=str(exc))
            self._dedup_stats["embedding_dedup_latency_ms"] += (
                (time.monotonic() - t_start) * 1000
            )
            return []

        # Pairwise cosine similarity — find clusters above threshold
        clusters: list[ProposalCluster] = []
        clustered: set[int] = set()

        for i in range(len(embeddings)):
            if i in clustered:
                continue
            group = [i]
            for j in range(i + 1, len(embeddings)):
                if j in clustered:
                    continue
                sim = cosine_similarity(embeddings[i], embeddings[j])
                if sim >= self._EMBEDDING_DEDUP_THRESHOLD:
                    group.append(j)
                    clustered.add(j)

            if len(group) >= 2:
                clustered.add(i)
                member_ids = [proposals[idx].id for idx in group]
                similarities = []
                for idx in group:
                    if idx == group[0]:
                        similarities.append(1.0)
                    else:
                        sim = cosine_similarity(embeddings[group[0]], embeddings[idx])
                        similarities.append(round(sim, 3))

                clusters.append(ProposalCluster(
                    representative_id=member_ids[0],
                    member_ids=member_ids,
                    similarity_scores=similarities,
                    merge_recommendation=(
                        f"Embedding similarity ≥{self._EMBEDDING_DEDUP_THRESHOLD} "
                        f"(voyage-code-3)"
                    ),
                ))
                self._dedup_stats["tier3_embedding_matches"] += len(group)

        latency_ms = (time.monotonic() - t_start) * 1000
        self._dedup_stats["embedding_dedup_latency_ms"] += latency_ms

        self._log.info(
            "embedding_dedup_complete",
            proposals_checked=len(proposals),
            clusters_found=len(clusters),
            latency_ms=round(latency_ms, 1),
        )
        return clusters

    async def _llm_deduplicate(
        self, proposals: list[EvolutionProposal],
    ) -> list[ProposalCluster]:
        """LLM-based semantic similarity check (fallback). ~300 tokens."""
        descriptions = "\n".join(
            f"{i+1}. [{p.id[:8]}] {p.category.value}: {p.description[:100]}"
            for i, p in enumerate(proposals[:10])
        )

        prompt = (
            "Below are evolution proposals for an AI system. "
            "Identify any that are semantically similar enough to be duplicates.\n\n"
            f"{descriptions}\n\n"
            "Reply with groups of similar proposals by their numbers.\n"
            "Format: GROUP: 1, 3 (reason)\n"
            "If no duplicates found, reply: NONE"
        )

        try:
            import asyncio
            response = await asyncio.wait_for(
                self._llm.evaluate(prompt=prompt, max_tokens=200, temperature=0.1),  # type: ignore[union-attr]
                timeout=8.0,
            )

            clusters: list[ProposalCluster] = []
            for line in response.text.strip().splitlines():
                line = line.strip()
                if line.upper() == "NONE" or "GROUP" not in line.upper():
                    continue
                try:
                    _, nums_part = line.split(":", 1)
                    reason_start = nums_part.find("(")
                    if reason_start > 0:
                        reason = nums_part[reason_start:].strip("() ")
                        nums_part = nums_part[:reason_start]
                    else:
                        reason = ""

                    indices = [
                        int(n.strip()) - 1 for n in nums_part.split(",") if n.strip().isdigit()
                    ]
                    valid = [i for i in indices if 0 <= i < len(proposals)]
                    if len(valid) >= 2:
                        members = [proposals[i].id for i in valid]
                        clusters.append(ProposalCluster(
                            representative_id=members[0],
                            member_ids=members,
                            similarity_scores=[0.6] * len(members),
                            merge_recommendation=reason or "LLM-detected similarity",
                        ))
                        self._dedup_stats["tier3_llm_fallback_matches"] += len(valid)
                except (ValueError, IndexError):
                    continue

            return clusters
        except Exception as exc:
            self._log.warning("llm_dedup_failed", error=str(exc))
            return []

    # ─── Dependency Analysis ─────────────────────────────────────────────────

    async def analyze_dependencies(
        self, proposals: list[EvolutionProposal],
    ) -> list[tuple[str, str, str]]:
        """
        Detect ordering dependencies between proposals.
        Returns list of (before_id, after_id, reason) tuples.

        Rule-based analysis — zero LLM tokens:
        - ADD_EXECUTOR should come before MODIFY_CONTRACT referencing axon
        - ADD_INPUT_CHANNEL before MODIFY_CONTRACT referencing atune
        - ADJUST_BUDGET after the thing it's budgeting for is added
        - ADD_SYSTEM_CAPABILITY is a superset that depends on components
        """
        if len(proposals) < 2:
            return []

        dependencies: list[tuple[str, str, str]] = []

        # Build lookup
        additive = [p for p in proposals if p.category in {
            ChangeCategory.ADD_EXECUTOR,
            ChangeCategory.ADD_INPUT_CHANNEL,
            ChangeCategory.ADD_PATTERN_DETECTOR,
        }]
        contracts = [p for p in proposals if p.category == ChangeCategory.MODIFY_CONTRACT]
        capabilities = [p for p in proposals if p.category == ChangeCategory.ADD_SYSTEM_CAPABILITY]
        budgets = [p for p in proposals if p.category == ChangeCategory.ADJUST_BUDGET]

        # Additive changes should come before contract modifications
        # that reference the same system
        for add_p in additive:
            add_systems = set(add_p.change_spec.affected_systems)
            for contract_p in contracts:
                contract_systems = set(contract_p.change_spec.affected_systems)
                overlap = add_systems & contract_systems
                if overlap:
                    dependencies.append((
                        add_p.id,
                        contract_p.id,
                        f"Add {add_p.category.value} before modifying contracts for {overlap}",
                    ))

        # Additive changes should come before capability additions
        for add_p in additive:
            for cap_p in capabilities:
                cap_systems = set(cap_p.change_spec.affected_systems)
                add_systems = set(add_p.change_spec.affected_systems)
                if cap_systems & add_systems:
                    dependencies.append((
                        add_p.id,
                        cap_p.id,
                        "Add component before adding system capability",
                    ))

        # Budget changes should come after the thing they budget for
        for budget_p in budgets:
            param = budget_p.change_spec.budget_parameter or ""
            for add_p in additive:
                # If the budget parameter references the additive system
                add_systems_list = add_p.change_spec.affected_systems
                for sys in add_systems_list:
                    if sys in param:
                        dependencies.append((
                            add_p.id,
                            budget_p.id,
                            f"Add component before adjusting its budget ({param})",
                        ))

        if dependencies:
            self._log.info(
                "dependencies_detected",
                count=len(dependencies),
            )
        return dependencies

    # ─── Cost Estimation ─────────────────────────────────────────────────────

    def estimate_cost(self, proposal: EvolutionProposal) -> float:
        """
        Heuristic cost estimation (0.0-1.0 scale).
        Zero LLM tokens — pure lookup + adjustment.
        """
        base_cost = _CATEGORY_COST.get(proposal.category, 0.5)

        # Adjust for complexity signals
        spec = proposal.change_spec
        if spec.affected_systems and len(spec.affected_systems) > 1:
            base_cost = min(1.0, base_cost + 0.1 * (len(spec.affected_systems) - 1))

        if spec.contract_changes and len(spec.contract_changes) > 2:
            base_cost = min(1.0, base_cost + 0.1)

        return round(base_cost, 2)

    # ─── Duplicate Detection Helper ──────────────────────────────────────────

    def get_dedup_stats(self) -> dict[str, Any]:
        """Return dedup precision benchmarking stats (Stage 1B.5)."""
        return dict(self._dedup_stats)

    def is_duplicate(
        self,
        proposal: EvolutionProposal,
        clusters: list[ProposalCluster],
    ) -> bool:
        """
        Check if a proposal appears in any cluster as a non-representative member.
        If it's in a cluster but not the representative, it's a duplicate.
        """
        for cluster in clusters:
            if proposal.id in cluster.member_ids and proposal.id != cluster.representative_id:
                return True
        return False

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\rollback.py =====

"""
EcodiaOS -- Simula Rollback Manager

Before any change is applied, RollbackManager snapshots all files
that might be modified. If the post-apply health check fails -- or if
any exception occurs during application -- the manager restores the
codebase to its pre-change state.

Rollback target: <=2s (from spec).
"""

from __future__ import annotations

from pathlib import Path

import structlog

from ecodiaos.systems.simula.types import ConfigSnapshot, FileSnapshot

logger = structlog.get_logger().bind(system="simula.rollback")


class RollbackError(RuntimeError):
    """Raised when restoring files to their pre-change state fails."""
    pass


class RollbackManager:
    """
    Captures file snapshots before structural changes are applied
    and restores them if the post-apply health check fails.
    """

    def __init__(self, codebase_root: Path) -> None:
        self._root = codebase_root
        self._log = logger

    async def snapshot(self, proposal_id: str, paths: list[Path]) -> ConfigSnapshot:
        """""""""
        Read each file's current content and package into a ConfigSnapshot.
        Files that do not exist are recorded with existed=False so rollback
        knows to delete them rather than restore content.
        """""""""
        snapshots: list[FileSnapshot] = []
        for path in paths:
            abs_path = path if path.is_absolute() else self._root / path
            content = await self._read_file_safe(abs_path)
            existed = content is not None
            snapshots.append(
                FileSnapshot(
                    path=str(abs_path),
                    content=content,
                    existed=existed,
                )
            )
            self._log.debug(
                "snapshot_captured",
                path=str(abs_path),
                existed=existed,
                size=len(content) if content else 0,
            )

        # We need a config_version from outside context; use 0 as placeholder.
        # The service layer will update this before persisting.
        cfg_snapshot = ConfigSnapshot(
            proposal_id=proposal_id,
            files=snapshots,
            config_version=0,
        )
        self._log.info(
            "snapshot_complete",
            proposal_id=proposal_id,
            files_captured=len(snapshots),
        )
        return cfg_snapshot

    async def restore(self, snapshot: ConfigSnapshot) -> list[str]:
        """""""""
        Restore all files to the state captured in the snapshot.
        Files that did not exist before are deleted.
        Returns the list of absolute paths that were restored.
        Raises RollbackError if any restore fails.
        """""""""
        restored: list[str] = []
        errors: list[str] = []

        for file_snap in snapshot.files:
            path = Path(file_snap.path)
            try:
                if not file_snap.existed:
                    # File was created by the change -- delete it
                    if path.exists():
                        path.unlink()
                        self._log.info("rollback_deleted", path=str(path))
                elif file_snap.content is not None:
                    path.parent.mkdir(parents=True, exist_ok=True)
                    path.write_text(file_snap.content, encoding="utf-8")
                    self._log.info("rollback_restored", path=str(path))
                restored.append(str(path))
            except Exception as exc:
                msg = f"Failed to restore {path}: {exc}"
                self._log.error("rollback_restore_failed", path=str(path), error=str(exc))
                errors.append(msg)

        if errors:
            raise RollbackError("Rollback incomplete. Failures: " + str(errors))

        self._log.info(
            "rollback_complete",
            proposal_id=snapshot.proposal_id,
            files_restored=len(restored),
        )
        return restored

    async def _read_file_safe(self, path: Path) -> str | None:
        """""""""
        Read file content; return None if file does not exist.
        """""""""
        try:
            return path.read_text(encoding="utf-8")
        except FileNotFoundError:
            return None
        except Exception as exc:
            self._log.warning("snapshot_read_failed", path=str(path), error=str(exc))
            return None

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\service.py =====

"""
EcodiaOS — Simula Service

The self-evolution system. Simula is the organism's capacity for
metamorphosis: structural change beyond parameter tuning.

Where Evo adjusts the knobs, Simula redesigns the dashboard.

Simula coordinates the full evolution proposal pipeline:
  1. DEDUPLICATE — check for duplicate/similar active proposals
  2. VALIDATE    — reject forbidden categories immediately
  3. SIMULATE    — deep multi-strategy impact prediction
  4. GATE        — route governed changes through community governance
  5. APPLY       — invoke the code agent or config updater with rollback
  6. VERIFY      — health check post-application
  7. RECORD      — write immutable history, increment version, update analytics

Interfaces:
  initialize()            — build sub-systems, load current version
  process_proposal()      — main entry point for rich proposals
  receive_evo_proposal()  — receive from Evo via bridge translation
  get_history()           — recent evolution records
  get_current_version()   — current config version number
  get_analytics()         — evolution quality metrics
  shutdown()              — graceful teardown
  stats                   — service-level metrics

Iron Rules (never violated — see SIMULA_IRON_RULES in types.py):
  - Cannot modify Equor, constitutional drives, invariants
  - Cannot modify its own logic
  - Must simulate before applying any change
  - Must maintain rollback capability
  - Evolution history is immutable
"""

from __future__ import annotations

import asyncio
import contextlib
import hashlib
from pathlib import Path
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.clients.embedding import EmbeddingClient, create_voyage_client
from ecodiaos.clients.llm import LLMProvider, create_thinking_provider
from ecodiaos.primitives.common import new_id, utc_now
from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine
from ecodiaos.systems.simula.applicator import ChangeApplicator
from ecodiaos.systems.simula.bridge import EvoSimulaBridge
from ecodiaos.systems.simula.code_agent import SimulaCodeAgent
from ecodiaos.systems.simula.health import HealthChecker
from ecodiaos.systems.simula.history import EvolutionHistoryManager
from ecodiaos.systems.simula.learning.grpo import GRPOTrainingEngine
from ecodiaos.systems.simula.learning.lilo import LiloLibraryEngine
from ecodiaos.systems.simula.proposal_intelligence import ProposalIntelligence
from ecodiaos.systems.simula.retrieval.swe_grep import SweGrepRetriever
from ecodiaos.systems.simula.rollback import RollbackManager
from ecodiaos.systems.simula.simulation import ChangeSimulator
from ecodiaos.systems.simula.types import (
    FORBIDDEN,
    GOVERNANCE_REQUIRED,
    ConfigVersion,
    EnrichedSimulationResult,
    EvolutionAnalytics,
    EvolutionProposal,
    EvolutionRecord,
    ProposalResult,
    ProposalStatus,
    RiskLevel,
    SimulationResult,
    TriageResult,
    TriageStatus,
)
from ecodiaos.systems.simula.verification.incremental import IncrementalVerificationEngine

if TYPE_CHECKING:
    from ecodiaos.clients.neo4j import Neo4jClient
    from ecodiaos.clients.redis import RedisClient
    from ecodiaos.clients.timescaledb import TimescaleDBClient
    from ecodiaos.config import SimulaConfig
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.simula.hunter.analytics import HunterAnalyticsEmitter
    from ecodiaos.systems.simula.hunter.service import HunterService
    from ecodiaos.systems.simula.hunter.types import HuntResult

logger = structlog.get_logger()


class SimulaService:
    """
    Simula — the EOS self-evolution system.

    Coordinates eight sub-systems:
      ChangeSimulator           — deep multi-strategy impact prediction
      SimulaCodeAgent           — Claude-backed code generation with 11 tools
      ChangeApplicator          — routes proposals to the right application strategy
      RollbackManager           — file snapshots and restore
      EvolutionHistoryManager   — immutable Neo4j history
      EvoSimulaBridge           — Evo→Simula proposal translation
      ProposalIntelligence      — deduplication, prioritization, dependency analysis
      EvolutionAnalyticsEngine  — evolution quality tracking
    """

    system_id: str = "simula"

    def __init__(
        self,
        config: SimulaConfig,
        llm: LLMProvider,
        neo4j: Neo4jClient | None = None,
        memory: MemoryService | None = None,
        codebase_root: Path | None = None,
        instance_name: str = "EOS",
        tsdb: TimescaleDBClient | None = None,
        redis: RedisClient | None = None,
    ) -> None:
        self._config = config
        self._llm = llm
        self._neo4j = neo4j
        self._memory = memory
        self._root = codebase_root or Path(config.codebase_root).resolve()
        self._instance_name = instance_name
        self._tsdb = tsdb
        self._redis = redis
        self._initialized: bool = False
        self._logger = logger.bind(system="simula")

        # Sub-systems (built in initialize())
        self._simulator: ChangeSimulator | None = None
        self._code_agent: SimulaCodeAgent | None = None
        self._applicator: ChangeApplicator | None = None
        self._rollback: RollbackManager | None = None
        self._history: EvolutionHistoryManager | None = None
        self._health: HealthChecker | None = None
        self._bridge: EvoSimulaBridge | None = None
        self._intelligence: ProposalIntelligence | None = None
        self._analytics: EvolutionAnalyticsEngine | None = None

        # Stage 3 sub-systems
        self._incremental: IncrementalVerificationEngine | None = None
        self._swe_grep: SweGrepRetriever | None = None
        self._lilo: LiloLibraryEngine | None = None

        # Stage 4 sub-systems
        self._lean_bridge: object | None = None  # LeanBridge (lazy import)
        self._grpo: GRPOTrainingEngine | None = None
        self._diffusion_repair: object | None = None  # DiffusionRepairAgent (lazy import)

        # Stage 5 sub-systems
        self._synthesis: object | None = None  # SynthesisStrategySelector (lazy import)
        self._repair_agent: object | None = None  # RepairAgent (lazy import)
        self._orchestrator: object | None = None  # MultiAgentOrchestrator (lazy import)
        self._causal_debugger: object | None = None  # CausalDebugger (lazy import)
        self._issue_resolver: object | None = None  # IssueResolver (lazy import)

        # Stage 6 sub-systems
        self._hash_chain: object | None = None  # HashChainManager (lazy import)
        self._content_credentials: object | None = None  # ContentCredentialManager (lazy import)
        self._governance_credentials: object | None = None  # GovernanceCredentialManager (lazy import)
        self._hard_negative_miner: object | None = None  # HardNegativeMiner (lazy import)
        self._adversarial_tester: object | None = None  # AdversarialTestGenerator (lazy import)
        self._formal_spec_generator: object | None = None  # FormalSpecGenerator (lazy import)
        self._egraph: object | None = None  # EqualitySaturationEngine (lazy import)
        self._symbolic_execution: object | None = None  # SymbolicExecutionEngine (lazy import)

        # Stage 7 sub-systems (Hunter — lazy runtime imports in initialize())
        self._hunter: HunterService | None = None
        self._hunter_analytics: HunterAnalyticsEmitter | None = None

        # State
        self._current_version: int = 0
        self._version_lock: asyncio.Lock = asyncio.Lock()
        self._active_proposals: dict[str, EvolutionProposal] = {}
        self._proposals_lock: asyncio.Lock = asyncio.Lock()

        # Metrics
        self._proposals_received: int = 0
        self._proposals_approved: int = 0
        self._proposals_rejected: int = 0
        self._proposals_rolled_back: int = 0
        self._proposals_awaiting_governance: int = 0
        self._proposals_deduplicated: int = 0
        self._proposals_applied_since_consolidation: int = 0

    # ─── Lifecycle ─────────────────────────────────────────────────────────────

    async def initialize(self) -> None:
        """
        Build all sub-systems and load current config version from history.
        Must be called before any other method.
        """
        if self._initialized:
            return

        # Build the rollback manager
        self._rollback = RollbackManager(codebase_root=self._root)

        # ── Stage 2: Verification bridges ─────────────────────────────────────
        from ecodiaos.systems.simula.verification.dafny_bridge import DafnyBridge
        from ecodiaos.systems.simula.verification.static_analysis import StaticAnalysisBridge
        from ecodiaos.systems.simula.verification.z3_bridge import Z3Bridge

        dafny_bridge: DafnyBridge | None = None
        if self._config.dafny_enabled:
            dafny_bridge = DafnyBridge(
                dafny_path=self._config.dafny_binary_path,
                verify_timeout_s=self._config.dafny_verify_timeout_s,
                max_rounds=self._config.dafny_max_clover_rounds,
            )
            self._logger.info("dafny_bridge_initialized")

        z3_bridge: Z3Bridge | None = None
        if self._config.z3_enabled:
            z3_bridge = Z3Bridge(
                check_timeout_ms=self._config.z3_check_timeout_ms,
                max_rounds=self._config.z3_max_discovery_rounds,
            )
            self._logger.info("z3_bridge_initialized")

        static_bridge: StaticAnalysisBridge | None = None
        if self._config.static_analysis_enabled:
            static_bridge = StaticAnalysisBridge(
                codebase_root=self._root,
            )
            self._logger.info("static_analysis_bridge_initialized")

        # Store for AgentCoder pipeline
        self._dafny_bridge = dafny_bridge
        self._z3_bridge = z3_bridge
        self._static_bridge = static_bridge

        # Build the health checker with Stage 2 bridges + Stage 3 Z3 blocking
        self._health = HealthChecker(
            codebase_root=self._root,
            test_command=self._config.test_command,
            dafny_bridge=dafny_bridge,
            z3_bridge=z3_bridge,
            static_analysis_bridge=static_bridge,
            llm=self._llm,
            z3_blocking=self._config.z3_blocking,
        )

        # ── Stage 1A: Extended-thinking provider for governance/high-risk ────
        thinking_provider = None
        if self._config.thinking_model_api_key:
            try:
                thinking_provider = create_thinking_provider(
                    api_key=self._config.thinking_model_api_key,
                    model=self._config.thinking_model,
                    provider=self._config.thinking_model_provider,
                    reasoning_budget=self._config.thinking_budget_tokens,
                )
                self._logger.info(
                    "thinking_provider_initialized",
                    model=self._config.thinking_model,
                    provider=self._config.thinking_model_provider,
                )
            except Exception as exc:
                self._logger.warning("thinking_provider_init_failed", error=str(exc))

        # ── Stage 1B: Voyage-code-3 embedding client ────────────────────────
        embedding_client: EmbeddingClient | None = None
        if self._config.embedding_api_key:
            try:
                embedding_client = create_voyage_client(
                    api_key=self._config.embedding_api_key,
                    model=self._config.embedding_model,
                )
                self._logger.info(
                    "embedding_client_initialized",
                    model=self._config.embedding_model,
                )
            except Exception as exc:
                self._logger.warning("embedding_client_init_failed", error=str(exc))

        # Store for shutdown cleanup
        self._embedding_client = embedding_client

        # Build the code agent with Stage 1 + 2 enhancements
        code_agent_llm = self._llm
        self._code_agent = SimulaCodeAgent(
            llm=code_agent_llm,
            codebase_root=self._root,
            max_turns=self._config.max_code_agent_turns,
            thinking_provider=thinking_provider,
            thinking_budget_tokens=self._config.thinking_budget_tokens,
            embedding_client=embedding_client,
            kv_compression_ratio=self._config.kv_compression_ratio,
            kv_compression_enabled=self._config.kv_compression_enabled,
            # Stage 2C: static analysis post-generation gate
            static_analysis_bridge=static_bridge,
            static_analysis_max_fix_iterations=self._config.static_analysis_max_fix_iterations,
        )

        # ── Stage 2D: AgentCoder pipeline agents ──────────────────────────────
        from ecodiaos.systems.simula.agents.test_designer import TestDesignerAgent
        from ecodiaos.systems.simula.agents.test_executor import TestExecutorAgent

        test_designer: TestDesignerAgent | None = None
        test_executor: TestExecutorAgent | None = None
        if self._config.agent_coder_enabled:
            test_designer = TestDesignerAgent(
                llm=self._llm,
                codebase_root=self._root,
            )
            test_executor = TestExecutorAgent(
                codebase_root=self._root,
                test_timeout_s=self._config.agent_coder_test_timeout_s,
            )
            self._logger.info("agent_coder_pipeline_initialized")

        # Build the applicator with Stage 2D pipeline
        self._applicator = ChangeApplicator(
            code_agent=self._code_agent,
            rollback_manager=self._rollback,
            health_checker=self._health,
            codebase_root=self._root,
            test_designer=test_designer,
            test_executor=test_executor,
            static_analysis_bridge=static_bridge,
            agent_coder_enabled=self._config.agent_coder_enabled,
            agent_coder_max_iterations=self._config.agent_coder_max_iterations,
        )

        # Build the history manager (requires Neo4j) with Stage 1B embedding support
        if self._neo4j is None:
            raise RuntimeError(
                "Simula requires a Neo4j client for evolution history, analytics, "
                "governance, and learning. Either supply a Neo4jClient or disable Simula."
            )
        self._history = EvolutionHistoryManager(
            neo4j=self._neo4j,
            embedding_client=embedding_client,
        )
        self._current_version = await self._history.get_current_version()

        # Build the analytics engine (depends on history)
        self._analytics = EvolutionAnalyticsEngine(history=self._history)

        # Build the deep simulator (depends on analytics for dynamic caution)
        self._simulator = ChangeSimulator(
            config=self._config,
            llm=self._llm,
            memory=self._memory,
            analytics=self._analytics,
            codebase_root=self._root,
        )

        # Build the Evo↔Simula bridge
        self._bridge = EvoSimulaBridge(
            llm=self._llm,
            memory=self._memory,
        )

        # Build the proposal intelligence layer with Stage 1B embedding dedup
        self._intelligence = ProposalIntelligence(
            llm=self._llm,
            analytics=self._analytics,
            embedding_client=embedding_client,
        )

        # ── Stage 3A: Incremental verification ─────────────────────────────────
        if self._config.incremental_verification_enabled:
            self._incremental = IncrementalVerificationEngine(
                codebase_root=self._root,
                redis=self._redis,
                neo4j=self._neo4j,
                hot_ttl_seconds=self._config.incremental_hot_ttl_seconds,
            )
            self._logger.info("incremental_verification_initialized")

        # ── Stage 3B: SWE-grep retrieval ──────────────────────────────────────
        if self._config.swe_grep_enabled:
            self._swe_grep = SweGrepRetriever(
                codebase_root=self._root,
                llm=self._llm,
                max_hops=self._config.swe_grep_max_hops,
            )
            self._logger.info("swe_grep_retriever_initialized")

        # ── Stage 3C: LILO library learning ───────────────────────────────────
        if self._config.lilo_enabled:
            self._lilo = LiloLibraryEngine(
                neo4j=self._neo4j,
                llm=self._llm,
                codebase_root=self._root,
            )
            self._logger.info("lilo_library_initialized")

        # ── Stage 4A: Lean 4 proof generation ────────────────────────────────
        lean_bridge_instance = None
        if self._config.lean_enabled:
            from ecodiaos.systems.simula.verification.lean_bridge import LeanBridge

            lean_bridge_instance = LeanBridge(
                lean_path=self._config.lean_binary_path,
                project_path=self._config.lean_project_path or "",
                verify_timeout_s=self._config.lean_verify_timeout_s,
                max_attempts=self._config.lean_max_attempts,
                copilot_enabled=self._config.lean_copilot_enabled,
                dojo_enabled=self._config.lean_dojo_enabled,
                max_library_size=self._config.lean_proof_library_max_size,
                neo4j=self._neo4j,
            )
            self._lean_bridge = lean_bridge_instance
            self._logger.info("lean_bridge_initialized")

        # Wire Lean bridge into health checker
        if lean_bridge_instance is not None and self._health is not None:
            self._health._lean = lean_bridge_instance
            self._health._lean_blocking = self._config.lean_blocking

        # ── Stage 4B: GRPO domain fine-tuning ─────────────────────────────────
        if self._config.grpo_enabled:
            self._grpo = GRPOTrainingEngine(
                config=self._config,
                neo4j=self._neo4j,
            )
            self._logger.info("grpo_engine_initialized")

        # ── Stage 4C: Diffusion-based code repair ────────────────────────────
        if self._config.diffusion_repair_enabled:
            from ecodiaos.systems.simula.agents.diffusion_repair import DiffusionRepairAgent

            self._diffusion_repair = DiffusionRepairAgent(
                llm=self._llm,
                codebase_root=self._root,
                max_denoise_steps=self._config.diffusion_max_denoise_steps,
                timeout_s=self._config.diffusion_timeout_s,
                sketch_first=self._config.diffusion_sketch_first,
            )
            self._logger.info("diffusion_repair_agent_initialized")

        # ── Stage 5A: Neurosymbolic synthesis ────────────────────────────────
        if self._config.synthesis_enabled:
            from ecodiaos.systems.simula.synthesis.chopchop import ChopChopEngine
            from ecodiaos.systems.simula.synthesis.hysynth import HySynthEngine
            from ecodiaos.systems.simula.synthesis.sketch_solver import SketchSolver
            from ecodiaos.systems.simula.synthesis.strategy_selector import (
                SynthesisStrategySelector,
            )

            hysynth = HySynthEngine(
                llm=self._llm,
                codebase_root=self._root,
                max_candidates=self._config.hysynth_max_candidates,
                beam_width=self._config.hysynth_beam_width,
                timeout_s=self._config.hysynth_timeout_s,
            )
            sketch = SketchSolver(
                llm=self._llm,
                z3_bridge=z3_bridge,
                max_holes=self._config.sketch_max_holes,
                solver_timeout_ms=self._config.sketch_solver_timeout_ms,
            )
            chopchop = ChopChopEngine(
                llm=self._llm,
                codebase_root=self._root,
                max_retries_per_chunk=self._config.chopchop_max_retries,
                chunk_size_lines=self._config.chopchop_chunk_size_lines,
                timeout_s=self._config.chopchop_timeout_s,
            )
            self._synthesis = SynthesisStrategySelector(
                hysynth=hysynth,
                sketch_solver=sketch,
                chopchop=chopchop,
                codebase_root=self._root,
            )
            self._logger.info("synthesis_subsystem_initialized")

        # ── Stage 5B: Neural program repair ──────────────────────────────────
        if self._config.repair_agent_enabled:
            from ecodiaos.systems.simula.agents.repair_agent import RepairAgent

            self._repair_agent = RepairAgent(
                reasoning_llm=self._llm,
                code_llm=self._llm,
                codebase_root=self._root,
                neo4j=self._neo4j,
                max_retries=self._config.repair_max_retries,
                cost_budget_usd=self._config.repair_cost_budget_usd,
                timeout_s=self._config.repair_timeout_s,
                use_similar_fixes=self._config.repair_use_similar_fixes,
            )
            self._logger.info("repair_agent_initialized")

        # ── Stage 5C: Multi-agent orchestration ─────────────────────────────
        if self._config.orchestration_enabled and self._code_agent is not None:
            from ecodiaos.systems.simula.orchestration.orchestrator import MultiAgentOrchestrator
            from ecodiaos.systems.simula.orchestration.task_planner import TaskPlanner

            task_planner = TaskPlanner(
                codebase_root=self._root,
                llm=self._llm,
                max_dag_nodes=self._config.orchestration_max_dag_nodes,
            )
            self._orchestrator = MultiAgentOrchestrator(
                llm=self._llm,
                codebase_root=self._root,
                code_agent=self._code_agent,
                task_planner=task_planner,
                max_agents_per_stage=self._config.orchestration_max_agents_per_stage,
                timeout_s=self._config.orchestration_timeout_s,
            )
            self._logger.info("orchestrator_initialized")

        # ── Stage 5D: Causal debugging ───────────────────────────────────────
        if self._config.causal_debugging_enabled:
            from ecodiaos.systems.simula.debugging.causal_dag import CausalDebugger

            self._causal_debugger = CausalDebugger(
                llm=self._llm,
                codebase_root=self._root,
                max_interventions=self._config.causal_max_interventions,
                fault_injection_enabled=self._config.causal_fault_injection_enabled,
                timeout_s=self._config.causal_timeout_s,
            )
            self._logger.info("causal_debugger_initialized")

        # ── Stage 5E: Autonomous issue resolution ────────────────────────────
        if self._config.issue_resolution_enabled:
            from ecodiaos.systems.simula.agents.repair_agent import RepairAgent
            from ecodiaos.systems.simula.resolution.issue_resolver import IssueResolver
            from ecodiaos.systems.simula.resolution.monitors import (
                DegradationMonitor,
                PerfRegressionMonitor,
                SecurityVulnMonitor,
            )

            perf_monitor = (
                PerfRegressionMonitor()
                if self._config.issue_perf_regression_enabled
                else None
            )
            security_monitor = (
                SecurityVulnMonitor(self._root)
                if self._config.issue_security_scan_enabled
                else None
            )
            degradation_monitor = DegradationMonitor(
                window_hours=self._config.issue_degradation_window_hours,
            )

            self._issue_resolver = IssueResolver(
                llm=self._llm,
                codebase_root=self._root,
                neo4j=self._neo4j,
                code_agent=self._code_agent,
                repair_agent=self._repair_agent if isinstance(self._repair_agent, RepairAgent) else None,
                perf_monitor=perf_monitor,
                security_monitor=security_monitor,
                degradation_monitor=degradation_monitor,
                max_autonomy_level=self._config.issue_max_autonomy_level,
                abstention_threshold=self._config.issue_abstention_confidence_threshold,
            )
            self._logger.info("issue_resolver_initialized")

        # ── Stage 6A: Cryptographic auditability ────────────────────────────
        if self._config.hash_chain_enabled:
            from ecodiaos.systems.simula.audit.hash_chain import HashChainManager

            self._hash_chain = HashChainManager(
                neo4j=self._neo4j,
            )
            self._logger.info("hash_chain_initialized")

        if self._config.c2pa_enabled:
            from ecodiaos.systems.simula.audit.content_credentials import ContentCredentialManager

            self._content_credentials = ContentCredentialManager(
                signing_key_path=self._config.c2pa_signing_key_path,
                issuer_name=self._config.c2pa_issuer_name,
            )
            self._logger.info("content_credentials_initialized")

        if self._config.verifiable_credentials_enabled:
            from ecodiaos.systems.simula.audit.verifiable_credentials import (
                GovernanceCredentialManager,
            )

            self._governance_credentials = GovernanceCredentialManager(
                neo4j=self._neo4j,
                signing_key_path=self._config.c2pa_signing_key_path,
            )
            self._logger.info("governance_credentials_initialized")

        # ── Stage 6B: Co-evolving agents ──────────────────────────────────────
        if self._config.coevolution_enabled:
            from ecodiaos.systems.simula.coevolution.hard_negative_miner import HardNegativeMiner

            self._hard_negative_miner = HardNegativeMiner(
                neo4j=self._neo4j,
                llm=self._llm,
                max_negatives_per_cycle=self._config.adversarial_max_tests_per_cycle,
            )
            self._logger.info("hard_negative_miner_initialized")

            if self._config.adversarial_test_generation_enabled:
                from ecodiaos.systems.simula.coevolution.adversarial_tester import (
                    AdversarialTestGenerator,
                )

                self._adversarial_tester = AdversarialTestGenerator(
                    llm=self._llm,
                    codebase_root=self._root,
                    max_tests_per_cycle=self._config.adversarial_max_tests_per_cycle,
                )
                self._logger.info("adversarial_tester_initialized")

        # ── Stage 6C: Formal spec generation ─────────────────────────────────
        if self._config.formal_spec_generation_enabled:
            from ecodiaos.systems.simula.formal_specs.spec_generator import FormalSpecGenerator

            self._formal_spec_generator = FormalSpecGenerator(
                llm=self._llm,
                dafny_bridge=dafny_bridge,
                tla_plus_path=self._config.tla_plus_binary_path,
                alloy_path=self._config.alloy_binary_path,
                dafny_bench_target=self._config.dafny_bench_coverage_target,
                tla_plus_timeout_s=self._config.tla_plus_model_check_timeout_s,
                alloy_scope=self._config.alloy_scope,
            )
            self._logger.info("formal_spec_generator_initialized")

        # ── Stage 6D: Equality saturation (E-graphs) ─────────────────────────
        if self._config.egraph_enabled:
            from ecodiaos.systems.simula.egraph.equality_saturation import EqualitySaturationEngine

            self._egraph = EqualitySaturationEngine(
                max_iterations=self._config.egraph_max_iterations,
                timeout_s=self._config.egraph_timeout_s,
            )
            self._logger.info("egraph_initialized")

        # ── Stage 6E: Hybrid symbolic execution ──────────────────────────────
        if self._config.symbolic_execution_enabled:
            from ecodiaos.systems.simula.verification.symbolic_execution import (
                SymbolicExecutionEngine,
            )

            self._symbolic_execution = SymbolicExecutionEngine(
                z3_bridge=z3_bridge,
                llm=self._llm,
                timeout_ms=self._config.symbolic_execution_timeout_ms,
                blocking=self._config.symbolic_execution_blocking,
            )
            self._logger.info("symbolic_execution_initialized")

        # Wire Stage 6D + 6E into health checker
        if self._health is not None:
            if self._egraph is not None:
                self._health._egraph = self._egraph  # type: ignore[assignment]
                self._health._egraph_blocking = self._config.egraph_blocking
            if self._symbolic_execution is not None:
                self._health._symbolic_execution = self._symbolic_execution  # type: ignore[assignment]
                self._health._symbolic_execution_blocking = self._config.symbolic_execution_blocking
                self._health._symbolic_execution_domains = self._config.symbolic_execution_domains

        # Wire SWE-grep into the bridge for pre-translation retrieval (3B.5)
        if self._bridge is not None and self._swe_grep is not None:
            self._bridge.set_swe_grep(self._swe_grep)

        # ── Stage 7: Hunter — Zero-Day Discovery Engine ─────────────────────
        if self._config.hunter_enabled and z3_bridge is not None:
            from ecodiaos.systems.simula.hunter.analytics import HunterAnalyticsEmitter
            from ecodiaos.systems.simula.hunter.prover import VulnerabilityProver
            from ecodiaos.systems.simula.hunter.service import HunterService
            from ecodiaos.systems.simula.hunter.types import HunterConfig

            hunter_config = HunterConfig(
                authorized_targets=self._config.hunter_authorized_targets,
                max_workers=self._config.hunter_max_workers,
                sandbox_timeout_seconds=self._config.hunter_sandbox_timeout_s,
                log_vulnerability_analytics=self._config.hunter_log_analytics,
                clone_depth=self._config.hunter_clone_depth,
            )

            hunter_prover = VulnerabilityProver(
                z3_bridge=z3_bridge,
                llm=self._llm,
            )

            # Phase 9: Build analytics emitter with optional TSDB persistence
            hunter_analytics: HunterAnalyticsEmitter | None = None
            if self._config.hunter_log_analytics:
                hunter_analytics = HunterAnalyticsEmitter(tsdb=self._tsdb)
                self._hunter_analytics = hunter_analytics
                # Initialize TSDB schema (creates hunter_events hypertable)
                await hunter_analytics.initialize()

            # Build optional remediation orchestrator
            hunter_remediation = None
            if self._config.hunter_remediation_enabled and self._repair_agent is not None:
                from ecodiaos.systems.simula.hunter.remediation import HunterRepairOrchestrator
                from ecodiaos.systems.simula.hunter.workspace import TargetWorkspace

                # Remediation needs a workspace; it's set per-hunt by HunterService
                placeholder_workspace = TargetWorkspace.internal(self._root)
                hunter_remediation = HunterRepairOrchestrator(
                    repair_agent=self._repair_agent,  # type: ignore[arg-type]
                    prover=hunter_prover,
                    workspace=placeholder_workspace,
                )

            self._hunter = HunterService(
                prover=hunter_prover,
                config=hunter_config,
                eos_root=self._root,
                analytics=hunter_analytics,
                remediation=hunter_remediation,
            )

            # Phase 9: Wire Hunter analytics into the unified EvolutionAnalyticsEngine
            if self._analytics is not None and self._hunter is not None:
                self._analytics.set_hunter_view(self._hunter.analytics_view)
                if hunter_analytics is not None and hunter_analytics._store is not None:
                    self._analytics.set_hunter_store(hunter_analytics._store)

            self._logger.info(
                "hunter_initialized",
                hunter="active",
                max_workers=hunter_config.max_workers,
                authorized_targets=len(hunter_config.authorized_targets),
                remediation=hunter_remediation is not None,
                tsdb_persistence=self._tsdb is not None,
            )

        # Pre-compute analytics from history
        if self._history is not None:
            try:
                await self._analytics.compute_analytics()
            except Exception as exc:
                self._logger.warning("initial_analytics_failed", error=str(exc))

        # Validate that all enabled external tool binaries are reachable.
        # Fail fast at startup rather than silently degrade on first use.
        await self._validate_tools()

        self._initialized = True
        self._logger.info(
            "simula_initialized",
            current_version=self._current_version,
            codebase_root=str(self._root),
            max_code_agent_turns=self._config.max_code_agent_turns,
            subsystems=[
                "simulator", "code_agent", "applicator", "rollback",
                "health", "bridge", "intelligence", "analytics",
                "history" if self._history else "history(disabled)",
                "dafny" if dafny_bridge else "dafny(disabled)",
                "z3" if z3_bridge else "z3(disabled)",
                "static_analysis" if static_bridge else "static_analysis(disabled)",
                "incremental" if self._incremental else "incremental(disabled)",
                "swe_grep" if self._swe_grep else "swe_grep(disabled)",
                "lilo" if self._lilo else "lilo(disabled)",
                "lean" if self._lean_bridge else "lean(disabled)",
                "grpo" if self._grpo else "grpo(disabled)",
                "diffusion_repair" if self._diffusion_repair else "diffusion_repair(disabled)",
                "synthesis" if self._synthesis else "synthesis(disabled)",
                "repair_agent" if self._repair_agent else "repair_agent(disabled)",
                "orchestrator" if self._orchestrator else "orchestrator(disabled)",
                "causal_debugger" if self._causal_debugger else "causal_debugger(disabled)",
                "issue_resolver" if self._issue_resolver else "issue_resolver(disabled)",
                "hash_chain" if self._hash_chain else "hash_chain(disabled)",
                "content_credentials" if self._content_credentials else "content_credentials(disabled)",
                "governance_credentials" if self._governance_credentials else "governance_credentials(disabled)",
                "hard_negative_miner" if self._hard_negative_miner else "hard_negative_miner(disabled)",
                "adversarial_tester" if self._adversarial_tester else "adversarial_tester(disabled)",
                "formal_spec_generator" if self._formal_spec_generator else "formal_spec_generator(disabled)",
                "egraph" if self._egraph else "egraph(disabled)",
                "symbolic_execution" if self._symbolic_execution else "symbolic_execution(disabled)",
                "hunter" if self._hunter else "hunter(disabled)",
            ],
            stage1_extended_thinking=thinking_provider is not None,
            stage1_embeddings=embedding_client is not None,
            stage1_kv_compression=self._config.kv_compression_enabled,
            stage1_kv_ratio=self._config.kv_compression_ratio,
            stage2_dafny=dafny_bridge is not None,
            stage2_z3=z3_bridge is not None,
            stage2_static_analysis=static_bridge is not None,
            stage2_agent_coder=self._config.agent_coder_enabled,
            stage3_incremental=self._incremental is not None,
            stage3_swe_grep=self._swe_grep is not None,
            stage3_lilo=self._lilo is not None,
            stage4_lean=self._lean_bridge is not None,
            stage4_grpo=self._grpo is not None,
            stage4_diffusion_repair=self._diffusion_repair is not None,
            stage5_synthesis=self._synthesis is not None,
            stage5_repair_agent=self._repair_agent is not None,
            stage5_orchestrator=self._orchestrator is not None,
            stage5_causal_debugger=self._causal_debugger is not None,
            stage5_issue_resolver=self._issue_resolver is not None,
            stage6_hash_chain=self._hash_chain is not None,
            stage6_content_credentials=self._content_credentials is not None,
            stage6_governance_credentials=self._governance_credentials is not None,
            stage6_coevolution=self._hard_negative_miner is not None,
            stage6_adversarial_tester=self._adversarial_tester is not None,
            stage6_formal_specs=self._formal_spec_generator is not None,
            stage6_egraph=self._egraph is not None,
            stage6_symbolic_execution=self._symbolic_execution is not None,
            stage7_hunter=self._hunter is not None,
            stage9_hunter_analytics=self._hunter_analytics is not None,
            stage9_tsdb_persistence=self._tsdb is not None,
        )

    async def _validate_tools(self) -> None:
        """
        Verify every enabled external tool binary exists and is executable.
        Raises RuntimeError on the first missing binary so the process crashes
        at startup rather than silently falling back to a degraded mode.
        """
        import shutil

        checks: list[tuple[bool, str, str]] = [
            # (enabled, binary_path, tool_name)
            (self._config.dafny_enabled, self._config.dafny_binary_path, "Dafny"),
            (self._config.lean_enabled, self._config.lean_binary_path, "Lean 4"),
            (
                self._config.formal_spec_generation_enabled and bool(self._config.tla_plus_binary_path),
                self._config.tla_plus_binary_path or "",
                "TLA+",
            ),
            (
                self._config.formal_spec_generation_enabled and bool(self._config.alloy_binary_path),
                self._config.alloy_binary_path or "",
                "Alloy",
            ),
        ]

        for enabled, binary_path, tool_name in checks:
            if not enabled or not binary_path:
                continue
            if not shutil.which(binary_path) and not Path(binary_path).is_file():
                raise RuntimeError(
                    f"Simula: {tool_name} is enabled but binary not found: '{binary_path}'. "
                    f"Install {tool_name} or set the correct path in SimulaConfig."
                )
            self._logger.debug("tool_binary_ok", tool=tool_name, path=binary_path)

    async def shutdown(self) -> None:
        """Graceful shutdown."""
        # Clean up Stage 1B embedding client
        if hasattr(self, "_embedding_client") and self._embedding_client is not None:
            with contextlib.suppress(Exception):
                await self._embedding_client.close()

        self._logger.info(
            "simula_shutdown",
            proposals_received=self._proposals_received,
            proposals_approved=self._proposals_approved,
            proposals_rejected=self._proposals_rejected,
            proposals_rolled_back=self._proposals_rolled_back,
            proposals_deduplicated=self._proposals_deduplicated,
            current_version=self._current_version,
        )

    # ─── Triage (Fast-Path Pre-Simulation) ─────────────────────────────────────

    def _triage_proposal(self, proposal: EvolutionProposal) -> TriageResult:
        """
        Fast-path proposal check. If trivial, skip expensive simulation.
        Trivial = budget tweaks <5% with sufficient data.

        Returns TriageResult with skip_simulation=True for trivial cases.
        """
        if proposal.category.value != "adjust_budget":
            return TriageResult(
                status=TriageStatus.REQUIRES_SIMULATION,
                skip_simulation=False,
            )

        spec = proposal.change_spec
        if not spec.budget_new_value or spec.budget_old_value is None:
            return TriageResult(
                status=TriageStatus.REQUIRES_SIMULATION,
                skip_simulation=False,
            )

        # Check delta < 5%
        old_val = spec.budget_old_value
        new_val = spec.budget_new_value
        if old_val == 0.0:
            delta_pct = 1.0  # Treat zero as 100% change
        else:
            delta_pct = abs(new_val - old_val) / abs(old_val)

        if delta_pct < 0.05:
            self._logger.info(
                "proposal_triaged",
                proposal_id=proposal.id,
                status="trivial",
                reason=f"Budget delta {delta_pct:.1%} < 5%",
            )
            return TriageResult(
                status=TriageStatus.TRIVIAL,
                assumed_risk=RiskLevel.LOW,
                reason=f"Budget delta {delta_pct:.1%} < 5%",
                skip_simulation=True,
            )

        return TriageResult(
            status=TriageStatus.REQUIRES_SIMULATION,
            skip_simulation=False,
        )

    # ─── Main Pipeline ─────────────────────────────────────────────────────────

    async def process_proposal(self, proposal: EvolutionProposal) -> ProposalResult:
        """
        Main entry point for evolution proposals.

        Pipeline:
          DEDUP → VALIDATE → SIMULATE → [GOVERNANCE GATE] → APPLY → VERIFY → RECORD

        Spec reference: Section III.3.2
        Performance target: validation ≤50ms, simulation ≤30s, apply ≤5s
        """
        self._proposals_received += 1
        log = self._logger.bind(proposal_id=proposal.id, category=proposal.category.value)
        log.info("proposal_received", source=proposal.source, description=proposal.description[:100])

        # ── STEP 0: Deduplication ────────────────────────────────────────────
        # Snapshot active proposals under lock (cheap), run async dedup
        # outside the lock to avoid blocking other proposals during the
        # potential LLM Tier-3 similarity call, then re-acquire the lock for
        # the atomic capacity-check + insert.
        if self._intelligence is not None:
            async with self._proposals_lock:
                snapshot = list(self._active_proposals.values())
            if snapshot:
                try:
                    all_proposals = [proposal] + snapshot
                    clusters = await self._intelligence.deduplicate(all_proposals)
                    if self._intelligence.is_duplicate(proposal, clusters):
                        self._proposals_deduplicated += 1
                        log.info("proposal_deduplicated")
                        return ProposalResult(
                            status=ProposalStatus.REJECTED,
                            reason="Duplicate of an active proposal",
                        )
                except Exception as exc:
                    log.warning("dedup_check_failed", error=str(exc))

        async with self._proposals_lock:
            if len(self._active_proposals) >= self._config.max_active_proposals:
                log.warning(
                    "proposal_rejected_queue_full",
                    active=len(self._active_proposals),
                    limit=self._config.max_active_proposals,
                )
                return ProposalResult(
                    status=ProposalStatus.REJECTED,
                    reason=(
                        f"Too many active proposals "
                        f"({len(self._active_proposals)}/{self._config.max_active_proposals}). "
                        "Try again later."
                    ),
                )
            self._active_proposals[proposal.id] = proposal

        # All remaining steps are wrapped in try/finally so any unexpected
        # exit (exception or early return) always removes the proposal from
        # _active_proposals and never leaves it stranded.
        try:
            return await asyncio.wait_for(
                self._run_pipeline(proposal, log),
                timeout=self._config.pipeline_timeout_s,
            )
        except asyncio.TimeoutError:
            log.error(
                "proposal_pipeline_timeout",
                timeout_s=self._config.pipeline_timeout_s,
            )
            proposal.status = ProposalStatus.REJECTED
            self._proposals_rejected += 1
            async with self._proposals_lock:
                self._active_proposals.pop(proposal.id, None)
            self._invalidate_analytics()
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Pipeline timed out after {self._config.pipeline_timeout_s}s",
            )
        finally:
            # Governance-approved proposals are intentionally left in
            # _active_proposals (awaiting approve_governed_proposal call).
            # All other terminal states remove themselves inside _run_pipeline;
            # this is a safety net for any path we missed.
            if proposal.status not in (
                ProposalStatus.AWAITING_GOVERNANCE,
                ProposalStatus.APPLIED,
                ProposalStatus.ROLLED_BACK,
            ):
                async with self._proposals_lock:
                    self._active_proposals.pop(proposal.id, None)

    async def _run_pipeline(
        self, proposal: EvolutionProposal, log: Any
    ) -> ProposalResult:
        """Inner pipeline body, always called from process_proposal's try/finally."""
        # ── STEP 1: Validate ────────────────────────────────────────────────
        if proposal.category in FORBIDDEN:
            proposal.status = ProposalStatus.REJECTED
            self._proposals_rejected += 1
            reason = (
                f"Category '{proposal.category.value}' is forbidden for Simula. "
                f"Iron rule: {self._get_iron_rule_for(proposal)}"
            )
            log.warning("proposal_rejected_forbidden", reason=reason)
            async with self._proposals_lock:
                self._active_proposals.pop(proposal.id, None)
            return ProposalResult(status=ProposalStatus.REJECTED, reason=reason)

        # ── STEP 1.5: Triage (fast-path for trivial cases) ──────────────────
        triage = self._triage_proposal(proposal)
        if triage.skip_simulation:
            # Build synthetic simulation result
            proposal.simulation = SimulationResult(
                episodes_tested=0,
                risk_level=triage.assumed_risk or RiskLevel.LOW,
                risk_summary=f"Triaged as trivial: {triage.reason}",
                benefit_summary=proposal.expected_benefit,
            )
            log.info("proposal_triaged_skipping_simulation", reason=triage.reason)
            # Skip STEP 2 (Simulate) and proceed directly to governance/apply

        # ── STEP 2: Simulate (deep multi-strategy) ─────────────────────────
        # Skip if already triaged (has synthetic simulation)
        if proposal.simulation is None:
            proposal.status = ProposalStatus.SIMULATING
            log.info("proposal_simulating")

            try:
                simulation = await self._simulate_change(proposal)
                proposal.simulation = simulation
            except Exception as exc:
                proposal.status = ProposalStatus.REJECTED
                self._proposals_rejected += 1
                reason = f"Simulation failed: {exc}"
                log.error("simulation_error", error=str(exc))
                async with self._proposals_lock:
                    self._active_proposals.pop(proposal.id, None)
                return ProposalResult(status=ProposalStatus.REJECTED, reason=reason)
        else:
            simulation = proposal.simulation

        if simulation.risk_level == RiskLevel.UNACCEPTABLE:
            proposal.status = ProposalStatus.REJECTED
            self._proposals_rejected += 1
            reason = f"Simulation shows unacceptable risk: {simulation.risk_summary}"
            log.warning("proposal_rejected_risk", risk_level=simulation.risk_level.value)
            async with self._proposals_lock:
                self._active_proposals.pop(proposal.id, None)
            return ProposalResult(status=ProposalStatus.REJECTED, reason=reason)

        # ── STEP 3: Governance gate ─────────────────────────────────────────
        if self.requires_governance(proposal):
            proposal.status = ProposalStatus.AWAITING_GOVERNANCE
            self._proposals_awaiting_governance += 1
            try:
                governance_id = await self._submit_to_governance(proposal, simulation)
                proposal.governance_record_id = governance_id
            except Exception as exc:
                # Governance submission failure is a hard stop: without a
                # governance record the change cannot be audited or approved.
                log.error("governance_submission_failed", error=str(exc))
                proposal.status = ProposalStatus.REJECTED
                self._proposals_rejected += 1
                self._proposals_awaiting_governance = max(0, self._proposals_awaiting_governance - 1)
                async with self._proposals_lock:
                    self._active_proposals.pop(proposal.id, None)
                return ProposalResult(
                    status=ProposalStatus.REJECTED,
                    reason=f"Governance submission failed: {exc}",
                )

            log.info("proposal_awaiting_governance", governance_id=governance_id)
            return ProposalResult(
                status=ProposalStatus.AWAITING_GOVERNANCE,
                governance_record_id=governance_id,
            )

        # ── STEP 4: Apply (self-applicable changes only) ───────────────────
        return await self._apply_change(proposal)

    async def receive_evo_proposal(
        self,
        evo_description: str,
        evo_rationale: str,
        hypothesis_ids: list[str],
        hypothesis_statements: list[str],
        evidence_scores: list[float],
        supporting_episode_ids: list[str],
        mutation_target: str = "",
        mutation_type: str = "",
    ) -> ProposalResult:
        """
        Receive a proposal from Evo via the bridge.
        Translates the lightweight Evo proposal into Simula's rich format,
        then feeds it into the main pipeline.

        This is the public API that Evo's ConsolidationOrchestrator calls.
        """
        if self._bridge is None:
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason="Simula bridge not initialized",
            )

        self._logger.info(
            "evo_proposal_received",
            description=evo_description[:80],
            hypotheses=len(hypothesis_ids),
        )

        try:
            translated = await self._bridge.translate_proposal(
                evo_description=evo_description,
                evo_rationale=evo_rationale,
                hypothesis_ids=hypothesis_ids,
                hypothesis_statements=hypothesis_statements,
                evidence_scores=evidence_scores,
                supporting_episode_ids=supporting_episode_ids,
                mutation_target=mutation_target,
                mutation_type=mutation_type,
            )
        except Exception as exc:
            self._logger.error("bridge_translation_failed", error=str(exc))
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Bridge translation failed: {exc}",
            )

        return await self.process_proposal(translated)

    async def approve_governed_proposal(
        self, proposal_id: str, governance_record_id: str
    ) -> ProposalResult:
        """
        Called when a governed proposal receives community approval.
        Resumes the pipeline from the application step.
        """
        proposal = self._active_proposals.get(proposal_id)
        if proposal is None:
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Proposal {proposal_id} not found in active proposals",
            )
        if proposal.status != ProposalStatus.AWAITING_GOVERNANCE:
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Proposal {proposal_id} is not awaiting governance (status: {proposal.status})",
            )

        proposal.status = ProposalStatus.APPROVED
        self._proposals_awaiting_governance = max(0, self._proposals_awaiting_governance - 1)
        self._logger.info("governed_proposal_approved", proposal_id=proposal_id)
        return await self._apply_change(proposal)

    def requires_governance(self, proposal: EvolutionProposal) -> bool:
        """Changes in the GOVERNANCE_REQUIRED category always need governance."""
        return proposal.category in GOVERNANCE_REQUIRED

    # ─── Query Interface ───────────────────────────────────────────────────────

    async def get_history(self, limit: int = 50) -> list[EvolutionRecord]:
        """Return the most recent evolution records."""
        if self._history is None:
            return []
        return await self._history.get_history(limit=limit)

    async def get_current_version(self) -> int:
        """Return the current config version number."""
        return self._current_version

    async def get_version_chain(self) -> list[ConfigVersion]:
        """Return the full version history chain."""
        if self._history is None:
            return []
        return await self._history.get_version_chain()

    def get_active_proposals(self) -> list[EvolutionProposal]:
        """Return all proposals currently in the pipeline."""
        return list(self._active_proposals.values())

    async def get_analytics(self) -> EvolutionAnalytics:
        """Return current evolution quality analytics."""
        if self._analytics is None:
            return EvolutionAnalytics()
        return await self._analytics.compute_analytics()

    async def get_prioritized_proposals(self) -> list[dict[str, Any]]:
        """Return active proposals ranked by priority score."""
        if self._intelligence is None or not self._active_proposals:
            return []
        priorities = await self._intelligence.prioritize(list(self._active_proposals.values()))
        return [p.model_dump() for p in priorities]

    # ─── Stats ────────────────────────────────────────────────────────────────

    @property
    def stats(self) -> dict[str, Any]:
        base: dict[str, Any] = {
            "initialized": self._initialized,
            "current_version": self._current_version,
            "proposals_received": self._proposals_received,
            "proposals_approved": self._proposals_approved,
            "proposals_rejected": self._proposals_rejected,
            "proposals_rolled_back": self._proposals_rolled_back,
            "proposals_deduplicated": self._proposals_deduplicated,
            "proposals_awaiting_governance": self._proposals_awaiting_governance,
            "active_proposals": len(self._active_proposals),
        }

        # Include cached analytics summary if available
        if self._analytics is not None and self._analytics._cached_analytics is not None:
            analytics = self._analytics._cached_analytics
            base["analytics"] = {
                "total_proposals": analytics.total_proposals,
                "evolution_velocity": analytics.evolution_velocity,
                "rollback_rate": analytics.rollback_rate,
                "mean_simulation_risk": analytics.mean_simulation_risk,
            }

        # Stage 3 subsystem status
        base["stage3"] = {
            "incremental_verification": self._incremental is not None,
            "swe_grep": self._swe_grep is not None,
            "lilo": self._lilo is not None,
        }

        # Stage 4 subsystem status
        base["stage4"] = {
            "lean": self._lean_bridge is not None,
            "grpo": self._grpo is not None,
            "diffusion_repair": self._diffusion_repair is not None,
        }

        # Stage 5 subsystem status
        base["stage5"] = {
            "synthesis": self._synthesis is not None,
            "repair_agent": self._repair_agent is not None,
            "orchestrator": self._orchestrator is not None,
            "causal_debugger": self._causal_debugger is not None,
            "issue_resolver": self._issue_resolver is not None,
        }

        # Stage 6 subsystem status
        base["stage6"] = {
            "hash_chain": self._hash_chain is not None,
            "content_credentials": self._content_credentials is not None,
            "governance_credentials": self._governance_credentials is not None,
            "hard_negative_miner": self._hard_negative_miner is not None,
            "adversarial_tester": self._adversarial_tester is not None,
            "formal_spec_generator": self._formal_spec_generator is not None,
            "egraph": self._egraph is not None,
            "symbolic_execution": self._symbolic_execution is not None,
        }

        # Stage 7 subsystem status
        base["stage7"] = {
            "hunter": self._hunter is not None,
        }
        if self._hunter is not None:
            base["stage7"]["hunter_stats"] = self._hunter.stats

        # Phase 9: Hunter analytics observability
        base["stage9_analytics"] = {
            "hunter_analytics_emitter": self._hunter_analytics is not None,
            "hunter_tsdb_persistence": (
                self._hunter_analytics is not None
                and self._hunter_analytics._store is not None
            ),
            "hunter_view_attached": (
                self._analytics is not None
                and self._analytics._hunter_view is not None
            ),
            "hunter_store_attached": (
                self._analytics is not None
                and self._analytics._hunter_store is not None
            ),
        }
        if self._hunter_analytics is not None:
            base["stage9_analytics"]["emitter_stats"] = self._hunter_analytics.stats

        return base

    # ─── Hunter API ───────────────────────────────────────────────────────────

    def _ensure_hunter(self) -> HunterService:
        """Validate that Hunter is enabled and return the typed service."""
        if self._hunter is None:
            raise RuntimeError(
                "Hunter is not enabled. Set hunter_enabled=True in SimulaConfig."
            )
        return self._hunter

    async def hunt_external_target(
        self,
        github_url: str,
        *,
        authorized_targets: list[str] | None = None,
        attack_goals: list[str] | None = None,
        generate_pocs: bool | None = None,
        generate_patches: bool | None = None,
    ) -> HuntResult:
        """
        Run Hunter against an external GitHub repository.

        Hunter is purely additive — it never modifies EOS files and all
        analysis happens in temporary workspaces.

        Args:
            github_url: HTTPS URL of the target repository.
            authorized_targets: Override config authorized targets for this hunt.
                Creates a scoped config copy — the shared config is never mutated.
            attack_goals: Custom attack goals (defaults to predefined set).
            generate_pocs: Generate exploit PoC scripts (default from config).
            generate_patches: Generate + verify patches (default from config).

        Returns:
            HuntResult with discovered vulnerabilities and optional patches.

        Raises:
            RuntimeError: If Hunter is not enabled.
        """
        hunter = self._ensure_hunter()

        # Scope-safe authorized target override: create a copy of the config
        # so concurrent hunts don't corrupt each other's authorization lists.
        if authorized_targets is not None:
            from ecodiaos.systems.simula.hunter.types import HunterConfig

            original_config = hunter._config
            hunter._config = HunterConfig(
                authorized_targets=authorized_targets,
                max_workers=original_config.max_workers,
                sandbox_timeout_seconds=original_config.sandbox_timeout_seconds,
                log_vulnerability_analytics=original_config.log_vulnerability_analytics,
                clone_depth=original_config.clone_depth,
            )
            try:
                return await hunter.hunt_external_repo(
                    github_url=github_url,
                    attack_goals=attack_goals,
                    generate_pocs=generate_pocs if generate_pocs is not None else self._config.hunter_generate_pocs,
                    generate_patches=generate_patches if generate_patches is not None else self._config.hunter_generate_patches,
                )
            finally:
                hunter._config = original_config

        return await hunter.hunt_external_repo(
            github_url=github_url,
            attack_goals=attack_goals,
            generate_pocs=generate_pocs if generate_pocs is not None else self._config.hunter_generate_pocs,
            generate_patches=generate_patches if generate_patches is not None else self._config.hunter_generate_patches,
        )

    async def hunt_internal(
        self,
        *,
        attack_goals: list[str] | None = None,
        generate_pocs: bool | None = None,
        generate_patches: bool | None = None,
    ) -> HuntResult:
        """
        Run Hunter against the internal EOS codebase for self-testing.

        Args:
            attack_goals: Custom attack goals (defaults to predefined set).
            generate_pocs: Generate exploit PoC scripts (default from config).
            generate_patches: Generate + verify patches (default from config).

        Returns:
            HuntResult with discovered vulnerabilities.

        Raises:
            RuntimeError: If Hunter is not enabled.
        """
        hunter = self._ensure_hunter()

        return await hunter.hunt_internal_eos(
            attack_goals=attack_goals,
            generate_pocs=generate_pocs if generate_pocs is not None else self._config.hunter_generate_pocs,
            generate_patches=generate_patches if generate_patches is not None else self._config.hunter_generate_patches,
        )

    async def generate_patches_for_hunt(
        self,
        hunt_result: HuntResult,
    ) -> dict[str, str]:
        """
        Generate patches for vulnerabilities found in a completed hunt.

        Useful when a hunt was run without generate_patches=True and you want
        to retroactively generate patches for the discovered vulnerabilities.

        Args:
            hunt_result: A completed HuntResult from hunt_external_target
                         or hunt_internal.

        Returns:
            Dict mapping vulnerability ID → unified diff patch string.

        Raises:
            RuntimeError: If Hunter or remediation is not enabled.
        """
        hunter = self._ensure_hunter()
        return await hunter.generate_patches(hunt_result)

    def get_hunter_analytics(self) -> dict[str, Any]:
        """Return aggregate Hunter analytics if available."""
        hunter = self._ensure_hunter()
        return hunter.analytics_view.summary

    async def get_unified_analytics(self) -> dict[str, Any]:
        """
        Return unified analytics combining evolution metrics and Hunter
        security metrics. This is the Phase 9 observability entry point.
        """
        if self._analytics is None:
            return {}
        return await self._analytics.get_unified_analytics()

    async def get_hunter_weekly_trends(
        self,
        *,
        weeks: int = 12,
        target_url: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Query weekly Hunter vulnerability trends from TSDB or in-memory view.
        """
        if self._analytics is None:
            return []
        return await self._analytics.get_hunter_weekly_trends(
            weeks=weeks, target_url=target_url,
        )

    async def get_hunter_error_summary(self, *, days: int = 7) -> list[dict[str, Any]]:
        """Query Hunter pipeline error summary from TSDB."""
        if self._analytics is None:
            return []
        return await self._analytics.get_hunter_error_summary(days=days)

    # ─── Evo Bridge Callback ──────────────────────────────────────────────────

    def get_evo_callback(self) -> Any:
        """
        Return a callback function for Evo's ConsolidationOrchestrator.
        This is wired during system initialization in main.py.

        The callback signature matches what Evo Phase 8 expects:
          async def callback(evo_proposal, hypotheses) -> ProposalResult
        """
        async def _evo_callback(evo_proposal: Any, hypotheses: list[Any]) -> ProposalResult:
            # Extract fields from Evo's lightweight types
            hypothesis_ids = [getattr(h, "id", "") for h in hypotheses]
            hypothesis_statements = [getattr(h, "statement", "") for h in hypotheses]
            evidence_scores = [getattr(h, "evidence_score", 0.0) for h in hypotheses]

            # Collect all supporting episode IDs across hypotheses
            episode_ids: list[str] = []
            for h in hypotheses:
                episode_ids.extend(getattr(h, "supporting_episodes", []))

            # Extract mutation info if available
            mutation_target = ""
            mutation_type = ""
            for h in hypotheses:
                mutation = getattr(h, "proposed_mutation", None)
                if mutation is not None:
                    mutation_target = getattr(mutation, "target", "")
                    mutation_type = getattr(mutation, "type", "")
                    if hasattr(mutation_type, "value"):
                        mutation_type = mutation_type.value
                    break

            return await self.receive_evo_proposal(
                evo_description=getattr(evo_proposal, "description", ""),
                evo_rationale=getattr(evo_proposal, "rationale", ""),
                hypothesis_ids=hypothesis_ids,
                hypothesis_statements=hypothesis_statements,
                evidence_scores=evidence_scores,
                supporting_episode_ids=episode_ids,
                mutation_target=mutation_target,
                mutation_type=mutation_type,
            )

        return _evo_callback

    # ─── Private: Application ──────────────────────────────────────────────────

    async def _apply_change(self, proposal: EvolutionProposal) -> ProposalResult:
        """
        Apply a validated, simulated, approved proposal.
        Includes health check and automatic rollback on failure.
        """
        assert self._applicator is not None
        assert self._health is not None
        assert self._rollback is not None

        proposal.status = ProposalStatus.APPLYING
        log = self._logger.bind(proposal_id=proposal.id, category=proposal.category.value)
        log.info("applying_change")

        # Stage 3C: Inject LILO library prompt into the code agent
        if self._lilo is not None and self._code_agent is not None:
            try:
                self._code_agent._lilo_prompt = await self._lilo.get_library_prompt()
            except Exception as exc:
                log.warning("lilo_prompt_error", error=str(exc))

        # Stage 4A: Inject proof library context into the code agent
        if self._lean_bridge is not None and self._code_agent is not None:
            try:
                from ecodiaos.systems.simula.verification.lean_bridge import LeanBridge
                if isinstance(self._lean_bridge, LeanBridge):
                    lib_stats = await self._lean_bridge.get_library_stats()
                    if lib_stats.total_lemmas > 0:
                        self._code_agent._proof_library_prompt = (
                            f"\n\n## Proof Library ({lib_stats.total_lemmas} proven lemmas)\n"
                            f"The Lean 4 proof library contains {lib_stats.total_lemmas} proven lemmas "
                            f"across domains: {', '.join(f'{d}: {c}' for d, c in lib_stats.by_domain.items())}.\n"
                            f"Mean Lean Copilot automation rate: {lib_stats.mean_copilot_automation:.0%}.\n"
                            f"Your implementation may benefit from existing verified properties."
                        )
            except Exception as exc:
                log.warning("proof_library_prompt_error", error=str(exc))

        # Stage 4B: GRPO A/B model routing
        grpo_model_used = ""
        grpo_ab_group = ""
        if self._grpo is not None and self._code_agent is not None:
            try:
                if self._grpo.should_use_finetuned():
                    grpo_model_used = self._config.grpo_base_model + "-finetuned"
                    grpo_ab_group = "finetuned"
                    self._code_agent._grpo_model_id = grpo_model_used
                    log.info("grpo_routing_finetuned", model=grpo_model_used)
                else:
                    grpo_ab_group = "base"
                    log.info("grpo_routing_base")
            except Exception as exc:
                log.warning("grpo_routing_error", error=str(exc))

        # ── Stage 5A: Synthesis-first (fast-path before CEGIS) ────────────────
        synthesis_result_stash = None
        if self._synthesis is not None:
            try:
                from ecodiaos.systems.simula.synthesis.strategy_selector import (
                    SynthesisStrategySelector,
                )
                from ecodiaos.systems.simula.synthesis.types import SynthesisStatus

                if isinstance(self._synthesis, SynthesisStrategySelector):
                    synth_result = await self._synthesis.synthesise(proposal)
                    synthesis_result_stash = synth_result
                    if synth_result.status == SynthesisStatus.SYNTHESIZED and synth_result.final_code:
                        log.info(
                            "synthesis_succeeded",
                            strategy=synth_result.strategy.value,
                            tokens=synth_result.total_llm_tokens,
                            duration_ms=synth_result.total_duration_ms,
                        )
                        # Write synthesised code and skip CEGIS
                        if synth_result.files_written:
                            for fpath in synth_result.files_written:
                                full_path = self._root / fpath
                                if full_path.exists():
                                    log.debug("synthesis_wrote_file", path=fpath)
                    else:
                        log.info(
                            "synthesis_fell_back_to_cegis",
                            strategy=synth_result.strategy.value,
                            status=synth_result.status.value,
                        )
            except Exception as exc:
                log.warning("synthesis_error", error=str(exc))

        # ── Stage 5C: Multi-agent orchestration for multi-file proposals ──────
        if self._orchestrator is not None:
            try:
                from ecodiaos.systems.simula.orchestration.orchestrator import (
                    MultiAgentOrchestrator,
                )

                if isinstance(self._orchestrator, MultiAgentOrchestrator):
                    # Estimate affected files from proposal target + code_hint
                    estimated_files = []
                    _target = getattr(proposal, "target", None)
                    if _target:
                        estimated_files.append(_target)
                    if hasattr(proposal, "affected_files"):
                        estimated_files.extend(proposal.affected_files)

                    threshold = self._config.orchestration_multi_file_threshold
                    if len(estimated_files) >= threshold:
                        log.info(
                            "orchestration_engaged",
                            files=len(estimated_files),
                            threshold=threshold,
                        )
                        orc_result = await self._orchestrator.orchestrate(
                            proposal=proposal,
                            files_to_change=estimated_files,
                        )
                        proposal._orchestration_result = orc_result  # type: ignore[attr-defined]
                        log.info(
                            "orchestration_complete",
                            success=not orc_result.error,
                            stages=orc_result.parallel_stages_executed,
                            agents=orc_result.total_agents_used,
                        )
            except Exception as exc:
                log.warning("orchestration_error", error=str(exc))

        code_result, snapshot = await self._applicator.apply(proposal)
        # Stamp the snapshot with the version that was current before this
        # change was applied, so rollback audit trails show the correct target.
        snapshot.config_version = self._current_version

        # ── Stage 5B: Neural repair agent (primary recovery before diffusion) ─
        if not code_result.success and self._repair_agent is not None:
            log.info("repair_agent_attempting")
            try:
                from ecodiaos.systems.simula.agents.repair_agent import (
                    RepairAgent as RepairAgentCls,
                )
                from ecodiaos.systems.simula.verification.types import RepairStatus

                if isinstance(self._repair_agent, RepairAgentCls):
                    broken_files = {
                        f: (self._root / f).read_text()
                        for f in code_result.files_written
                        if (self._root / f).exists()
                    }
                    repair_result = await self._repair_agent.repair(
                        proposal=proposal,
                        broken_files=broken_files,
                        test_output=code_result.test_output or code_result.error,
                    )
                    if repair_result.status == RepairStatus.REPAIRED:
                        log.info(
                            "repair_agent_succeeded",
                            attempts=repair_result.total_attempts,
                            cost=f"${repair_result.total_cost_usd:.4f}",
                        )
                        code_result.success = True
                        code_result.files_written = repair_result.files_repaired
                        code_result.error = ""
                        code_result.repair_attempted = True
                        code_result.repair_succeeded = True
                        code_result.repair_cost_usd = repair_result.total_cost_usd
                        proposal._repair_result = repair_result  # type: ignore[attr-defined]
                    else:
                        log.info("repair_agent_insufficient", status=repair_result.status.value)
                        code_result.repair_attempted = True
                        code_result.repair_succeeded = False
                        proposal._repair_result = repair_result  # type: ignore[attr-defined]
            except Exception as exc:
                log.warning("repair_agent_error", error=str(exc))

        # Stage 4C: Diffusion repair fallback when code agent fails
        if not code_result.success and self._diffusion_repair is not None:
            log.info("diffusion_repair_fallback_attempting")
            try:
                from ecodiaos.systems.simula.agents.diffusion_repair import DiffusionRepairAgent
                if isinstance(self._diffusion_repair, DiffusionRepairAgent):
                    broken_files_dr = {
                        f: (self._root / f).read_text()
                        for f in code_result.files_written
                        if (self._root / f).exists()
                    }
                    dr_result = await self._diffusion_repair.repair(
                        proposal=proposal,
                        broken_files=broken_files_dr,
                        test_output=code_result.test_output or code_result.error or "",
                    )
                    if dr_result.status.value == "repaired":
                        log.info(
                            "diffusion_repair_succeeded",
                            steps=len(dr_result.denoise_steps),
                            improvement=f"{dr_result.improvement_rate:.0%}",
                        )
                        # Mark as success — diffusion repair saved the change
                        code_result.success = True
                        code_result.files_written = dr_result.files_repaired
                        code_result.error = ""
                        # Stash repair metadata on proposal for history recording
                        proposal._diffusion_repair_result = dr_result  # type: ignore[attr-defined]
                    else:
                        log.info("diffusion_repair_insufficient", status=dr_result.status.value)
            except Exception as exc:
                log.warning("diffusion_repair_error", error=str(exc))

        if not code_result.success:
            proposal.status = ProposalStatus.ROLLED_BACK
            self._proposals_rolled_back += 1
            log.warning("apply_failed_no_success", error=code_result.error)
            async with self._proposals_lock:
                self._active_proposals.pop(proposal.id, None)
            self._invalidate_analytics()
            return ProposalResult(
                status=ProposalStatus.ROLLED_BACK,
                reason=f"Application failed: {code_result.error}",
            )

        # Stash GRPO metadata for history recording
        proposal._grpo_model_used = grpo_model_used  # type: ignore[attr-defined]
        proposal._grpo_ab_group = grpo_ab_group  # type: ignore[attr-defined]

        # Stash Stage 5A synthesis metadata
        if synthesis_result_stash is not None:
            proposal._synthesis_result = synthesis_result_stash  # type: ignore[attr-defined]
            code_result.synthesis_strategy = synthesis_result_stash.strategy.value
            code_result.synthesis_speedup = synthesis_result_stash.speedup_vs_cegis

        # ── Health check (with Stage 2 formal verification) ────────────────
        try:
            health = await self._health.check(
                code_result.files_written, proposal=proposal,
            )
        except Exception as exc:
            log.error("health_check_unhandled_error", error=str(exc))
            proposal.status = ProposalStatus.REJECTED
            self._proposals_rejected += 1
            async with self._proposals_lock:
                self._active_proposals.pop(proposal.id, None)
            self._invalidate_analytics()
            return ProposalResult(
                status=ProposalStatus.REJECTED,
                reason=f"Health check failed unexpectedly: {exc}",
            )

        # Stash formal verification result for history recording
        if health.formal_verification is not None:
            proposal._formal_verification_result = health.formal_verification  # type: ignore[attr-defined]

        # Stash Lean 4 verification result for history recording
        if health.lean_verification is not None:
            proposal._lean_verification_result = health.lean_verification  # type: ignore[attr-defined]

        # Stash Stage 6 formal guarantees result for history recording
        if health.formal_guarantees is not None:
            proposal._formal_guarantees_result = health.formal_guarantees  # type: ignore[attr-defined]

        if not health.healthy:
            recovered = False

            # ── Stage 5D: Causal debugging before repair ──────────────────
            causal_diagnosis = None
            if self._causal_debugger is not None:
                log.info("causal_debugging_starting", issues=health.issues)
                try:
                    from ecodiaos.systems.simula.debugging.causal_dag import (
                        CausalDebugger as CausalDbgCls,
                    )

                    if isinstance(self._causal_debugger, CausalDbgCls):
                        causal_diagnosis = await self._causal_debugger.diagnose(
                            files_written=code_result.files_written,
                            health_issues=health.issues,
                            test_output=code_result.test_output or "",
                        )
                        log.info(
                            "causal_diagnosis_complete",
                            root_cause=causal_diagnosis.root_cause_node,
                            confidence=f"{causal_diagnosis.confidence:.2f}",
                            interventions=causal_diagnosis.total_interventions,
                        )
                        # Stash for history recording
                        proposal._causal_diagnosis = causal_diagnosis  # type: ignore[attr-defined]
                        health.causal_diagnosis = causal_diagnosis
                except Exception as exc:
                    log.warning("causal_debugging_error", error=str(exc))

            # ── Stage 5B: Repair agent recovery after causal diagnosis ────
            if self._repair_agent is not None:
                log.info("repair_agent_post_health_attempting")
                try:
                    from ecodiaos.systems.simula.agents.repair_agent import (
                        RepairAgent as RepairAgentCls,
                    )
                    from ecodiaos.systems.simula.verification.types import RepairStatus

                    if isinstance(self._repair_agent, RepairAgentCls):
                        broken_files = {
                            f: (self._root / f).read_text()
                            for f in code_result.files_written
                            if (self._root / f).exists()
                        }
                        # Feed causal diagnosis context to the repair agent
                        diag_context = ""
                        if causal_diagnosis is not None:
                            diag_context = (
                                f"Root cause: {causal_diagnosis.root_cause_node}\n"
                                f"Fix location: {causal_diagnosis.root_cause_file}\n"
                                f"Confidence: {causal_diagnosis.confidence:.2f}\n"
                                f"Reasoning: {' → '.join(causal_diagnosis.reasoning_chain)}"
                            )
                        repair_result = await self._repair_agent.repair(
                            proposal=proposal,
                            broken_files=broken_files,
                            test_output=(
                                code_result.test_output
                                or "; ".join(health.issues)
                            ),
                            lint_output=diag_context or "",
                        )
                        if repair_result.status == RepairStatus.REPAIRED:
                            log.info(
                                "repair_agent_post_health_succeeded",
                                attempts=repair_result.total_attempts,
                                cost=f"${repair_result.total_cost_usd:.4f}",
                            )
                            code_result.repair_attempted = True
                            code_result.repair_succeeded = True
                            code_result.repair_cost_usd = repair_result.total_cost_usd
                            proposal._repair_result = repair_result  # type: ignore[attr-defined]

                            # Re-check health after repair
                            health_recheck = await self._health.check(
                                repair_result.files_repaired, proposal=proposal,
                            )
                            if health_recheck.healthy:
                                log.info("health_recheck_passed_after_repair")
                                health = health_recheck
                                code_result.files_written = repair_result.files_repaired
                                code_result.success = True
                                recovered = True
                            else:
                                log.warning(
                                    "health_recheck_still_failing",
                                    issues=health_recheck.issues,
                                )
                        else:
                            log.info(
                                "repair_agent_post_health_insufficient",
                                status=repair_result.status.value,
                            )
                            code_result.repair_attempted = True
                            code_result.repair_succeeded = False
                except Exception as exc:
                    log.warning("repair_agent_post_health_error", error=str(exc))

            # ── Rollback only if all recovery failed ──────────────────────
            if not recovered:
                log.warning("health_check_failed_rolling_back", issues=health.issues)
                await self._rollback.restore(snapshot)
                proposal.status = ProposalStatus.ROLLED_BACK
                self._proposals_rolled_back += 1

                # Record the rollback in history
                await self._record_evolution(
                    proposal, code_result.files_written,
                    rolled_back=True,
                    rollback_reason="; ".join(health.issues),
                )

                async with self._proposals_lock:
                    self._active_proposals.pop(proposal.id, None)
                self._invalidate_analytics()
                return ProposalResult(
                    status=ProposalStatus.ROLLED_BACK,
                    reason=f"Post-apply health check failed: {'; '.join(health.issues)}",
                )

        # ── Stage 6A.2: Sign generated files with content credentials ────────
        if self._content_credentials is not None:
            try:
                from ecodiaos.systems.simula.audit.content_credentials import (
                    ContentCredentialManager,
                )

                if isinstance(self._content_credentials, ContentCredentialManager):
                    cc_result = await self._content_credentials.sign_files(
                        files=code_result.files_written,
                        codebase_root=self._root,
                    )
                    proposal._content_credential_result = cc_result  # type: ignore[attr-defined]
                    log.info(
                        "content_credentials_signed",
                        signed=len(cc_result.credentials),
                        unsigned=len(cc_result.unsigned_files),
                    )
            except Exception as exc:
                log.warning("content_credentials_error", error=str(exc))

        # ── Stage 6C: Generate formal specs for changed code ─────────────────
        if self._formal_spec_generator is not None:
            try:
                from ecodiaos.systems.simula.formal_specs.spec_generator import FormalSpecGenerator

                if isinstance(self._formal_spec_generator, FormalSpecGenerator):
                    fsg_result = await self._formal_spec_generator.generate_all(
                        files=code_result.files_written,
                        proposal=proposal,
                        codebase_root=self._root,
                        dafny_enabled=self._config.dafny_spec_generation_enabled,
                        tla_plus_enabled=self._config.tla_plus_enabled,
                        alloy_enabled=self._config.alloy_enabled,
                        self_spec_enabled=self._config.self_spec_dsl_enabled,
                    )
                    proposal._formal_spec_result = fsg_result  # type: ignore[attr-defined]
                    log.info(
                        "formal_specs_generated",
                        specs=len(fsg_result.specs),
                        coverage=f"{fsg_result.overall_coverage_percent:.0%}",
                    )
            except Exception as exc:
                log.warning("formal_spec_generation_error", error=str(exc))

        # ── Stage 3A: Incremental verification cache update ─────────────────
        if self._incremental is not None:
            try:
                incr_result = await self._incremental.verify_incremental(
                    files_changed=code_result.files_written,
                    proposal_id=proposal.id,
                )
                log.info(
                    "incremental_verification_complete",
                    checked=incr_result.functions_checked,
                    skipped=incr_result.functions_skipped_early_cutoff,
                    cache_hit_rate=f"{incr_result.cache_hit_rate:.0%}",
                )
            except Exception as exc:
                log.warning("incremental_verification_error", error=str(exc))

        # ── Success ───────────────────────────────────────────────────────────
        proposal.status = ProposalStatus.APPLIED
        self._proposals_approved += 1

        async with self._version_lock:
            from_version = self._current_version
            self._current_version += 1

        await self._record_evolution(
            proposal,
            code_result.files_written,
            rolled_back=False,
            from_version=from_version,
        )

        # ── Stage 6A.1: Append to hash chain ─────────────────────────────────
        if self._hash_chain is not None:
            try:
                from ecodiaos.systems.simula.audit.hash_chain import HashChainManager

                if isinstance(self._hash_chain, HashChainManager):
                    # Build a record-like object for hashing (use the same fields as the recording)
                    from ecodiaos.systems.simula.types import EvolutionRecord as ERec

                    hash_record = ERec(
                        proposal_id=proposal.id,
                        category=proposal.category,
                        description=proposal.description,
                        from_version=from_version,
                        to_version=self._current_version,
                        files_changed=code_result.files_written,
                        simulation_risk=RiskLevel.LOW,
                        rolled_back=False,
                    )
                    hce = await self._hash_chain.append(hash_record)
                    proposal._hash_chain_entry = hce  # type: ignore[attr-defined]
                    log.info(
                        "hash_chain_appended",
                        chain_hash=hce.chain_hash[:16],
                        position=hce.chain_position,
                    )
            except Exception as exc:
                log.warning("hash_chain_append_error", error=str(exc))

        # ── Stage 6B: Co-evolution cycle (fire-and-forget) ───────────────────
        if self._hard_negative_miner is not None:
            try:
                import asyncio as _aio

                _aio.create_task(
                    self._coevolution_background(
                        files=code_result.files_written,
                        proposal_id=proposal.id,
                    ),
                )
                log.info("coevolution_cycle_scheduled")
            except Exception as exc:
                log.warning("coevolution_schedule_error", error=str(exc))

        # ── Stage 4B: Record GRPO training data ──────────────────────────────
        if self._grpo is not None:
            try:
                self._grpo.record_proposal_applied()
                # Check if retraining is warranted
                if self._grpo.should_retrain():
                    log.info("grpo_retrain_triggered")
                    # Fire-and-forget: retraining is expensive and non-blocking
                    import asyncio
                    asyncio.create_task(self._grpo_retrain_background())
            except Exception as exc:
                log.warning("grpo_record_error", error=str(exc))

        # ── Stage 3C: LILO abstraction extraction ────────────────────────────
        self._proposals_applied_since_consolidation += 1
        if self._lilo is not None:
            try:
                extraction = await self._lilo.extract_from_proposals(
                    proposal_ids=[proposal.id],
                    files_changed={proposal.id: code_result.files_written},
                )
                if extraction.extracted:
                    log.info(
                        "lilo_extraction_complete",
                        extracted=len(extraction.extracted),
                        merged=extraction.merged_into_existing,
                    )
                # Periodic consolidation
                if (
                    self._proposals_applied_since_consolidation
                    >= self._config.lilo_consolidation_interval_proposals
                ):
                    await self._lilo.consolidate()
                    self._proposals_applied_since_consolidation = 0
                    log.info("lilo_consolidation_complete")
            except Exception as exc:
                log.warning("lilo_extraction_error", error=str(exc))

        # Clean up active proposals
        async with self._proposals_lock:
            self._active_proposals.pop(proposal.id, None)
        self._invalidate_analytics()

        log.info(
            "change_applied",
            from_version=from_version,
            to_version=self._current_version,
            files_changed=len(code_result.files_written),
        )

        return ProposalResult(
            status=ProposalStatus.APPLIED,
            version=self._current_version,
            files_changed=code_result.files_written,
        )

    async def _simulate_change(self, proposal: EvolutionProposal) -> SimulationResult:
        """Delegate to the deep ChangeSimulator."""
        if self._simulator is None:
            return SimulationResult(risk_level=RiskLevel.LOW, risk_summary="Simulator not initialized")
        return await self._simulator.simulate(proposal)

    async def _submit_to_governance(
        self, proposal: EvolutionProposal, simulation: SimulationResult
    ) -> str:
        """
        Submit a governed proposal to the community governance system.
        Returns a governance record ID. Enriches the governance record
        with deep simulation data for community review.
        """
        record_id = f"gov_{new_id()}"

        if self._neo4j is not None:
            try:
                # Include enriched simulation data for governance reviewers
                risk_summary = simulation.risk_summary
                benefit_summary = simulation.benefit_summary

                # Add counterfactual and alignment data if available (enriched simulation)
                enrichment = []
                if isinstance(simulation, EnrichedSimulationResult):
                    if simulation.constitutional_alignment != 0.0:
                        enrichment.append(f"Constitutional alignment: {simulation.constitutional_alignment:+.2f}")
                    if simulation.dependency_blast_radius > 0:
                        enrichment.append(f"Blast radius: {simulation.dependency_blast_radius} files")
                if enrichment:
                    risk_summary = f"{risk_summary} [{'; '.join(enrichment)}]"

                await self._neo4j.execute_write(
                    """
                    CREATE (:GovernanceProposal {
                        id: $id,
                        proposal_id: $proposal_id,
                        category: $category,
                        description: $description,
                        risk_level: $risk_level,
                        risk_summary: $risk_summary,
                        benefit_summary: $benefit_summary,
                        submitted_at: $submitted_at,
                        status: 'pending'
                    })
                    """,
                    {
                        "id": record_id,
                        "proposal_id": proposal.id,
                        "category": proposal.category.value,
                        "description": proposal.description,
                        "risk_level": simulation.risk_level.value,
                        "risk_summary": risk_summary,
                        "benefit_summary": benefit_summary,
                        "submitted_at": utc_now().isoformat(),
                    },
                )
            except Exception as exc:
                self._logger.warning("governance_neo4j_write_failed", error=str(exc))

        return record_id

    async def _record_evolution(
        self,
        proposal: EvolutionProposal,
        files_changed: list[str],
        rolled_back: bool = False,
        rollback_reason: str = "",
        from_version: int | None = None,
    ) -> None:
        """Write an immutable evolution record and update the version chain.

        from_version should be the pre-apply version captured atomically inside
        _version_lock. If omitted it is derived from self._current_version for
        backwards compatibility (rolled-back path, where the version was never
        incremented).
        """
        if self._history is None:
            return

        if from_version is None:
            # Rollback path: version was never incremented, so from == to.
            from_version = self._current_version
        to_version = self._current_version

        risk_level = (
            proposal.simulation.risk_level
            if proposal.simulation
            else RiskLevel.LOW
        )

        # Extract simulation detail fields if enriched simulation was performed
        sim_detail: dict[str, Any] = {
            "simulation_episodes_tested": 0,
            "counterfactual_regression_rate": 0.0,
            "dependency_blast_radius": 0,
            "constitutional_alignment": 0.0,
            "resource_tokens_per_hour": 0,
            "caution_reasoning": "",
        }
        if isinstance(proposal.simulation, EnrichedSimulationResult):
            sim_detail["simulation_episodes_tested"] = proposal.simulation.episodes_tested
            sim_detail["counterfactual_regression_rate"] = proposal.simulation.counterfactual_regression_rate
            sim_detail["dependency_blast_radius"] = proposal.simulation.dependency_blast_radius
            sim_detail["constitutional_alignment"] = proposal.simulation.constitutional_alignment
            if proposal.simulation.resource_cost_estimate:
                sim_detail["resource_tokens_per_hour"] = (
                    proposal.simulation.resource_cost_estimate.estimated_additional_llm_tokens_per_hour
                )
            if proposal.simulation.caution_adjustment:
                sim_detail["caution_reasoning"] = proposal.simulation.caution_adjustment.reasoning

        record = EvolutionRecord(
            proposal_id=proposal.id,
            category=proposal.category,
            description=proposal.description,
            from_version=from_version,
            to_version=to_version,
            files_changed=files_changed,
            simulation_risk=risk_level,
            rolled_back=rolled_back,
            rollback_reason=rollback_reason,
            **sim_detail,
        )

        # Stage 2: Attach formal verification metadata if available
        if hasattr(proposal, "_formal_verification_result"):
            fv = proposal._formal_verification_result
            if fv is not None:
                if fv.dafny and fv.dafny.status:
                    record.formal_verification_status = fv.dafny.status.value
                    record.dafny_rounds = fv.dafny.rounds_attempted
                if fv.z3 and fv.z3.valid_invariants:
                    record.discovered_invariants_count = len(fv.z3.valid_invariants)
                if fv.static_analysis:
                    record.static_analysis_findings = len(fv.static_analysis.findings)

        # Stage 4A: Attach Lean 4 proof metadata if available
        if hasattr(proposal, "_lean_verification_result"):
            lean_r = proposal._lean_verification_result
            if lean_r is not None:
                record.lean_proof_status = lean_r.status.value
                record.lean_proof_rounds = len(lean_r.attempts)
                record.lean_proven_lemmas_count = len(lean_r.proven_lemmas)
                record.lean_copilot_automation_rate = lean_r.copilot_automation_rate
                record.lean_library_lemmas_reused = len(lean_r.library_lemmas_used)

        # Stage 4B: Attach GRPO model routing metadata
        if hasattr(proposal, "_grpo_model_used"):
            record.grpo_model_used = proposal._grpo_model_used
        if hasattr(proposal, "_grpo_ab_group"):
            record.grpo_ab_group = proposal._grpo_ab_group

        # Stage 4C: Attach diffusion repair metadata if used
        if hasattr(proposal, "_diffusion_repair_result"):
            dr = proposal._diffusion_repair_result
            if dr is not None:
                record.diffusion_repair_used = True
                record.diffusion_repair_status = dr.status.value
                record.diffusion_repair_steps = len(dr.denoise_steps)
                record.diffusion_improvement_rate = dr.improvement_rate

        # Stage 5A: Attach synthesis metadata
        if hasattr(proposal, "_synthesis_result"):
            sr = proposal._synthesis_result
            if sr is not None:
                record.synthesis_strategy_used = sr.strategy.value
                record.synthesis_status = sr.status.value
                record.synthesis_speedup_vs_baseline = sr.speedup_vs_cegis
                record.synthesis_candidates_explored = sr.candidates_explored

        # Stage 5B: Attach repair agent metadata
        if hasattr(proposal, "_repair_result"):
            rr = proposal._repair_result
            if rr is not None:
                record.repair_agent_used = True
                record.repair_agent_status = rr.status.value
                record.repair_attempts = rr.total_attempts
                record.repair_cost_usd = rr.total_cost_usd

        # Stage 5C: Attach orchestration metadata
        if hasattr(proposal, "_orchestration_result"):
            orc = proposal._orchestration_result
            if orc is not None:
                record.orchestration_used = True
                record.orchestration_dag_nodes = orc.dag_nodes
                record.orchestration_agents_used = orc.agents_used
                record.orchestration_parallel_stages = orc.parallel_stages

        # Stage 5D: Attach causal debugging metadata
        if hasattr(proposal, "_causal_diagnosis"):
            cd = proposal._causal_diagnosis
            if cd is not None:
                record.causal_debug_used = True
                record.causal_root_cause = cd.root_cause
                record.causal_confidence = cd.confidence
                record.causal_interventions = cd.interventions_performed

        # Stage 5E: Attach issue resolution metadata
        if hasattr(proposal, "_issue_resolution_result"):
            ir = proposal._issue_resolution_result
            if ir is not None:
                record.issue_resolution_used = True
                record.issue_autonomy_level = ir.autonomy_level.value
                record.issue_abstained = ir.status.value == "abstained"

        # Stage 6A: Attach hash chain metadata
        if hasattr(proposal, "_hash_chain_entry"):
            hce = proposal._hash_chain_entry
            if hce is not None:
                record.hash_chain_hash = hce.chain_hash
                record.hash_chain_position = hce.chain_position

        # Stage 6A: Content credentials count
        if hasattr(proposal, "_content_credential_result"):
            ccr = proposal._content_credential_result
            if ccr is not None:
                record.content_credentials_signed = len(ccr.credentials)

        # Stage 6A: Governance credential status
        if hasattr(proposal, "_governance_credential_result"):
            gcr = proposal._governance_credential_result
            if gcr is not None:
                record.governance_credential_status = gcr.status.value

        # Stage 6B: Co-evolution metadata
        if hasattr(proposal, "_coevolution_result"):
            cr = proposal._coevolution_result
            if cr is not None:
                record.coevolution_hard_negatives_mined = cr.hard_negatives_mined
                record.coevolution_adversarial_tests = cr.adversarial_tests_generated
                record.coevolution_bugs_found = cr.tests_found_bugs

        # Stage 6C: Formal spec metadata
        if hasattr(proposal, "_formal_spec_result"):
            fsr = proposal._formal_spec_result
            if fsr is not None:
                record.formal_specs_generated = len(fsr.specs)
                record.formal_spec_coverage_percent = fsr.overall_coverage_percent
                if fsr.tla_plus_results:
                    record.tla_plus_states_explored = sum(
                        r.states_explored for r in fsr.tla_plus_results
                    )

        # Stage 6D: E-graph metadata
        if hasattr(proposal, "_formal_guarantees_result"):
            fg = proposal._formal_guarantees_result
            if fg is not None and fg.egraph is not None:
                er = fg.egraph
                record.egraph_used = True
                record.egraph_status = er.status.value
                record.egraph_rules_applied = len(er.rules_applied)

        # Stage 6E: Symbolic execution metadata
        if hasattr(proposal, "_formal_guarantees_result"):
            fg = proposal._formal_guarantees_result
            if fg is not None and fg.symbolic_execution is not None:
                se = fg.symbolic_execution
                record.symbolic_execution_used = True
                record.symbolic_properties_proved = se.properties_proved
                record.symbolic_counterexamples = len(se.counterexamples)

        try:
            await self._history.record(record)
        except Exception as exc:
            self._logger.error("history_write_failed", error=str(exc))
            return

        if not rolled_back:
            config_hash = self._compute_config_hash(files_changed)
            version = ConfigVersion(
                version=self._current_version,
                proposal_ids=[proposal.id],
                config_hash=config_hash,
            )
            try:
                await self._history.record_version(version, previous_version=from_version)
            except Exception as exc:
                self._logger.error("version_write_failed", error=str(exc))

    # ─── Stage 4B: GRPO Background Retraining ──────────────────────────────

    async def _grpo_retrain_background(self) -> None:
        """
        Background task to run the GRPO retraining pipeline.
        Collects data → SFT → GRPO RL → evaluate → deploy if improved.
        """
        if self._grpo is None:
            return
        try:
            self._logger.info("grpo_retrain_starting")
            training_examples = await self._grpo.collect_training_data()
            if not training_examples:
                self._logger.info("grpo_retrain_skipped_insufficient_data")
                return

            training_run = await self._grpo.run_sft(training_examples)
            training_run = await self._grpo.run_grpo(training_run)
            evaluation = await self._grpo.evaluate()

            if evaluation.statistically_significant and evaluation.improvement_percent > 0:
                self._logger.info(
                    "grpo_retrain_deployed",
                    improvement=f"{evaluation.improvement_percent:.1f}%",
                    pass_at_1_base=f"{evaluation.base_model_pass_at_1:.2f}",
                    pass_at_1_finetuned=f"{evaluation.finetuned_model_pass_at_1:.2f}",
                )
            else:
                self._logger.info(
                    "grpo_retrain_no_improvement",
                    improvement=f"{evaluation.improvement_percent:.1f}%",
                    significant=evaluation.statistically_significant,
                )
        except Exception as exc:
            self._logger.warning("grpo_retrain_error", error=str(exc))

    # ─── Stage 6B: Co-Evolution Background Cycle ────────────────────────────

    async def _coevolution_background(
        self,
        files: list[str],
        proposal_id: str,
    ) -> None:
        """
        Background task to run a co-evolution cycle:
        mine hard negatives from history and adversarial tests,
        then feed into GRPO training.
        """
        if self._hard_negative_miner is None:
            return
        try:
            from ecodiaos.systems.simula.coevolution.hard_negative_miner import HardNegativeMiner

            if not isinstance(self._hard_negative_miner, HardNegativeMiner):
                return

            # Import adversarial tester if available
            adversarial_gen = None
            if self._adversarial_tester is not None:
                from ecodiaos.systems.simula.coevolution.adversarial_tester import (
                    AdversarialTestGenerator,
                )

                if isinstance(self._adversarial_tester, AdversarialTestGenerator):
                    adversarial_gen = self._adversarial_tester

            cycle_result = await self._hard_negative_miner.run_cycle(
                adversarial_generator=adversarial_gen,
                files=files,
            )

            self._logger.info(
                "coevolution_cycle_complete",
                proposal_id=proposal_id,
                hard_negatives=cycle_result.hard_negatives_mined,
                adversarial_tests=cycle_result.adversarial_tests_generated,
                bugs_found=cycle_result.tests_found_bugs,
                grpo_examples=cycle_result.grpo_examples_produced,
                duration_ms=cycle_result.duration_ms,
            )

            # Feed hard negatives into GRPO if available
            if self._grpo is not None and cycle_result.grpo_examples_produced > 0:
                grpo_batch = await self._hard_negative_miner.prepare_grpo_batch(
                    await self._hard_negative_miner.mine_from_history(),
                )
                self._logger.info(
                    "coevolution_grpo_batch_ready",
                    examples=len(grpo_batch),
                )

        except Exception as exc:
            self._logger.warning(
                "coevolution_background_error",
                error=str(exc),
                proposal_id=proposal_id,
            )

    # ─── Helpers ──────────────────────────────────────────────────────────────

    def _compute_config_hash(self, files_changed: list[str]) -> str:
        """Compute a stable hash of the current config state."""
        hasher = hashlib.sha256()
        for rel_path in sorted(files_changed):
            full_path = self._root / rel_path
            hasher.update(rel_path.encode())
            if full_path.exists():
                hasher.update(str(full_path.stat().st_mtime).encode())
        return hasher.hexdigest()[:16]

    def _get_iron_rule_for(self, proposal: EvolutionProposal) -> str:
        """Return the relevant iron rule for a forbidden category."""
        rule_map = {
            "modify_equor": "Simula CANNOT modify Equor in any way.",
            "modify_constitution": "Simula CANNOT modify constitutional drives.",
            "modify_invariants": "Simula CANNOT modify invariants.",
            "modify_self_evolution": "Simula CANNOT modify its own logic (no self-modifying code).",
        }
        return rule_map.get(proposal.category.value, "Category is forbidden.")

    def _invalidate_analytics(self) -> None:
        """Invalidate analytics cache after a proposal completes."""
        if self._analytics is not None:
            self._analytics.invalidate_cache()

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\simulation.py =====

"""
EcodiaOS -- Simula Deep Simulation Engine

Before any change is applied, the simulator performs multi-strategy
impact prediction. This is the brain of Simula's decision-making.

Strategy stack (per proposal):
  1. Category-specific validation (static analysis / budget check / LLM reasoning)
  2. Counterfactual episode replay — "What if this existed during episode X?"
  3. Dependency graph analysis — blast radius via import-graph traversal
  4. Resource cost estimation — heuristic compute/memory/token impact
  5. Constitutional alignment prediction — drive alignment scoring
  6. Risk synthesis — combine all signals into a unified assessment

Budget efficiency:
  - Counterfactual replay: batches 30 episodes into ONE LLM call (~800 tokens)
  - Constitutional alignment: single call, 100 tokens max output
  - Dependency analysis: pure Python ast module, zero LLM tokens
  - Resource cost: heuristic lookup table, zero LLM tokens
  - Analytics-informed caution: uses cached history, zero LLM tokens

Target latency: <=30s for full simulation (spec requirement).
"""

from __future__ import annotations

import ast
import asyncio
import re
from pathlib import Path
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.systems.simula.types import (
    GOVERNANCE_REQUIRED,
    SELF_APPLICABLE,
    CautionAdjustment,
    ChangeCategory,
    CounterfactualResult,
    DependencyImpact,
    EnrichedSimulationResult,
    EvolutionProposal,
    ImpactType,
    ResourceCostEstimate,
    RiskLevel,
    SimulationResult,
)

if TYPE_CHECKING:
    from ecodiaos.clients.llm import LLMProvider
    from ecodiaos.config import SimulaConfig
    from ecodiaos.systems.memory.service import MemoryService
    from ecodiaos.systems.simula.analytics import EvolutionAnalyticsEngine

from ecodiaos.clients.context_compression import ContextCompressor
from ecodiaos.clients.optimized_llm import OptimizedLLMProvider

logger = structlog.get_logger().bind(system="simula.simulation")

# Valid Python identifier pattern for names
_VALID_NAME = re.compile(r"^[A-Za-z][A-Za-z0-9_]*$")
_SNAKE_CASE = re.compile(r"^[a-z][a-z0-9_]*$")
_PASCAL_CASE = re.compile(r"^[A-Z][A-Za-z0-9]*$")

# Resource cost heuristics per category (zero LLM tokens)
_RESOURCE_COST_HEURISTICS: dict[ChangeCategory, dict[str, int | float]] = {
    ChangeCategory.ADD_EXECUTOR: {
        "llm_tokens_per_hour": 500,
        "compute_ms_per_cycle": 5,
        "memory_mb": 2.0,
    },
    ChangeCategory.ADD_INPUT_CHANNEL: {
        "llm_tokens_per_hour": 200,
        "compute_ms_per_cycle": 10,
        "memory_mb": 5.0,
    },
    ChangeCategory.ADD_PATTERN_DETECTOR: {
        "llm_tokens_per_hour": 0,
        "compute_ms_per_cycle": 3,
        "memory_mb": 1.0,
    },
    ChangeCategory.ADJUST_BUDGET: {
        "llm_tokens_per_hour": 0,
        "compute_ms_per_cycle": 0,
        "memory_mb": 0.0,
    },
}

# System directories for dependency analysis
_SYSTEM_DIRS: dict[str, str] = {
    "memory": "src/ecodiaos/systems/memory",
    "equor": "src/ecodiaos/systems/equor",
    "atune": "src/ecodiaos/systems/atune",
    "voxis": "src/ecodiaos/systems/voxis",
    "nova": "src/ecodiaos/systems/nova",
    "axon": "src/ecodiaos/systems/axon",
    "evo": "src/ecodiaos/systems/evo",
    "simula": "src/ecodiaos/systems/simula",
}


class ChangeSimulator:
    """
    Deep multi-strategy impact simulator. Combines category-specific
    validation with counterfactual replay, dependency analysis, resource
    estimation, and constitutional alignment prediction.

    All strategies run concurrently where possible (asyncio.gather)
    to stay within the 30s latency target.
    """

    def __init__(
        self,
        config: SimulaConfig,
        llm: LLMProvider,
        memory: MemoryService | None = None,
        analytics: EvolutionAnalyticsEngine | None = None,
        codebase_root: Path | None = None,
    ) -> None:
        self._config = config
        self._llm = llm
        self._memory = memory
        self._analytics = analytics
        self._root = codebase_root or Path(config.codebase_root).resolve()
        self._log = logger
        # Optimization: detect optimized provider for budget checks + cache tagging
        self._optimized = isinstance(llm, OptimizedLLMProvider)
        # KVzip compression for large counterfactual replay prompts
        self._compressor = ContextCompressor(
            prune_ratio=config.kv_compression_ratio,
            enabled=config.kv_compression_enabled,
        )

    async def simulate(self, proposal: EvolutionProposal) -> EnrichedSimulationResult:
        """
        Main simulation entry point. Runs category-specific validation
        plus cross-cutting deep analysis, then synthesizes a unified
        risk assessment.

        All independent analyses run concurrently via asyncio.gather.
        """
        self._log.info(
            "deep_simulation_started",
            proposal_id=proposal.id,
            category=proposal.category.value,
        )

        # Forbidden categories are rejected before reaching simulation,
        # but defend in depth
        from ecodiaos.systems.simula.types import FORBIDDEN
        if proposal.category in FORBIDDEN:
            return EnrichedSimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=f"Category {proposal.category.value} is forbidden.",
            )

        # Run all strategies concurrently
        base_task = self._simulate_by_category(proposal)
        counterfactual_task = self._counterfactual_replay(proposal)
        dependency_task = self._analyze_dependencies(proposal)
        alignment_task = self._predict_constitutional_alignment(proposal)

        base_result, counterfactuals, dependency_impacts, alignment = await asyncio.gather(
            base_task,
            counterfactual_task,
            dependency_task,
            alignment_task,
            return_exceptions=True,
        )

        # Handle exceptions gracefully -- individual strategy failures
        # should not prevent the simulation from completing
        if isinstance(base_result, BaseException):
            self._log.error("base_simulation_failed", error=str(base_result))
            base_result = SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary=f"Base simulation failed: {base_result}",
            )
        if isinstance(counterfactuals, BaseException):
            self._log.warning("counterfactual_replay_failed", error=str(counterfactuals))
            counterfactuals = []
        if isinstance(dependency_impacts, BaseException):
            self._log.warning("dependency_analysis_failed", error=str(dependency_impacts))
            dependency_impacts = []
        if isinstance(alignment, BaseException):
            self._log.warning("alignment_prediction_failed", error=str(alignment))
            alignment = 0.0

        # Resource cost estimation (pure heuristic, synchronous)
        cost_estimate = self._estimate_resource_cost(proposal)

        # Synthesize all signals
        result = self._synthesize_risk(
            base_result=base_result,
            counterfactuals=counterfactuals,
            dependency_impacts=dependency_impacts,
            cost_estimate=cost_estimate,
            constitutional_alignment=alignment,
            proposal=proposal,
        )

        self._log.info(
            "deep_simulation_complete",
            proposal_id=proposal.id,
            risk_level=result.risk_level.value,
            counterfactuals=len(result.counterfactuals),
            dependency_blast_radius=result.dependency_blast_radius,
            constitutional_alignment=round(result.constitutional_alignment, 2),
            episodes_tested=result.episodes_tested,
        )
        return result

    # ─── Category-Specific Simulation ────────────────────────────────────────

    async def _simulate_by_category(self, proposal: EvolutionProposal) -> SimulationResult:
        """Dispatch to the right category-specific strategy."""
        if proposal.category in SELF_APPLICABLE:
            if proposal.category == ChangeCategory.ADJUST_BUDGET:
                return await self._simulate_budget(proposal)
            else:
                return await self._simulate_additive(proposal)
        elif proposal.category in GOVERNANCE_REQUIRED:
            return await self._simulate_governance(proposal)
        else:
            return SimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=f"Category {proposal.category.value} is forbidden.",
            )

    async def _simulate_additive(self, proposal: EvolutionProposal) -> SimulationResult:
        """
        Enhanced static analysis for additive changes.
        Beyond name validation: checks naming conventions, system existence,
        existing overlap detection, and spec completeness.
        """
        spec = proposal.change_spec
        issues: list[str] = []

        # Determine the relevant name and validate by category
        name: str | None = None

        if proposal.category == ChangeCategory.ADD_EXECUTOR:
            name = spec.executor_name
            if not spec.executor_action_type:
                issues.append("executor_action_type is required")
            if name and not _SNAKE_CASE.match(name):
                issues.append(
                    f"Executor module name {name!r} should be snake_case "
                    f"(e.g., 'email_sender', not 'EmailSender')"
                )
            # Check if executor with this action_type already exists
            if spec.executor_action_type:
                existing = await self._check_existing_executor(spec.executor_action_type)
                if existing:
                    issues.append(
                        f"Executor for action_type {spec.executor_action_type!r} "
                        f"already exists: {existing}"
                    )
            # Verify the axon executors directory exists
            executors_dir = self._root / "src" / "ecodiaos" / "systems" / "axon" / "executors"
            if not executors_dir.exists():
                issues.append("Axon executors directory not found -- system may not be built yet")

        elif proposal.category == ChangeCategory.ADD_INPUT_CHANNEL:
            name = spec.channel_name
            if not spec.channel_type:
                issues.append("channel_type is required")
            if name and not _SNAKE_CASE.match(name):
                issues.append(f"Channel module name {name!r} should be snake_case")

        elif proposal.category == ChangeCategory.ADD_PATTERN_DETECTOR:
            name = spec.detector_name
            if not spec.detector_pattern_type:
                issues.append("detector_pattern_type is required")
            if name and not _PASCAL_CASE.match(name):
                issues.append(
                    f"Detector class name {name!r} should be PascalCase "
                    f"(e.g., 'FrequencyDetector')"
                )

        if name is None:
            issues.append("No name provided for additive change")
        elif not _VALID_NAME.match(name):
            issues.append(f"Name {name!r} is not a valid Python identifier")

        if issues:
            return SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary="Spec validation failed: " + "; ".join(issues),
                benefit_summary=proposal.expected_benefit,
            )

        return SimulationResult(
            episodes_tested=0,
            risk_level=RiskLevel.LOW,
            risk_summary="Additive change passes enhanced static analysis.",
            benefit_summary=proposal.expected_benefit,
        )

    async def _simulate_budget(self, proposal: EvolutionProposal) -> SimulationResult:
        """Validate budget parameter range and assess risk magnitude."""
        from ecodiaos.systems.evo.types import TUNABLE_PARAMETERS

        spec = proposal.change_spec
        if not spec.budget_parameter:
            return SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary="ADJUST_BUDGET proposal missing budget_parameter.",
            )
        if spec.budget_parameter not in TUNABLE_PARAMETERS:
            return SimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=f"Parameter {spec.budget_parameter!r} is not in TUNABLE_PARAMETERS.",
            )

        param_spec = TUNABLE_PARAMETERS[spec.budget_parameter]
        new_val = spec.budget_new_value
        old_val = spec.budget_old_value

        if new_val is None:
            return SimulationResult(
                risk_level=RiskLevel.HIGH,
                risk_summary="ADJUST_BUDGET proposal missing budget_new_value.",
            )
        if new_val < param_spec.min_val or new_val > param_spec.max_val:
            return SimulationResult(
                risk_level=RiskLevel.UNACCEPTABLE,
                risk_summary=(
                    f"{spec.budget_parameter} new value {new_val} is outside allowed range "
                    f"[{param_spec.min_val}, {param_spec.max_val}]."
                ),
            )

        delta = abs(new_val - (old_val or 0.0))
        risk = RiskLevel.MODERATE if delta > 0.05 else RiskLevel.LOW
        return SimulationResult(
            episodes_tested=0,
            risk_level=risk,
            risk_summary=(
                f"{spec.budget_parameter}: {old_val} -> {new_val} "
                f"(delta={delta:.4f}). Risk: {risk.value}."
            ),
            benefit_summary=proposal.expected_benefit,
        )

    async def _simulate_governance(self, proposal: EvolutionProposal) -> SimulationResult:
        """
        LLM-based impact assessment for governance-required changes.
        Retrieves up to 30 recent episode summaries and asks the LLM
        to reason about the impact with explicit risk dimensions.
        """
        episode_context = ""
        episodes_count = 0

        if self._memory is not None:
            try:
                episodes = await asyncio.wait_for(
                    self._memory.retrieve_recent_episodes(limit=30),  # type: ignore[attr-defined]
                    timeout=5.0,
                )
                episodes_count = len(episodes)
                episode_context = self._build_episode_context(episodes)
            except Exception as exc:
                self._log.warning("episode_fetch_failed", error=str(exc))

        # Build an explicit multi-dimension assessment prompt
        prompt = (
            "You are evaluating a proposed structural change to EcodiaOS, "
            "a computational cognitive architecture.\n\n"
            f"PROPOSAL\n"
            f"Category: {proposal.category.value}\n"
            f"Description: {proposal.description}\n"
            f"Expected benefit: {proposal.expected_benefit}\n"
            f"Affected systems: {', '.join(proposal.change_spec.affected_systems) or 'unspecified'}\n\n"
            f"RECENT EPISODE CONTEXT ({episodes_count} episodes):\n{episode_context}\n\n"
            "Assess this change across four dimensions:\n"
            "1. BEHAVIORAL_RISK: Would existing behaviors regress? (LOW/MODERATE/HIGH)\n"
            "2. INTEGRATION_RISK: Could this break inter-system contracts? (LOW/MODERATE/HIGH)\n"
            "3. RESOURCE_RISK: Would this significantly increase resource consumption? (LOW/MODERATE/HIGH)\n"
            "4. REVERSIBILITY: How easy is rollback? (EASY/MODERATE/HARD)\n\n"
            "Reply with:\n"
            "RISK: <overall level: LOW|MODERATE|HIGH>\n"
            "BEHAVIORAL: <level>\n"
            "INTEGRATION: <level>\n"
            "RESOURCE: <level>\n"
            "REVERSIBILITY: <level>\n"
            "REASONING: <2-3 sentences>\n"
            "BENEFIT: <1 sentence>"
        )

        # Budget gate: simulation is STANDARD priority — skip in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.simulation", estimated_tokens=400):
                self._log.info("governance_simulation_skipped_budget", proposal_id=proposal.id)
                return SimulationResult(
                    episodes_tested=episodes_count,
                    risk_level=RiskLevel.HIGH,
                    risk_summary="LLM budget exhausted (RED tier) — defaulting to HIGH risk.",
                    benefit_summary=proposal.expected_benefit,
                )

        try:
            if self._optimized:
                response = await asyncio.wait_for(
                    self._llm.evaluate(  # type: ignore[call-arg]
                        prompt=prompt, max_tokens=400, temperature=0.2,
                        cache_system="simula.simulation", cache_method="governance_impact",
                    ),
                    timeout=10.0,
                )
            else:
                response = await asyncio.wait_for(
                    self._llm.evaluate(prompt=prompt, max_tokens=400, temperature=0.2),
                    timeout=10.0,
                )
            risk_level, risk_summary, benefit_summary = self._parse_llm_risk(response.text)
        except TimeoutError:
            self._log.warning("simulation_llm_timeout", proposal_id=proposal.id)
            risk_level = RiskLevel.HIGH
            risk_summary = "LLM assessment timed out; defaulting to HIGH risk."
            benefit_summary = proposal.expected_benefit
        except Exception as exc:
            self._log.error("simulation_llm_error", error=str(exc))
            risk_level = RiskLevel.HIGH
            risk_summary = f"LLM assessment failed: {exc}"
            benefit_summary = proposal.expected_benefit

        return SimulationResult(
            episodes_tested=episodes_count,
            risk_level=risk_level,
            risk_summary=risk_summary,
            benefit_summary=benefit_summary,
        )

    # ─── Counterfactual Episode Replay ───────────────────────────────────────

    async def _counterfactual_replay(
        self, proposal: EvolutionProposal,
    ) -> list[CounterfactualResult]:
        """
        For additive changes, ask: 'If this had existed during recent episodes,
        when would it have been invoked? What would have changed?'

        Token-efficient: batches up to 30 episodes into a single LLM call
        with structured output (~800 tokens total).

        Returns empty list for non-additive changes or when Memory is unavailable.
        """
        # Only meaningful for additive changes
        if proposal.category not in {
            ChangeCategory.ADD_EXECUTOR,
            ChangeCategory.ADD_INPUT_CHANNEL,
            ChangeCategory.ADD_PATTERN_DETECTOR,
        }:
            return []

        if self._memory is None:
            return []

        # Retrieve recent episodes
        try:
            episodes = await asyncio.wait_for(
                self._memory.retrieve_recent_episodes(limit=30),  # type: ignore[attr-defined]
                timeout=5.0,
            )
        except Exception as exc:
            self._log.warning("counterfactual_episode_fetch_failed", error=str(exc))
            return []

        if not episodes:
            return []

        # Build the batch counterfactual prompt
        episode_summaries = []
        for i, ep in enumerate(episodes[:30], start=1):
            summary = getattr(ep, "summary", "") or getattr(ep, "raw_content", "")[:150]
            source = getattr(ep, "source", "unknown")
            episode_summaries.append(f"{i}. [{source}] {summary[:150]}")

        change_desc = self._describe_additive_change(proposal)

        # KVzip: compress episode summaries when batch is large to reduce tokens.
        # Truncate individual episode summaries more aggressively if many episodes.
        max_summary_chars = 150 if len(episode_summaries) <= 15 else 80
        if max_summary_chars < 150:
            episode_summaries = [s[:max_summary_chars] for s in episode_summaries]

        prompt = (
            f"EcodiaOS is considering adding a new capability:\n{change_desc}\n\n"
            f"Below are {len(episode_summaries)} recent episodes. For each, determine:\n"
            f"- Would this new capability have been triggered/relevant? (yes/no)\n"
            f"- If yes, what would have been different? (improvement/regression/neutral)\n\n"
            f"EPISODES:\n" + "\n".join(episode_summaries) + "\n\n"
            "Reply as a numbered list matching the episode numbers:\n"
            "<number>. <yes|no> | <improvement|regression|neutral> | <1 sentence reason>\n"
            "Only include episodes where the answer is 'yes'."
        )

        # Budget gate: skip counterfactual replay in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.simulation", estimated_tokens=500):
                self._log.info("counterfactual_replay_skipped_budget")
                return []

        try:
            if self._optimized:
                response = await asyncio.wait_for(
                    self._llm.evaluate(  # type: ignore[call-arg]
                        prompt=prompt, max_tokens=500, temperature=0.2,
                        cache_system="simula.simulation", cache_method="counterfactual",
                    ),
                    timeout=15.0,
                )
            else:
                response = await asyncio.wait_for(
                    self._llm.evaluate(prompt=prompt, max_tokens=500, temperature=0.2),
                    timeout=15.0,
                )
            return self._parse_counterfactual_response(response.text, episodes)
        except Exception as exc:
            self._log.warning("counterfactual_llm_failed", error=str(exc))
            return []

    def _describe_additive_change(self, proposal: EvolutionProposal) -> str:
        """Human-readable description of an additive change for counterfactual prompt."""
        spec = proposal.change_spec
        if proposal.category == ChangeCategory.ADD_EXECUTOR:
            return (
                f"New Axon Executor: {spec.executor_name or 'unnamed'}\n"
                f"Action type: {spec.executor_action_type or 'unspecified'}\n"
                f"Description: {spec.executor_description or proposal.description}"
            )
        elif proposal.category == ChangeCategory.ADD_INPUT_CHANNEL:
            return (
                f"New Atune Input Channel: {spec.channel_name or 'unnamed'}\n"
                f"Channel type: {spec.channel_type or 'unspecified'}\n"
                f"Description: {spec.channel_description or proposal.description}"
            )
        elif proposal.category == ChangeCategory.ADD_PATTERN_DETECTOR:
            return (
                f"New Evo Pattern Detector: {spec.detector_name or 'unnamed'}\n"
                f"Pattern type: {spec.detector_pattern_type or 'unspecified'}\n"
                f"Description: {spec.detector_description or proposal.description}"
            )
        return proposal.description

    def _parse_counterfactual_response(
        self, text: str, episodes: list[Any],
    ) -> list[CounterfactualResult]:
        """Parse the LLM's batch counterfactual response into structured results."""
        results: list[CounterfactualResult] = []
        for line in text.strip().splitlines():
            line = line.strip()
            if not line or not line[0].isdigit():
                continue
            try:
                # Expected: "3. yes | improvement | Would have handled email notifications"
                num_part, rest = line.split(".", 1)
                idx = int(num_part.strip()) - 1
                if idx < 0 or idx >= len(episodes):
                    continue

                parts = [p.strip() for p in rest.split("|")]
                if len(parts) < 2:
                    continue

                triggered = parts[0].lower().strip() in ("yes", "y", "true")
                if not triggered:
                    continue

                impact_str = parts[1].lower().strip() if len(parts) > 1 else "neutral"
                if "improvement" in impact_str:
                    impact = ImpactType.IMPROVEMENT
                elif "regression" in impact_str:
                    impact = ImpactType.REGRESSION
                else:
                    impact = ImpactType.NEUTRAL

                reasoning = parts[2].strip() if len(parts) > 2 else ""

                ep = episodes[idx]
                results.append(CounterfactualResult(
                    episode_id=getattr(ep, "id", f"ep_{idx}"),
                    would_have_triggered=True,
                    predicted_outcome=reasoning[:200],
                    impact=impact,
                    confidence=0.6,
                    reasoning=reasoning[:300],
                ))
            except (ValueError, IndexError):
                continue

        return results

    # ─── Dependency Graph Analysis ───────────────────────────────────────────

    async def _analyze_dependencies(
        self, proposal: EvolutionProposal,
    ) -> list[DependencyImpact]:
        """
        Static analysis of the affected system's import graph.
        Uses the ast module to parse Python files and trace imports.
        Zero LLM tokens -- pure computation.
        """
        affected_systems = proposal.change_spec.affected_systems
        if not affected_systems:
            affected_systems = self._infer_affected_systems(proposal)

        impacts: list[DependencyImpact] = []

        for sys_name in affected_systems:
            sys_dir = self._root / _SYSTEM_DIRS.get(sys_name, f"src/ecodiaos/systems/{sys_name}")
            if not sys_dir.exists():
                continue

            # Find all Python files in the affected system
            py_files = list(sys_dir.rglob("*.py"))

            # For each file, find what imports it from other systems
            for py_file in py_files:
                rel_path = str(py_file.relative_to(self._root))
                module_name = self._path_to_module(rel_path)
                if not module_name:
                    continue

                # Check how many other files import this module
                importers = await self._find_importers(module_name)
                if importers:
                    impacts.append(DependencyImpact(
                        file_path=rel_path,
                        impact_type="import_dependency",
                        risk_contribution=min(1.0, len(importers) * 0.1),
                    ))

            # Check for test coverage
            test_dir = self._root / "tests" / "unit" / "systems" / sys_name
            if test_dir.exists():
                test_files = list(test_dir.rglob("*.py"))
                impacts.append(DependencyImpact(
                    file_path=str(test_dir.relative_to(self._root)),
                    impact_type="test_coverage",
                    risk_contribution=0.0 if test_files else 0.3,
                ))

        return impacts

    def _infer_affected_systems(self, proposal: EvolutionProposal) -> list[str]:
        """Infer which systems a change affects from the category."""
        category_to_systems: dict[ChangeCategory, list[str]] = {
            ChangeCategory.ADD_EXECUTOR: ["axon"],
            ChangeCategory.ADD_INPUT_CHANNEL: ["atune"],
            ChangeCategory.ADD_PATTERN_DETECTOR: ["evo"],
            ChangeCategory.ADJUST_BUDGET: [],
            ChangeCategory.MODIFY_CONTRACT: [],
            ChangeCategory.ADD_SYSTEM_CAPABILITY: [],
            ChangeCategory.MODIFY_CYCLE_TIMING: ["synapse"],
            ChangeCategory.CHANGE_CONSOLIDATION: ["evo"],
        }
        return category_to_systems.get(proposal.category, [])

    async def _find_importers(self, module_name: str) -> list[str]:
        """Find files that import the given module. Scans src/ directory."""
        importers: list[str] = []
        src_dir = self._root / "src"
        if not src_dir.exists():
            return importers

        # Extract the short module name for import matching
        parts = module_name.split(".")
        parts[-1] if parts else module_name

        for py_file in src_dir.rglob("*.py"):
            try:
                source = py_file.read_text(encoding="utf-8")
                tree = ast.parse(source, filename=str(py_file))
            except Exception:
                continue

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        if module_name in alias.name:
                            importers.append(str(py_file.relative_to(self._root)))
                            break
                elif isinstance(node, ast.ImportFrom):
                    if node.module and module_name in node.module:
                        importers.append(str(py_file.relative_to(self._root)))

        return importers

    def _path_to_module(self, rel_path: str) -> str | None:
        """Convert a relative file path to a dotted module name."""
        parts = rel_path.replace("\\", "/").split("/")
        if parts and parts[0] == "src":
            parts = parts[1:]
        if not parts:
            return None
        if parts[-1].endswith(".py"):
            parts[-1] = parts[-1][:-3]
        if parts[-1] == "__init__":
            parts = parts[:-1]
        return ".".join(parts) if parts else None

    # ─── Resource Cost Estimation ────────────────────────────────────────────

    def _estimate_resource_cost(
        self, proposal: EvolutionProposal,
    ) -> ResourceCostEstimate:
        """
        Heuristic estimation of ongoing resource impact.
        Zero LLM tokens -- pure lookup + arithmetic.
        """
        heuristics = _RESOURCE_COST_HEURISTICS.get(proposal.category)
        if heuristics is None:
            # Governance-required changes: estimate moderate cost
            return ResourceCostEstimate(
                estimated_additional_llm_tokens_per_hour=1000,
                estimated_additional_compute_ms_per_cycle=10,
                estimated_memory_mb=5.0,
                budget_headroom_percent=90.0,
            )

        tokens = int(heuristics.get("llm_tokens_per_hour", 0))
        compute = int(heuristics.get("compute_ms_per_cycle", 0))
        memory = float(heuristics.get("memory_mb", 0.0))

        # Budget headroom: what percent of the relevant system's budget remains
        # after adding this cost
        system_budget = self._get_system_budget(proposal)
        headroom = 100.0
        if system_budget > 0 and tokens > 0:
            headroom = max(0.0, 100.0 * (1.0 - tokens / system_budget))

        return ResourceCostEstimate(
            estimated_additional_llm_tokens_per_hour=tokens,
            estimated_additional_compute_ms_per_cycle=compute,
            estimated_memory_mb=memory,
            budget_headroom_percent=round(headroom, 1),
        )

    def _get_system_budget(self, proposal: EvolutionProposal) -> int:
        """Get the affected system's hourly token budget."""
        category_to_system: dict[ChangeCategory, str] = {
            ChangeCategory.ADD_EXECUTOR: "axon",
            ChangeCategory.ADD_INPUT_CHANNEL: "atune",
            ChangeCategory.ADD_PATTERN_DETECTOR: "evo",
        }
        sys_name = category_to_system.get(proposal.category, "")
        # Default system budgets (from config/default.yaml)
        default_budgets: dict[str, int] = {
            "atune": 60000,
            "equor": 30000,
            "nova": 120000,
            "voxis": 120000,
            "evo": 60000,
            "axon": 60000,
            "simula": 10000,
        }
        return default_budgets.get(sys_name, 60000)

    # ─── Constitutional Alignment Prediction ─────────────────────────────────

    async def _predict_constitutional_alignment(
        self, proposal: EvolutionProposal,
    ) -> float:
        """
        Predict how well this change aligns with the four constitutional drives.
        Single LLM call, 100 tokens max output. Returns -1.0 to 1.0.

        Budget: ~200 tokens total (prompt + response).
        """
        prompt = (
            "EcodiaOS has four constitutional drives: "
            "coherence (make sense), care (orient toward wellbeing), "
            "growth (become more capable), honesty (represent reality truthfully).\n\n"
            f"Proposed change: {proposal.description[:200]}\n"
            f"Category: {proposal.category.value}\n"
            f"Expected benefit: {proposal.expected_benefit[:100]}\n\n"
            "Score the alignment of this change with the drives from -1.0 to 1.0.\n"
            "Reply with a single number only (e.g., 0.7)."
        )

        # Budget gate: skip alignment prediction in RED tier
        if self._optimized:
            assert isinstance(self._llm, OptimizedLLMProvider)
            if not self._llm.should_use_llm("simula.simulation", estimated_tokens=100):
                self._log.info("alignment_prediction_skipped_budget")
                return 0.0

        try:
            if self._optimized:
                response = await asyncio.wait_for(
                    self._llm.evaluate(  # type: ignore[call-arg]
                        prompt=prompt, max_tokens=20, temperature=0.1,
                        cache_system="simula.simulation", cache_method="constitutional_alignment",
                    ),
                    timeout=5.0,
                )
            else:
                response = await asyncio.wait_for(
                    self._llm.evaluate(prompt=prompt, max_tokens=20, temperature=0.1),
                    timeout=5.0,
                )
            # Extract the float from the response
            text = response.text.strip()
            # Handle potential formatting like "0.7" or "Score: 0.7"
            for token in text.split():
                try:
                    score = float(token.strip(".,;:"))
                    return max(-1.0, min(1.0, score))
                except ValueError:
                    continue
            return 0.0
        except Exception as exc:
            self._log.warning("alignment_prediction_failed", error=str(exc))
            return 0.0

    # ─── Risk Synthesis ──────────────────────────────────────────────────────

    def _synthesize_risk(
        self,
        base_result: SimulationResult,
        counterfactuals: list[CounterfactualResult],
        dependency_impacts: list[DependencyImpact],
        cost_estimate: ResourceCostEstimate,
        constitutional_alignment: float,
        proposal: EvolutionProposal,
    ) -> EnrichedSimulationResult:
        """
        Combine all simulation signals into a unified risk assessment.

        Risk factors (weighted):
          - Base category simulation: 40%
          - Counterfactual regression rate: 20%
          - Dependency blast radius: 15%
          - Resource cost: 10%
          - Constitutional alignment: 15% (negative alignment increases risk)

        Dynamic adjustment: if analytics show high rollback rate for this
        category, bump the risk level up one notch.
        """
        # Counterfactual regression rate
        cf_regressions = sum(1 for cf in counterfactuals if cf.impact == ImpactType.REGRESSION)
        cf_total = len(counterfactuals) if counterfactuals else 1
        cf_regression_rate = cf_regressions / max(1, cf_total)

        # Dependency blast radius
        blast_radius = len(dependency_impacts)
        total_risk_contribution = sum(d.risk_contribution for d in dependency_impacts)

        # Resource risk (0-1 scale based on budget consumption)
        resource_risk = 1.0 - (cost_estimate.budget_headroom_percent / 100.0) if cost_estimate else 0.0

        # Constitutional risk (alignment < 0 adds risk)
        alignment_risk = max(0.0, -constitutional_alignment)

        # Base risk as numeric
        base_risk_numeric = {
            RiskLevel.LOW: 0.1,
            RiskLevel.MODERATE: 0.4,
            RiskLevel.HIGH: 0.7,
            RiskLevel.UNACCEPTABLE: 1.0,
        }.get(base_result.risk_level, 0.4)

        # Weighted composite risk score (0.0 - 1.0)
        composite_risk = (
            0.40 * base_risk_numeric
            + 0.20 * cf_regression_rate
            + 0.15 * min(1.0, total_risk_contribution)
            + 0.10 * resource_risk
            + 0.15 * alignment_risk
        )

        # Dynamic caution adjustment from analytics history
        caution_adj: CautionAdjustment | None = None
        if self._analytics is not None:
            caution_adj = self._analytics.should_increase_caution(proposal.category)
            if caution_adj.should_adjust:
                composite_risk = min(1.0, composite_risk + caution_adj.magnitude)
                self._log.info(
                    "caution_increased",
                    category=proposal.category.value,
                    composite_risk=round(composite_risk, 3),
                    magnitude=caution_adj.magnitude,
                    factors=caution_adj.factors,
                    reasoning=caution_adj.reasoning,
                )

        # Map composite score to RiskLevel
        if composite_risk >= 0.75:
            final_risk = RiskLevel.UNACCEPTABLE
        elif composite_risk >= 0.50:
            final_risk = RiskLevel.HIGH
        elif composite_risk >= 0.25:
            final_risk = RiskLevel.MODERATE
        else:
            final_risk = RiskLevel.LOW

        # Emit decision audit log with all signal values and weights
        self._log.info(
            "simulation_decision_audit",
            proposal_id=proposal.id,
            category=proposal.category.value,
            base_risk=f"{0.40 * base_risk_numeric:.3f} (0.40×{base_risk_numeric:.2f})",
            counterfactual_risk=f"{0.20 * cf_regression_rate:.3f} (0.20×{cf_regression_rate:.2f})",
            dependency_risk=f"{0.15 * min(1.0, total_risk_contribution):.3f} (0.15×{total_risk_contribution:.2f})",
            resource_risk=f"{0.10 * resource_risk:.3f} (0.10×{resource_risk:.2f})",
            alignment_risk=f"{0.15 * alignment_risk:.3f} (0.15×{alignment_risk:.2f})",
            weighted_sum=round(composite_risk, 3),
            caution_adjustment=caution_adj.magnitude if caution_adj and caution_adj.should_adjust else 0.0,
            final_risk=final_risk.value,
            episodes_tested=base_result.episodes_tested,
            blast_radius=blast_radius,
            constitutional_alignment=round(constitutional_alignment, 2),
        )

        # Build summary
        summary_parts = [base_result.risk_summary]
        if counterfactuals:
            cf_improvements = sum(1 for cf in counterfactuals if cf.impact == ImpactType.IMPROVEMENT)
            summary_parts.append(
                f"Counterfactual: {cf_improvements} improvements, "
                f"{cf_regressions} regressions across {len(counterfactuals)} triggered episodes."
            )
        if blast_radius > 0:
            summary_parts.append(f"Blast radius: {blast_radius} affected files/modules.")
        if constitutional_alignment != 0.0:
            summary_parts.append(f"Constitutional alignment: {constitutional_alignment:+.2f}.")

        return EnrichedSimulationResult(
            episodes_tested=base_result.episodes_tested,
            differences=base_result.differences,
            improvements=base_result.improvements + sum(
                1 for cf in counterfactuals if cf.impact == ImpactType.IMPROVEMENT
            ),
            regressions=base_result.regressions + cf_regressions,
            neutral_changes=base_result.neutral_changes + sum(
                1 for cf in counterfactuals if cf.impact == ImpactType.NEUTRAL
            ),
            risk_level=final_risk,
            risk_summary=" ".join(summary_parts),
            benefit_summary=base_result.benefit_summary,
            counterfactuals=counterfactuals,
            dependency_impacts=dependency_impacts,
            resource_cost_estimate=cost_estimate,
            constitutional_alignment=constitutional_alignment,
            counterfactual_regression_rate=round(cf_regression_rate, 3),
            dependency_blast_radius=blast_radius,
            caution_adjustment=caution_adj,
        )

    # ─── Helpers ─────────────────────────────────────────────────────────────

    async def _check_existing_executor(self, action_type: str) -> str | None:
        """Check if an executor for this action_type already exists."""
        executors_dir = self._root / "src" / "ecodiaos" / "systems" / "axon" / "executors"
        if not executors_dir.exists():
            return None

        for py_file in executors_dir.glob("*.py"):
            if py_file.name == "__init__.py":
                continue
            try:
                source = py_file.read_text(encoding="utf-8")
                if f'"{action_type}"' in source or f"'{action_type}'" in source:
                    return str(py_file.name)
            except Exception:
                continue
        return None

    async def _check_name_conflict(self, name: str, category: ChangeCategory) -> bool:
        """Returns True if the name would cause a conflict."""
        return bool(not _VALID_NAME.match(name))

    def _build_episode_context(self, episodes: list[Any]) -> str:
        """Build concise context string from episode objects."""
        lines: list[str] = []
        for i, ep in enumerate(episodes[:30], start=1):
            summary = getattr(ep, "summary", "") or getattr(ep, "raw_content", "")[:200]
            source = getattr(ep, "source", "")
            lines.append(f"{i}. [{source}] {summary[:200]}")
        return "\n".join(lines)

    def _parse_llm_risk(self, text: str) -> tuple[RiskLevel, str, str]:
        """Parse the LLM response to extract risk level, reasoning, and benefit."""
        risk_level = RiskLevel.MODERATE
        risk_summary = text[:500]
        benefit_summary = ""

        for line in text.splitlines():
            line = line.strip()
            upper = line.upper()
            if upper.startswith("RISK:"):
                level_str = line.split(":", 1)[-1].strip().upper()
                if level_str == "LOW":
                    risk_level = RiskLevel.LOW
                elif level_str == "MODERATE":
                    risk_level = RiskLevel.MODERATE
                elif level_str == "HIGH":
                    risk_level = RiskLevel.HIGH
                elif level_str == "UNACCEPTABLE":
                    risk_level = RiskLevel.UNACCEPTABLE
            elif upper.startswith("REASONING:"):
                risk_summary = line.split(":", 1)[-1].strip()
            elif upper.startswith("BENEFIT:"):
                benefit_summary = line.split(":", 1)[-1].strip()

        return risk_level, risk_summary, benefit_summary

===== D:\.code\EcodiaOS\backend\ecodiaos\systems\simula\types.py =====

"""
EcodiaOS -- Simula Internal Types

All data types internal to the Simula self-evolution system.
Simula is the organism's capacity for metamorphosis: structural change
beyond parameter tuning. These types model the full lifecycle of an
evolution proposal -- from reception through simulation, governance,
application, and immutable history.
"""

from __future__ import annotations

from datetime import datetime
import enum
from typing import Any

from pydantic import Field

from ecodiaos.primitives.common import (
    EOSBaseModel,
    Identified,
    Timestamped,
    utc_now,
)


# --- Enums -------------------------------------------------------------------


class ChangeCategory(enum.StrEnum):
    ADD_EXECUTOR = "add_executor"
    ADD_INPUT_CHANNEL = "add_input_channel"
    ADD_PATTERN_DETECTOR = "add_pattern_detector"
    ADJUST_BUDGET = "adjust_budget"
    MODIFY_CONTRACT = "modify_contract"
    ADD_SYSTEM_CAPABILITY = "add_system_capability"
    MODIFY_CYCLE_TIMING = "modify_cycle_timing"
    CHANGE_CONSOLIDATION = "change_consolidation"
    MODIFY_EQUOR = "modify_equor"
    MODIFY_CONSTITUTION = "modify_constitution"
    MODIFY_INVARIANTS = "modify_invariants"
    MODIFY_SELF_EVOLUTION = "modify_self_evolution"


class ProposalStatus(enum.StrEnum):
    PROPOSED = "proposed"
    SIMULATING = "simulating"
    AWAITING_GOVERNANCE = "awaiting_governance"
    APPROVED = "approved"
    APPLYING = "applying"
    APPLIED = "applied"
    ROLLED_BACK = "rolled_back"
    REJECTED = "rejected"


class RiskLevel(enum.StrEnum):
    LOW = "low"
    MODERATE = "moderate"
    HIGH = "high"
    UNACCEPTABLE = "unacceptable"


class ImpactType(enum.StrEnum):
    IMPROVEMENT = "improvement"
    REGRESSION = "regression"
    NEUTRAL = "neutral"


class TriageStatus(enum.StrEnum):
    """Status of proposal triage (fast-path pre-simulation check)."""
    TRIVIAL = "trivial"
    REQUIRES_SIMULATION = "requires_simulation"


# --- Models -----------------------------------------------------------------


class ChangeSpec(EOSBaseModel):
    """
    Formal specification of what to change.
    One model covers every ChangeCategory -- fields are optional by category.
    """

    # ADD_EXECUTOR
    executor_name: str | None = None
    executor_description: str | None = None
    executor_action_type: str | None = None
    executor_input_schema: dict[str, Any] | None = None

    # ADD_INPUT_CHANNEL
    channel_name: str | None = None
    channel_type: str | None = None
    channel_description: str | None = None

    # ADD_PATTERN_DETECTOR
    detector_name: str | None = None
    detector_description: str | None = None
    detector_pattern_type: str | None = None

    # ADJUST_BUDGET
    budget_parameter: str | None = None
    budget_old_value: float | None = None
    budget_new_value: float | None = None

    # MODIFY_CONTRACT
    contract_changes: list[str] = Field(default_factory=list)

    # ADD_SYSTEM_CAPABILITY
    capability_description: str | None = None

    # MODIFY_CYCLE_TIMING
    timing_parameter: str | None = None
    timing_old_value: float | None = None
    timing_new_value: float | None = None

    # CHANGE_CONSOLIDATION
    consolidation_schedule: str | None = None

    # Cross-cutting
    affected_systems: list[str] = Field(default_factory=list)
    additional_context: str = ""
    code_hint: str = ""  # optional hint of what the code should look like


class SimulationDifference(EOSBaseModel):
    """Describes how one episode's outcome would differ under the proposed change."""

    episode_id: str
    original_outcome: str
    simulated_outcome: str
    impact: ImpactType
    reasoning: str = ""


class SimulationResult(EOSBaseModel):
    """Aggregate outcome of simulating a proposal against recent episodes."""

    episodes_tested: int = 0
    differences: int = 0
    improvements: int = 0
    regressions: int = 0
    neutral_changes: int = 0
    risk_level: RiskLevel = RiskLevel.LOW
    risk_summary: str = ""
    benefit_summary: str = ""
    simulated_at: datetime = Field(default_factory=utc_now)


class CautionAdjustment(EOSBaseModel):
    """
    Transparent caution adjustment logic explaining WHY a proposal's risk
    was bumped. Returned by EvolutionAnalyticsEngine.should_increase_caution().
    """

    should_adjust: bool
    magnitude: float  # 0.0-0.5 additive risk bump
    factors: dict[str, float] = Field(default_factory=dict)  # {factor_name: contribution}
    reasoning: str = ""


class TriageResult(EOSBaseModel):
    """Result of fast-path proposal triage (pre-simulation check)."""

    status: TriageStatus
    assumed_risk: RiskLevel | None = None
    reason: str = ""
    skip_simulation: bool = False


class ProposalResult(EOSBaseModel):
    """Final outcome recorded once a proposal reaches a terminal state."""

    status: ProposalStatus
    reason: str = ""
    version: int | None = None
    governance_record_id: str | None = None
    files_changed: list[str] = Field(default_factory=list)


class EvolutionProposal(Identified, Timestamped):
    """
    The full proposal lifecycle object -- richer than Evo's simplified version.
    Owns the proposal from receipt through simulation, governance, and application.
    """

    source: str  # "evo" | "governance"
    category: ChangeCategory
    description: str
    change_spec: ChangeSpec
    evidence: list[str] = Field(default_factory=list)  # hypothesis IDs / episode IDs
    expected_benefit: str = ""
    risk_assessment: str = ""
    status: ProposalStatus = ProposalStatus.PROPOSED
    simulation: SimulationResult | None = None
    governance_record_id: str | None = None
    result: ProposalResult | None = None


class FileSnapshot(EOSBaseModel):
    """
    One file's state immediately before a change was applied, enabling rollback.
    content is None when the file did not previously exist -- rollback deletes it.
    """

    path: str  # absolute path
    content: str | None  # None means file did not exist before
    existed: bool = True


class ConfigSnapshot(Identified, Timestamped):
    """Full snapshot of all affected files captured before applying a change."""

    proposal_id: str
    files: list[FileSnapshot] = Field(default_factory=list)
    config_version: int  # the version at snapshot time


class ConfigVersion(EOSBaseModel):
    """Tracks one step in the config version chain."""

    version: int
    timestamp: datetime = Field(default_factory=utc_now)
    proposal_ids: list[str] = Field(default_factory=list)  # evolution proposal IDs
    config_hash: str  # SHA256 hash of the canonical config state


class EvolutionRecord(Identified, Timestamped):
    """Immutable history entry written to Neo4j after each successful application."""

    proposal_id: str
    category: ChangeCategory
    description: str
    from_version: int
    to_version: int
    files_changed: list[str] = Field(default_factory=list)
    simulation_risk: RiskLevel
    applied_at: datetime = Field(default_factory=utc_now)
    rolled_back: bool = False
    rollback_reason: str = ""
    # Simulation detail persisted for audit trail and learning
    simulation_episodes_tested: int = 0
    counterfactual_regression_rate: float = 0.0
    dependency_blast_radius: int = 0
    constitutional_alignment: float = 0.0
    resource_tokens_per_hour: int = 0
    caution_reasoning: str = ""
    # Stage 2: Formal verification metadata
    formal_verification_status: str = ""  # "verified"|"failed"|"skipped"|""
    discovered_invariants_count: int = 0
    dafny_rounds: int = 0
    static_analysis_findings: int = 0
    # Stage 4A: Lean 4 proof metadata
    lean_proof_status: str = ""  # "proved"|"failed"|"timeout"|"skipped"|""
    lean_proof_rounds: int = 0
    lean_proven_lemmas_count: int = 0
    lean_copilot_automation_rate: float = 0.0
    lean_library_lemmas_reused: int = 0
    # Stage 4B: GRPO fine-tuning metadata
    grpo_model_used: str = ""  # "" = base model, else fine-tuned model id
    grpo_ab_group: str = ""  # "base"|"finetuned"|""
    # Stage 4C: Diffusion repair metadata
    diffusion_repair_used: bool = False
    diffusion_repair_status: str = ""  # "repaired"|"partial"|"failed"|"skipped"|""
    diffusion_repair_steps: int = 0
    diffusion_improvement_rate: float = 0.0
    # Stage 5A: Neurosymbolic synthesis metadata
    synthesis_strategy_used: str = ""  # "hysynth"|"sketch_solve"|"chopchop"|"cegis_fallback"|""
    synthesis_status: str = ""  # "synthesized"|"partial"|"failed"|"timeout"|"skipped"|""
    synthesis_speedup_vs_baseline: float = 0.0
    synthesis_candidates_explored: int = 0
    # Stage 5B: Neural repair metadata
    repair_agent_used: bool = False
    repair_agent_status: str = ""  # "repaired"|"partial"|"failed"|"timeout"|"skipped"|"budget_exceeded"|""
    repair_attempts: int = 0
    repair_cost_usd: float = 0.0
    # Stage 5C: Orchestration metadata
    orchestration_used: bool = False
    orchestration_dag_nodes: int = 0
    orchestration_agents_used: int = 0
    orchestration_parallel_stages: int = 0
    # Stage 5D: Causal debugging metadata
    causal_debug_used: bool = False
    causal_root_cause: str = ""
    causal_confidence: float = 0.0
    causal_interventions: int = 0
    # Stage 5E: Issue resolution metadata
    issue_resolution_used: bool = False
    issue_autonomy_level: str = ""  # "lint"|"dependency"|"test_fix"|"logic_bug"|""
    issue_abstained: bool = False
    # Stage 6A: Cryptographic auditability metadata
    hash_chain_hash: str = ""  # SHA-256 chain hash for this record
    hash_chain_position: int = 0  # position in the hash chain
    content_credentials_signed: int = 0  # number of files signed with C2PA
    governance_credential_status: str = ""  # "valid"|"revoked"|"expired"|"unverified"|""
    # Stage 6B: Co-evolution metadata
    coevolution_hard_negatives_mined: int = 0
    coevolution_adversarial_tests: int = 0
    coevolution_bugs_found: int = 0
    # Stage 6C: Formal spec generation metadata
    formal_specs_generated: int = 0
    formal_spec_coverage_percent: float = 0.0
    tla_plus_states_explored: int = 0
    # Stage 6D: E-graph metadata
    egraph_used: bool = False
    egraph_status: str = ""  # "saturated"|"partial"|"timeout"|"failed"|"skipped"|""
    egraph_rules_applied: int = 0
    # Stage 6E: Symbolic execution metadata
    symbolic_execution_used: bool = False
    symbolic_properties_proved: int = 0
    symbolic_counterexamples: int = 0


class CodeChangeResult(EOSBaseModel):
    """What the code agent returns after implementing a structural change."""

    success: bool
    files_written: list[str] = Field(default_factory=list)
    summary: str = ""
    error: str = ""
    lint_passed: bool = True
    tests_passed: bool = True
    test_output: str = ""
    # Stage 1A: Extended-thinking model metrics
    used_extended_thinking: bool = False
    reasoning_tokens: int = 0
    # Stage 1C: KVzip context compression metrics
    kv_compression_ratio: float = 0.0  # 0.0 = no savings, 1.0 = maximum
    kv_messages_compressed: int = 0
    kv_original_tokens: int = 0
    kv_compressed_tokens: int = 0
    # Stage 2C: Static analysis metrics
    static_analysis_findings: int = 0
    static_analysis_fix_iterations: int = 0
    # Stage 2D: AgentCoder metrics
    agent_coder_iterations: int = 0
    test_designer_test_count: int = 0
    # Stage 4B: GRPO model routing metrics
    grpo_model_used: str = ""
    grpo_ab_group: str = ""  # "base"|"finetuned"|""
    # Stage 4C: Diffusion repair metrics
    diffusion_repair_attempted: bool = False
    diffusion_repair_succeeded: bool = False
    # Stage 5A: Synthesis metrics
    synthesis_strategy: str = ""  # "hysynth"|"sketch_solve"|"chopchop"|"cegis_fallback"|""
    synthesis_speedup: float = 0.0
    # Stage 5B: Repair metrics
    repair_attempted: bool = False
    repair_succeeded: bool = False
    repair_cost_usd: float = 0.0
    # Stage 5C: Orchestration metrics
    orchestration_used: bool = False
    orchestration_agents: int = 0


class HealthCheckResult(EOSBaseModel):
    """Result of a post-apply codebase health check."""

    healthy: bool
    issues: list[str] = Field(default_factory=list)
    checked_at: datetime = Field(default_factory=utc_now)
    # Stage 2: Formal verification result (attached when verification runs)
    formal_verification: object | None = None  # FormalVerificationResult
    # Stage 4A: Lean 4 proof verification result (attached when Lean verification runs)
    lean_verification: object | None = None  # LeanVerificationResult
    # Stage 5D: Causal debugging result (attached when causal debug runs)
    causal_diagnosis: object | None = None  # CausalDiagnosis
    # Stage 6: Formal guarantees result (attached when Stage 6 checks run)
    formal_guarantees: object | None = None  # FormalGuaranteesResult


# --- Enriched Simulation Models ----------------------------------------------


class CounterfactualResult(EOSBaseModel):
    """
    Result of asking: 'If this change had existed during episode X,
    what would have been different?'

    Batched into a single LLM call across multiple episodes for
    token efficiency (~800 tokens per 30-episode batch).
    """

    episode_id: str
    would_have_triggered: bool = False
    predicted_outcome: str = ""
    impact: ImpactType = ImpactType.NEUTRAL
    confidence: float = 0.5
    reasoning: str = ""


class DependencyImpact(EOSBaseModel):
    """
    A file or module affected by a proposed change, discovered
    via static import-graph analysis (zero LLM tokens).
    """

    file_path: str
    impact_type: str = "import_dependency"  # "direct_modification" | "import_dependency" | "test_coverage"
    risk_contribution: float = 0.0


class ResourceCostEstimate(EOSBaseModel):
    """
    Heuristic estimation of the ongoing resource cost a change
    would add to the running system. Computed without LLM calls.
    """

    estimated_additional_llm_tokens_per_hour: int = 0
    estimated_additional_compute_ms_per_cycle: int = 0
    estimated_memory_mb: float = 0.0
    budget_headroom_percent: float = 100.0


class EnrichedSimulationResult(SimulationResult):
    """
    Extended simulation result with deep multi-strategy analysis.
    Produced by the upgraded ChangeSimulator, consumed by SimulaService
    for richer risk/benefit decision-making.
    """

    counterfactuals: list[CounterfactualResult] = Field(default_factory=list)
    dependency_impacts: list[DependencyImpact] = Field(default_factory=list)
    resource_cost_estimate: ResourceCostEstimate | None = None
    constitutional_alignment: float = 0.0
    counterfactual_regression_rate: float = 0.0
    dependency_blast_radius: int = 0
    caution_adjustment: CautionAdjustment | None = None


# --- Bridge Models -----------------------------------------------------------


class EvoProposalEnriched(EOSBaseModel):
    """
    Evo proposal enriched with hypothesis evidence and inferred context.
    Produced by EvoSimulaBridge, consumed by SimulaService.translate().
    """

    evo_description: str
    evo_rationale: str
    hypothesis_ids: list[str] = Field(default_factory=list)
    hypothesis_statements: list[str] = Field(default_factory=list)
    evidence_scores: list[float] = Field(default_factory=list)
    supporting_episode_ids: list[str] = Field(default_factory=list)
    mutation_target: str = ""
    mutation_type: str = ""
    inferred_category: ChangeCategory | None = None
    inferred_change_spec: ChangeSpec | None = None


# --- Proposal Intelligence Models --------------------------------------------


class ProposalPriority(EOSBaseModel):
    """
    Priority score for a proposal, enabling intelligent processing order.
    Higher priority_score = process first.

    Formula: evidence_strength * expected_impact / max(0.1, estimated_risk * estimated_cost)
    """

    proposal_id: str
    priority_score: float = 0.0
    evidence_strength: float = 0.0
    expected_impact: float = 0.0
    estimated_risk: float = 0.0
    estimated_cost: float = 0.0
    reasoning: str = ""


class ProposalCluster(EOSBaseModel):
    """
    Group of semantically similar proposals that could be deduplicated.
    Detected via cheap heuristics first, LLM only for ambiguous cases.
    """

    representative_id: str
    member_ids: list[str] = Field(default_factory=list)
    similarity_scores: list[float] = Field(default_factory=list)
    merge_recommendation: str = ""


# --- Analytics Models --------------------------------------------------------


class CategorySuccessRate(EOSBaseModel):
    """Success rate tracking for a specific change category."""

    category: ChangeCategory
    total: int = 0
    approved: int = 0
    rejected: int = 0
    rolled_back: int = 0

    @property
    def success_rate(self) -> float:
        return self.approved / max(1, self.total)

    @property
    def rollback_rate(self) -> float:
        return self.rolled_back / max(1, self.total)


class EvolutionAnalytics(EOSBaseModel):
    """
    Aggregate evolution quality metrics computed from Neo4j history.
    Enables Simula to learn from its own performance over time.
    Zero LLM tokens -- pure computation from stored records.
    """

    category_rates: dict[str, CategorySuccessRate] = Field(default_factory=dict)
    total_proposals: int = 0
    evolution_velocity: float = 0.0  # proposals per day
    mean_simulation_risk: float = 0.0
    rollback_rate: float = 0.0
    recent_rollback_rates: dict[str, float] = Field(default_factory=dict)  # per-category 7-day rate
    last_updated: datetime = Field(default_factory=utc_now)


# --- Constants ---------------------------------------------------------------

SELF_APPLICABLE: frozenset[ChangeCategory] = frozenset({
    ChangeCategory.ADD_EXECUTOR,
    ChangeCategory.ADD_INPUT_CHANNEL,
    ChangeCategory.ADD_PATTERN_DETECTOR,
    ChangeCategory.ADJUST_BUDGET,
})

GOVERNANCE_REQUIRED: frozenset[ChangeCategory] = frozenset({
    ChangeCategory.MODIFY_CONTRACT,
    ChangeCategory.ADD_SYSTEM_CAPABILITY,
    ChangeCategory.MODIFY_CYCLE_TIMING,
    ChangeCategory.CHANGE_CONSOLIDATION,
})

FORBIDDEN: frozenset[ChangeCategory] = frozenset({
    ChangeCategory.MODIFY_EQUOR,
    ChangeCategory.MODIFY_CONSTITUTION,
    ChangeCategory.MODIFY_INVARIANTS,
    ChangeCategory.MODIFY_SELF_EVOLUTION,
})

SIMULA_IRON_RULES: list[str] = [
    "Simula CANNOT modify Equor in any way.",
    "Simula CANNOT modify constitutional drives.",
    "Simula CANNOT modify invariants.",
    "Simula CANNOT modify its own logic (no self-modifying code).",
    "Simula CANNOT bypass governance for governed changes.",
    "Simula CANNOT apply changes without rollback capability.",
    "Simula CANNOT delete evolution history records.",
    "Simula MUST simulate before applying any change.",
    "Simula MUST maintain version continuity -- no identity-breaking changes.",
]

# Paths the code agent is NEVER allowed to write to
FORBIDDEN_WRITE_PATHS: list[str] = [
    "src/ecodiaos/systems/equor",
    "src/ecodiaos/systems/simula",
    "src/ecodiaos/primitives/constitutional.py",
    "src/ecodiaos/primitives/common.py",
    "src/ecodiaos/config.py",
]
