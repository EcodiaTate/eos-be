"""
EcodiaOS -- Simula TestExecutor Agent (Stage 2D)

Runs test files generated by TestDesigner against code generated by
the CodeAgent. Collects structured pass/fail/coverage results.

This is the third agent in the AgentCoder pipeline:
  TestDesigner → Coder → TestExecutor → iterate

The executor:
  1. Writes test files to the codebase (temporary, tracked)
  2. Runs pytest with --json-report for structured output
  3. Optionally runs coverage measurement
  4. Parses results into TestExecutionResult
  5. Cleans up test files if requested
"""

from __future__ import annotations

import asyncio
import json
import time
from typing import TYPE_CHECKING

import structlog

if TYPE_CHECKING:
    from pathlib import Path

from ecodiaos.systems.simula.verification.types import TestExecutionResult

logger = structlog.get_logger().bind(system="simula.agents.test_executor")


class TestExecutorAgent:
    """
    Runs generated tests and collects structured results.

    Writes test files to disk, invokes pytest as a subprocess,
    parses JSON output, and returns a TestExecutionResult.
    """

    def __init__(
        self,
        codebase_root: Path,
        test_timeout_s: float = 60.0,
        coverage_enabled: bool = True,
    ) -> None:
        self._root = codebase_root.resolve()
        self._test_timeout_s = test_timeout_s
        self._coverage_enabled = coverage_enabled
        self._log = logger
        self._written_files: list[Path] = []

    async def execute_tests(
        self,
        test_files: dict[str, str],
        cleanup: bool = False,
    ) -> TestExecutionResult:
        """
        Write test files, run pytest, return structured results.

        Args:
            test_files: Mapping of relative path → file content.
            cleanup: If True, delete written test files after execution.

        Returns:
            TestExecutionResult with pass/fail counts and details.
        """
        start = time.monotonic()
        self._written_files = []

        if not test_files:
            return TestExecutionResult()

        # 1. Write test files to disk
        written_paths = self._write_test_files(test_files)
        if not written_paths:
            return TestExecutionResult(
                raw_output="Error: No test files could be written.",
            )

        try:
            # 2. Run pytest
            result = await self._run_pytest(written_paths)
            result.execution_time_ms = int((time.monotonic() - start) * 1000)

            self._log.info(
                "test_execution_complete",
                passed=result.passed,
                failed=result.failed,
                errors=result.errors,
                total=result.total,
                coverage=result.coverage_percent,
                elapsed_ms=result.execution_time_ms,
            )
            return result

        finally:
            if cleanup:
                self._cleanup_test_files()

    def _write_test_files(
        self, test_files: dict[str, str],
    ) -> list[Path]:
        """Write test files to the codebase. Returns paths of written files."""
        paths: list[Path] = []
        for rel_path, content in test_files.items():
            full_path = self._root / rel_path
            try:
                full_path.parent.mkdir(parents=True, exist_ok=True)
                full_path.write_text(content, encoding="utf-8")
                paths.append(full_path)
                self._written_files.append(full_path)
                self._log.debug("test_file_written", path=rel_path)
            except Exception as exc:
                self._log.warning(
                    "test_file_write_failed",
                    path=rel_path,
                    error=str(exc),
                )
        return paths

    async def _run_pytest(
        self, test_paths: list[Path],
    ) -> TestExecutionResult:
        """
        Run pytest on the given test files with JSON reporting.

        Uses --json-report for structured output parsing.
        Falls back to stdout parsing if json-report is not available.
        """
        # Build the pytest command
        json_report_path = self._root / ".pytest_report.json"
        cmd: list[str] = [
            "python", "-m", "pytest",
            "--tb=short",
            "-q",
            "--json-report",
            f"--json-report-file={json_report_path}",
        ]

        # Add coverage if enabled
        if self._coverage_enabled:
            cmd.extend([
                "--cov=ecodiaos",
                "--cov-report=json",
                "--no-cov-on-fail",
            ])

        # Add test file paths
        cmd.extend(str(p) for p in test_paths)

        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=str(self._root),
            )
            try:
                stdout, stderr = await asyncio.wait_for(
                    proc.communicate(), timeout=self._test_timeout_s,
                )
            except TimeoutError:
                proc.kill()
                await proc.communicate()
                self._log.warning("pytest_timeout", timeout=self._test_timeout_s)
                return TestExecutionResult(
                    raw_output=f"Pytest timed out after {self._test_timeout_s}s",
                )

            stdout_text = stdout.decode("utf-8", errors="replace")
            stderr_text = stderr.decode("utf-8", errors="replace")
            raw_output = stdout_text + ("\n" + stderr_text if stderr_text else "")

            # Try to parse JSON report first
            result = self._parse_json_report(json_report_path, raw_output)

            # If JSON report failed, fall back to stdout parsing
            if result.total == 0 and stdout_text:
                result = self._parse_stdout_output(stdout_text, raw_output)

            # Parse coverage if available
            if self._coverage_enabled:
                result.coverage_percent = self._parse_coverage()

            return result

        except FileNotFoundError:
            self._log.warning("pytest_not_found")
            return TestExecutionResult(
                raw_output="Error: pytest not found. Install with: pip install pytest",
            )
        except Exception as exc:
            self._log.warning("pytest_error", error=str(exc))
            return TestExecutionResult(raw_output=f"Error: {exc}")
        finally:
            # Clean up report files
            try:
                json_report_path.unlink(missing_ok=True)
                cov_file = self._root / "coverage.json"
                cov_file.unlink(missing_ok=True)
            except Exception:
                pass

    def _parse_json_report(
        self, report_path: Path, raw_output: str,
    ) -> TestExecutionResult:
        """Parse pytest-json-report output."""
        if not report_path.is_file():
            return TestExecutionResult(raw_output=raw_output)

        try:
            data = json.loads(report_path.read_text(encoding="utf-8"))
        except (json.JSONDecodeError, OSError):
            return TestExecutionResult(raw_output=raw_output)

        summary = data.get("summary", {})
        passed = summary.get("passed", 0)
        failed = summary.get("failed", 0)
        errors = summary.get("error", 0)
        total = summary.get("total", passed + failed + errors)

        # Extract failure details
        failure_details: list[str] = []
        for test in data.get("tests", []):
            if test.get("outcome") in ("failed", "error"):
                node_id = test.get("nodeid", "unknown")
                call_info = test.get("call", {})
                longrepr = call_info.get("longrepr", "")
                if longrepr:
                    # Truncate long tracebacks
                    if len(longrepr) > 500:
                        longrepr = longrepr[:500] + "..."
                    failure_details.append(f"{node_id}:\n{longrepr}")
                else:
                    failure_details.append(f"{node_id}: {test.get('outcome', 'unknown')}")

        return TestExecutionResult(
            passed=passed,
            failed=failed,
            errors=errors,
            total=total,
            failure_details=failure_details,
            raw_output=raw_output,
        )

    @staticmethod
    def _parse_stdout_output(
        stdout: str, raw_output: str,
    ) -> TestExecutionResult:
        """
        Fallback: parse pytest stdout when JSON report is unavailable.

        Looks for the summary line like "5 passed, 2 failed, 1 error".
        """
        passed = 0
        failed = 0
        errors = 0

        # Parse the pytest summary line
        for line in reversed(stdout.splitlines()):
            line = line.strip()
            if not line:
                continue
            # e.g., "=== 5 passed, 2 failed in 0.45s ==="
            # or "5 passed, 2 failed"
            import re
            m_passed = re.search(r"(\d+)\s+passed", line)
            m_failed = re.search(r"(\d+)\s+failed", line)
            m_errors = re.search(r"(\d+)\s+error", line)
            if m_passed or m_failed or m_errors:
                passed = int(m_passed.group(1)) if m_passed else 0
                failed = int(m_failed.group(1)) if m_failed else 0
                errors = int(m_errors.group(1)) if m_errors else 0
                break

        total = passed + failed + errors

        # Extract failure lines
        failure_details: list[str] = []
        in_failure = False
        current_failure: list[str] = []
        for line in stdout.splitlines():
            if line.startswith("FAILED ") or line.startswith("ERROR "):
                if current_failure:
                    failure_details.append("\n".join(current_failure))
                current_failure = [line]
                in_failure = True
            elif in_failure and line.strip():
                current_failure.append(line)
            elif in_failure and not line.strip():
                if current_failure:
                    failure_details.append("\n".join(current_failure))
                current_failure = []
                in_failure = False
        if current_failure:
            failure_details.append("\n".join(current_failure))

        return TestExecutionResult(
            passed=passed,
            failed=failed,
            errors=errors,
            total=total,
            failure_details=failure_details,
            raw_output=raw_output,
        )

    def _parse_coverage(self) -> float:
        """Parse coverage.json for total coverage percentage."""
        cov_file = self._root / "coverage.json"
        if not cov_file.is_file():
            return 0.0
        try:
            data = json.loads(cov_file.read_text(encoding="utf-8"))
            totals = data.get("totals", {})
            return float(totals.get("percent_covered", 0.0))
        except (json.JSONDecodeError, OSError):
            return 0.0

    def _cleanup_test_files(self) -> None:
        """Remove test files written by this executor."""
        for path in self._written_files:
            try:
                if path.is_file():
                    path.unlink()
                    self._log.debug("test_file_cleaned", path=str(path))
                # Clean up empty parent directories
                parent = path.parent
                while parent != self._root:
                    try:
                        parent.rmdir()  # Only removes if empty
                        parent = parent.parent
                    except OSError:
                        break
            except Exception as exc:
                self._log.debug(
                    "test_file_cleanup_failed",
                    path=str(path),
                    error=str(exc),
                )
        self._written_files.clear()

    @staticmethod
    def format_failures_for_feedback(result: TestExecutionResult) -> str:
        """
        Format test failures as text for code agent feedback.

        Used in the AgentCoder iterate loop: test failures become
        feedback for the code agent to fix its implementation.
        """
        if not result.failure_details:
            if result.total == 0:
                return "No tests were executed."
            return f"All {result.passed} tests passed."

        lines = [
            f"Test results: {result.passed} passed, {result.failed} failed, "
            f"{result.errors} errors out of {result.total} total.",
            "",
            "Failures:",
        ]
        for detail in result.failure_details:
            lines.append(f"\n{detail}")

        lines.extend([
            "",
            "Fix the implementation so all tests pass. "
            "Do NOT modify the test files — they test the specification.",
        ])
        return "\n".join(lines)
