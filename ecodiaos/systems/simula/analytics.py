"""
EcodiaOS -- Simula Evolution Analytics Engine

Tracks evolution quality metrics over time, enabling Simula to learn
from its own history. All analytics are computed from Neo4j evolution
records -- zero LLM tokens required.

Key metrics:
  - Per-category success/rollback rates
  - Evolution velocity (proposals per day)
  - Rollback pattern analysis (which categories fail most, why)
  - Dynamic caution adjustment (increase risk thresholds for
    categories with high recent rollback rates)

Used by:
  - ChangeSimulator: dynamic risk threshold adjustment
  - SimulaService: enhanced stats reporting
  - ProposalIntelligence: cost/risk estimation
"""

from __future__ import annotations

from datetime import datetime, timedelta, timezone
from typing import TYPE_CHECKING, Any

import structlog

from ecodiaos.primitives.common import utc_now
from ecodiaos.systems.simula.types import (
    CategorySuccessRate,
    ChangeCategory,
    EvolutionAnalytics,
    EvolutionRecord,
    RiskLevel,
)

if TYPE_CHECKING:
    from ecodiaos.systems.simula.history import EvolutionHistoryManager

logger = structlog.get_logger().bind(system="simula.analytics")

# Rollback rate above this threshold triggers increased caution
_CAUTION_THRESHOLD: float = 0.30

# Window for "recent" rollback rate calculation
_RECENT_WINDOW_DAYS: int = 7

# Risk level to numeric mapping for mean calculation
_RISK_LEVEL_NUMERIC: dict[RiskLevel, float] = {
    RiskLevel.LOW: 0.1,
    RiskLevel.MODERATE: 0.4,
    RiskLevel.HIGH: 0.7,
    RiskLevel.UNACCEPTABLE: 1.0,
}


class EvolutionAnalyticsEngine:
    """
    Tracks evolution quality metrics over time.
    Enables Simula to learn from its own history and dynamically
    adjust risk thresholds based on past performance.

    All computation is from Neo4j records -- no LLM tokens consumed.
    """

    def __init__(self, history: EvolutionHistoryManager | None = None) -> None:
        self._history = history
        self._log = logger
        self._cached_analytics: EvolutionAnalytics | None = None
        self._cache_ttl_seconds: int = 300  # 5 minutes
        self._last_computed: datetime | None = None

    async def compute_analytics(self) -> EvolutionAnalytics:
        """
        Compute current analytics from the full evolution history.
        Results are cached for 5 minutes to avoid repeated Neo4j queries.
        """
        now = utc_now()
        if (
            self._cached_analytics is not None
            and self._last_computed is not None
            and (now - self._last_computed).total_seconds() < self._cache_ttl_seconds
        ):
            return self._cached_analytics

        if self._history is None:
            return EvolutionAnalytics()

        records = await self._history.get_history(limit=500)

        if not records:
            analytics = EvolutionAnalytics(last_updated=now)
            self._cached_analytics = analytics
            self._last_computed = now
            return analytics

        # Per-category rates
        category_rates: dict[str, CategorySuccessRate] = {}
        total_risk_numeric: float = 0.0
        risk_count: int = 0

        for record in records:
            cat_key = record.category.value
            if cat_key not in category_rates:
                category_rates[cat_key] = CategorySuccessRate(category=record.category)

            rate = category_rates[cat_key]
            rate.total += 1

            if record.rolled_back:
                rate.rolled_back += 1
            else:
                rate.approved += 1

            total_risk_numeric += _RISK_LEVEL_NUMERIC.get(record.simulation_risk, 0.4)
            risk_count += 1

        # Evolution velocity: proposals per day over the record span
        velocity = 0.0
        if len(records) >= 2:
            newest = records[0].created_at
            oldest = records[-1].created_at
            span_days = max(1.0, (newest - oldest).total_seconds() / 86400.0)
            velocity = len(records) / span_days

        # Aggregate rollback rate
        total_rolled_back = sum(r.rolled_back for r in category_rates.values())
        total_proposals = sum(r.total for r in category_rates.values())
        rollback_rate = total_rolled_back / max(1, total_proposals)

        # Mean simulation risk
        mean_risk = total_risk_numeric / max(1, risk_count)

        analytics = EvolutionAnalytics(
            category_rates=category_rates,
            total_proposals=total_proposals,
            evolution_velocity=round(velocity, 3),
            mean_simulation_risk=round(mean_risk, 3),
            rollback_rate=round(rollback_rate, 3),
            last_updated=now,
        )

        self._cached_analytics = analytics
        self._last_computed = now
        self._log.info(
            "analytics_computed",
            total_proposals=total_proposals,
            velocity=analytics.evolution_velocity,
            rollback_rate=analytics.rollback_rate,
            categories=len(category_rates),
        )
        return analytics

    async def get_category_success_rate(self, category: ChangeCategory) -> float:
        """
        Success rate for a specific change category.
        Used by ChangeSimulator for dynamic risk weighting.
        Returns 0.5 (neutral) if no history exists for this category.
        """
        analytics = await self.compute_analytics()
        rate = analytics.category_rates.get(category.value)
        if rate is None or rate.total == 0:
            return 0.5  # no data -- assume neutral
        return rate.success_rate

    async def get_recent_rollback_rate(self, category: ChangeCategory) -> float:
        """
        Rollback rate for a category within the recent window (7 days).
        More responsive to recent trends than the all-time rate.
        """
        if self._history is None:
            return 0.0

        records = await self._history.get_history(limit=200)
        cutoff = utc_now() - timedelta(days=_RECENT_WINDOW_DAYS)

        recent = [
            r for r in records
            if r.category == category and r.created_at >= cutoff
        ]

        if not recent:
            return 0.0

        rolled_back = sum(1 for r in recent if r.rolled_back)
        return rolled_back / len(recent)

    async def get_rollback_patterns(self) -> list[dict[str, Any]]:
        """
        Analyze rollback history for actionable patterns:
        - Which categories roll back most often
        - Common rollback reasons
        - Trend direction (getting better or worse)
        """
        analytics = await self.compute_analytics()
        patterns: list[dict[str, Any]] = []

        for cat_key, rate in analytics.category_rates.items():
            if rate.rolled_back == 0:
                continue
            patterns.append({
                "category": cat_key,
                "rollback_rate": round(rate.rollback_rate, 3),
                "total": rate.total,
                "rolled_back": rate.rolled_back,
                "severity": "high" if rate.rollback_rate > _CAUTION_THRESHOLD else "normal",
            })

        # Sort by rollback rate descending
        patterns.sort(key=lambda p: p["rollback_rate"], reverse=True)
        return patterns

    def should_increase_caution(self, category: ChangeCategory) -> bool:
        """
        Synchronous check against cached analytics.
        If recent rollback rate for this category exceeds the threshold,
        recommend that simulation use stricter risk assessment.
        """
        if self._cached_analytics is None:
            return False

        rate = self._cached_analytics.category_rates.get(category.value)
        if rate is None or rate.total < 3:
            return False  # not enough data to judge

        return rate.rollback_rate > _CAUTION_THRESHOLD

    def invalidate_cache(self) -> None:
        """Force recomputation on next analytics request."""
        self._cached_analytics = None
        self._last_computed = None
