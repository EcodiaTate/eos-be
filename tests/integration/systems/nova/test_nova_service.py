"""
Integration tests for NovaService.

Tests the full deliberation pipeline end-to-end with:
- Real sub-components (BeliefUpdater, GoalManager, PolicyGenerator, EFEEvaluator,
  DeliberationEngine, IntentRouter)
- Mocked external dependencies (LLMProvider, MemoryService, EquorService, VoxisService)

No real API calls, no real Neo4j, no real Redis.
"""

from __future__ import annotations

import asyncio
import json
from typing import Any
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from ecodiaos.clients.llm import LLMProvider, LLMResponse
from ecodiaos.config import NovaConfig
from ecodiaos.primitives.affect import AffectState
from ecodiaos.primitives.common import DriveAlignmentVector, new_id
from ecodiaos.primitives.intent import Intent
from ecodiaos.primitives.memory_trace import SelfNode
from ecodiaos.systems.atune.types import SalienceVector, WorkspaceBroadcast, WorkspaceContext
from ecodiaos.systems.nova.service import NovaService
from ecodiaos.systems.nova.types import (
    Goal,
    GoalSource,
    GoalStatus,
    IntentOutcome,
)


# ─── Mock Factories ───────────────────────────────────────────────


def make_llm_mock(
    policy_response: dict | None = None,
    efe_pragmatic: dict | None = None,
    efe_epistemic: dict | None = None,
) -> AsyncMock:
    """
    LLM mock that returns policy candidates for generate() and EFE estimates for evaluate().
    """
    llm = AsyncMock(spec=LLMProvider)

    if policy_response is None:
        policy_response = {
            "policies": [
                {
                    "name": "Respond helpfully",
                    "reasoning": "Direct response is appropriate",
                    "steps": [{"action_type": "express", "description": "Reply to user"}],
                    "risks": [],
                    "epistemic_value": "Confirms context",
                    "estimated_effort": "low",
                    "time_horizon": "immediate",
                }
            ]
        }

    llm.generate.return_value = LLMResponse(
        text=json.dumps(policy_response),
        model="mock-model",
        input_tokens=200,
        output_tokens=300,
        finish_reason="stop",
    )

    pr = efe_pragmatic or {"success_probability": 0.75, "confidence": 0.7, "reasoning": "Good match"}
    er = efe_epistemic or {"info_gain": 0.3, "uncertainties_addressed": 1, "novelty": 0.2}

    call_count_ref = [0]

    async def evaluate_side_effect(prompt: str, max_tokens: int = 200, temperature: float = 0.2) -> LLMResponse:
        call_count_ref[0] += 1
        if call_count_ref[0] % 2 == 1:
            return LLMResponse(text=json.dumps(pr), model="mock", input_tokens=50, output_tokens=50, finish_reason="stop")
        return LLMResponse(text=json.dumps(er), model="mock", input_tokens=50, output_tokens=50, finish_reason="stop")

    llm.evaluate.side_effect = evaluate_side_effect
    return llm


def make_memory_mock(
    self_node_name: str = "EOS",
    constitution: dict | None = None,
) -> AsyncMock:
    """MemoryService mock."""
    memory = AsyncMock()
    self_node = MagicMock(spec=SelfNode)
    self_node.name = self_node_name
    memory.get_self.return_value = self_node
    memory.get_constitution.return_value = constitution or {
        "drives": {"coherence": 1.0, "care": 1.2, "growth": 1.0, "honesty": 1.5}
    }
    retrieve_result = MagicMock()
    retrieve_result.traces = []
    retrieve_result.entities = []
    memory.retrieve.return_value = retrieve_result
    return memory


def make_equor_mock(verdict: str = "APPROVED") -> AsyncMock:
    """EquorService mock that approves intents by default."""
    equor = AsyncMock()
    review_result = MagicMock()
    review_result.verdict = verdict
    review_result.approved = (verdict == "APPROVED")
    review_result.reasoning = f"Mock: {verdict}"
    review_result.modifications = []
    review_result.suggested_modifications = []
    equor.review_intent.return_value = review_result
    return equor


def make_voxis_mock() -> AsyncMock:
    """VoxisService mock."""
    voxis = AsyncMock()
    expr = MagicMock()
    expr.is_silence = False
    expr.content = "I'm here to help."
    voxis.express.return_value = expr
    return voxis


def make_broadcast(
    text: str = "Hello, can you help me?",
    precision: float = 0.7,
    care_activation: float = 0.0,
    novelty: float = 0.3,
    emotional_score: float = 0.0,
    composite: float = 0.6,
    speaker_id: str | None = None,
) -> WorkspaceBroadcast:
    """Build a realistic WorkspaceBroadcast."""
    affect = AffectState.neutral().model_copy(update={
        "care_activation": care_activation,
        "curiosity": 0.4,
    })

    scores: dict[str, float] = {"novelty": novelty}
    if emotional_score > 0:
        scores["emotional"] = emotional_score

    # Nested content structure
    content_inner = MagicMock()
    content_inner.content = text
    if speaker_id:
        content_inner.speaker_id = speaker_id

    content_outer = MagicMock()
    content_outer.content = content_inner

    return WorkspaceBroadcast(
        content=content_outer,
        salience=SalienceVector(scores=scores, composite=composite),
        affect=affect,
        precision=precision,
        context=WorkspaceContext(),
    )


def make_nova_service(
    llm: LLMProvider | None = None,
    memory: Any | None = None,
    equor: Any | None = None,
    voxis: Any | None = None,
    config: NovaConfig | None = None,
) -> NovaService:
    return NovaService(
        memory=memory or make_memory_mock(),
        equor=equor or make_equor_mock(),
        voxis=voxis or make_voxis_mock(),
        llm=llm or make_llm_mock(),
        config=config or NovaConfig(),
    )


# ─── Initialization Tests ─────────────────────────────────────────


class TestInitialization:
    @pytest.mark.asyncio
    async def test_initialize_loads_instance_name(self) -> None:
        memory = make_memory_mock(self_node_name="Lumi")
        nova = make_nova_service(memory=memory)
        await nova.initialize()
        assert nova._instance_name == "Lumi"

    @pytest.mark.asyncio
    async def test_initialize_loads_constitution(self) -> None:
        memory = make_memory_mock(constitution={
            "drives": {"coherence": 1.5, "care": 2.0, "growth": 0.8, "honesty": 1.2}
        })
        nova = make_nova_service(memory=memory)
        await nova.initialize()
        assert nova._drive_weights["care"] == pytest.approx(2.0)
        assert nova._drive_weights["coherence"] == pytest.approx(1.5)

    @pytest.mark.asyncio
    async def test_initialize_default_when_no_self_node(self) -> None:
        memory = make_memory_mock()
        memory.get_self.return_value = None
        nova = make_nova_service(memory=memory)
        await nova.initialize()
        assert nova._instance_name == "EOS"  # Default

    @pytest.mark.asyncio
    async def test_initialize_builds_all_sub_components(self) -> None:
        nova = make_nova_service()
        await nova.initialize()
        assert nova._goal_manager is not None
        assert nova._policy_generator is not None
        assert nova._efe_evaluator is not None
        assert nova._deliberation_engine is not None
        assert nova._intent_router is not None

    @pytest.mark.asyncio
    async def test_shutdown_runs_cleanly(self) -> None:
        nova = make_nova_service()
        await nova.initialize()
        await nova.shutdown()  # Should not raise


# ─── Broadcast Processing ─────────────────────────────────────────


class TestBroadcastProcessing:
    @pytest.mark.asyncio
    async def test_broadcast_before_init_does_nothing(self) -> None:
        nova = make_nova_service()
        # Not initialized — should just return
        broadcast = make_broadcast()
        await nova.receive_broadcast(broadcast)
        assert nova._total_broadcasts == 0

    @pytest.mark.asyncio
    async def test_broadcast_increments_counter(self) -> None:
        nova = make_nova_service()
        await nova.initialize()
        broadcast = make_broadcast()
        await nova.receive_broadcast(broadcast)
        assert nova._total_broadcasts == 1

    @pytest.mark.asyncio
    async def test_broadcast_updates_beliefs(self) -> None:
        nova = make_nova_service()
        await nova.initialize()
        broadcast = make_broadcast("the code has a bug in the algorithm")
        await nova.receive_broadcast(broadcast)
        # Beliefs should have been updated (context summary non-empty)
        assert nova._belief_updater.beliefs.current_context.summary != ""

    @pytest.mark.asyncio
    async def test_broadcast_updates_current_affect(self) -> None:
        nova = make_nova_service()
        await nova.initialize()
        affect = AffectState.neutral().model_copy(update={"care_activation": 0.8})
        broadcast = make_broadcast(care_activation=0.8)
        await nova.receive_broadcast(broadcast)
        # Affect should be stored from the broadcast
        assert nova._current_affect.care_activation == pytest.approx(0.8)

    @pytest.mark.asyncio
    async def test_high_salience_creates_goal(self) -> None:
        nova = make_nova_service()
        await nova.initialize()
        # First broadcast creates goal; second + deliberation exercises it
        broadcast = make_broadcast(
            "Please help me urgently with this problem",
            precision=0.9,
            composite=0.85,
        )
        await nova.receive_broadcast(broadcast)
        # May or may not create a goal depending on deliberation path
        # Just verify no exception is thrown
        assert nova._total_broadcasts == 1


# ─── Deliberation Path Tests ──────────────────────────────────────


class TestDeliberationPaths:
    @pytest.mark.asyncio
    async def test_direct_message_triggers_fast_or_slow_path(self) -> None:
        """A direct user message should result in some deliberation path being taken."""
        nova = make_nova_service()
        await nova.initialize()

        # Add an active goal first so deliberation can work with it
        goal = Goal(
            id=new_id(),
            description="Respond helpfully to: Hello, can you help me?",
            source=GoalSource.USER_REQUEST,
            priority=0.7,
            drive_alignment=DriveAlignmentVector(coherence=0.3, care=0.3, growth=0.1, honesty=0.2),
        )
        await nova.add_goal(goal)

        broadcast = make_broadcast("Hello, can you help me?", precision=0.8, composite=0.7)
        await nova.receive_broadcast(broadcast)

        # At least one of the path counters should be non-zero
        total_paths = nova._total_fast_path + nova._total_slow_path + nova._total_do_nothing
        assert total_paths == 1

    @pytest.mark.asyncio
    async def test_do_nothing_counted_when_no_goal_active(self) -> None:
        """Without an active goal, Nova should do nothing."""
        nova = make_nova_service()
        await nova.initialize()

        broadcast = make_broadcast("Some ambient background content", precision=0.2, composite=0.15)
        await nova.receive_broadcast(broadcast)

        # With very low salience and no existing goals → do nothing
        # (actual path depends on implementation details)
        assert nova._total_broadcasts == 1

    @pytest.mark.asyncio
    async def test_equor_block_prevents_intent_dispatch(self) -> None:
        """When Equor blocks an intent, it should not be routed."""
        equor = make_equor_mock(verdict="BLOCKED")
        nova = make_nova_service(equor=equor)
        await nova.initialize()

        goal = Goal(
            id=new_id(),
            description="Respond helpfully to: Help me now",
            source=GoalSource.USER_REQUEST,
            priority=0.9,
            drive_alignment=DriveAlignmentVector(coherence=0.3, care=0.5, growth=0.1, honesty=0.2),
        )
        await nova.add_goal(goal)

        broadcast = make_broadcast("Help me now", precision=0.9, composite=0.85)
        await nova.receive_broadcast(broadcast)

        # Blocked intents should increment blocked counter (not issued counter)
        # The exact count depends on how many policies were generated and blocked
        # Just verify no exception
        assert nova._total_broadcasts == 1


# ─── Goal Management via Service ─────────────────────────────────


class TestGoalManagementViaService:
    @pytest.mark.asyncio
    async def test_add_goal_directly(self) -> None:
        nova = make_nova_service()
        await nova.initialize()

        goal = Goal(
            id=new_id(),
            description="Test goal",
            source=GoalSource.USER_REQUEST,
        )
        added = await nova.add_goal(goal)
        assert added.id == goal.id
        assert added.status == GoalStatus.ACTIVE

    @pytest.mark.asyncio
    async def test_active_goal_summaries_property(self) -> None:
        nova = make_nova_service()
        await nova.initialize()

        goal = Goal(
            id=new_id(),
            description="Test goal for summary",
            source=GoalSource.USER_REQUEST,
            priority=0.6,
        )
        await nova.add_goal(goal)

        summaries = nova.active_goal_summaries
        # Summaries is a list; may be empty if goal has no embedding
        assert isinstance(summaries, list)


# ─── Outcome Processing ───────────────────────────────────────────


class TestOutcomeProcessing:
    @pytest.mark.asyncio
    async def test_success_outcome_increments_counter(self) -> None:
        nova = make_nova_service()
        await nova.initialize()

        outcome = IntentOutcome(
            intent_id=new_id(),
            success=True,
            episode_id="ep_123",
        )
        await nova.process_outcome(outcome)
        assert nova._total_outcomes_success == 1

    @pytest.mark.asyncio
    async def test_failure_outcome_increments_counter(self) -> None:
        nova = make_nova_service()
        await nova.initialize()

        outcome = IntentOutcome(
            intent_id=new_id(),
            success=False,
            failure_reason="Target unavailable",
        )
        await nova.process_outcome(outcome)
        assert nova._total_outcomes_failure == 1

    @pytest.mark.asyncio
    async def test_success_increases_belief_confidence(self) -> None:
        nova = make_nova_service()
        await nova.initialize()
        initial_conf = nova._belief_updater.beliefs.overall_confidence

        outcome = IntentOutcome(intent_id=new_id(), success=True, episode_id="ep_ok")
        await nova.process_outcome(outcome)

        assert nova._belief_updater.beliefs.overall_confidence >= initial_conf

    @pytest.mark.asyncio
    async def test_outcome_with_known_intent_updates_goal(self) -> None:
        """If outcome maps to a pending intent with a goal, goal progress updates."""
        from ecodiaos.systems.nova.types import PendingIntent
        nova = make_nova_service()
        await nova.initialize()

        goal = Goal(
            id=new_id(),
            description="Test goal",
            source=GoalSource.USER_REQUEST,
        )
        await nova.add_goal(goal)

        intent_id = new_id()
        nova._pending_intents[intent_id] = PendingIntent(
            intent_id=intent_id,
            goal_id=goal.id,
            routed_to="voxis",
            policy_name="Respond helpfully",
        )

        outcome = IntentOutcome(
            intent_id=intent_id,
            success=True,
            episode_id="ep_done",
        )
        await nova.process_outcome(outcome)
        # Intent should have been removed from pending
        assert intent_id not in nova._pending_intents


# ─── Health Metrics ───────────────────────────────────────────────


class TestHealthMetrics:
    @pytest.mark.asyncio
    async def test_health_returns_expected_keys(self) -> None:
        nova = make_nova_service()
        await nova.initialize()
        h = await nova.health()

        assert "total_broadcasts" in h
        assert "fast_path_decisions" in h
        assert "slow_path_decisions" in h
        assert "do_nothing_decisions" in h
        assert "intents_issued" in h
        assert "goals" in h
        assert "status" in h

    @pytest.mark.asyncio
    async def test_health_always_returns_status(self) -> None:
        """health() is always callable and returns a status."""
        nova = make_nova_service()
        h = await nova.health()
        assert "status" in h

    @pytest.mark.asyncio
    async def test_health_counts_after_broadcasts(self) -> None:
        nova = make_nova_service()
        await nova.initialize()

        for _ in range(3):
            await nova.receive_broadcast(make_broadcast())

        h = await nova.health()
        assert h["total_broadcasts"] == 3


# ─── EFE Weight Updates ───────────────────────────────────────────


class TestEFEWeightUpdates:
    @pytest.mark.asyncio
    async def test_update_efe_weights(self) -> None:
        nova = make_nova_service()
        await nova.initialize()

        # update_efe_weights takes dict[str, float]
        nova.update_efe_weights({"pragmatic": 0.50, "epistemic": 0.10})
        assert nova._efe_evaluator.weights.pragmatic == pytest.approx(0.50)
        assert nova._efe_evaluator.weights.epistemic == pytest.approx(0.10)

    @pytest.mark.asyncio
    async def test_update_weights_partial_dict(self) -> None:
        """Only specified weights change; others keep their current values."""
        nova = make_nova_service()
        await nova.initialize()

        original_feasibility = nova._efe_evaluator.weights.feasibility
        nova.update_efe_weights({"pragmatic": 0.45})
        assert nova._efe_evaluator.weights.pragmatic == pytest.approx(0.45)
        assert nova._efe_evaluator.weights.feasibility == pytest.approx(original_feasibility)


# ─── System Identity ──────────────────────────────────────────────


class TestSystemIdentity:
    def test_system_id_is_nova(self) -> None:
        nova = make_nova_service()
        assert nova.system_id == "nova"

    @pytest.mark.asyncio
    async def test_receive_broadcast_method_exists(self) -> None:
        """NovaService implements the BroadcastSubscriber protocol."""
        nova = make_nova_service()
        assert callable(nova.receive_broadcast)


# ─── Somatic Threshold Modulation (Stage 0, Task 0.4/0.7) ────────


class TestNovaSomaticThresholds:
    """
    Verify that Soma's allostatic state modulates EFE deliberation thresholds.

    Tests the update_somatic_thresholds() pathway from Nova service.receive_broadcast()
    through to DeliberationEngine. This is the Stage 0 Task 0.4 integration.
    """

    @pytest.mark.asyncio
    async def test_update_somatic_thresholds_low_urgency_low_arousal(self) -> None:
        """Below-threshold urgency and arousal → no deltas applied."""
        nova = make_nova_service()
        await nova.initialize()

        engine = nova._deliberation_engine
        engine.update_somatic_thresholds(urgency=0.1, arousal=0.3)

        # Below urgency threshold (0.3) and below arousal threshold (0.6) → no change
        assert engine._novelty_threshold_delta == pytest.approx(0.0)
        assert engine._precision_threshold_delta == pytest.approx(0.0)
        assert engine._do_nothing_efe_override is None

    @pytest.mark.asyncio
    async def test_update_somatic_thresholds_high_arousal(self) -> None:
        """High arousal lowers precision threshold (more sensitive to broadcasts)."""
        nova = make_nova_service()
        await nova.initialize()

        engine = nova._deliberation_engine
        engine.update_somatic_thresholds(urgency=0.0, arousal=0.8)

        # arousal=0.8 → delta = -min(0.15, (0.8 - 0.6) * 0.375) = -min(0.15, 0.075) = -0.075
        assert engine._precision_threshold_delta == pytest.approx(-0.075, abs=1e-6)
        assert engine._novelty_threshold_delta == pytest.approx(0.0)
        assert engine._do_nothing_efe_override is None

    @pytest.mark.asyncio
    async def test_update_somatic_thresholds_max_arousal(self) -> None:
        """Maximum arousal caps precision delta at -0.15."""
        nova = make_nova_service()
        await nova.initialize()

        engine = nova._deliberation_engine
        engine.update_somatic_thresholds(urgency=0.0, arousal=1.0)

        assert engine._precision_threshold_delta == pytest.approx(-0.15, abs=1e-6)

    @pytest.mark.asyncio
    async def test_update_somatic_thresholds_high_urgency(self) -> None:
        """High urgency lowers novelty threshold and raises do-nothing EFE baseline."""
        nova = make_nova_service()
        await nova.initialize()

        engine = nova._deliberation_engine
        engine.update_somatic_thresholds(urgency=1.0, arousal=0.0)

        # urgency=1.0 → urgency_excess = (1.0 - 0.3) / 0.7 = 1.0
        # novelty_delta = -min(0.20, 1.0 * 0.20) = -0.20
        assert engine._novelty_threshold_delta == pytest.approx(-0.20, abs=1e-6)
        # do_nothing_override = -0.10 + min(0.15, 1.0 * 0.15) = -0.10 + 0.15 = +0.05
        assert engine._do_nothing_efe_override == pytest.approx(0.05, abs=1e-4)

    @pytest.mark.asyncio
    async def test_update_somatic_thresholds_moderate_urgency(self) -> None:
        """Moderate urgency (0.65) produces intermediate deltas."""
        nova = make_nova_service()
        await nova.initialize()

        engine = nova._deliberation_engine
        engine.update_somatic_thresholds(urgency=0.65, arousal=0.0)

        # urgency_excess = (0.65 - 0.3) / 0.7 = 0.5
        # novelty_delta = -min(0.20, 0.5 * 0.20) = -0.10
        assert engine._novelty_threshold_delta == pytest.approx(-0.10, abs=1e-6)
        assert engine._do_nothing_efe_override is not None
        assert engine._do_nothing_efe_override < 0.0  # Still negative (inaction harder but not positive)

    @pytest.mark.asyncio
    async def test_update_somatic_thresholds_resets_on_recovery(self) -> None:
        """After stress, returning to low urgency/arousal resets deltas to zero."""
        nova = make_nova_service()
        await nova.initialize()

        engine = nova._deliberation_engine

        # Stress state
        engine.update_somatic_thresholds(urgency=1.0, arousal=1.0)
        assert engine._novelty_threshold_delta < 0.0
        assert engine._precision_threshold_delta < 0.0
        assert engine._do_nothing_efe_override is not None

        # Recovery
        engine.update_somatic_thresholds(urgency=0.0, arousal=0.2)
        assert engine._novelty_threshold_delta == pytest.approx(0.0)
        assert engine._precision_threshold_delta == pytest.approx(0.0)
        assert engine._do_nothing_efe_override is None

    @pytest.mark.asyncio
    async def test_receive_broadcast_with_mocked_soma_calls_thresholds(self) -> None:
        """
        NovaService.receive_broadcast() reads Soma signal and calls
        update_somatic_thresholds() on the deliberation engine.
        """
        nova = make_nova_service()
        await nova.initialize()

        # Build a Soma mock whose get_current_signal() returns an AllostaticSignal-like object
        from ecodiaos.systems.soma.types import (
            ALL_DIMENSIONS,
            InteroceptiveDimension,
            InteroceptiveState,
        )

        sensed = {d: 0.5 for d in ALL_DIMENSIONS}
        sensed[InteroceptiveDimension.AROUSAL] = 0.85  # High arousal

        mock_state = MagicMock()
        mock_state.sensed = sensed

        mock_signal = MagicMock()
        mock_signal.urgency = 0.6
        mock_signal.state = mock_state
        mock_signal.dominant_error = InteroceptiveDimension.AROUSAL
        mock_signal.precision_weights = {d: 1.0 for d in ALL_DIMENSIONS}
        mock_signal.nearest_attractor = None
        mock_signal.trajectory_heading = "transient"

        soma_mock = MagicMock()
        soma_mock.get_current_signal.return_value = mock_signal
        soma_mock.urgency_threshold = 0.5

        nova.set_soma(soma_mock)

        engine = nova._deliberation_engine
        initial_precision_delta = engine._precision_threshold_delta

        broadcast = make_broadcast("Test with somatic state", precision=0.7)
        await nova.receive_broadcast(broadcast)

        # High arousal (0.85) should have lowered precision threshold
        assert engine._precision_threshold_delta < initial_precision_delta
        assert engine._precision_threshold_delta < 0.0


# ─── Multi-Broadcast Scenarios ────────────────────────────────────


class TestMultiBroadcastScenarios:
    @pytest.mark.asyncio
    async def test_multiple_broadcasts_accumulate_beliefs(self) -> None:
        nova = make_nova_service()
        await nova.initialize()

        await nova.receive_broadcast(make_broadcast("I feel sad today", care_activation=0.5))
        await nova.receive_broadcast(make_broadcast("I really need help", care_activation=0.7))
        await nova.receive_broadcast(make_broadcast("This is urgent", precision=0.9))

        assert nova._total_broadcasts == 3

    @pytest.mark.asyncio
    async def test_decision_records_ring_buffer(self) -> None:
        """Decision records should be capped at max_decision_records."""
        nova = make_nova_service()
        await nova.initialize()

        # Add a goal so deliberation can happen
        goal = Goal(
            id=new_id(),
            description="Respond to many messages",
            source=GoalSource.USER_REQUEST,
            priority=0.8,
            drive_alignment=DriveAlignmentVector(coherence=0.3, care=0.3, growth=0.1, honesty=0.1),
        )
        await nova.add_goal(goal)

        # Process many broadcasts
        for i in range(10):
            await nova.receive_broadcast(make_broadcast(f"Message {i}", precision=0.7))

        # Ring buffer should not exceed max
        assert len(nova._decision_records) <= nova._max_decision_records

    @pytest.mark.asyncio
    async def test_care_broadcast_creates_care_goal(self) -> None:
        """High care_activation broadcast should create a CARE_RESPONSE goal."""
        nova = make_nova_service()
        await nova.initialize()

        broadcast = make_broadcast(
            "I'm feeling really hurt and alone today",
            care_activation=0.9,
            precision=0.8,
            composite=0.85,
        )
        await nova.receive_broadcast(broadcast)

        # Check if a care goal was created
        active = nova._goal_manager.active_goals if nova._goal_manager else []
        care_goals = [g for g in active if g.source == GoalSource.CARE_RESPONSE]
        # May or may not create based on create_from_broadcast threshold
        assert isinstance(care_goals, list)
